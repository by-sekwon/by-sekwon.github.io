![myself.png](media/image1.png){width="4.076208442694663in"
height="2.9841404199475066in"}Chapter 1. 데이터 축소 원칙

vol 3. 추론통계

section 6. 추정과 검정

wolfpack se kwon

<https://sites.google.com/view/wolfpack61>

권세혁교수 통계노트시리즈

확률표본 $X_{1},\ldots,X_{n}$으로부터 미지의 모수 $\theta$에 대해 추론을
시도한다. 표본크기 $n$이 크면 관찰된 표본 데이터 $x_{1},\ldots,x_{n}$은
해석하기 어려운 긴 수열이 될 수 있으므로, 데이터 내 정보를 요약하기 위해
확률표본의 함수인 통계량을 계산한다. 데이터 축소 또는 요약의 한 형태인
$T(\mathbf{X})$는 동일한 값을 갖더라도 상이한 표본일 수 있다.

여기서는 데이터 축소의 세 가지 원리에 대해 살펴본다. 미지의 모수
$\theta$에 대한 중요한 정보를 버리지 않고 데이터 요약을 수행하는 방법과,
반대로 $\theta$에 대한 지식 획득에 무관한 정보를 성공적으로 제거하는
방법이다.

충분성 원리 sufficiency principle: 데이터를 요약하는 과정에서도
$\theta$에 관한 정보를 버리지 않는 방법을 제시

우도 원리 likelihood principle: 관찰된 확률표본을 통해 얻어진 $\theta$에
대한 모든 정보를 담고 있는 파라미터의 함수를 기술

등분산성 원리 equivariance principle: 모형의 중요한 특성들을 유지하면서
또 다른 형태의 데이터 축소를 가능하게 하는 방법을 제시

1충분성 원리

충분통계량은 어떤 모수 $\theta$에 대해, 확률표본에 포함된 $\theta$에
관한 모든 정보를 포착하는 통계량을 의미한다. 확률표본의 충분통계량 값
이외에 추가로 얻을 수 있는 표본의 다른 정보는 $\theta$에 대해 더 이상
아무런 정보를 제공하지 않는다.

$T(\mathbf{X})$가 모수 $\theta$에 대한 충분통계량이라면 $\theta$에 대한
모든 추론은 확률표본 $\mathbf{X}$의 전체 값이 아니라 $T(\mathbf{X})$의
값만을 통해 이루어져야 한다. 즉, 두 표본 점 $\mathbf{x}$와
$\mathbf{y}$가 $T(\mathbf{x}) = T(\mathbf{y})$를 만족하면, $\theta$에
대한 추론은 $\mathbf{X} = \mathbf{x}$가 관측되었을 때와
$\mathbf{X} = \mathbf{y}$가 관측되었을 때 동일해야 한다.

정의

충분통계량

통계량 $T(\mathbf{X})$가 모수 $\theta$에 대한 충분통계량이 되기 위한
조건은 다음과 같다: 조건부 분포
$f_{\mathbf{X}|T(\mathbf{X})}(\mathbf{x}|T(\mathbf{X}) = t)$가 모수
$\theta$에 의존하지 않을 때, $T(\mathbf{X})$는 $\theta$에 대한
충분통계량이다.

정의

즉, 확률표본 $\mathbf{X}$에 대한 조건부 분포가 통계량 $T(\mathbf{X})$의
값만 주어진 상태에서 $\theta$와 무관하다면, $T(\mathbf{X})$만 가지고도
$\theta$에 대해 모든 정보를 담고 있다고 본다.

$p(\mathbf{x}|\theta)$를 확률표본 $\mathbf{X}$의 결합확률밀도함수라
하고, 통계량 $T(\mathbf{X})$의 확률밀도함수를 $q(t|\theta)$라 하자.
통계량 $T(\mathbf{X})$가 모수 $\theta$에 대한 충분통계량이 되기 위한
필요충분조건은, 모든 표본 $\mathbf{x}$에 대하여 비율
$\frac{p(\mathbf{x}|\theta)}{q(T(\mathbf{X})|\theta)}$이 모수 $\theta$의
함수로 상수가 되는 것이다.

정리

【예제 ①】 $X_{1},\ldots,X_{n}$을 모수 $\theta \in (0,1)$인 베르누이
분포를 따르는 확률표본이라 하자. $T(\mathbf{X}) = \sum X_{i}$는
$\theta$에 대한 충분통계량이다.

$T(\mathbf{X}) = \sum X_{i} \sim B(n,\theta)$이므로
$q(T(\mathbf{x})|\theta) \sim \binom{n}{t}\theta^{t}(1 - \theta)^{n - t}$

결합확률밀도함수는
$p(\mathbf{x}|\theta) = \prod\theta^{x_{i}}(1 - \theta)^{1 - x_{i}}$이다.

$$\frac{p(\mathbf{x}|\theta)}{q(T(\mathbf{x})|\theta)} = = \frac{\theta^{t}(1 - \theta)^{n - t}}{\binom{n}{t}\theta^{t}(1 - \theta)^{n - t}} = \frac{1}{\binom{n}{t}} = \frac{1}{\binom{n}{\sum x_{i}}}$$

【예제 ②】 $X_{1},\ldots,X_{n}$을 $\sigma^{2}$가 알려진
$N(\mu,\sigma^{2})$ 정규분포를 따르는 확률표본이라 하자.
$T(\mathbf{X}) = \sum X_{i}/n$는 $\mu$에 대한 충분통계량이다.

$$f(\mathbf{x}|\mu) = \overset{n}{\prod_{i = 1}}(2\pi\sigma^{2})^{- 1/2}\exp\left( - \frac{(x_{i} - \mu)^{2}}{2\sigma^{2}} \right)$$

$$\overline{X} \sim N(\mu,\sigma^{2}/n)$$

$$\frac{f(\mathbf{x}|\theta)}{q(T(\mathbf{x})|\theta)} = (2\pi\sigma^{2})^{- n/2}\exp\left( - \frac{\sum_{i = 1}^{n}(x_{i} - \overline{x})^{2} + n(\overline{x} - \mu)^{2}}{2\sigma^{2}} \right)/(2\pi\sigma^{2}/n)^{- 1/2}\exp\left( - \frac{n(\overline{x} - \mu)^{2}}{2\sigma^{2}} \right)$$

$$= n^{- 1/2}(2\pi\sigma^{2})^{- (n - 1)/2}\exp\left( - \frac{\sum_{i = 1}^{n}(x_{i} - \overline{x})^{2}}{2\sigma^{2}} \right)$$

【예제 ③】 지수족 분포를 벗어나면 순서통계량보다 작은 차원의
충분통계량을 찾는 것은 불가능 하다. 비모수 검정이 필요하다.

$X_{1},\ldots,X_{n}$이 임의의 확률밀도함수 $f(x)$로부터 확률분포로
추출되었다고 하자. 이때 $f(x)$에 대한 추가적인 정보가 없는 경우(비모수
추정 상황)에는 확률표본의 순서통계량만이 정보를 담는다. 즉,

$f(\mathbf{x}) = \overset{n}{\prod_{i = 1}}f(x_{i}) = \overset{n}{\prod_{i = 1}}f(x_{(i)})$가
성립하므로 위의 정리에 따라 순서통계량이 충분통계량이 된다.

(분해 정리, Factorization Theorem) $f(\mathbf{x}|\theta)$를 표본
$\mathbf{X}$의 결합확률밀도함수라 하자. 통계량 $T(\mathbf{X})$가
$\theta$에 대한 충분통계량이 되기 위한 필요충분조건은 다음과 같다. 모든
$\mathbf{x}$와 $\theta$에 대해, 함수 $g(t|\theta)$와 $h(\mathbf{x})$가
존재하여 $f(\mathbf{x}|\theta) = g(T(\mathbf{x})|\theta)h(\mathbf{x})$를
만족할 때, $T(\mathbf{X})$는 $\theta$에 대한 충분통계량이다.

정리

표본의 확률함수를 충분통계량만을 통한 함수 $g$와 $\mathbf{x}$에만
의존하는 함수 $h$로 분해할 수 있으면, 그 통계량은 충분하다. 분해정리를
이용하여 충분통계량을 찾기 위해서는 확률표본의 결합확률밀도함수를 두
부분으로 분해한다. 한 부분은 모수 $\theta$에 의존하지 않는 부분이고 다른
부분은 $\theta$에 의존하는 부분이다.

【예제 ② 계속】 $X_{1},\ldots,X_{n}$을 $\sigma^{2}$가 알려진
$N(\mu,\sigma^{2})$ 정규분포를 따르는 확률표본이라 하자.
$T(\mathbf{X}) = \sum X_{i}/n$는 $\mu$에 대한 충분통계량이다.

$$f(\mathbf{x}|\mu) = (2\pi\sigma^{2})^{- n/2}\exp\left( - \overset{n}{\sum_{i = 1}}(x_{i} - \overline{x})^{2}/(2\sigma^{2}) \right)\exp\left( - n(\overline{x} - \mu)^{2}/(2\sigma^{2}) \right)$$

$$h(\mathbf{x}) = (2\pi\sigma^{2})^{- n/2}\exp\left( - \frac{1}{2\sigma^{2}}\overset{n}{\sum_{i = 1}}(x_{i} - \overline{x})^{2} \right)$$

$g(t|\mu) = \exp\left( - \frac{n(t - \mu)^{2}}{2\sigma^{2}} \right)$이므로
$f(\mathbf{x}|\mu) = g(t|\mu)h(\mathbf{x})$

【예제 ④】 $X_{1},\ldots,X_{n}$을
$f(x|\theta) = \frac{1}{\theta},x = 1,2,...,\theta$ 이산형 균일분포를
따르는 확률표본이라 하자. $max(x_{i})$는 $\theta$에 대한 충분통계량이다.

$$f(\mathbf{x}|\theta) = \{\begin{matrix}
\theta^{- n}, & \text{if}x_{i} \in \{ 1,2,\ldots,\theta\}\text{for all}i = 1,\ldots,n \\
0, & \text{otherwise}
\end{matrix}$$

$$h(\mathbf{x}) = \{\begin{matrix}
1, & \text{if}x_{i} \in \{ 1,2,\ldots\}\text{for all}i = 1,\ldots,n \\
0, & \text{otherwise}
\end{matrix}$$

$T(\mathbf{x}) = \max_{i}x_{i}$이면, $g(t|\theta) = \{\begin{matrix}
\theta^{- n}, & \text{if}t \leq \theta \\
0, & \text{otherwise}
\end{matrix}$

【예제 ⑤】 $X_{1},\ldots,X_{n}$을 $N(\mu,\sigma^{2})$ 정규분포를 따르는
확률표본이라 하자. $T_{1}(\mathbf{x}) = \overline{X}$,
$T_{2}(\mathbf{x}) = S^{2} = \frac{1}{n - 1}\overset{n}{\sum_{i = 1}}(X_{i} - \overline{X})^{2}$은
모수 $(\mu,\sigma^{2})$에 대한 충분통계량이다.

$T_{1}(\mathbf{x}) = \overline{X}$, $T_{2}(\mathbf{x}) = S^{2}$에 대하여
$h(x) = 1$이고

$$g(t_{1},t_{2} \mid \mu,\sigma^{2}) = (2\pi\sigma^{2})^{- n/2}\exp\left( - \frac{n(t_{1} - \mu)^{2} + (n - 1)t_{2}}{2\sigma^{2}} \right)$$

$$f(\mathbf{x} \mid \mu,\sigma^{2}) = g\left( T_{1}(\mathbf{x}),T_{2}(\mathbf{x}) \mid \mu,\sigma^{2} \right)h(\mathbf{x})$$

$f(x \mid \theta) = h(x)c(\mathbf{\theta})\exp\left( \overset{k}{\sum_{i = 1}}w_{i}(\mathbf{\theta})t_{i}(x) \right)$
지수족 분포로부터의 충분통계량은
$T(\mathbf{X}) = \left( \overset{n}{\sum_{j = 1}}t_{1}(X_{j}),\overset{n}{\sum_{j = 1}}t_{2}(X_{j}),\ldots,\overset{n}{\sum_{j = 1}}t_{k}(X_{j}) \right)$이다.

정리

최소 충분통계량

충분통계량은 모수 $\theta$에 대한 정보를 표본에서 손실 없이 요약할 수
있는 통계량이다. 그런데 모든 충분통계량이 ["]{dir="rtl"}작거나 간단한"
것은 아닙니다. 어떤 충분통계량은 더 많은 정보를 담고 있을 수도 있다.
최소 minimal 충분통계량은 다음을 만족한다.

정보를 모두 보존하면서 가장 작고 요약된 형태로 되어 있는 충분통계량이다.

즉, 중복 없이 핵심 정보만 유지하는 가장 효율적인 통계량이다.

어떤 충분통계량 $T(\mathbf{X})$이 모든 다른 충분통계량
$T'(\mathbf{X})$에 대해, $T(\mathbf{X})$이 $T'(\mathbf{X})$의 함수로
표현될 수 있으면 $T(\mathbf{X})$을 최소 충분통계량이라고 한다.

정의

확률표본 $\mathbf{X}$의 확률밀도함수가 $f(\mathbf{x}|\theta)$로
주어졌다고 하자.

정리

어떤 함수 $T(\mathbf{x})$가 존재하여, 모든 표본점
$\mathbf{x},\mathbf{y}$에 대해
$\frac{f(\mathbf{x}|\theta)}{f(\mathbf{y}|\theta)}$가 $\theta$에 대해
상수가 되는 경우가 $T(\mathbf{x}) = T(\mathbf{y})$일 때와 정확히
일치한다면, $T(\mathbf{X})$는 $\theta$에 대한 최소 충분통계량이다.

【예제 ⑤】 확률표본 $X_{1},\ldots,X_{n}$이
$\text{N}(\mu,\sigma^{2})$에서 추출되었고 모수 $(\mu,\sigma^{2})$ 둘 다
모를 경우 $(\overline{x},s^{2})$는 최소 충분통계량이다.

$$\frac{f(\mathbf{x}|\mu,\sigma^{2})}{f(\mathbf{y}|\mu,\sigma^{2})} = \exp\left( \left\lbrack - n({\overline{x}}^{2} - {\overline{y}}^{2}) + 2n\mu(\overline{x} - \overline{y}) - (n - 1)(s_{x}^{2} - s_{y}^{2}) \right\rbrack/(2\sigma^{2}) \right)$$

이 비가 $(\mu,\sigma^{2})$에 대해 상수가 되려면
$\overline{x} = \overline{y},s_{x}^{2} = s_{y}^{2}$

이어야 한다. 따라서, 표본평균 $\overline{X}$와 표본분산 $S^{2}$는
$(\mu,\sigma^{2})$

에 대한 최소 충분통계량이다.

【예제 ⑥】 확률표본 $X_{1},\ldots,X_{n}$이 $U(\theta,\theta + 1)$에서
추출되었다고 하자. $T(\mathbf{X}) = (X_{(1)},X_{(n)})$은 모수 $\theta$에
대한 최소 충분통계량이다.

$$f(\mathbf{x}|\theta) = \{\begin{matrix}
1 & \text{if}\theta < x_{i} < \theta + 1,\text{for all}i = 1,\ldots,n \\
0 & \text{otherwise}
\end{matrix}$$

$$f(\mathbf{x}|\theta) = \{\begin{matrix}
1 & \text{if}\max_{i}x_{i} - 1 < \theta < \min_{i}x_{i} \\
0 & \text{otherwise}
\end{matrix}$$

두 표본 $\mathbf{x},\mathbf{y}$에 대하여, 비율
$\frac{f(\mathbf{x}|\theta)}{f(\mathbf{y}|\theta)}$이 $\theta$에 대해
항상 일정하려면
$\min_{i}x_{i} = \min_{i}y_{i},\max_{i}x_{i} = \max_{i}y_{i}$이어야
한다.

최소 충분통계량은 유일하지 않다.

$T'(\mathbf{X}) = (X_{(n)} - X_{(1)},(X_{(n)} + X_{(1)})/2)$,

$T'(\mathbf{X}) = \left( \overset{n}{\sum_{i = 1}}X_{i},\overset{n}{\sum_{i = 1}}X_{i}^{2} \right)$
또한 최소 충분통계량이다.

보조 통계량

통계량 $S(\mathbf{X})$의 분포가 모수 $\theta$와 무관할 때, 이를 보조
ancillary 통계량이라고 한다.

정의

【예제 ⑥ 계속】 확률표본 $X_{1},\ldots,X_{n}$이
$U(\theta,\theta + 1)$에서 추출되었다고 하자.
$T(\mathbf{X}) = (X_{(1)},X_{(n)})$은 모수 $\theta$에 대한 최소
충분통계량이므로 $R = X_{(n)} - X_{(1)}$은 보조 통계량이다.

$$g(x_{(1)},x_{(n)} \mid \theta) = \{\begin{matrix}
n(n - 1)(x_{(n)} - x_{(1)})^{n - 2} & \text{if}\theta < x_{(1)} < x_{(n)} < \theta + 1 \\
0 & \text{otherwise}.
\end{matrix}$$

$R = X_{(n)} - X_{(1)},M = \frac{X_{(1)} + X_{(n)}}{2}$ 변수변환 하면,

$$h(r,m \mid \theta) = \{\begin{matrix}
n(n - 1)r^{n - 2}, & \text{if}0 < r < 1,\theta + \frac{r}{2} < m < \theta + 1 - \frac{r}{2} \\
0, & \text{otherwise}.
\end{matrix}$$

$$h(r \mid \theta) = n(n - 1)r^{n - 2}(1 - r),0 < r < 1 \sim Beta(n - 1,2)$$

$R = X_{(n)} - X_{(1)}$의 확률밀도함수는 모수 $\theta$에 의존하지
않는다.

완비 통계량

최소 충분통계량은 표본으로부터 모수 $\theta$에 대한 모든 정보를
유지하면서, 그 외의 불필요한 정보를 최대한 제거한 통계량이다. 즉,
표본에서 모수와 관련된 핵심 정보만을 남기는 데이터 축약 방법이다. 반면,
보조 통계량은 그 분포가 모수 $\theta$에 의존하지 않는 통계량으로 모수에
대한 정보를 전혀 담고 있지 않다.

이 둘은 개념적으로 구별되지만, 반드시 독립적이지는 않다. 예를 들어,
$uniform(\theta,\theta + 1)$에서, 최소값과 최대값의 조합인
$(X_{(n)} - X_{(1)},(X_{(n)} + X_{(1)})/2)$은 최소 충분통계량이 되고, 그
중 $X_{(n)} - X_{(1)}$은 보조 통계량이 된다. 이 경우, 최소 충분통계량과
보조 통계량은 서로 독립하지 않으며, 오히려 하나의 구성요소가 된다.

보조 통계량은 모수 추정 정밀도에 기여

$X_{1},X_{2}$가 다음 이산분포 에서 독립적으로
관측되었다.$P_{\theta}(X = \theta) = P_{\theta}(X = \theta + 1) = P_{\theta}(X = \theta + 2) = \frac{1}{3}$.
순서 통계량 $X_{(1)},X_{(2)}$으로
$R = X_{(2)} - X_{(1)},M = (X_{(1)} + X_{(2)})/2$를 정의하면, $(R,M)$은
최소 충분통계량이고, $R$은 보조 통계량이다.

그러나 $R$이 보조 통계량임에도 불구하고 모수 $\theta$에 대해 간접적으로
중요한 정보를 제공할 수 있다. 예를 들어, 단순히 $M = m$이라는 정보만
알고 있을 때, 가능한 $\theta$ 값은 $m,m - 1,m - 2$ 세 가지가 된다.
하지만 추가로 $R = 2$라는 정보를 알게 되면,
$X_{(1)} = m - 1,X_{(2)} = m + 1$이 되어, 가능한 $\theta$ 값이 유일하게
$m - 1$로 결정된다. 즉, 보조 통계량 $R$이 모수 추정의 정밀도를 높이는 데
기여한 것이다.

어떤 통계량 $T(\mathbf{X})$에 대해 확률분포족 $f(t|\theta)$가 있을 때,
이 분포족을 완비라고 부른다. 완비란, 모든 $\theta$에 대해
$\mathbb{E}\theta\lbrack g(T)\rbrack = 0$이면서도,
$P_{\theta}(g(T) = 0) = 1$이 되는 경우를 말한다. 즉, 기대값이 0인 함수
$g(T)$는 거의 확률 1로 항상 0이어야 한다는 뜻이다. 이 경우,
$T(\mathbf{X})$를 완비 통계량 complete 이라고 한다.

정의

【예제 ①】 $X_{1},\ldots,X_{n}$을 모수 $\theta \in (0,1)$인 베르누이
분포를 따르는 확률표본이라 하자.
$T(\mathbf{X}) = \sum X_{i} \sim B(n,\theta)$는 $\theta$에 대한 완비
충분 통계량이다.

【예제 ②】 확률표본 $X_{1},\ldots,X_{n}$이 $U(0,\theta)$에서
추출되었다고 하자. $T(\mathbf{X}) = max(x_{i}) = x_{(n)}$은 모수
$\theta$에 대한 완비 충분 통계량이다.

(Basu[']{dir="rtl"}s Theorem) 만약 $T(\mathbf{X})$가 완비하고 최소 충분
통계량이라면, $T(\mathbf{X})$는 모든 보조 통계량과 서로 독립이다.

정리

완비성과 최소충분성이라는 강력한 조건을 만족할 경우, 매개변수 $\theta$와
무관하게 분포하는 보조통계량들과 $T(\mathbf{X})$사이에는 어떠한 의존성도
존재하지 않음을 의미한다. 최소충분 통계량과 모수와 무관한
정보보조통계량를 분리할 수 있게 해주기 때문에, 통계 추론이나 신뢰구간
설정에 매우 유용하게 사용된다.

$X_{1},\ldots,X_{n}$이 지수족 분포를 따르는 확률표본이라고 하자.

정리

$f(x|\theta) = h(x)c(\theta)\exp\left( \overset{k}{\sum_{j = 1}}w(\theta_{j})t_{j}(x) \right)$.
다음 $T(\mathbf{X})$는 완비통계량이다.
$T(\mathbf{X}) = \left( \overset{n}{\sum_{i = 1}}t_{1}(X_{i}),\overset{n}{\sum_{i = 1}}t_{2}(X_{i}),\ldots,\overset{n}{\sum_{i = 1}}t_{k}(X_{i}) \right)$

(Minimal Complete Statistic) 만약 최소 충분통계량이 존재한다면, 모든
완비통계량도 최소 충분통계량이다.

정리

【예제 ③】 $X_{1},\ldots,X_{n}$을 $exp(\theta)$, 지수분포(지수족)를
따르는 확률표본이라 하자.
$T(\mathbf{X}) = \sum X_{i} \sim Gamma(n,\theta)$는 $\theta$에 대한 완비
최소 충분 통계량이다.

【예제 ④】 $X_{1},\ldots,X_{n}$을 $N(\mu,\sigma^{2})$,
정규분포(지수족)를 따르는 확률표본이라 하자.
$T(\mathbf{X}) = (\overline{X},S^{2})$는 $\theta$에 대한 완비 최소 충분
통계량이다.

2우도함수 원리

통계적 추론에서는 데이터로부터 정보를 요약하는 방법이 중요하다.
우도함수는 단순히 하나의 요약 방법이 아니라, 특정 원칙을 수용할 경우
필수적인 데이터 축약 장치로 간주된다.

충분성 원칙: 관찰된 데이터가 어떤 충분한 통계량에 의해서만 정보를
제공한다면, 모든 추론은 이 충분한 통계량에만 의존해야 한다.

조건화 원칙: 실험 설계상 복수의 실험이 가능한 경우, 실제로 수행된 실험의
결과만을 기반으로 추론해야 한다.

우도 원칙: 주어진 데이터에 대한 우도함수의 형태만이 추론에 중요하며,
데이터가 관찰된 경로는 중요하지 않다.

위의 원칙들을 받아들인다면, 우도함수는 주어진 데이터로부터 정보를
요약하는 유일하고 필수적인 수단이 된다.

우도함수

확률표본 $\mathbf{X} = (X_{1},\ldots,X_{n})$의 결합 확률밀도함수를
$f(\mathbf{x}|\theta)$라고 하자. 이때 표본 데이터
$\mathbf{X} = \mathbf{x}$가 관측되었을 때,

정의

모수 $\theta$의 함수로 정의되는
$L(\theta|\mathbf{x}) = f(\mathbf{x}|\theta)$를 우도함수 likelihood
function 라고 한다.

우도함수는 관측된 데이터 $\mathbf{x}$를 기준으로 다양한 $\theta$ 값들에
대해 상대적 타당성을 비교하는 도구이다. 이산형, 연속형 모두 우도비를
통해 두 모수에 대한 비교가 가능하다. 즉, 실제 데이터가 관측되어 우도
값이 계산된다면 우도 값이 큰 모수가 진짜 모수일 가능성이 높다.

【예제 ①】 $NB(r = 3,p)$, 음이항분포로부터 $X_{1} = 2$ 관측되었다면
우도함수는 $P_{p}(X = 2) = \binom{4}{2}p^{3}(1 - p)^{2}$이다.

【우도 원리】 표본점 $\mathbf{x},\mathbf{y}$가 다음 조건을 만족한다고
하자.

두 표본에 대해 우도함수 $L(\theta|\mathbf{x})$, $L(\theta|\mathbf{y})$가
서로 비례한다.

즉, 모든 $\theta$에 대해 다음을 만족하는 상수
$C(\mathbf{x},\mathbf{y})$가 존재한다.

$$L(\theta|\mathbf{x}) = C(\mathbf{x},\mathbf{y})L(\theta|\mathbf{y}),\text{for all}\theta$$

두 표본 $\mathbf{x},\mathbf{y}$가 관찰되었을 때, 만약 이들의 우도함수가
비례한다면, 이 두 표본은 동일한 정보를 제공한다. 통계적 결론은 오직
우도함수 에만 의존해야 하며, 표본의 다른 세부사항에는 의존하지 않는다.

공식 formal 충분 통계량 원칙

어떤 실험 $E = (X,\theta,\{ f(x|\theta)\})$이 수행되었고, $T(X)$이
$\theta$에 대한 충분통계량이라 할 때, 만약 두 관측값 $x$와 $y$가
$T(x) = T(y)$를 만족한다면, 이 두 관측값이 제공하는 증거는 동일해야
한다는 것이다. 즉, 관측 데이터 전체 $x$ 자체가 아니라, 그로부터 계산된
충분통계량 $T(x)$만이 $\theta$에 관한 모든 정보를 요약하므로, 두
데이터가 동일한 충분통계량 값을 가질 때는, 둘 모두 $\theta$에 대해
동일한 결론을 가져야 한다.

조건화 conditionality 원칙

여러 개의 가능한 실험이 있을 때, 어떤 실험이 실제로 수행되었는지가 매우
중요하다는 사실을 강조한다. 예를 들어, 두 개의 실험 $E_{1},E_{2}$ 중
무작위로 하나를 선택하여 시행한다고 가정하자. 이때, 어느 실험이
선택되었는지는 관측값과 함께 반드시 고려되어야 하며, 실제로 수행된
실험에 기반하여 추론이 이루어져야 한다.

보다 공식적으로, 혼합 실험 $E^{*}$이 정의될 때, 실험 $E_{j}(j = 1,2)$가
수행되고 관측값 $x_{j}$가 주어진 경우,
$\text{Ev}(E^{,}(j,x_{j})) = \text{Ev}(E_{j},x_{j})$이어야 한다. 즉,
실험 $E$로부터 얻어진 데이터라도 실제로 수행된 $E_{j}$에 기반하여
해석되어야 한다. 조건화 원칙은 ["]{dir="rtl"}오직 수행된 실험만이
중요하며, 선택되지 않은 실험들은 전혀 고려되어서는 안 된다"는 점을
명확히 한다. 이는 실험 설계 단계에서 무작위성이 개입되더라도, 실제로
수행된 실험만이 추론의 근거가 되어야 한다는 점에서 자연스럽고 설득력
있는 원칙이다.

1)  우도 원칙

공식 충분성 원칙과 조건화 원칙을 함께 받아들이면, 우도 원칙이 도출된다.
즉, 두 실험에서 수집된 두 데이터 $x_{1}^{*},x_{2}^{*}$가 생성하는
우도함수가 다음과 같은 비례 관계를 만족할 때,
$L(\theta|x_{2}^{*}) = CL(\theta|x_{1}^{*})$이 두 데이터는 $\theta$에
대해 동일한 증거를 제공해야 한다.

따라서 관측 데이터가 생성하는 우도함수만이 파라미터에 관한 모든 정보를
담고 있으며, 우도함수가 같으면 추론 결과도 같아야 한다는 결론에
도달한다. 이는 바로 우도 원칙의 본질이다.

2)  동등성 Equivariance 원칙

동등성 원칙에서는 함수 $T(x)$가 지정되지만, $T(x) = T(y)$일 때 $x$를
관찰했을 경우와 $y$를 관찰했을 경우 추론 결과가 ["]{dir="rtl"}일정한
관계["]{dir="rtl"}를 가져야 한다고 요구한다. 반드시 동일할 필요는
없지만, 정해진 관계를 따라야 한다는 점이 특징이다. 또한 동등성 원칙은
실제로 두 가지 다른 고려사항을 결합한 것으로 이해할 수 있다:

Measurement Equivariance

측정 단위에 의존하지 않는 추론을 요구한다. 예를 들어, 두 산림 조사원이
각각 나무의 평균 직경을 측정한다고 하자. 한 명은 인치 단위로, 다른 한
명은 미터 단위로 데이터를 수집하였다. 비록 단위가 다르더라도, 최종적으로
동일한 추정값을 제시해야 한다. 즉, 단위 변환(예: 인치를 미터로 변환)
이후 결과가 일치해야 한다.

Formal Invariance

수학적 모델의 구조가 동일하다면 추론 절차 역시 동일해야 한다고 요구한다.
이는 물리적 의미(예: 단위 등)와는 무관하게, 다음 세 가지가 같다면,
동일한 추론 방법을 사용해야 한다는 것이다.

모수 공간 $\Theta$

확률밀도함수 $f(x|\theta)$

허용 가능한 추론 및 오차

만약 $Y = g(X)$가 $X$의 측정 단위 변환이고, $Y$의 모델이 $X$의 모델과
동일한 수학적 구조를 갖는다면, 추론 절차는 측정 단위 변화에 대해
불변하며 동시에 수학적 구조에 대해 불변해야 한다.

【예제 ①】 $X \sim \text{Binomial}(n,p)$일 때, 성공 횟수 $x$를 관찰한
경우를 생각한다. 실패 횟수는 $Y = n - X$로 표현할 수 있으며, 역시
$\text{Binomial}(n,q = 1 - p)$분포를 따른다.

Measurement Equivariance 요구

성공 수 $x$를 기반으로 한 추정값 $T(x)$와 실패 수 $y = n - x$를 기반으로
한 추정값 $T(y)$는 다음을 만족해야 한다.

$$T(x) = 1 - T(n - x)\text{or}T(x) = 1 - T(n - x)$$

Chapter 2. 점 추정

1개념

첫 번째 부분은 추정량을 찾는 방법을, 두 번째 부분은 추정량(및 기타 다른
추정량)을 평가하는 방법을 다룬다. 점추정의 논리는 매우 단순하다.
모집단이 확률밀도함수 $f(x|\theta)$로 기술될 때 $\theta$에 대한 지식은
모집단 전체에 대한 정보를 제공한다. 따라서, $\theta$의 좋은 추정량을
찾는 방법을 모색하는 것은 자연스러운 일이다. 또한, 경우에 따라서는
$\theta$의 함수, 즉 $\tau(\theta)$가 관심 대상이 될 수도 있다.

점추정량은 확률표본의 함수 $W(X_{1},\ldots,X_{n})$이다. 즉, 모든
통계량은 점추정량이다.

정의

모집단 확률분포함수 $f(x;\theta),\theta \in \Omega$의 확률표본에서 얻은
통계량이 추정에 사용된다면 이를 추정량 estimator 이라 한다. 근사할
것이라고 생각하는 하나의 값으로 제시한다면 이를 점추정 point estimate,
$\theta$을 포함하고 있을 가능성이 높은 구간을 제시하는 것은 구간추정
interval estimate이라 한다. 계산되는 공식을 추정량, 실제 데이터를
이용하여 계산된 값을 추정치 estimates 이라 한다.

모집단 확률분포함수 $f(x;\theta),\theta \in \Omega$의 확률표본에서 얻은
통계량 $T = T\left( X_{1},X_{2},\ldots,X_{n} \right)$이 모수 추정에
사용되면 이를 추정량 이라 하고 $\overset{\hat{}}{\theta}$이라 표현한다.

정의

추정량 : $\overset{\hat{}}{\theta} = T(X_{1},X_{2},\ldots,X_{n})$
대문자로 표현

추정치 : : $t(x_{1},x,\ldots,x_{n})$ 관측된 값으로 소문자로 표현

2추정량 구하는 방법

적률법

가장 오랜 방법으로 적률을 이용하여 모수 추정하는 방법으로 매우 간단하나
좋은 추정량의 조건을 갖추지 않을 수 있다.

알려지지 않은 모집단 확률분포함수 $f(x;\theta),\theta \in \Omega$,
확률표본 $\left( X_{1},X_{2},\ldots,X_{n} \right)$에서 모집단의 $k$차
적률 ${\mu'}_{k} = E(X^{k})$과 표본의 $k$차 적률
$m_{k}' = E(X_{i}^{k})$이라 하자.

$\mu_{k}' = m_{k}$이라 놓고 풀면 모수 추정량 얻게 된다.

만약 모수 한 개 이상이면 적률에 의한 방정식을 모수 수만큼 얻어 사용하면
된다.

【예제 ①】 모집단 $B(n,p)$으로부터 확률표본
$\left( X_{1},X_{2},\ldots,X_{n} \right)$이다. 적률방법으로 추정량
$\overset{\hat{}}{n},\overset{\hat{}}{p}$ 구하라.

모집단 적률 :
$\mu_{1}' = E(X) = np,\mu_{2}' = E\left( X^{2} \right) = np(1 - p) + {(np)}^{2}$

표본적률 : $m_{1}' = E(X) = \overset{¯}{X}$,
$m_{2}' = E\left( X^{2} \right) = \frac{1}{n}\sum X_{i}^{2}$

방정식 : $np = \overset{¯}{X}$,
$\frac{1}{n}\sum X_{i}^{2}$=$= np(1 - p) + {(np)}^{2}$

모비율 추정량 : $\widehat{p} = \overline{x}$

【예제 ②】 모집단 $N(\mu,\sigma^{2})$으로부터 확률표본
$\left( X_{1},X_{2},\ldots,X_{n} \right)$이다. 적률방법으로
 $추정량\overset{\hat{}}{\mu},\overset{\hat{}}{\sigma^{2}}$ 구하라.

모집단 적률 :
$\mu_{1}' = E(X) = \mu,\mu_{2}' = E\left( X^{2} \right) = \sigma^{2} + \mu^{2}$

표본 적률 : $m_{1}' = E(X) = \overset{¯}{X}$,
$m_{2}' = E\left( X^{2} \right) = \frac{1}{n}\sum X_{i}^{2}$

방정식 : $\mu = \overset{¯}{X}$,
$\sigma^{2} + \mu^{2} = \frac{\sum X_{i}^{2}}{n}$

평균 추정량 : $\overset{\hat{}}{\mu} = \overset{¯}{x}$

분산 추정량 :
$\overset{\hat{}}{\sigma^{2}} = \frac{1}{n}\sum X_{i}^{2} - {\overset{¯}{X}}^{2} = \frac{1}{n}\sum\left( X_{i} - \overset{¯}{X} \right)^{2}$

【예제 ③】 모집단 $U(0,\theta)$으로부터 확률표본
$\left( X_{1},X_{2},\ldots,X_{n} \right)$이다. 적률방법으로  추정량
$\widehat{\theta}$ 구하라.

모집단 적률 : $\mu_{1}' = E(X) = \frac{\theta}{2}$

표본 적률 : $m_{1}' = \overset{¯}{X}$

$\overset{¯}{X} = \frac{\theta}{2}$ 이므로 적률에 의한 추정량은
$\overset{\hat{}}{\theta} = 2\overset{¯}{X}$이다.

  -------------------------------------------------------------------------------------------------------------
  추정량 $\overset{\hat{}}{\theta} = 2\overset{¯}{X}$은 불편 추정량은
  ($E\left( 2\overset{¯}{X} \right) = 2\theta \neq \theta$)아니지만 일치 추정량이다. 【정리】 만약
  $\lim_{n \rightarrow \infty}{V\left( \overset{\hat{}}{\theta} \right)( = 4\frac{\theta^{2}}{12n}) = 0}$이면
   $\overset{\hat{}}{\theta}$는 일치 추정량이다.

  -------------------------------------------------------------------------------------------------------------

【예제 ④】 모집단 $Gamma(\alpha,\beta)$으로부터 확률표본
$\left( X_{1},X_{2},\ldots,X_{n} \right)$이다. 적률방법으로 모수
$\alpha,\beta$ 추정량을 구하라.

모집단 적률 :
$\mu_{1}' = E(X) = \alpha\beta,\mu_{2}' = E\left( X^{2} \right) = \alpha\beta^{2} + (\alpha{\beta)}^{2}$

표본 적률 : $m_{1}' = E(X) = \overset{¯}{X}$,
$m_{2}' = E\left( X^{2} \right) = \frac{1}{n}\sum X_{i}^{2}$

방정식: $\alpha\beta = \overset{¯}{X}$,
$\alpha\beta^{2} + \alpha\beta^{2} = \frac{\sum X_{i}^{2}}{n}$

추정량 :
$\overset{\hat{}}{\alpha} = \frac{n\overset{¯}{X}}{n\sum\left( X_{i} - \overset{¯}{X} \right)^{2}}$,
$\overset{\hat{}}{\beta} = \frac{\sum\left( X_{i} - \overset{¯}{X} \right)^{2}}{n\overset{¯}{X}}$

불편 추정량은 아니지만 $\overset{¯}{X}$는 $\alpha\beta$의 일치
추정량이고 $\frac{1}{n}\sum X_{i}^{2}$은
$\alpha\beta^{2} + (\alpha{\beta)}^{2}$의 일치 추정량이다. 적률에 의해
구한 추정량은 일치 추정량이기는 하지만 불편성 보장은 물론 MVUE라는
보장이 없다. 쉽게 얻을 수 있다는 장점으로 인하여 추정량을 이해하기
위하여 시작점이 된다.

최대우도 추정량 MLE

개념

최종적으로 최량 추정량, MVUE(minimum variance unbiased estimator
최소분산 불편 추정량)를 구하기 위하여 ⑴Factorial criterion에 의해 충분
통계량을 구하고 ⑵충분 통계량의 함수이면서 불편성을 갖는 추정량을 구하면
Rao-Blackwell 정리에 의해 이것이 MVUE이다. 그러나 불편 추정량을 구하는
것이 그렇게 쉽지만은 않다.

한편, 적률에 의한 추정량은 일치성은 보장하지만 불편성, MVUE는 아닐
가능성이 높다. 이제 MVUE일 가능성이 높은 추정 방법을 소개하고자 한다.
추출된(수집된) 확률표본(데이터)이 어떤 모수 값일 경우 그 값들이 추정될
가능성이 가장 높은가? 이를 최대 우도 추정량이라 한다. 통계추론에서
사용되는 추정량은 대부분 MLE이다.

주머니 속에 공이 3개 들어 있다. 공의 색깔은 하양, 파랑일 수 있다. 그러나
각 몇 개씩 들어 있는지는 모른다고 가정하자. 2개의 공을 뽑아 색을 보고
주머니에 있는 공의 색을 맞춘다고 하자. 공 2개를 뽑았더니 파랑이었다.
그럼? 주머니의 공은?

하얀 공일 확률: 1/3($= \binom{2}{2}\binom{1}{0}/\binom{3}{2}$),

파란 공일 확률: 1($= \binom{3}{2}/\binom{3}{2}$)

파랑 공이 가능성이 높다. 이렇게 모수에 대한 추정량을 구하는 방법이
최대우도 추정법이다.

MLE 구하기

【우도함수】 알려지지 않은 모집단 확률분포함수
$f(x;\theta),\theta \in \Omega$에 대한 정보를 얻기 위하여 추출한 크기
$n$의 (확률)표본
$\overline{X} = \left( X_{1},X_{2},\ldots,X_{n} \right)$의 결합
확률밀도함수를 모수 포함한 함수로 표현한 것을 우도함수 likelihood
function 이라 하며 모수의 함수이다.

정의

우도함수 :
$L\left( \theta;x_{1},x_{2},\ldots,x_{n} \right) = L(\theta;\overline{x}) = \prod_{i}^{n}{f(x_{i};}\theta)$

확률표본(데이터) 결합 확률(표본 데이터가 수집되었다면 어떤 모수 값일
가능성이 가장 높은가)을 최대화 하는 모수 값을 MLE라 한다.

【MLE】 우도함수 최대화 하는 $\theta$를 최대우도 maximum likelihood
추정량 이라 한다.

정의

$\frac{\partial L(\theta)}{\partial\theta} = 0$을 만족하는 추정량
$\overset{\hat{}}{\theta}(\overline{x})$을 $MLE$ 이라 한다.

【로그 우도함수】 우도 함수는 항상 0보다 크므로 우도함수 최대화 하는
$\theta$ 계산 ⬄ 로그 우도함수 최대화 하는 $\theta$ 계산

【예제 ①】 어느 지역의 암 환자 비율 $p$을 추정하려고 한다. 모수 $p$인
베르누이 확률밀도함수로부터 확률표본을 추출하였다고 하자.

모집단 확률밀도함수 : $f(x;\theta = p) = p^{x}(1 - p)^{1 - x},x = 0,1$

우도 함수 :
$L(\theta;\overline{x}) = \prod_{i}^{n}{f(x_{i};p)} = \sum_{i}^{n}{p^{x_{i}}(1 - p)^{1 - x_{i}}} = p^{\sum x_{i}}(1 - p)^{n - \sum x_{i}}$

로그 우도함수 :
$l(\theta) = \ln\left( L(\theta) \right) = \sum x_{i}\ln(p) + (n - \sum x_{i})ln(1 - p)$

MLE :
$\frac{\partial l(\theta)}{\partial\theta} = \sum x_{i}\left( \frac{1}{p} \right) + (n - \sum x_{i})\frac{1}{1 - p}( - 1) = 0$,

그러므로 $\overset{\hat{}}{p} = \frac{\sum x_{i}}{n}$이다.

【예제 ②】 빼빼로 중량이 $N(\mu,\sigma^{2})$을 따른다고 하자. 중량
평균을 추정하기 위하여 확률표본을 추출하였다고 하였다. 모집단 모수
$\overline{\theta} = (\mu,\sigma)$는 2개이나 평균에 관심이 있으므로
$\mu$는 목표 모수, 분산 $\sigma^{2}$은 불필요 nuisance 모수 이다.

모집단 확률밀도함수 :
$f(x;\mu,\sigma) = \frac{1}{2\sqrt{}\pi\sigma}\exp\left( - \frac{(x - \mu)^{2}}{2\sigma^{2}} \right), - \infty < x < \infty$

로그 우도함수 :
$l\left( \mu,\sigma^{2} \right) = \frac{n}{2}\ln(2\pi) - \frac{1}{2}\sum\left( \frac{x_{i} - \mu}{\sigma} \right)^{2}$

MLE :
$\frac{\partial l(\mu,\sigma)}{\partial\mu} = 0,\frac{\partial l\left( \mu,\sigma^{2} \right)}{\partial\sigma^{2}} = 0$
그러므로
$\overset{\hat{}}{\mu} = \overset{¯}{X},{\overset{\hat{}}{\sigma}}^{2} = \frac{\sum\left( x_{i} - \overset{¯}{x} \right)^{2}}{n}$이다.

【예제 ③】 모집단 확률분포 $U(0,\theta)$, 확률표본에서 MLE을 구하시오.

모집단 확률밀도함수 : $f(x;\theta) = \frac{1}{\theta},0 < x < \theta$

우도함수 :
$L(\theta) = \Pi\left( \frac{1}{\theta} \right)I_{(x_{i},\theta)} = \Pi\left( \frac{1}{\theta} \right)I_{(\max\left\{ x_{i} \right\},\theta)}$

$I_{\lbrack a,b\rbrack}$은 지시 indicator 함수로 $(a < b)$이면 1, 그렇지
않으면 0이다. 우도함수 최대화 되려면
$\overset{\hat{}}{\theta} = \max\left\{ x_{i} \right\} = x_{(n)}$

【예제 ④】 라플라스분포
$f(x;\theta) = \frac{1}{2}e^{- |x - \theta|}, - \infty < x < \infty, - \infty < \theta < \infty$을
따르는 확률표본을 이용하여 $\theta$에 대한 MLE 구하라.

로그 우도함수 : $l(\theta) = - nln(2) - \sum_{i}^{n}{|x_{i} - \theta|}$

미분 :
$\frac{\partial l}{\partial\theta} = \sum_{i}^{n}{sgn(x_{i} - \theta)} = 0,wheresgn(t) = \left\{ \begin{array}{r}
 - 1,t < 0 \\
0,t = 0 \\
1,t > 0
\end{array} \right.\ $

그러므로 $\overset{\hat{}}{\theta} = Median$, MLE이다.

【예제 ⑤】 $N(\mu,1),where\mu > 0$을 따르는 확률표본을 이용하여 $\mu$에
대한 MLE 구하라.

로그 우도함수 최대화 하는 MLE $\overset{\hat{}}{\mu} = \overline{X}$
이므로 $\overset{\hat{}}{\mu} = \left\{ \begin{array}{r}
\overline{X}if\overline{X} \geq 0 \\
0if\overline{X} < 0
\end{array} \right.\ $

【예제 ⑥】 $B(n,p)$을 따르는 확률표본을 이용하여 $n$에 대한 MLE
구하라(단, $p$는 알려져 있음). (적용) 동전의 공정성을 평가하기 위하여 몇
번을 던져야 하나?

우도함수 :
$L\left( k;\overline{x},p \right) = \prod_{i}^{n}{\binom{k}{x_{i}}p^{x_{i}}(1 - p)^{k - x_{i}}}$

$k$에 대한 우도함수 미분은 쉽지 않다. 만약 $k < x_{(n)}$이면
$L\left( k;\overline{x},p \right) = 0$ 이므로 다음 조건을 만족하는
$k \geq x_{(n)}$이 MLE이다.

$\frac{L\left( k;\overline{x},p \right)}{L\left( k - 1;\overline{x},p \right)} \geq 1,\frac{L\left( k + 1;\overline{x},p \right)}{L\left( k;\overline{x},p \right)} < 1$.

그러므로 최대화 조건은 다음과
같다.$\left( k(1 - p) \right)^{n} \geq \prod_{1}^{n}\left( k - x_{i} \right)and\left( (k + 1)(1 - p) \right)^{n} \geq \prod_{1}^{n}{(k + 1 - x_{i})}$이다.
결론적으로 $(1 - p)^{n} = \overset{n}{\prod_{i = 1}}(1 - x_{i}z)$. 구간
$0 \leq z \leq 1/\max_{i}x_{i}$ 범위 내에서 MLE 구하면
$\widehat{k} = \lfloor 1/\widehat{z}\rfloor$ 소숫점 버리고 내림한
값이다.

MLE 성질

【invariance property】 $f(x;\theta),\theta \in \Omega$을 따르는
확률표본으로부터 $\overset{\hat{}}{\theta}$은 MLE,
$\theta \rightarrow \tau(\theta)$ 일대일 맵핑이라면 $\tau(\theta)$ MLE은
$\tau\left( \overset{\hat{}}{\theta} \right)$이다.

정리

【예제 ①】 $N\left( \mu,\sigma^{2} \right)$에서
$\overline{\theta} = (\mu,\sigma^{2})$ MLE는
$\overset{\hat{}}{\mu} = \overset{¯}{X},{\overset{\hat{}}{\sigma}}^{2} = \frac{\sum\left( x_{i} - \overset{¯}{x} \right)^{2}}{n}$.

invariance property에 의해 표준편차
$\sigma = \sqrt{\sigma^{2}}(\tau(\theta))$ MLE는
$\overset{\hat{}}{\sigma} = \sqrt{\frac{\sum\left( x_{i} - \overset{¯}{x} \right)^{2}}{n}}$이다.

【예제 ②】 $B(p)$에서 확률표본 $(X_{1},X_{2},\ldots,X_{n})$이다.
$V(X)$의 MLE 구하라.

$V(X) = p(1 - p)$이고 모수 $p$에 대한 MLE은
$\overset{\hat{}}{p} = \frac{\sum X_{i}}{n}$ 이므로 분산의 MLE 는
$\overset{\hat{}}{V(X)} = \frac{\sum X_{i}}{n}(1 - \frac{\sum X_{i}}{n})$이다.

모수 $\theta$에 대한 MLE ${\overset{\hat{}}{\theta}}_{mle}$은 일치
추정량이다.

정리

베이즈 추정량

베이지안 접근법은 통계학에 대한 고전적인 접근법과 근본적으로 다른데
고전적인 접근법에서는 모수 $\theta$가 알려지지 않지만 고정된 값으로
간주된다. 모수 $\theta$에 대한 정보는 확률표본
$\overline{X} = \left( X_{1},X_{2},\ldots,X_{n} \right)$을 추출한 후
계산된 통계량을 기초하여 얻어진다.

베이지안 접근법에서는 모수 $\theta$는 확률변수로서의 변동성을 갖는
양으로 간주되며 이를 사전 확률밀도함수라 한다. 이는 분석자의 믿음에
기반한 주관적인 분포로서 데이터가 관찰되기 전에 정의하고 그런 다음 모수
$\theta$인 모집단에서 표본을 추출하고 이 표본 정보를 사용하여 사전
분포를 업데이트한다. 이 업데이트된 사전 분포를 사후 분포라고 하고 이러한
업데이트는 베이즈 정리를 사용하여 수행한다.

【사후확률】 $\pi(\theta)$ 모수 사전 prior 확률밀도함수,
$L(\theta;\overline{x})$을 우도 함수이면 확률표본
$\overline{x} = \left( x_{1},x_{2},\ldots,x_{n} \right)$이 주어진 경우
모수에 대한 조건부 확률밀도함수를 사후 posterior 확률밀도함수라 한다.

정의

> $$\pi\left( \theta \middle| \overline{x} \right) = \frac{\pi(\theta)L(\theta;\overline{x})}{m(\overline{x})} \propto \pi(\theta)L(\theta;\overline{x}),wherem\left( \overline{x} \right) = \int\pi(\theta)L\left( \theta;\overline{x} \right)d\theta$$

【bayes estimator 베이지안 추정량】

정의

최소 squared error loss function(제곱 오차 손실함수)
$Loss\left( \theta,\overset{\hat{}}{\theta} \right) = \left( \theta - \overset{\hat{}}{\theta} \right)^{2}$
: 사후 확률함수 평균

최소 absolute error loss function(절대 오차 손실함수)
$Loss\left( \theta,\overset{\hat{}}{\theta} \right) = |\theta - \overset{\hat{}}{\theta}|$
: 사후 확률함수 중앙값

【예제 ①】 $B(p)$에서 추출한 확률표본
$\overline{X} = \left( X_{1},X_{2},\ldots,X_{n} \right)$을 이용하여
베이즈 추정량 구하라.

우도함수 :
$L\left( p;\overline{x} \right) = \binom{n}{y}p^{y}(1 - p)^{n - y}wherey = \sum_{i}^{n}x_{i}$

사전확률 : (1) uniform prior $\pi(p) \sim U(0,1)$, (2) conjugate prior
$\pi(p) \sim Beta(\alpha,\beta)$

conjugate prior: 사후 확률밀도함수와 동일한 분포를 갖는 사전
확률밀도함수를 conjugate prior라 한다. 비율의 사후 확률밀도함수가
베타분포 이므로 사전 확률밀도함수를 베타분포이면 이를 conjugate prior라
한다.

uniform prior 사후확률 :
$\pi\left( p \middle| \overline{x} \right) \propto 1_{(0,1)}^{p}\binom{n}{y}p^{y}(1 - p)^{n - y} \sim Beta(y + 1,n - y + 1)$

conjugate prior 사후확률 :
$\pi\left( p \middle| \overline{x} \right) \propto Beta(\alpha,\beta)\binom{n}{y}p^{y}(1 - p)^{n - y} \sim Beta(\alpha + y,\beta + n - y)$

베이즈 추정량 (제곱 오차 손실 함수 적용) :
$\overset{\hat{}}{p} = \frac{y + 1}{(n + 2)}$(uniform prior),
$\overset{\hat{}}{p} = \frac{\alpha + y}{(n + \alpha + \beta)}$(conjugate
prior)

【예제 ②】 $N\left( \theta,\sigma^{2} \right)$에서 추출한 확률표본
$\overline{X} = \left( X_{1},X_{2},\ldots,X_{n} \right)$을 이용하여
$\left( \theta,\sigma^{2} \right)$베이즈 추정량 구하라.

Conjugate 사전 확률밀도 함수 : $\pi(\theta) \sim N(\mu,\tau^{2})$

사후 확률밀도함수 :
$\pi\left( \theta \middle| \overline{x} \right) \sim N(\frac{\tau^{2}}{\tau^{2} + \sigma^{2}}\overline{x} + \frac{\sigma^{2}}{\tau^{2} + \sigma^{2}}\mu,\frac{\sigma^{2}\tau^{2}}{\tau^{2} + \sigma^{2}})$

제곱 오차 손실함수 최소화 베이즈 추정량 :
$\overset{\hat{}}{\theta} = \frac{\tau^{2}}{\tau^{2} + \sigma^{2}}\overline{x} + \frac{\sigma^{2}}{\tau^{2} + \sigma^{2}}\mu$.

3추정량 평가

앞 절에서는 모수의 점추정량을 찾기 위한 합리적인 방법들을 소개하였다.
하지만 문제는, 특정 상황에서는 여러 방법을 적용할 수 있다는 점이다.
따라서 여러 추정량 후보 중에서 어떤 추정량을 선택할지 결정해야 하는
과제가 생긴다.

점추정은 과녁에 화살을 쏘는 것과 같다. 모집단으로부터 확률표본을 얻고
이로부터 모수에 대한 예측 값(추정치)을 얻는다. 즉 과녁에 한 발의 화살을
쏘는 것과 같다. 과연 bull-eye일까? 좋은 추정치라면 한 번에 bull-eye일까?

화살을 한 발 쏘아![라인, 도표이(가) 표시된 사진 자동 생성된
설명](media/image2.png){width="2.754861111111111in"
height="1.6868055555555554in"} bull-eye를 했다고 하자. 명궁이라 할 수
있나? 아닐 것이다. 2발, 아니 20발쯤 연속 명중시킨다면 명궁이라 할 수
있을 것이다. 이처럼 한 번의 추정치로는 그 추정치가 좋은지를 판단할 수는
없다. 추정치 good 여부를 판단하려면 추정치를 여러 번 구해야 한다. 즉
추정치의 평균과 분산이 필요하게 되는 것이다.

목표 모수 $\theta$에 대한 추정량 $\overset{\hat{}}{\theta}$을 여러 번
얻는다면($\overset{\hat{}}{\theta}$ 확률밀도함수,
$f(\overset{\hat{}}{\theta})$,샘플링 분포도 얻을 수 있음) 그 추정량은
모수 $\theta$을 중심으로 흩어져 있을 것이다. 모수 부근에 있을 가능성은
높고 멀어질수록 가능성은 떨어질 것이다.

평균제곱오차

【MSE】 추정량 $W$와 모수 $\theta$에 대해, 평균제곱오차(Mean Squared
Error, MSE)는 $MSE(W) = E_{\theta}(W - \theta)^{2}$로 정의된다.

정의

평균 절대오차 $E_{\theta}(|W - \theta|)$)도 점추정량 성능 척도의 대안이
될 수 있으나, MSE는 다음과 같은 두 가지 강점을 가진다.

수학적으로 다루기 쉬움

분산과 편향이라는 명확한 해석 가능

$$MSE = E_{\theta}(W - \theta)^{2} = {Var}_{\theta}(W) + (E_{\theta}W - \theta)^{2}$$

${Var}_{\theta}(W)$: 추정량 $W$의 분산(추정분산) - 추정량의 변동성

$(E_{\theta}W - \theta)^{2}$: 추정량 $W$의 편향 bias의 제곱 - 추정량이
모수에 얼마나 가까운지

【편향】 추정량 $W$ 의 편향은
${Bias}_{\theta}(W) = E_{\theta}W - \theta$

정의

으로 정의된다.

만약 ${Bias}_{\theta}(W) = 0$이면, 추정량 W는 불편 unbiased 추정량이라
한다. 이는 모든 $\theta$에 대해 $E_{\theta}W = \theta$를 만족한다는
뜻이다. 불편 추정량인 경우 $MSE_{\theta}(W) = V_{\theta}(W)$이다.

【예제 ①】 $f(x;\theta) \sim N(\mu,\sigma^{2})$에서 표본크기 $n$인
확률표본을 추출하였다. MLE 추정량 $\overline{X},S^{2}$이 불편 추정량
인지 보이고 MSE 구하라.

$$E(\overline{X}) = \mu,E(S^{2}) = \sigma^{2}$$

$$MSE(\overline{X}) = E(\overline{X} - \mu)^{2} = Var(\overline{X}) = \frac{\sigma^{2}}{n}$$

$$MSE(S^{2}) = E(S^{2} - \sigma^{2})^{2} = Var(S^{2}) = \frac{2\sigma^{4}}{n - 1}$$

【예제 ②】 $f(x;\theta) \sim U(\theta,\theta + 1)$에서 표본크기 $n$인
확률표본을 추출하였다. 추정량 $\overset{¯}{X}$가 편의 추정량 임을 보이고
MSE 구하라.

$E(X) = \frac{2\theta + 1}{2},V(X) = \frac{1}{12}$.

편의
$B\left( \overline{x} \right) = \frac{2\theta + 1}{2} - \theta = \frac{1}{2}$.

추정분산 $V\left( \overline{x} \right) = \frac{1}{12n}$ 이므로
$MSE\left( \overline{x} \right) = \frac{1}{12n} + \frac{1}{4}$이다.

【예제 ③】 $f(x;\theta) = \frac{1}{\theta}e^{- x/\theta},0 < x$ 에서
표본크기 3인 확률표본 $(X_{1},X_{2},X_{3})$ 추출하였다. 4개 추정량 중
MSE가 가장 작은 것은?

$$(1){\overset{\hat{}}{\theta}}_{1} = X_{1}(2){\overset{\hat{}}{\theta}}_{2} = \frac{X_{1} + X_{2}}{2}(3){\overset{\hat{}}{\theta}}_{3} = \frac{X_{1} + 2X_{2}}{3}(4){\overset{\hat{}}{\theta}}_{4} = \frac{X_{1} + X_{2} + X_{3}}{3}$$

$E(X) = \theta,V(X) = \theta^{2}$.

$E\left( {\overset{\hat{}}{\theta}}_{1} \right) = \theta,E\left( {\overset{\hat{}}{\theta}}_{2} \right) = \theta,E\left( {\overset{\hat{}}{\theta}}_{3} \right) = \theta,E\left( {\overset{\hat{}}{\theta}}_{4} \right) = \theta$
이므로 모두 불편 추정량 이다.

$MSE\left( {\overset{\hat{}}{\theta}}_{1} \right) = V\left( {\overset{\hat{}}{\theta}}_{1} \right) = \theta^{2}$,
$MSE\left( {\overset{\hat{}}{\theta}}_{2} \right) = \frac{\theta^{2}}{2}$,
$MSE\left( {\overset{\hat{}}{\theta}}_{3} \right) = \frac{5\theta^{2}}{9}$,
$MSE\left( {\overset{\hat{}}{\theta}}_{4} \right) = \frac{\theta^{2}}{3}$
4번째 추정량 MSE가 최소

【예제 ④】 $f(x;p) = p^{x}(1 - p)^{1 - x},x = 0,1$ 베르누이 분포에서
표본크기 n인 확률표본을 추출하였다. MLE 추정량과 베이지 추정량의 MLE을
구하시오.

모집단 평균 및 분산: $E(X) = p,V(X) = p(1 - p)$

MLE : $\widehat{p} = \frac{\sum X_{i}}{n}$,
$MSE = E_{p}(\widehat{p} - p)^{2} = {Var}_{p}(\overline{X}) = \frac{p(1 - p)}{n}$

베이지 추정량 :
${\widehat{p}}_{B} = \frac{Y + \alpha}{\alpha + \beta + n}$

$$E_{p}({\widehat{p}}_{B} - p)^{2} = {Var}_{p}({\widehat{p}}_{B}) + ({Bias}_{p}({\widehat{p}}_{B}))^{2}$$

$$= {Var}_{p}\left( \frac{Y + \alpha}{\alpha + \beta + n} \right) + \left( E_{p}\left( \frac{Y + \alpha}{\alpha + \beta + n} \right) - p \right)^{2} = \frac{np(1 - p)}{(\alpha + \beta + n)^{2}} + \left( \frac{np + \alpha}{\alpha + \beta + n} - p \right)^{2}$$

상대효율

앞에서 살펴보았듯이 모수 $\theta$에 대한 불편 추정량은 무수히 많이
존재한다. 두 불편 추정량
${\overset{\hat{}}{\theta}}_{1},{\overset{\hat{}}{\theta}}_{2}$을 생각해
보자. 만약
${V(\overset{\hat{}}{\theta}}_{1}) \leq V({\overset{\hat{}}{\theta}}_{2})$라면
${\overset{\hat{}}{\theta}}_{1}$은 ${\overset{\hat{}}{\theta}}_{2}$에
비해 상대적으로 효율적 efficient 이라고 정의한다.

【상대효율】
${eff(\overset{\hat{}}{\theta}}_{1},{\overset{\hat{}}{\theta}}_{2}) = \frac{{V(\overset{\hat{}}{\theta}}_{2})}{V({\overset{\hat{}}{\theta}}_{1})}$
: 추정량 ${\overset{\hat{}}{\theta}}_{2}$에 대한
${\overset{\hat{}}{\theta}}_{1}$의 상대효율이라 한다.

정의

${\overset{\hat{}}{\theta}}_{1},{\overset{\hat{}}{\theta}}_{2}$가 불편
추정량이면 추정 분산과 추정 평균제곱오차은 동일하므로 추정 분산이 적은
추정량이 (즉 효율적인 추정량) 좋은 추정량이다.

【예제 ①】 $f(x;\theta) \sim U(0,\theta)$에서 표본크기 $n$인 확률표본을
추출하였다.
$({\overset{\hat{}}{\theta}}_{1} = 2\overline{X},{\overset{\hat{}}{\theta}}_{2} = \frac{n + 1}{n}X_{(n)})$
추정량이 불편 추정량임을 보이고 상대효율을 구하라.

$E(X) = \frac{\theta}{2},V(\theta) = \frac{\theta^{2}}{12}$.
$E\left( {\overset{\hat{}}{\theta}}_{1} \right) = E\left( 2\overline{X} \right) = 2\frac{\theta}{2} = \theta$
불편 추정량이다.
$V\left( {\overset{\hat{}}{\theta}}_{1} \right) = V\left( 2\overline{X} \right) = 4\frac{\theta^{2}}{12n} = \frac{\theta^{2}}{3n}$.

순서 통계량 $Y = X_{(n)}$ 확률밀도함수 :
$f(y) = n\left( \frac{y}{\theta} \right)^{n - 1}\left( \frac{1}{\theta} \right),0 < y < \theta$.

$E(Y) = \frac{n}{n + 1}\theta,V(Y) = \left( \frac{n}{n + 2} - \left( \frac{n}{n + 2} \right)^{2} \right)\theta^{2}$.

$E\left( {\overset{\hat{}}{\theta}}_{2} \right) = E\left( \frac{n + 1}{n}X_{(n)} \right) = \frac{n + 1}{n}\frac{n}{n + 1}\theta = \theta$
불편 추정량이다. .
$V\left( {\overset{\hat{}}{\theta}}_{2} \right) = \frac{1}{n(n + 2)}\theta^{2}$.

${eff(\overset{\hat{}}{\theta}}_{1},{\overset{\hat{}}{\theta}}_{2}) = \frac{{V(\overset{\hat{}}{\theta}}_{2})}{V({\overset{\hat{}}{\theta}}_{1})} = \frac{3}{n + 2}$
이므로 $n \geq 1$ 이면 ${\overset{\hat{}}{\theta}}_{1}$이
${\overset{\hat{}}{\theta}}_{2}$에 비해 상대적으로 효율적이다.

【예제 ②】 $f(x;\theta) = \frac{1}{\theta}e^{- x/\theta},0 < x$에서
표본크기 $n$인 확률표본 $(X_{1},X_{2},\ldots,X_{n})$ 추출하였다.
$({\overset{\hat{}}{\theta}}_{1} = \frac{X_{1} + X_{2}}{2},{\overset{\hat{}}{\theta}}_{2} = {\overline{X}}_{n})$
불편 추정량임을 보이고 상대효율을 구하라.

$E(X) = \theta,V(\theta) = \theta^{2}$.

$E\left( {\overset{\hat{}}{\theta}}_{1} \right) = E\left( \frac{X_{1} + X_{2}}{2} \right) = \theta$
불편 추정량이다.
$V\left( {\overset{\hat{}}{\theta}}_{1} \right) = V\left( \frac{X_{1} + X_{2}}{2} \right) = \frac{\theta^{2}}{2}$.

$E\left( {\overset{\hat{}}{\theta}}_{2} \right) = E\left( \frac{X_{1} + X_{2} + \ldots + X_{n}}{n} \right) = \theta$
불편 추정량이다.
$V\left( {\overset{\hat{}}{\theta}}_{1} \right) = V\left( {\overline{X}}_{n} \right) = \frac{\theta^{2}}{n}$.

${eff(\overset{\hat{}}{\theta}}_{1},{\overset{\hat{}}{\theta}}_{2}) = \frac{{V(\overset{\hat{}}{\theta}}_{2})}{V({\overset{\hat{}}{\theta}}_{1})} = \frac{n}{2}$
이므로 $n \geq 2$ 이면 ${\overset{\hat{}}{\theta}}_{2}$이
${\overset{\hat{}}{\theta}}_{1}$에 비해 상대적으로 효율적이다.

최량 불편 추정량

최소 MSE을 갖는 추정량을 최량 추정량으로 정의하였는데 실제 MSE을 최소화
하는 추정량을 구하는 것은 쉽지 않거나(수학적 접근 매우 어려움) 실제
["]{dir="rtl"}최고의 MSE 추정량"은 존재하지 않는다. 이는 후보 추정량의
범위가 너무 넓기 때문인데, $\widehat{\theta} = 17$은 $\theta = 17$일 때
MSE가 최솟값이지만 다른 값에서는 매우 나쁜 추정량이다.

불편추정량으로 범위를 제한하면 후보 추정량의 범위를 불편추정량으로
제한한다. MSE 비교는 단순히 분산 비교로 귀결되므로 추정분산이 더 작은
불편추정량을 선택하면 된다.

추정량 $W^{*}$가 다음 조건을 만족하면 $\tau(\theta)$에 대한 최량
불편추정량 best unbiased estimator 이라 한다.

정의

모든 $\theta$에 대해 $E_{\theta}W^{*} = \tau(\theta)$

임의의 다른 추정량 $W$에 대해
$Var\theta(W^{*}) \leq Var\theta(W)\text{for all}\theta$

【예제 ①】 $f(x;\theta) \sim B(n,p)$에서 표본크기 $n$인 확률표본을
추출하였다. 다음 2개 추정량에 대하여 (1) 불편 추정량인지 보이고 (2)
MSE을 비교하라.
$(1){\overset{\hat{}}{p}}_{1} = \frac{\sum X_{i}}{n}(2){\overset{\hat{}}{p}}_{2} = \frac{\sum X_{i} + 1}{n + 2}$.

$E(X) = p,V(X) = p(1 - p)$.

$E\left( \frac{\sum X_{i}}{n} \right) = p$ 이므로
${\overset{\hat{}}{p}}_{1}$는 불편 추정량 이다.

$E\left( \frac{\sum X_{i} + 1}{n + 2} \right) = \frac{p + 1}{n + 2}$
이므로 ${\overset{\hat{}}{p}}_{2}$는 불편 추정량 아니다.
$B\left( {\overset{\hat{}}{p}}_{2} \right) = \frac{1 - np - p}{n + 2}$.

$$MSE\left( {\overset{\hat{}}{p}}_{1} \right) = MSE\left( \frac{\sum X_{i}}{n} \right) = V\left( \frac{\sum X_{i}}{n} \right) = \frac{p(1 - p)}{n}$$

$$MSE\left( {\overset{\hat{}}{p}}_{2} \right) = MSE\left( \frac{\sum X_{i} + 1}{n + 2} \right) = V\left( \frac{\sum X_{i} + 1}{n + 2} \right) + B^{2}\left( \frac{\sum X_{i} + 1}{n + 2} \right) = \frac{np(1 - p)}{(n + 2)^{2}} + \frac{(1 - np - p)^{2}}{(n + 2)^{2}}$$

그러므로
$MSE\left( {\overset{\hat{}}{p}}_{1} \right) > MSE\left( {\overset{\hat{}}{p}}_{2} \right)for0 < p < 1$.

【예제 ②】 $f(x;\theta) \sim Poisson(\lambda)$에서 표본크기 $n$인
확률표본을 추출하였다. 포아송 분포는 평균, 분산이 동일하므로
표본평균($\overline{x}$), 표본분산($S^{2}$) 모두 불편 추정량이다. 어느
추정량을 사용할 것인가? 추정 분산이 적은 통계량을 사용해야 한다.

표본평균 추정분산 :
$V\left( \overset{¯}{x} \right) = \frac{\lambda}{n}$.

표본분산 추정분산 :
$\frac{(n - 1)S^{2}}{\sigma^{2}( = \lambda)} \sim \chi^{2}(n - 1)$
이므로$V(S^{2}) = \frac{2\lambda^{2}}{n - 1}$이다.
$V(\overline{X}) \leq V(S^{2})$

최량 불편추정량을 찾는 과정은 매우 복잡하다. 만약 어떤 분포
$f(x|\theta)$에 대해 모수 $\tau(\theta)$의 불편추정량의 분산에 대한 하한
$B(\theta)$를 설정할 수 있다면,${Var}_{\theta}(W) = B(\theta)$를
만족하는 추정량을 찾으면 최량 불편추정량을 찾은 것이 된다.

【Cramér--Rao Lower Bound, CRLB】 확률밀도함수 $f(x|\theta)$의
확률표본으로부터의 추정량 $W(\mathbf{X}) = W(X_{1},\ldots,X_{n})$는
다음을 만족한다면,
${Var}_{\theta}(W(\mathbf{X})) \geq \frac{\left( \frac{d}{d\theta}E_{\theta}W(\mathbf{X}) \right)^{2}}{nE_{\theta}\left( \left( \frac{\partial}{\partial\theta}\log f(\mathbf{X}|\theta) \right)^{2} \right)}$

정리

1)  $$\frac{d}{d\theta}E_{\theta}W(\mathbf{X}) = \int_{x}\frac{\partial}{\partial\theta}\lbrack W(x)f(x|\theta)\rbrack dx$$

2)  $${Var}_{\theta}(W(\mathbf{X})) < \infty$$

【Fisher Information】 확률밀도함수 $f(x|\theta)$가 지수족을 따르다면
$E_{\theta}\left( \left( \frac{\partial}{\partial\theta}\log f(X|\theta) \right)^{2} \right) = - E_{\theta}\left( \frac{\partial^{2}}{\partial\theta^{2}}\log f(X|\theta) \right)$

따름정리

【예제 ② 계속】 $f(x;\theta) \sim Poisson(\lambda)$에서 표본크기 $n$인
확률표본을 추출하였다. 모수 $\lambda$에 대한 추정량의 분산 그레머 라오
하한을 구하시오.

포아송분포는 지수족이므로 Fisher Information은 다음과 같다.

$$E_{\lambda}\left( \left( \frac{\partial}{\partial\lambda}\log\overset{n}{\prod_{i = 1}}f(X_{i}|\lambda) \right)^{2} \right) = - nE_{\lambda}\left( \frac{\partial^{2}}{\partial\lambda^{2}}\log f(X|\lambda) \right)$$

$$= - nE_{\lambda}\left( \frac{\partial^{2}}{\partial\lambda^{2}}\log\left( \frac{e^{- \lambda}\lambda^{X}}{X!} \right) \right) = - nE_{\lambda}\left( \frac{\partial^{2}}{\partial\lambda^{2}}\left( - \lambda + X\log\lambda - \log X! \right) \right) = \frac{n}{\lambda}$$

$V_{\lambda}(\overline{X}) = \frac{\lambda}{n}$이므로 표본평균이 크레머
라오 하한을 보장한다.

【예제 ③】 $f(x;\theta) \sim N(\mu,\sigma^{2})$에서 표본크기 $n$인
확률표본을 추출하였다. 모수 $\sigma^{2}$에 대한 추정량의 분산 그레머
라오 하한을 구하시오.

정규분포는 지수족이므로

$$\frac{\partial^{2}}{\partial(\sigma^{2})^{2}}\log\left( \frac{1}{(2\pi\sigma^{2})^{\frac{1}{2}}}e^{- (1/2)(x - \mu)^{2}/\sigma^{2}} \right) = \frac{1}{2\sigma^{4}} - \frac{(x - \mu)^{2}}{\sigma^{6}}$$

$$- E\left( \frac{\partial^{2}}{\partial(\sigma^{2})^{2}}\log f(X|\mu,\sigma^{2}) \right) = - E\left( \frac{1}{2\sigma^{4}} - \frac{(X - \mu)^{2}}{\sigma^{6}} \right) = \frac{1}{2\sigma^{4}}$$

표본분산의 추정분산은
$Var(S^{2} \mid \mu,\sigma^{2}) = \frac{2\sigma^{4}}{n - 1}$이므로
표본분산은 그레머 라오 하한을 만족하지 못한다.

4Rao balckwell 정리 & MVUE

【MVUE 정의】 $f(x;\theta)$ 에서 표본크기 $n$인 확률표본
$(X_{1},X_{2},\ldots,X_{n})$ 추출하였고 $T(\overline{x})$은 모수
$\theta$의 충분 통계량이다. 만약 $T(\overline{x})$ 불편 추정량이고 다른
불편 추정량의 추정 분산보다 적은 추정 분산을 가진다면
$T(\overline{x})$를 최소분산 불편 추정량 minimum variance unbiased
estimator 이라 한다.

정의

충분 통계량은 모수에 대한 좋은 추정량을 발견하는데 주요 역할을 한다.
추정량 $\widehat{\theta}$을 모수 $\theta$의 불편 추정량, 통계량 $U$을
모수 $\theta$에 대한 충분 통계량이라 하자. 불편 추정량인 충분 통계량
함수는 불편 추정량 중 최소 분산을 갖는다. 만약 최소 분산을 갖는 불편
추정량을 찾는 것은 충분 통계량의 함수인 추정량에 한정하며 된다. 이에
관련된 이론이 Rao-Blackwell 정리라 한다.

【rao-blackwell theorem】 추정량  $\overset{\hat{}}{\theta}$는 모수
$\theta$의 불편 추정량이고 추정 분산을 $V(\overset{\hat{}}{\theta})$이라
하자. 만약 통계량 $U$을 모수 $\theta$에 대한 충분 통계량이라 하면
$E(\overset{\hat{}}{\theta}|U)$은 불편 추정량이고 불편 추정량 중 최소
분산을 갖는다.

정리

$X_{1},X_{2}$ 확률변수에 대하여

\(1\) $E\left( X_{2} \right) = E(E\left( X_{2}|X_{1} \right))$ (2)
$V\left( X_{2} \right) \geq V(E\left( X_{2}|X_{1} \right))$

$X_{1}$=모수 $\theta$ 충분 통계량 $U$, $X_{2}$=모수 $\theta$ 불편 통계량
$\overset{\hat{}}{\theta}$이라 하자.

$E\left( \overset{\hat{}}{\theta} \right) = E(E\left( \overset{\hat{}}{\theta}|U \right))$
이므로 $E(\overset{\hat{}}{\theta}|U)$ 불편 추정량이다.

$V\left( \overset{\hat{}}{\theta} \right) \geq V(E(\overset{\hat{}}{\theta}|U))$
이므로 불편 추정량이면서 이전보다 추정분산이 적은 추정량을 얻는다.

R-B 정리는 최소분산을 갖는 불편 추정량은 충분 통계량으로 만들어질 수
있다. 만약 우리가 불편 추정량을 갖고 있다면 R-B 정리를 이용하여 이 불편
추정량을 향상 시킬 수 있다. 이렇게 얻는 추정량에 R-B 정리를 반복
적용하면 된다. 그러나 만약 동일한 충분 통계량을 사용한다면 더 이상
나아지는 것도 없다.

${\overset{\hat{}}{\theta}}^{*} = E(\overset{\hat{}}{\theta}|U)$을 새로
얻은 불편 추정량이라 하자.
$E\left( {\overset{\hat{}}{\theta}}^{*} \middle| U \right) = {\overset{\hat{}}{\theta}}^{*}$
이므로 충분 통계량은 수없이 많다. 그럼 어떤 충분 통계량을 시작점으로
하여 R-B 정리에 사용될까? Factorization criterion이 가장 좋은 충분
통계량을 얻게 한다. 가장 좋은 통계량이란 데이터(확률표본)에 있는 모수에
대한 정보를 가장 잘(best) 요약한 것을 의미하며 이를 Minimal 충분
통계량이라 한다.

$f(x;\theta)$ 에서 표본크기 $n$인 확률표본 $(X_{1},X_{2},\ldots,X_{n})$
추출하였고 $T(\overline{x})$을 모수 $\theta$의 충분 통계량이라 하자. 또
다른 확률표본 $(Y_{1},Y_{2},\ldots,Y_{n})$에 대하여
$\frac{L(x_{1},x_{2},\ldots,x_{n};\theta)}{L(y_{1},y_{2},\ldots,y_{n};\theta)}$가
모수 $\theta$의 함수가 성립한다. ⬄ (필요 충분 조건)
$T\left( \overline{x} \right) = T(\overline{y})$. 그리고
$T(\overline{x})$을 최소 minimal 충분 통계량이라 한다.

정리

일반적으로 Factorization criterion에서 얻은 충분 통계량과 Minimal 충분
통계량은 같다. 이런 통계량이 갖는 성질을 Completeness(완비성)라 한다.

【ancillary statistics】 모수 $\theta$에 의존하지 않는 통계량
$S(\overline{x})$을 보조 ancillary 통계량이라 한다.확률 분포의 모수에
관련된 정보가 아닌 추가적인 정보를 제공하는 통계량을 나타내고 모수
추정이나 가설 검정과 같은 통계적 추론에서 사용되는데, 주로 추정된
모수들의 분포나 특성을 이해하고 분석하는 데 활용된다.

정의

【예제】 $f(x;\theta) \sim U(\theta,\theta + 1)$에서 표본크기 $n$인
확률표본 $(X_{1},X_{2},\ldots,X_{n})$ 추출하였다. 통계량
$R = x_{(n)} - x_{(1)}$의 확률밀도함수가 모수 $\theta$에 의존하지
않으므로 보조 통계량이다.

【예제】 $f(x;\theta) \sim B(\theta = p)$에서 표본크기 $n$인 확률표본
$(X_{1},X_{2},\ldots,X_{n})$ 추출하였다. 충분 통계량 $\sum X_{i}$의
확률밀도함수는 $B(np,np(1 - p))$로 모수 $\theta = p$에 의존하므로 보조
통계량은 아니다. 그러나 $\frac{\sum X_{i} - np}{\sqrt{np(1 - p)}}$
확률밀도함수는 표준정규분포($N(0,1)$)에 근사하므로 모수에 의존하지 않아
보조 통계량이다. 다음 장에서 이를 검정 통계량이라 한다.

【완비성 completeness 】 충분 통계량
$T\left( \overline{x} \right) \sim f(t;\theta)$을 갖는다고 하자. 만약
$E_{\theta}\left( g(T) \right) = 0forall\theta$가
$P_{\theta}\left( g(T) = 0 \right) = 1forall\theta$을 포함하면
$T\left( \overline{x} \right)$는 완비 통계량이다.

정의

【예제】 $f(x;\theta) \sim B(\theta = p)$에서 표본크기 $n$인 확률표본을
추출하였다. 충분 통계량 $\sum X_{i}$은 완비 통계량임을 보이시오.

$T = \sum X_{i} \sim B(n,p)$ 이다.

$0 = E_{p}\left( g(T) \right) = \sum_{t}^{n}{g(t)\binom{n}{t}p^{t}(1 - p)^{n - t} = (1 - p)^{n}}\sum_{t}^{n}{g(t)\binom{n}{t}{(\frac{p}{1 - p})}^{t}forall0 < p < 1}$.

$0 = \sum_{t}^{n}{g(t)\binom{n}{t}{(\frac{p}{1 - p})}^{t}}$ 이 조건이
만족하기 위해서는 $P_{\theta}\left( g(T) = 0 \right) = 1$ 이므로 완비
통계량이다.

【예제】 $f(x;\theta) \sim U(0,\theta)$에서 표본크기 $n$인 확률표본
$(X_{1},X_{2},\ldots,X_{n})$ 추출하였다. 충분 통계량 ${T = x}_{(n)}$은
완비 통계량이다.

$T \sim f(t;\theta) = nt^{n - 1}\theta^{- n},0 < t < \theta$ 이다.

$E_{\theta}\left( g(T) \right)$은 모수 $\theta$의 함수이고 상수이므로
$E_{p}\left( g(T) \right) = 0$을 보이는 것은
${\frac{\partial}{\partial\theta}E}_{\theta}\left( g(T) \right) = 0$을
보이는 것은 동일하다.

$${0 = \frac{\partial}{\partial\theta}E}_{\theta}\left( g(T) \right) = \frac{\partial}{\partial\theta}\int_{0}^{\theta}{g(t)}nt^{n - 1}\theta^{- n}dt = \theta^{- 1}ng(\theta)$$

$\theta^{- 1}n \neq 0$ 이므로 $g(\theta) = 0$이어야 한다. 그러므로
${T = x}_{(n)}$ 완비 통계량이다.

【basu theorem】 최소 충분 통계량이고 완비 통계량
$T\left( \overline{x} \right)$는 다른 모든 보조 통계량과 독립이다.

정리

완비 통계량 $T\left( \overline{x} \right)$는 최소 충분 통계량이다.

정리

⑴ Factorization에 의해 충분 통계량 $U$을 구하고 (2) $U$ 확률밀도함수의
완비성을 보이고 (3) 완비 충분 통계량 $U$의 함수로 된 불편 추정량을
얻으면 이것이 Rao-Blackwell 정리에 의하여 MVUE가 된다.

【lehmann and scheffe theorem】 $f(x;\theta),\theta \in \Omega$ 에서
표본크기 $n$인 확률표본을 추출하였고 $T(x_{1},x_{2},\ldots,x_{n})$은
모수 $\theta$의 충분 통계량이라 하자. $T(\overline{x})$의 확률밀도함수
$f(t;\theta)$가 완비성을 갖는다면 불편성을 갖는 $T(\overline{x})$ 함수,
$g(T\left( \overline{x} \right),E(g\left( T\left( \overline{x} \right) \right) = \theta)$는
유일 최소분산불편 추정량(MVUE)이다.

정리

$f(x;\theta)$ 에서 표본크기 $n$인 확률표본을 추출하였고
$T(\overline{x})$을 모수 $\theta$의 충분 통계량, 그리고
${\overset{\hat{}}{\theta}}_{mle}$는 MLE 추정량이라 하자.
${\overset{\hat{}}{\theta}}_{mle}$는 충분 통계량, $T(\overline{x})$의
함수이다.

정리

충분 통계량의 완비성을 증명하는 것은 쉽지 않다. 단 지수족 모집단으로부터
확률표본의 통계량 $T = \sum K(X_{i})$는 모수 $\theta$의 완비 충분
통계량이다.

지수족 확률밀도함수를 갖는 경우 $c(\theta)$의 최소 충분 통계량은
$\sum K(x)$이다.

정리

지수족 확률밀도함수를 다음과 같이 쓸 수 있다.

> $$f(x;\theta) = h(x)g(\theta)\exp{\left( c(\theta)K(x) \right) \Longleftrightarrow}exp(c(\theta)K(x) + h(x) + g(\theta))$$

$h(x),K(x)$ : 확률변수 $x$의 함수, $g(\theta),c(\theta)$ : 모수
$\theta$의 함수

【예제】 $f(x;\theta) = \frac{1}{\theta},0 < x < \theta$에서 표본크기
$n$인 확률표본을 추출하였다. MVUE 구하라.

1)  ${T = X}_{(n)} \sim f(t;\theta) = \frac{nt^{n - 1}}{\theta^{n}},0 < t < \theta$는
    충분 통계량

<!-- -->

1)  $$E_{\theta}\left( g(T) \right) = \int_{0}^{\theta}{g(t)\frac{nt^{n - 1}}{\theta^{n}}dt} = (\theta > 0,n \geq 1)\int_{0}^{\theta}{g(t)t^{n - 1}dt} = 0$$

$0 = g(\theta)\theta^{n - 1}$을 만족하려면 $g(\theta) = 0$이어야 하므로
${T = X}_{(n)}$ 완비 통계량이다.

3)  $E(T) = \int_{0}^{\theta}{t\frac{nt^{n - 1}}{\theta^{n}}dt = \frac{n}{n + 1}\theta}$
    이므로 $\frac{n + 1}{n}X_{(n)}$은 MVUE

【예제】 $f(x;\theta) = B(\theta = p)$에서 표본크기 $n$인 확률표본을
추출하였다. MVUE 구하라.

$f(x;\theta = p) = p^{x}(1 - p)^{1 - x} = 1(1 - p)exp(ln(\frac{p}{1 - p})x)$
이므로 지수족이고 $K\left( x_{i} \right) = x_{i}$. 그러므로 $\sum x_{i}$
완비 충분 통계량이고 $\sum x_{i} \sim B(n,p)$ 이다.
$E\left( \sum x_{i} \right) = np$ 이므로
$\overline{X} = \frac{\sum x_{i}}{n}$는 MVUE이다.

【예제】
$f(x;\theta = \lambda) = e^{- \lambda}\frac{\lambda^{x}}{x!}$에서
표본크기 $n$인 확률표본을 추출하였다. MVUE 구하라.

$f(x;\lambda) = {\frac{1}{x!}e}^{- \lambda}(ln(\lambda)x)$ 이므로
지수족이고 $K\left( x_{i} \right) = x_{i}$.

그러므로 $\sum x_{i}$ 완비 충분 통계량이고 $\sum x_{i} \sim P(n\lambda)$
이다.

$E\left( \sum x_{i} \right) = n\lambda$ 이므로
$\overline{X} = \frac{\sum x_{i}}{n}$는 MVUE이다.

【예제】
$f(x;\theta) = \left( \frac{2x}{\theta} \right)e^{- x^{2}/\theta} \sim Weibull(\gamma = 2,\theta)$에서
표본크기 $n$인 확률표본을 추출하였다. MVUE 구하라.

$f(x;\theta) = 2x\left( \frac{1}{\theta} \right)exp( - x^{2}/\theta)$
이므로 지수족이고 $K\left( x_{i} \right) = x_{i}^{2}$. 그러므로
$\sum x_{i}^{2}$ 완비 충분 통계량이다.

변수 변환 방법 $W = X^{2},X = \sqrt{W},J = \frac{1}{2\sqrt{w}}$ 이므로
$W \sim exponential(\theta)$이다.

$E\left( \sum x_{i}^{2} \right) = n\theta$ 이므로
$\frac{\sum x_{i}^{2}}{n}$ 은 MVUE이다.

【예제】
$f(x;\theta = \mu) = N\left( \mu,\sigma^{2} \right),where\sigma^{2}isknown$에서
표본크기 $n$인 확률표본을 추출하였다. 모수 $\theta = \mu$MVUE 구하라.

$f(x;\theta) = \frac{1}{\sqrt{2\pi}\sigma}e^{( - \mu/2\sigma^{2})}exp(\frac{\mu}{\sigma^{2}}x - \frac{x^{2}}{2\sigma^{2}})$
이므로 지수족이고 $K\left( x_{i} \right) = x_{i}$.
$E\left( \sum x_{i} \right) = n\mu$ 이므로
$\overline{X} = \frac{\sum x_{i}}{n}$는 MVUE이다.

추정량 $\overset{\hat{}}{\theta}$은 모수 $\theta$에 대한 MVUE이고
$g(.)$은 일대일 함수이면 $g(\theta)$의 MVUE는
$g(\overset{\hat{}}{\theta})$ 중 불편성을 갖는 추정량이다.

정리

【예제】 $f(x;\theta) = B(\theta = p)$에서 표본크기 $n$인 확률표본을
추출하였다. $\frac{p(1 - p)}{n}$에 대한 MVUE 구하라.

모수 $\theta = p$에 대한 MVUE는 $\overline{X} = \frac{\sum x_{i}}{n}$
임을 보였다.

그러므로 $\frac{p(1 - \theta p)}{n}$에 대한 MVUE을 구하기 위하여 분포를
알고 있는 $Y = \sum x_{i} \sim B(n,p)$ 함수를 이용하는 것이 적절하다.
$Y$는 완비 충분 통계량이므로 $Y(1 - Y)$도 완비 충분 통계량이다.

$E\left( Y(1 - Y) \right) = E(Y) - E\left( Y^{2} \right) = E(Y) - \left( V(X) + E(Y)^{2} \right) = np - np(1 - p) - n^{2}p^{2}$

$= (n - 1)p(1 - p)$ 이므로 $\frac{\theta(1 - \theta)}{n}$ 의 MVUE는
$\frac{Y(1 - Y)}{n(n - 1)}$

【예제】
$f(x;\theta) = \left( \frac{1}{\theta} \right)e^{- x/\theta} \sim exponential(\theta)$에서
표본크기 $n$인 확률표본을 추출하였다. $V(X) = \theta^{2}$ MVUE 구하라.

지수분포는 지수족이고 $K\left( x_{i} \right) = x_{i}$이므로$\sum x_{i}$
완비 충분 통계량이므로 $\overline{X}$는 모수 $\theta$의 MVUE이다.
그러므로 $V(X) = \theta^{2}$의 MVUE을 ${\overline{X}}^{2}$의 함수 중
불편 추정량을 찾으면
된다.$E\left( {\overline{X}}^{2} \right) = V\left( \overline{X} \right) + E\left( \overline{X} \right)^{2} = \frac{\theta^{2}}{n} + \theta^{2} = \frac{n + 1}{n}\theta^{2}$
이므로 $\frac{n\overline{X}}{n + 1}$ 은 MVUE이다.

Chapter 3. 가설검정

1기초

개념

추론 통계학 inference statistics 은 모집단의 특성치인 모수 $\theta$에
대한 추정치(점 추정치, 구간 추정치)를 구하는 추정 과 모수 값에 대한
가설의 진위 여부(통계적 유의성)를 알아보는 가설 검정 hypothesis
testing이 있다.

가설 검정은 과학적 연구와 유사하다. ⑴자연 현상을 관찰하여 ⑵이론을
정립하고 (임의의 모수 값) ⑶관측치를 통하여 이론 진위 여부를 테스트 한다.
이론의 옳고 그름은 표본 관측치에 의해 판단된다.

가설검정은 무죄 추정의 원칙으로 범인의 유죄여부를 배심원의 결정과정과
동일하다. [']{dir="rtl"}범인은 무죄이다[']{dir="rtl"}를 시작으로
[']{dir="rtl"}검사가 제시한 여러 증거[']{dir="rtl"}(표본 데이터)를
기반하여 범인의 유죄, 무죄를 판결한다. 가설검정은 연역적 추론 과정과
유사하여 1) 자연현상, 사회현상을 관찰하여 이론을 도출하고 2)이를 통계적
가설로 만든 후 3)적절한 (확률표본) 데이터를 수집 분석하여 4)가설의
진위를 검증한다.

통계적 가설

통계적 가설은 모수에 관한 가정이다.
$X \sim f(x;\theta,\theta \in \Omega)$의 모수 $\theta$가 가지는 값에
대한 가정이다. 설정한 통계적 가설은 통계적 방법으로 테스트 할 수 있도록
한 것으로 모수의 값에 관한 것이다.

불량률이 5%인 제조 공정을 개선하기 위한 새로운 공정이 제안되었다. 모수는
모집단 비율 $\theta = p$

기존의 약에 비해 두통에 효과적인 새로운 약을 개발하였다고 하자. 두
모집단 평균 차이 $\theta = \mu_{old} - \mu_{new}$

수능성적과 GPA의 관계? $GPA = a + b \times SAT$, $b = 0$?

모수
$(\theta \in \Omega = \left\{ \theta;\omega_{0} \cup \omega_{1} \right\},\omega_{0} \cap \omega_{1} = \phi)$에
관한 통계적 가설은 다음과 같이 2개의 가설로 나뉜다.

정의

귀무가설 null hypothesis : $H_{0}:\theta \in \omega_{0}$, 집합
$\omega_{0}$에는 하나의 값 $\omega_{0}$만 있다.

대립가설 alternative hypothesis : $H_{1}:\theta \in \omega_{1}$, 집합
$\omega_{1} = \{\theta;\theta \neq \theta_{0}\}$이다.

양측 대립가설 : $\omega_{1} = \{\theta;\theta \neq \theta_{0}\}$

단측 대립가설 : $\omega_{1} = \{\theta;\theta < \theta_{0}\}$,
$\omega_{1} = \{\theta;\theta > \theta_{0}\}$

우리가 지지하는 가설은 연구가설 research hypothesis 혹은 대립가설이라
한다. 대립가설과 반대되는 가설을 귀무가설라 한다. 어떤 가설이
지지되느냐는 관측된 표본 데이터에 의존한다. ["]{dir="rtl"}귀무"(null)의
의미는 아무 것도 없다는 것이다.

검정통계량

검정통계량은 통계적 가설 검정에서 사용되는 중요한 개념으로 확률표본
데이터를 기반으로 계산되며 귀무가설을 평가하는데 사용된다. 검정통계량은
일반적으로 확률표본 데이터의 요약된 값으로 귀무가설이 참일 때 얼마나
불일치하는지를 나타내는 역할을 한다. 검정통계량의 값은 귀무가설의 가정과
확률표본 데이터 간의 차이를 측정하며, 이 값을 기반으로 가설 검정의
결과를 판단한다.

예를 들어, 평균값의 차이에 대한 가설 검정을 수행한다고 가정해보자. 두
그룹의 평균을 비교하려면 먼저 평균의 차이를 계산하고, 이 차이를 그룹
간의 변동을 나타내는 표준 오차로 나누어 표준화된 차이를 얻을 수 있는데.
이 표준화된 차이가 검정통계량이 된다.

검정통계량의 값은 주어진 데이터에 따라 달라지며 이 값을 귀무가설에 대한
기대값과 비교하여 가설 검정의 결과를 결정한다. 일반적으로 검정통계량이
특정한 임계값보다 크거나 작으면 귀무가설을 기각하고 대립가설을 지지하는
결론을 내린다. 이 때 임계값은 유의수준에 따라 결정되며, 유의수준은
연구자가 허용하는 오류의 수준을 나타낸다.

【검정통계량】

$f(x;\theta)$에서 확률표본 $(X_{1},X_{2},\ldots,X_{n})$의 함수인 통계량
$t(x_{1},x_{2},\ldots,x_{n})$이 가설 검정에 사용되면 이를 검정
통계량이라 한다.

【가설 검정 방법】

채택 영역 acceptance region: 어떤 확률표본 값들에 대하여 귀무가설을
참으로 받아들일지 결정

기각 영역 rejection region : 어떤 확률표본 값들에 대하여 귀무가설을
기각하고 대립가설을 참으로 받아들일지 결정

\"귀무가설 기각\"과 \"대립가설 수용\" 사이의 차이를 이해하는 것은 쉽지
않다. 첫 번째 경우에는 실험자가 어떤 상태를 수용하는지에 대한 내용은
함축되지 않으며, 오직 $H_{0}$에 의해 정의된 상태가 기각된다는 것만을
나타낸다. 마찬가지로 \"귀무가설 수용\"과 \"귀무가설 기각하지 않음\"
사이에도 차이가 있을 수 있다. 첫 번째 구문은 실험자가 $H_{0}$로 지정된
상태(단일 모수 값)를 주장하려고 하는 것을 의미하며, 두 번째 구문은
실험자가 실제로 $H_{0}$를 믿지 않지만 기각할 증거가 없다는 것을
의미한다.

기각역

확률표본 $(X_{1},X_{2},\ldots,X_{n})$의 영역을 $\mathcal{D}$라 하자.
$\mathcal{D}$의 부분 집합 $RR$ 에 확률표본 값이 포함되면 귀무가설을
기각한다면 $RR$ 영역을 기각역 critical region 이라 한다.

귀무가설 : $H_{0}:\theta \in \omega_{0} = \{\theta;\theta_{0}\}$ -- 단일
simple 가설

대립가설 :
$H_{1},H_{a}:\theta \in \omega_{1} = \{\theta;{\theta \neq \theta}_{0}\}$
복합 composite 가설

만약 $t\left( x_{1},x_{2},\ldots,x_{n} \right) \in RR$, 귀무가설
기각한다. ⬄ 대립가설 채택한다. 만약
$t\left( x_{1},x_{2},\ldots,x_{n} \right) \in {RR}^{c}$, 귀무가설
채택한다. ⬄ 대립가설![라인, 도표, 디자인이(가) 표시된 사진 자동 생성된
설명](media/image3.png){width="2.934884076990376in"
height="1.0768350831146107in"} 기각한다.

귀무가설을 기각하는 검정통계량의 값이 포함된 영역을 기각역 이라 한다.
만약 확률표본으로부터 계산된 검정통계량의 값이 기각역에 포함되면
귀무가설을 기각(대립가설 채택)하고 그렇지 않으면 귀무가설을 채택한다.
빨간 영역 부분이 기각역이다. 영역이 시작되는 값을 기각값, 임계값
critical value 이라 한다.

1종오류, 2종 오류

【1종 오류, 2종 오류】

1종 오류 : 귀무가설이 사실일 때 귀무가설을 기각할 확률이다. type I
error, $\alpha$ 라고 표기한다.

2종 오류 : 대립가설이 사실일 때 귀무가설을 채택하는 확률이다. type II
error, $\beta$라 표기한다.

즉 $\alpha,\beta$는 통계적 가설 검정의 정확도를 측정하는데 매우
유용하다.

+---------------------:+:---------------------:+:---------------------:+
| 실제 모집단          | 귀무가설($H_{0}$)     | 대립가설($H_{1}$)     |
|                      | 진실                  | 진실                  |
| 가설 판단            |                       |                       |
+----------------------+-----------------------+-----------------------+
| 귀무가설 기각        | 1종 오류 $\alpha$     | 옳은 판단             |
+----------------------+-----------------------+-----------------------+
| 귀무가설 채택        | 옳은 판단             | 2종 오류 $\beta$      |
+----------------------+-----------------------+-----------------------+

가설 검정의 목표는 모든 가능한 기각 영역 중에서 이러한 오류의 확률을
최소화하는 영역을 선택하는 것이지만 일반적으로 이것은 불가능한다. 이러한
오류의 확률은 종종 풍선 효과를 가지고 있는데 이것은 극단적인 경우에 즉시
볼 수 있다.

단순히 $RR = \phi$라 가정해 보자. 이 기각 영역에서는 결코 귀무가설을
기각하지 않을 것이므로 제1종 오류의 확률은 0이 될 것이지만, 제2종 오류의
확률은 1이다. 종종 우리는 두 오류 중에서 제1종 오류를 더 나쁜 오류로
간주한다. 따라서 우리는 제1종 오류의 확률을 제한하는 기각 영역을 선택한
다음 이러한 기각 영역 중에서 제2종 오류의 확률을 최소화하는 하나를
선택하려고 노력하게 된다. 검정 시 설정한 1종 오류(검정 크기)을 유의수준
significance level이라 한다.

【검정 크기】

만약
$\alpha = \max_{\theta \in \omega_{0}}{P_{\theta}(t\left( X_{1},X_{2},\ldots,X_{n} \right) \in RR)}$
이면 기각역의 크기 size는 $\alpha$라 한다. 크기 $\alpha$의 모든 기각
영역을 고려할 때, 우리는 제2종 오류의 확률이 더 낮은 영역을 고려한다.
또한 제2종 오류의 여집합을 살펴볼 수 있는데 즉 귀무가설이 참일 때
귀무가설을 기각하는 것이며 이것은 옳은 결정이다. 우리는 이후의 결정의
확률을 최대화하려고 하므로 이 확률이 최대한 크도록 하려고 합니다. 즉,
$\theta \in \omega_{1}$인 경우 우리는 다음을 최대화하고자 한다.

$$1 - P_{\theta}(typeIIerror) = P_{\theta}\left( \left( X_{1},X_{2},\ldots,X_{n} \right) \in RR \right)$$

【1종 오류】

1종 오류는 유의수준, 검정 크기와 동일하다. 기각역 $C$에 검정통계량 값이
속할 확률이다.
$P_{\theta_{0}}(t\left( X_{1},X_{2},\ldots,X_{n} \right) \in RR)$

【2종 오류 계산】

귀무가설 $H_{0}:\theta = \theta_{0}$, 대립가설
$H_{1}:\theta = \theta_{1} < \theta_{0}$의 경우 2종 오류을 계산하는
방법을 살펴 보자. 2종 오류를 계산하려면 대립가설에 속한 모수의 하나의
값이 필요하다. 대립가설의 모수 값은 구간이기 때문이다. 대립가설의 하나의
값을 $\theta_{1}$라 하자.

2종 오류 :
$P_{\theta_{1}}(t\left( X_{1},X_{2},\ldots,X_{n} \right) \in CR)$ 검정
통계량이 기각역($CR$)에 속하지 않을 확률

대립가설의 $\theta_{1}$이 $\theta_{0}$에 가깝다면 대립가설이 사실임에도
불구하고 귀무가설 받아들일 가능성이 높다. 즉 2종 오류는 커진다.

검정력

【검정력 함수】

다음은 기각역의 검정력 함수 power function로 정의한다. 검정력 함수는
모수 함수이다.

$$\gamma_{C}(\theta) = P_{\theta}(t\left( X_{1},X_{2},\ldots,X_{n} \right) \in RR;\theta \in \omega_{1})$$

귀무가설 하의 모수 값에서 검정력은 1 종 오류(유의수준)이다.

【예제】
$f(x;\theta) = \frac{1}{\theta},0 < x < \theta \sim U(0,\theta)$에서
모수 $\theta$에 대한 다음 가설을 검정하기 위하여 확률표본을 추출하였다.

귀무가설 : $H_{0}:\theta = \theta_{0}$

대립가설 : $H_{1}:\theta = \theta_{1} < \theta_{0}$ 단측 대립가설

검정통계량 : MVUE의 함수이며 확률밀도함수가 알려진 통계량을 이용한다.
\*) 모집단 비율의 MVUE는 $\sum X_{i}/n$이지만 $\sum X_{i} \sim B(n,p)$
이므로 $T = \sum X_{i}$을 검정 통계량으로 사용한다.

기각역 및 임계값 : 만약 $RR = \{ t;T \leq k\}$이면 귀무가설 기각한다.
임계값 $\alpha = P_{\theta_{0}}(T \leq k)$에 의해 결정된다.

검정력 함수 :
$\gamma_{C}(\theta) = P_{\theta}(T \leq k),\theta \leq \theta_{0}$,
대립가설의 참 모수 값이 귀무가설 $\theta_{0}$에서 멀어질수록 검정력은
커진다.

【예제】 A 교수는 수리통계학 시험 점수가 평균 70점 수준으로 출제했다고
했다. 수강생이 시험을 보고 나와 어려워 70점 미만이라고 주장했다. 25명
수강생 점수를 확인한 결과 평균 69점, 표준편차 9점이었다. 수강생들의
주장을 유의수준 5%에서 검정하라. 수리통계학 점수는 정규분포를 따른다.

귀무가설 : $H_{0}:\mu_{0} = 70$

1.  대립가설 : $H_{1}:\mu_{1} < 70$ 단측 대립가설

2.  검정통계량 : 모집단 평균 MVUE는 $\overline{X}$이고
    $\frac{\overline{X} - \mu_{0}}{\frac{S}{\sqrt{n}}} = \frac{69 - 70}{\frac{9}{\sqrt{25}}} = - 1.67 \sim t(n - 1 = 24)$이다.

3.  기각역 및 임계값 : $0.05 = P_{\mu_{0}}(t(24) \leq k)$ 식에서 의해
    결정된다. $k = - 1.71$

4.  검정력 함수 :
    $\gamma_{C}(\theta) = P_{\mu}\left( \frac{69 - \mu}{\frac{9}{\sqrt{25}}} \leq - 1.71 \right)$

    유의확률

1종 오류, 2종 오류, 검정력을 계산하는 방법에 대해 다루었다. 가설검정
절차를 보면 ① 가설 설정과 유의수준 $\alpha$을 설정하고 ② 적절한
검정통계량 계산 ③ 기각역(유의수준과 검정통계량의 분포에 따라 계산된다)을
얻고 이에 따라 귀무가설 채택 여부를 결정한다.

설정된 1종 오류를 유의수준 significant level, test level 이라 하고
$(1 - \beta)$을 검정력 power of test, test power 이라 한다. 유의수준은
일반적으로 1%, 5%, 10% (물론 5%을 가장 많이 사용하지만)을 사용하게
되므로 기각역이 달라진다. 만약 어떤 사람이 유의수준 5%에서 귀무가설을
기각하지 못했다고 발표했다. 그럼? 10%일 때는... 우리는 다시 가설 검정을
해야 한다. why? 기각역이 달라지므로...

이런 문제를 해결할 방법이 없을까? p-값 p-value, 유의확률 significant
probability 개념을 도입하자. 검정통계량이 계산되면 귀무가설을 더![도표,
라인, 그래프, 스케치이(가) 표시된 사진 자동 생성된
설명](media/image4.png){width="2.921738845144357in"
height="1.6623687664041995in"} 기각할 영역의 확률을 p-값이라 한다.
다음은 대립가설이 $H_{1}:\theta > \theta_{0}$일 경우 유의확률을 계산
방법이다.

【유의확률】

대립가설은 $H_{1}:\theta = \theta_{1} > \theta_{0}$이고 확률표본에 의해
계산된 검정통계량 값을 $t^{*}$ 하자.

$RR = \{{T > t}^{*}\}$.

유의확률 :
${p - value = P}_{\theta_{0}}(t\left( X_{1},X_{2},\ldots,X_{n} \right) \in RR)$

계산된 검정통계량이 귀무가설이 진실이라는 가정 하에 귀무가설을 기각할
최소의 확률을 유의확률 혹은 p-값이라 한다. 그러므로 유의확률을 계산된
유의수준이다. 계산된 검정통계량이 귀무가설을 기각할 방향(대립가설을
채택할 방향)의 영역 확률을 유의확률이라 한다.

유의확률이 주어지면 귀무가설 채택 여부를 알 수 있다. 유의확률이
유의수준보다 크다면 검정통계량의 값은 기각역에 속한 것이 아니므로
귀무가설이 채택되고 유의확률이 유의수준보다 적다면 검정통계량의 값이
기각역에 속하므로 귀무가설을 기각한다.

【예제】 모집단 $f(x;\theta) \sim N(\mu,\sigma^{2})$에서 표본 크기 $n$인
확률표본을 추출하였다. 모분산 $\sigma^{2}$을 알지 못할 때 nuisance
parameter 귀무가설 $H_{0}:\mu = \mu_{0}$와 대립가설
$H_{1}:\mu = \mu_{1} > \mu_{0}$을 유의수준
![image9.pdf](media/image5.png){width="0.15625in"
height="0.1388888888888889in"}에서 우도비 검정방법은
$T(\overline{x}) = \frac{\sqrt{n}(\overset{¯}{x} - \mu_{0})}{S} \sim t(n - 1)$이다.(자세한
내용은 다음 2절. 우도비 검정을 참고하기 바란다.)

> $${p - value = P}_{\theta_{0}}\left( t\left( X_{1},X_{2},\ldots,X_{n} \right) \in RR \right) = P(t(n - 1) \geq T(\overline{x}))$$

만약 대립가설 $H_{1}:\mu = \mu_{1} \neq \mu_{0}$ (양측 대립가설)이면
$p - value = 2P(t(n - 1) \geq |T\left( \overline{x} \right)|)$.

【유의확률과 대립가설 형태】![그림
1](media/image6.png){width="3.088888888888889in"
height="1.895138888888889in"}

유의확률을 계산할 때 계산된 검정 통계량 값이 귀무가설에 설정한 모수
값(확률밀도함수의 중앙)으로부터 멀어지는 구간에 속할 확률을 구하면 된다.
단측 대립가설이면 구한 유의확률을 그대로 사용하면 되지만 양측 대립가설일
경우에는 계산된 유의확률을 2배 하여 유의확률로 사용해야 한다. 그러므로
양측 대립가설이 채택되면 단측 대립가설도 항상 성립한다.

가설검정 관련 코멘트

1)  양측검정이냐? 단측검정이냐? 우리가 무엇에 관심을 갖느냐 하는데 있다.
    불량률과 같이 \~값 미만에 관심이 있다면 왼쪽이 단측 검정, 수익과
    같이 \~값 이상에 관심이 있다면 우측이 대립가설이 단측 검정을
    실시한다. ["]{dir="rtl"}차이가 없다" 같이 양쪽 모두에 관심이 있다면
    양측검정을 실시한다.

<!-- -->

2)  양측 대립가설이 기각되면 단측 대립가설은 항상 기각된다. 그러므로
    "차이가 없다"가 기각되면 "차이가 있다"는 물론 "크다" 혹은 "적다"로
    검정 결과를 내려도 된다.

3)  1종 오류는 기각역이 정해지면 계산 가능하지만 2종 오류는 대립 가설의
    값 중 임의의 값이 설정되어야 가능하다. 검정력은 (1-2종 오류)
    정의된다. 또한 우리의 관심이 대립가설에 있으므로 1종 오류를
    설정하고(이를 유의수준이라 한다) 2종 오류를 최소화 하는 검정 방법을
    구하게 된다. (1종, 2종 오류 모두 줄이는 방법은 없다.)

4)  단측 검정일 때 귀무가설에 대하여 대립가설이
    $H_{1}:\theta < \theta_{0}$라면 귀무가설이
    $H_{0}:\theta \geq \theta_{0}$은 아닌가? 1종 오류는
    $\alpha = \max_{\theta \in \omega_{0}}{P_{\theta}(t\left( X_{1},X_{2},\ldots,X_{n} \right) \in CR)}$에
    해당된다. 그러므로 Boundary 값이 $\theta_{0}$에서 최대화되므로
    $H_{0}:\theta = \theta_{0}$이 적절하다. 그리고 우리의 관심은
    귀무가설이 아니므로 귀무가설이 채택된다면 우리는 헛수고 한 것이다.

    가설검정과 신뢰수준![텍스트, 도표, 라인, 폰트이(가) 표시된 사진 자동
    생성된 설명](media/image7.png){width="2.7905380577427823in"
    height="1.887716535433071in"}

유의수준 $\alpha$의 양측 가설 검정과 $100(1 - \alpha)\%$신뢰구간은
일대일 대응관계가 있다. 모집단 $f(x;\theta = \mu)$, 확률표본, 추정량
$\overset{\hat{}}{\theta} = \overset{¯}{X}$, 표본분산 $S^{2}$, 귀무가설
$H_{0}:\theta = \theta_{0}$인 경우 대표본 이론에 의해
$T = \frac{\overset{\hat{}}{\theta} - \theta_{0}}{\frac{S}{\sqrt{n}}}$는
$N(0,1)$을 따르므로 다음이 성립한다.

$100(1 - \alpha)\%$ 신뢰구간 :
$\theta \pm z_{\left( 1 - \frac{\alpha}{2} \right)}\frac{s}{\sqrt{n}}$

유의수준 $\alpha$인 경우 기각역 :
$C = \{|\frac{\overset{\hat{}}{\theta} - \theta}{\frac{s}{\sqrt{n}}}| > z_{\left( 1 - \frac{\alpha}{2} \right)}\}$

유의수준 $\alpha$인 경우 채택역 :
$C^{c} = \{\left| \frac{\overset{\hat{}}{\theta} - \theta}{\frac{s}{\sqrt{n}}} \right| \leq z_{\left( 1 - \frac{\alpha}{2} \right)}\}$![텍스트,
라인, 도표, 폰트이(가) 표시된 사진 자동 생성된
설명](media/image8.png){width="2.957529527559055in"
height="1.9789359142607175in"}

즉 신뢰구간과 채택역은 동일함을 알 수 있다. 신뢰구간에 속한 모수 값이
귀무가설에 설정되면 그 귀무가설은 기각되지 않는다. 표본으로부터 통계량이
계산되면 신뢰구간의 식에의해 모수에 대한 구간이 결정된다. 그 부분이 빨간
영역 부분이다.귀무가설에 빨간 영역 안에 있는 모수 값이 설정되면
귀무가설을 기각하지 못한다.

단측 가설 검정과 상한(혹은 하한) 신뢰구간도 일대일 관계가 있다.
$100(1 - \alpha)\%$ 하한 신뢰구간은 유의수준 $\alpha$, 귀무가설
$H_{0}:\theta = \theta_{0}$, 대립가설: $H_{0}:\theta_{1} > \theta_{0}$의
채택역과 동일하다. 또한 상한신뢰구간 대립가설
$H_{0}:\theta_{1} < \theta_{0}$의 채택역과 동일하다.

【예제】 정부는 PC 구입 가격이 900\$이라고 발표했다. 이를 알아보기
위하여 소비자 55명을 임의 조사하였더니 평균 885\$, 표준편차는
50\$이었다.

모집단 평균에 대한 99%(양측)신뢰구간을 구하시오.

($\overset{¯}{X} \pm Z_{0.995}\frac{S}{\sqrt{n}}) \rightarrow (885 \pm 2.58\frac{50}{\sqrt{25}}) \rightarrow (859.2,910.8)$

정부의 발표가 진실이라고 할 수 있나? 유의수준은 1%

유의수준 1% 검정 통계량 :
$\frac{\overset{¯}{X} - \mu_{0}}{\frac{S}{\sqrt{n}}} = \frac{885 - 900}{\frac{50}{\sqrt{25}}} = - 0.5$

기각역 : $C = \{ t;|t| < 2.58\}$ 검정 통계량 값이 기각역에 속하지
않으므로 귀무가설을 채택한다. 유의수준 1%에서 귀무가설 값이 99%
신뢰구간에 속하므로 귀무가설 채택하게 되므로 신뢰구간을 계산한 경우에는
가설검정을 할 필요가 없다.

랜덤화 검정

가설 검정에서 설정한 유의수준에 의해 기각역이 설정되는데 이산형
확률분포의 경우 설정된 유의수준 정확한 값에 대응하는 기각역을 설정하지
못하는 경우가 발생하는 경우 랜덤화 검정을 실시한다. 예제로 이 개념을
설명하기로 한다.

【예제】 $f(x;\theta = \lambda) \sim Poisson(\theta)$의 모수 $\theta$에
대한 가설 검정을 위하여 표본크기 $n = 10$인 확률표본
$(X_{1},X_{2},\ldots,X_{10})$을 추출하였다. 귀무가설 $\theta = 0.1$,
귀무가설 $\theta > 0.1$을 유의수준 5%에서 검정하려고 할 때 기각역을
구하라

검정 통계량 : $T = \sum_{i}^{10}X_{i} \sim Poisson(\lambda = 10\theta)$

기각역 : $P_{\theta_{0} = 0.1}(T \geq 3) = 0.08$,
$P_{\theta_{0} = 0.1}(T \geq 4) = 0.019$ 이처럼 유의수준 5%에 해당하는
기각역 얻을 수 없다. 보간법에 의해
$\frac{0.05 - 0.019}{0.08 - 0.019} = \frac{31}{61}$ 이다. 그러므로
유의수준 5% 기각역은 다음과 같다. 만약 $(T \geq 4)$ 귀무가설을 기각하고
$(T = 3)$이면 $\frac{31}{61}$ 확률로 기각한다.

2가설검정 방법

최강 검정법

검정방법의 정도 goodness는 1종 오류와 2종 오류로 판단하는데 가설검정
시작 시 1종 오류는 유의수준으로 설정되므로 좋은 검정 방법이란 2종 오류를
최소화 하는 것이다. 즉 (1-2종 오류), 검정력 최대화 하는 것이다. 이를
최강 검정법 most powerful test 이라 한다.

모집단 확률변수
$X \sim f(x;\theta),\theta \in \Omega = \{\omega_{0} \cup \omega_{1}\}$이고
다음 2개의 단일 귀무가설과 대립가설을 가정하자.

귀무가설 : $H_{0}:\theta \in \omega_{o}$

대립가설 : $H_{1}:\theta \in \omega_{1}$

모집단 확률변수 $X \sim f(x;\theta)$ 에서 확률표본
${\overline{X}}' = (X_{1},X_{2},\ldots,X_{n})$이라 하고 $\mathcal{S}$을
확률표본 영역이라 하자.

【유의수준】 확률표본의 영역을 $\mathcal{S}$, 확률표본 영역의 부분 집합
$RR \in \mathcal{S}$이라 하자. 다음 $\alpha$을 1종 오류 type I error,
유의수준 significant level 이라 한다.
$\alpha = \max_{\theta \in \omega_{0}}{P_{\theta}\left( T\left( X_{1},X_{2},\ldots,X_{n} \right) \in RR \right)}$.

정의

【검정력】 다음을 검정력 power function 이라 정의한다.

정의

$${{P(\theta) = \gamma}_{RR}(\theta) = P}_{\theta}\left( T\left( X_{1},X_{2},\ldots,X_{n} \right) \in RR \right);\theta \in \omega_{1}$$

【예제】 표본크기 5인 확률표본
$\left( X_{1},X_{2},\ldots,X_{5} \right) \sim B(p)$을 이용하여
$H_{0}:p = 0.5vs.H_{1}:p > 0.5$ 가설을 검정한다고 하자.

\(1\) 기각역을 $RR = \{\sum x_{i} = 5\}$

- 1종 오류:
  $\alpha = P(\sum x_{i} = 5,\sum x_{i} \sim B(5,0.5)) = 0.031$

- 2종 오류 :
  $\beta(p) = P(\sum x_{i} \leq 4,\sum x_{i} \sim B(5,p > 0.5))$

- 검정력 : $Power(p) = 1 - \beta(p)$이므로 $p = 0.5$에서 멀어질수록
  검정력은 커진다. $p = 1$일 때 검정력은 0이다.

\(2\) 기각역을 $RR = \{\sum x_{i} \geq 3\}$

- 1종 오류 :
  $\alpha = P(\sum x_{i} \geq 5,\sum x_{i} \sim B(5,0.5)) = 0.5$

- 2종 오류 :
  $\beta(p) = P(\sum x_{i} \leq 4,\sum x_{i} \sim B(5,p > 0.5))$

- $RR = \{\sum x_{i} = 5\}$에 비해 1종 오류는 늘었으나 2종 오류는
  줄었다. 검정력은 커졌다. 즉, 1종 오류와 2종 오류를 동시에 줄이는
  검정방법은 존재하지 않는다.

【최강 검정력】 확률표본의 영역을 $\mathcal{S}$, 확률표본 영역의 부분
집합 $RR \in \mathcal{S}$이라 하자. 다음 조건을 만족하는 $RR$을 검정
크기 $\alpha$에서 귀무가설 $H_{0}:\theta = \theta_{0}$, 대립가설
$H_{1}:\theta = \theta_{1}$을 검정하는 최량 기각역 best critical
region이다.

정의

$$\max_{\theta \in \omega_{0}}{P_{\theta}\left( T\left( X_{1},X_{2},\ldots,X_{n} \right) \in RR \right)} = \alpha$$

임의의 부분집합 $A \in \mathcal{D}$에 대하여 다음이 성립한다.

> $$P_{\theta_{0}}\left( T\left( X_{1},X_{2},\ldots,X_{n} \right) \in A \right) = > P_{\theta_{1}}\left( T\left( X_{1},X_{2},\ldots,X_{n} \right) \in RR \right) \geq P_{\theta_{1}}\left( T\left( X_{1},X_{2},\ldots,X_{n} \right) \in A \right)$$

【예제】 모집단 $X \sim B(p = \theta)$ 에서 귀무가설 $H_{0}:p = 0.1$,
대립가설 $H_{0}:p = 0.2$ 가설을 검정하기 위하여 표본크기3인 확률표본을
추출한 결과는 $(1,0,0)$ 이다.

기각역을 $RR = \{ T(\overline{x});\sum X_{i} \geq 1\}$이라 하자.
$\sum X_{i} \sim B(n = 3,p)$분포를 따른다.

유의수준 :
${P_{p = 0.1}\left( T\left( X_{1},X_{2},\ldots,X_{n} \right) \in RR \right)}{= 0.243 + 0.027 + 0.001 = 0.271}$

2종 오류 :
${P_{p = 0.2}\left( T\left( X_{1},X_{2},\ldots,X_{n} \right) \in RR^{C} \right)}{= 0.5121}$

검정력 : $1 - 0.5121 = 0.4879$

  --------------------------------------- ------------- ------------- ------------- -------------
  $$\sum x$$                                    0             1             2             3

  $$f(x;p = 0.1)$$                            0.729         0.243         0.027         0.001

  $$f(x;p = 0.2)$$                            0.512         0.384         0.096         0.008

  $$\frac{f(x;p = 0.1)}{f(x;p = 0.2)}$$       1.424         0.633         0.281         0.125
  --------------------------------------- ------------- ------------- ------------- -------------

만약 대립가설의 모수 하나의 값(대립가설은 귀무가설과는 달리 모수의
영역이다)을 $\theta_{1}$라 한다면 검정력은
${P(\theta) = P}_{\theta}\left( T\left( X_{1},X_{2},\ldots,X_{n} \right) \in RR^{C}|\theta = \theta_{1} \right)$이고
2종오류는 $\beta(\theta) = 1 - P(\theta)$ 이다.
$H_{0}:\theta = \theta_{0}vs.H_{1}:\theta = \theta_{1} \neq \theta_{0}$
가설검정의 경우 전형적인 검정력 함수는 다음과 같다.

![라인, 도표, 그래프이(가) 표시된 사진 자동 생성된
설명](media/image9.png){width="4.4998611111111115in"
height="1.7407250656167979in"}

검정력은 귀무가설 단일 설정 값 $\theta_{0}$에서 최저였다가 멀어질수록
증가한다. 그러므로 유의수준
![image14.pdf](media/image10.png){width="0.15625in"
height="0.1388888888888889in"}을 설정하고 2종 오류를 최소화할 수 있는
검정 방법(기각역 찾음)을 찾는다.

most powerful test 최강력 검정법

가설에 설정된 모수 값이 하나인 경우 이를 단순가설이라 하고 설정된 모수가
영역인 경우 이를 복합가설이라 한다. 단순 귀무가설
$H_{0}:\theta = \theta_{0}$와 단순 대립가설
$H_{1}:\theta = \theta_{1}$을 검정한다고 하자. 모수 영역은
$\Omega = \{\theta;\theta_{0},\theta_{1}\}$이고 $P(\theta) = \alpha$인
기각역을 설정하고 $\beta(\theta) = 1 - p(\theta)$ 을 최소화 하는
기각역을 구한다. 이는 $P(\theta_{1})$을 최대화 하는 최강 검정력 검정
most powerful test을 찾는 것과 동일하다.

【Neyman-Pearson Lemma】 단순 귀무가설 $H_{0}:\theta = \theta_{0}$와
단순 대립가설 $H_{1}:\theta = \theta_{1}$을 검정한다고 하자. 가설 검정을
확률표본
$\overline{X} = \left( X_{1},X_{2},\ldots,X_{n} \right)$으로부터 구한
통계량을 이용한다. 이 통계량의 확률밀도함수(샘플링 분포)는 모수
$\theta$의 함수이며 이를 우도 함수 $L(\overline{x};\theta)$라 한다.
유의수준 $\alpha$ 하에서 검정력을 최대화 하는 기각역 $RR$은 다음에 의해
정해진다. 이렇게 얻은 검정 방법(기각역)을 최강력 검정법이라 한다.

정리

$\frac{L({\overline{x};\theta}_{0})}{L({\overline{x};\theta}_{1})} \leq k,for\overline{x} \in RR$.

$\frac{L({\overline{x};\theta}_{0})}{L({\overline{x};\theta}_{1})} \geq k,for\overline{x} \in RR^{C}$.

$${{\alpha = P}_{H}}_{0}(\overline{x} \in RR)$$

【불편 검정방법】 모집단 $X \sim f(x;\theta),\theta \in \Omega$ 에서
확률표본 $\overline{X} = \left( X_{1},X_{2},\ldots,X_{n} \right)$,
기각역은 ${{\alpha = P}_{H}}_{0}(\overline{x} \in RR)$이라 하면 다음을
불편 unbiased 검정방법이다.

정의

$$P_{\theta}\left( \overline{X} \in RR \right) \geq \alpha,forall\theta \in \omega_{1}$$

【예제】 확률표본
$\overline{X} = \left( X_{1},X_{2},\ldots,X_{n} \right) \sim f(x;\theta = \mu)\sim N(\mu,1)$이고
귀무가설 $H_{0}:\mu = 0$, 대립가설 $H_{0}:\mu = 1$ 검정하는 최강
기각역을 구하라.

N-P Lemma 의해 최강 기각역 :
$\frac{L({\overline{x};\theta}_{0} = 0)}{L({\overline{x};\theta}_{1} = 1)} = \frac{N(0,1)}{N(1,1)} = \exp\left( - \sum x_{i} + \frac{n}{2} \right) \leq k$.

$$\Leftrightarrow - \sum x_{i} + \frac{n}{2} \leq \ln(k) \Leftrightarrow \sum x_{i} > - \frac{n}{2} - \ln{(k) \Leftrightarrow}\frac{\sum x_{i}}{n} > c$$

그러므로 최강 기각역은
$RR = \left\{ \left( x_{1},x_{2},\ldots,x_{n} \right);\overline{x} \geq c \right\},\overline{x} \sim N(0,\frac{1}{n})$.

유의수준 $\alpha$ 임계값 critical value, $c_{1}$ 결정 :
${P_{H}}_{0}\left( \overline{x} \geq c \middle| \mu = 0 \right) = \int_{c}^{\infty}{\frac{1}{\sqrt{2\pi}\sqrt{1/n}}\exp\left( - \frac{\left( \overset{¯}{x} - 0 \right)^{2}}{2(1/n)} \right)d\overset{¯}{x}} = \alpha$을
만족하는 $c$ 값을 기각역의 시작 값인 임계값 $c_{1}$이다.

검정력 :
${P_{H}}_{1}\left( \overline{x} \geq c_{1} \middle| \mu = 1 \right) = \int_{c_{1}}^{\infty}{\frac{1}{\sqrt{2\pi}\sqrt{1/n}}\exp\left( - \frac{\left( \overset{¯}{x} - 1 \right)^{2}}{2(1/n)} \right)d\overset{¯}{x}}$

【예제】 확률표본
$\overline{X} = \left( X_{1},X_{2},\ldots,X_{n} \right) \sim f(x)$이고
귀무가설, 대립가설이 확률분포함수인 경우 최강 기각역을 구하라.

귀무가설 :
$H_{0}:f_{0}(x) = \frac{e^{- 1}}{x!}\sim Poisson(\lambda = 1),x = 0,1,2,\ldots$

대립가설 :
$H_{1}:f_{1}(x) = \left( \frac{1}{2} \right)^{x + 1},x = 0,1,2,\ldots$

우도비 :
$\frac{f_{0}(x)}{f_{1}(x)} = \frac{\left( 2e^{- 1} \right)^{n}2^{\sum x_{i}}}{\Pi x_{i}!}$

최강 기각역 :
$\frac{\left( 2e^{- 1} \right)^{n}2^{\sum x_{i}}}{\Pi x_{i}!} \leq k \Leftrightarrow \left( \sum x_{i} \right)\ln(2) - \ln\left( \Pi x_{i}! \right) \leq ln)k) - nln(2e^{- 1}) = c$

【예제】 모집단 $f(x;\theta) \sim B(p),x = 0,1$에서 모수 $\theta = p$에
대한 가설 검정을 위하여 표본 크기 $n$인 확률표본
$\overline{X} = \left( X_{1},X_{2},\ldots,X_{n} \right)$ 을 추출하였다.
귀무가설 $H_{0}:p_{0} = 0.5$ 대립가설 $H_{1}:p_{1} = 0.1$을 유의수준
0.05에서 검정하는 최강 검정방법을 찾아라.

N-P Lemma에 의해
$\frac{L\left( \overline{x}{;\theta}_{0} \right)}{L\left( \overline{x}{;\theta}_{1} \right)} = \frac{{0.5}^{\sum x_{i}}{0.5}^{n - \sum x_{i}}}{{0.2}^{\sum x_{i}}{0.8}^{n - \sum x_{i}}} \leq k \Leftrightarrow \sum x_{i} \leq c$
이 일양 최강 기각역이다.

$\sum x_{i} \sim B(n,p)$이므로
$0.05 = P(\sum x_{i} \leq c|\sum x_{i} \sim B(n,0.5))$을 만족하는 정수
$c$를 찾을 수 없다. 앞에서 살펴본 랜덤화 검정방법을 이용하여 검정하면
된다.

일양 최강 검정

【일양 최강 검정법】 귀무가설 $H_{0}:\theta = \theta_{0}$, 단순 대립가설
$H_{1}:\theta = \theta_{1} < \theta_{0}$의 최강 검정 기각역 $RR$이 모든
복합 대립가설 $H_{1}:\theta = \theta_{1} < \theta_{0}$의 최강 기각역이면
$RR$에 의한 검정방법을 일양 최강 검정법 uniformly most powerful test
이라 한다.

정의

【예제】 모집단 $f(x;\theta) = \frac{1}{\theta}e^{- x/\theta},x > 0$에서
모수 $\theta$에 대한 가설 검정을 위하여 표본 크기 $n$인 확률표본
$\overline{X} = \left( X_{1},X_{2},\ldots,X_{n} \right)$ 을 추출하였다.
귀무가설 $H_{0}:{\theta = \theta}_{0}$ 대립가설
$H_{1}:{\theta = \theta}_{1} < \theta_{0}$을 유의수준 $\alpha$에서
검정하는 일양 최강 검정방법을 찾아라.

N-P Lemma에 의해
$\frac{L\left( \overline{x}{;\theta}_{0} \right)}{L\left( \overline{x}{;\theta}_{1} \right)} = \frac{\frac{1}{\theta_{0}^{n}}e^{- \frac{\sum x}{\theta_{0}}}}{\frac{1}{\theta_{1}^{n}}e^{- \frac{\sum x}{\theta_{1}}}} \leq k \Leftrightarrow \sum x_{i} \leq c$
이 일양 최강 기각역이다. $RR = \{\sum x_{i} \leq c\}$ 이 UMPT
기각역이다. 항상 기각역에 사용되는 통계량의 분포는 알려져 있어야
가능하다. $\sum x_{i} \sim Gamma(n,\theta)$. 임계값은
$0.05 = P(\sum x_{i} \geq c|\sum x_{i} \sim Gamma(n,\theta_{0}))$을
만족하는 $c$값이다.

동일한 방법으로 귀무가설 $H_{0}:{\theta = \theta}_{0}$ 대립가설
$H_{1}:{\theta = \theta}_{1} > \theta_{0}$을 유의수준 $\alpha$에서
검정하는 일양 최강 검정력은
$RR = \{\sum x_{i} \geq c|\sum x_{i} \sim Gamma(n,\theta)\}$.

그러나 대립가설 $H_{0}:{\theta = \theta}_{1} \neq \theta_{0}$이면
Uniformly Most Powerful Test는 존재하지 않는다.

【예제】 모집단 $f(x;\theta) \sim N(0,\theta)$에서 모수 $\theta$(분산)에
대한 가설 검정을 위하여 표본 크기 $n$인 확률표본
$\overline{X} = \left( X_{1},X_{2},\ldots,X_{n} \right)$ 을 추출하였다.
귀무가설 $H_{0}:{\theta = \theta}_{0}$ 대립가설
$H_{1}:{\theta = \theta}_{1} > \theta_{0}$을 유의수준 $\alpha$에서
검정하는 일양 최강 검정방법을 찾아라.

N-P Lemma에 의해
$\frac{L\left( \overline{x}{;\theta}_{0} \right)}{L\left( \overline{x}{;\theta}_{1} \right)} = \frac{\left( \frac{1}{2\pi\theta_{0}} \right)^{\frac{n}{2}}exp( - \frac{1}{2\theta_{0}}\sum x_{i}^{2})}{\left( \frac{1}{2\pi\theta_{1}} \right)^{\frac{n}{2}}exp( - \frac{1}{2\theta_{1}}\sum x_{i}^{2})} \leq k \Leftrightarrow \sum x_{i}^{2} \geq c$이
일양 최강 기각역이다.

그러나 $\sum x_{i}^{2}$의 확률밀도함수는 알 수 없으므로 기각역에
사용되는 검정 통계량은
$\frac{\sum x_{i}^{2}}{\theta_{0}} \sim \chi^{2}(n - 1)$ 분포를 따르므로
$\frac{\sum x_{i}^{2}}{\theta_{0}}$을 이용하는데 이 통계량을 주축
pivotal 검정 통계량이라 한다.

【단조우도비】 만약 우도비
$\frac{L\left( \overline{x}{;\theta}_{0} \right)}{L\left( \overline{x}{;\theta}_{1} \right)},for\theta_{0} < \theta_{1}$가
$y = u(x)$의 단조 함수이면 우도함수
$L\left( \overline{x};\theta \right)$은 통계량 $y = u(x)$의 단조 우도비
monotone likelihood ratio(mlr)을 갖는다.

정의

$\frac{L\left( \overline{x}{;\theta}_{0} \right)}{L\left( \overline{x}{;\theta}_{1} \right)} \leq k \Leftrightarrow t\left( x_{1},x_{2},\ldots,x_{n} \right) \in RR$.
$\alpha = P_{\theta_{0}}(t\left( x_{1},x_{2},\ldots,x_{n} \right) \in RR)$을
만족하는 $k$가 임계값이다.

$\frac{L\left( \overline{x}{;\theta}_{0} \right)}{L\left( \overline{x}{;\theta}_{1} \right)} = g(Y) \leq k \Leftrightarrow Y \geq g^{- 1}(k)$.
$g^{- 1}(k)$는 $\alpha = P_{\theta_{0}}(Y \geq g^{- 1}(k))$을 만족한다.

【예제】 모집단 $f(x;\theta) \sim B(\theta = p),0 < \theta < 1$에서 모수
$\theta$에 대한 가설 검정을 위하여 표본 크기 $n$인 확률표본
$\overline{X} = \left( X_{1},X_{2},\ldots,X_{n} \right)$ 을 추출하였다.
귀무가설 $H_{0}:{\theta = \theta}_{0}$ 대립가설
$H_{1}:{\theta = \theta}_{1} > \theta_{0}$을 유의수준 $\alpha$에서
검정하는 일양 최강 검정방법을 찾아라.

$\frac{L\left( \overline{x}{;\theta}_{0} \right)}{L\left( \overline{x}{;\theta}_{1} \right)} = \frac{{\theta_{0}}^{\sum x_{i}}{\theta_{0}}^{n - \sum x_{i}}}{{\theta_{1}}^{\sum x_{i}}{\theta_{1}}^{n - \sum x_{i}}} = \left( \frac{\theta_{0}(1 - \theta_{1})}{\theta_{1}(1 - \theta_{0})} \right)^{\sum x_{i}}\left( \frac{1 - \theta_{0}}{1 - \theta_{1}} \right)^{n}$이므로
우도비는$y = \sum x_{i}$의 단조 함수이다. 그러므로 UMPT 기각역은
$RR = \{\sum x_{i} \geq c\}$이다.

【지수족 UMPT】

모집단 확률밀도함수가 지수족이면 다음 가설의 일양 최강 검정 기각역은
$RR = \{ y = \sum K\left( x_{i} \right) \geq c\}$이다.

귀무가설 : $H_{0}:\theta = \theta_{0}$ vs. 대립가설 :
$H_{1}:\theta = \theta_{1} > \theta_{0}$

만약 대립가설 : $H_{1}:\theta = \theta_{1} < \theta_{0}$ 이면 일양 최강
검정 기각역은 $RR = \{ y = \sum K\left( x_{i} \right) \leq c\}$이다.

1)  우도비 검정

단순 귀무가설과 단순 대립가설에 대한 Most powerful test을 얻으려면
Neyman-Pearson 정리를 이용하면 된다. 물론 대립가설이 단측 가설(한 쪽
방향)에 Most powerful 검정을 얻을 수 있지만... 양측 검정이나 Nuisance
parameter가 있는 경우에는 N-P 정리를 사용할 수 없다.

모수 θ 영역을 $\Omega$라 하자. 귀무가설에 설정된 모수 값은 단일 값으로
$\Omega_{0} = \{\theta_{0}\}$이고 대립가설의 모수 공간은
$\Omega_{1} = \Omega_{0}^{C}$이다. 전체 모수 공간 $\Omega$에서
우도함수를 $L\left( \overline{x};\Omega \right)$라 정의하고 모수에 대한
MLE(최대우도추정량)을 추정치로 사용한 우도함수를
$L\left( \overline{x};\overset{\hat{}}{\Omega} \right)$라 하자.

【우도비 검정】 귀무가설 $H_{0}:\theta \in \Omega_{0}$, 대립가설
$H_{1}:\theta \in \Omega_{0}^{C}$ 검정하는 우도 통계량은
$\lambda\left( \overline{x} \right) = \frac{\max_{\Omega_{0}}{L\left( \overline{x};\Omega_{0} \right)}}{\max_{\Omega}{L\left( \overline{x};\Omega \right)}}$이다.
우도비 검정 likelihood ratio test 기각역은
$\left\{ \lambda\left( \overline{x} \right) \leq c \right\}$이다. $c$는
$0 < c < 1$의 값을 갖는다.

정의

만약 $\lambda \rightarrow 0$이면 귀무가설 하의 우도함수 값이 매우 작다는
것을 의미하므로 대립가설에 비해 귀무가설을 기각하는 것이 적절하다. 만약
$\lambda \rightarrow 1$이면 귀무가설이 진실일 가능성이 높다. 그러므로
기각역이 $\lambda \leq c$ 이다.

【예제】 모집단 $f(x;\theta) \sim N(\mu,\sigma^{2})$에서 표본 크기 $n$인
확률표본 $\overline{X} = \left( X_{1},X_{2},\ldots,X_{n} \right)$ 을
추출하였다. 모분산 $\sigma^{2}$을 알지 못할 때 nuisance parameter
귀무가설 $H_{0}:\mu = \mu_{0}$와 대립가설
$H_{1}:\mu = \mu_{1} > \mu_{0}$을 유의수준
![image9.pdf](media/image11.png){width="0.15625in"
height="0.1388888888888889in"}에서 우도비 검정방법을 찾으시오.

귀무가설 하 모수 공간은
$\Omega_{0} = \{\mu,\sigma_{0};\mu = \mu_{0},\sigma^{2} > 0\}$, 대립가설
하에서 모수 공간은
$\Omega_{1} = \{\mu,\sigma_{1};\mu > \mu_{0},\sigma_{1}^{2} > 0\}$이므로
모수 전체 공간은
$\Omega = \{\mu,\sigma;\mu \geq \mu_{0},\sigma^{2} > 0\}$이다.

$L\left( \overline{x};\Omega_{0} \right) = \Pi\frac{1}{\sqrt{2\pi}\sigma}exp( - \frac{\left( x_{i} - \mu_{0} \right)^{2}}{2\sigma^{2}})$이고
귀무가설 모평균은 $\mu_{0}$로 주어져 있으니 모분산에 대한 MLE 추정치를
구하면
$\overset{\hat{}}{\sigma_{0}^{2}} = \frac{1}{n}\sum\left( x_{i} - \mu_{0} \right)^{2}$이다.

$L\left( \overline{x};\Omega \right)$에서 $\mu$의 MLE는
$\overset{\hat{}}{\mu} = max(\overset{¯}{x},\mu_{0})$이다. 왜냐하면 모수
공간 $\Omega$의 $\mu \geq \mu_{0}$이기 때문이다. 모분산에 대한 MLE
추정치는
$\overset{\hat{}}{\sigma} = \frac{1}{n}\sum\left( x_{i} - \overset{\hat{}}{\mu} \right)^{2}$이다.

$\lambda = \frac{L(\overset{\hat{}}{\Omega_{0}})}{L(\overset{\hat{}}{\Omega})} = \left( \frac{\overset{\hat{}}{\sigma_{0}^{2}}}{\overset{\hat{}}{\sigma^{2}}} \right)^{\frac{n}{2}} = \left\{ \begin{array}{r}
\left( \frac{\sum\left( x_{i} - \overset{¯}{x} \right)^{2}}{\sum\left( x_{i} - \mu_{0} \right)^{2}} \right)^{\frac{n}{2}} \\
1,if\overline{x} \leq \mu_{0}
\end{array},if\overline{x} > \mu_{0} \right.\ $ .

$\sum\left( x_{i} - \mu_{0} \right)^{2} = \sum\left( x_{i} - \overset{¯}{x} \right)^{2} + \sum\left( \overset{¯}{x} - \mu_{0} \right)^{2}$
이므로 정리하면

$$\lambda = \left( \frac{\sum\left( x_{i} - \overset{¯}{x} \right)^{2}}{\sum\left( x_{i} - \mu_{0} \right)^{2}} \right)^{\frac{n}{2}} \leq k \Leftrightarrow \frac{\sum\left( x_{i} - \overset{¯}{x} \right)^{2}}{\sum\left( x_{i} - \mu_{0} \right)^{2}} \leq k^{\frac{2}{n}} \Leftrightarrow \frac{\sum\left( x_{i} - \overset{¯}{x} \right)^{2}}{\sum\left( x_{i} - \overset{¯}{x} \right)^{2} + \sum\left( \overset{¯}{x} - \mu_{0} \right)^{2}} \leq k^{\frac{2}{n}}$$

$$\Rightarrow \frac{\sqrt{n}(\overset{¯}{x} - \mu_{0})}{S} > c,whereS^{2} = \sum\left( x_{i} - \overset{¯}{x} \right)^{2}/(n - 1)$$

$\frac{\sqrt{n}(\overset{¯}{x} - \mu_{0})}{S}$은 t-분포를 따르므로
우도비 검정은 t-검정과 일치한다.

【예제】 모집단 $f(x;\theta) = e^{- (x - \theta)},x \geq 0$에서 표본
크기 $n$인 확률표본
$\overline{X} = \left( X_{1},X_{2},\ldots,X_{n} \right)$을 추출하였다.
귀무가설 $H_{0}:\theta = \theta_{0}$와 대립가설
$H_{1}:\theta > \theta_{0}$을 유의수준
![image9.pdf](media/image11.png){width="0.15625in"
height="0.1388888888888889in"}에서 우도비 검정방법을 찾으시오.

우도 함수는
$L\left( \overline{x};\theta \right) = \Pi e^{- (x - \theta)} = e^{- \sum(x_{i} - \theta)},\theta \leq x_{(1)}$이므로
우도 함수는 모수 $\theta$의 증가함수이다.

$\lambda = \frac{L(\overset{\hat{}}{\Omega_{0}})}{L(\overset{\hat{}}{\Omega})} = \frac{e^{- \sum(x_{i} - \theta_{0})}}{e^{- \sum(x_{i} - x_{(1)})}} = \left\{ \begin{array}{r}
e^{- n\left( x_{(1)} - \theta_{0} \right)},ifx_{(1)} > \theta_{0} \\
1,ifx_{(1)} \leq \theta_{0}
\end{array} \right.\ $

기각역
$\{\lambda \leq k\} \Leftrightarrow \{\overline{x};x_{(1)} \geq \theta_{0} - \frac{ln(c)}{n}\}$

일반적으로 우도비 검정의 경우 검정통계량의 분포를 아는 것은 쉽지 않다.
이런 경우 다음 정리를 이용하여 검정하게 된다.

$\lambda = \frac{L(\overset{\hat{}}{\Omega_{0}})}{L(\overset{\hat{}}{\Omega})}$라
정의하자. 표본의 크기 $n$이 충분히 크다면 귀무가설
$H_{0}:\theta \in \Omega_{0}$, 대립가설 $H_{1}:\theta \in \Omega_{1}$에
대한 우도비(LTR) 검정은 검정통계량 $\lambda$에 대해 다음이 성립한다.

정리

$- 2\ln(\lambda) \sim (app)\chi^{2}(r_{0} - r)$, $r$은 모수 공간
$\Omega$하에서 설정된 모수의 개수이고 $r_{0}$은 모수 공간 $\Omega_{0}$
하에서 설정된 모수의 개수이다.

【예제】 작업 라인이 2개 있다. 각 작업을 일주일 단위로 생산된 제품에
대한 불량 개수를 조사하였더니 작업라인1은 평균 20, 작업라인 2는
22이다(표본의 개수는 각각 100이다). 불량 개수는 포아송 분포를 따르고
작업 라인 1의 평균을 $\lambda_{1}$, 작업 라인 2의 평균을 $\lambda_{2}$라
하자. 귀무가설 $H_{0}:\lambda_{1} = \lambda_{2}$와 대립가설
$H_{0}:\lambda_{1} \neq \lambda_{2}$을 검정하는 우도비 검정을 유의수준
0.01에서 실시하자.

작업라인 1의 확률표본을 $(X_{1},X_{2},...,X_{n})$, 작업라인 2의
확률표본을 $(Y_{1},Y_{2},...,y_{n})$라 하자.

귀무가설 모수 공간은
$\Omega_{0} = \{\lambda_{1} = \lambda_{2} = \lambda\}$이고 전체 모수
공간은 $\Omega = \{\lambda_{1} = \lambda_{2} > 0\}$이다.

전체 모수 공간 결합 우도비는
![image23.pdf](media/image12.png){width="3.9974179790026247in"
height="0.560165135608049in"}이고

귀무가설 모수 공간 결합 우도비는
![image24.pdf](media/image13.png){width="3.751160323709536in"
height="0.6114774715660543in"}이다.

![image25.pdf](media/image14.png){width="4.303011811023622in"
height="0.4998053368328959in"}➔ MLE는
${\widehat{\lambda}}_{1} = \overline{X} = 20,{\widehat{\lambda}}_{2} = \overline{Y} = 22$이다.

![image28.pdf](media/image15.png){width="4.303011811023622in"
height="0.5143263342082239in"}➔ MLE는
$\widehat{\lambda} = \frac{\overline{X} + \overline{Y}}{2} = 21$이다.

![image30.pdf](media/image16.png){width="2.1146041119860017in"
height="0.7042213473315836in"}이므로 $- 2ln\lambda = 9.53$이다.

귀무가설에서 설정된 귀무가설의 수는 1이고 전체 모수의 수는 2이다.
그러므로 $- 2ln\lambda \sim \chi^{2}(df = 2 - 1 = 1)$이 성립한다. 자유도
1이고 유의수준이 0.01인 경우 기각치(임계치)는 $\chi^{2}$-분포표에 의해
6.635이다. 9.53이 기각역에 속하므로 귀무가설은 기각된다.

Chapter 4. 구간 추정

앞에서는 모수 $\theta$의 점 추정에 대해 논의했는데 $\theta$의 값을
하나의 추정치로 추측하는 것입니다. 여기서는 구간 추정 및 더 일반적으로
집합 추정에 대해 논의한다. 집합 추정 문제에서의 추론은 $\theta$이 구간
$C(\overline{x})$에 속한다.\"라는 추정됩니다. 즉,
$P\left( \overset{\hat{}}{\theta} = \theta \right) = 0$(점 추정차와
모수가 같을 확률은 0이지만 $P(\theta \in C(\overline{x}))$는 0보다 큰
확률을 가지게 되는데 이를 신뢰수준이라 한다.

1구간추정

【구간 추정량】

모집단 $f(x;\theta),\overline{x} \in \mathcal{S}$에서 모수 $\theta$
추론을 위하여 표본 크기 $n$인 확률표본
$\overline{X} = \left( X_{1},X_{2},\ldots,X_{n} \right)$을 추출하였다.
확률표본의 함수, 통계량
$L\left( \overline{x} \right) \leq U\left( \overline{x} \right)$을
정의하자.
$(L\left( \overline{x} \right),U\left( \overline{x} \right))$을 모수
$\theta$ 구간 추정량 interval estimator 이라 한다.

그리고
$\min_{\theta}{P_{\theta}(\theta \in (L\left( \overline{x} \right),U\left( \overline{x} \right)))}$을
신뢰수준이라 한다. 좌우 대칭인 분포의 경우에는 양쪽 꼬리 부분에 동일한
확률 값을 배분해야 동일 신뢰수준 하에서 신뢰구간 폭이 가장
작아진다(정도는 높아짐).

【예제】 모집단 $f(x;\theta) \sim N(\mu,1)$에서 모수 $\theta$ 추론을
위하여 표본 크기 2인 확률표본
$\overline{X} = \left( X_{1},X_{2} \right)$ 추출하였다. 모수 $\theta$에
대한 신뢰구간을 구하라.

모수 $\theta$의 MVUE가 $\overset{¯}{X}$이므로
$(\overset{¯}{X} - 1,\overset{¯}{X} + 2)$ 신뢰구간 중 하나이다. 신뢰수준
: 91.9%
$P\left( \overset{¯}{X} - 1 \leq \mu \leq \overset{¯}{X} + 2 \middle| \overset{¯}{X} \sim N\left( \mu,\frac{1}{2} \right) \right) = P\left( - \frac{2}{\sqrt{1/2}} \leq z = \frac{\overset{¯}{X} - \mu}{\sqrt{1/2}} \leq \frac{1}{\sqrt{1/2}} \right) = P\left( - 2\sqrt{2} \leq z \leq \sqrt{2} \right) = 0.921 - 0.002 = 0.919$

【comment】

1)  유의수준 $\alpha$의 양측 대립가설 검정방법과 신뢰수준
    $100(1 - \alpha)\%$ 구간 추정은 동일하다.

2)  앞에서 설명하였듯이 신뢰수준 $100(1 - \alpha)\%$ 구간의 의미는
    신뢰수준 $100(1 - \alpha)\%$ 구간 내에 모수 $\theta$가 있을 확률이
    $100(1 - \alpha)\%$이 아니라 100번의 신뢰구간을 구했을 때
    $100(1 - \alpha)\%$개 신뢰구간이 모수를 포함하고 있다는 것으로
    신뢰구간의 모수 커버리지 coverage 확률이다.

【예제】 모집단 $f(x;\theta) \sim U(0,\theta)$에서 표본 크기 $n$인
확률표본 $\overline{X} = \left( X_{1},X_{2},\ldots,X_{n} \right)$
추출하였다. 모수 $\theta$에 대한 신뢰구간으로 다음 2개를 생각해 보자.
모수 $\theta$의 MLE는 $Y = x_{(n)}$ 이다.

$Y = x_{(n)}$ 확률밀도함수는
$f(y) = ny^{n - 1}\left( \frac{1}{\theta} \right)^{n},0 < y < \theta$
이므로 변수변환 $T = \frac{Y}{\theta}$ 확률밀도함수는
$f(t) = nt^{n - 1},0 < y < 1$ 이다.

\(1\) 신뢰구간 $(aY,bY)$의 신뢰수준 :
$P_{\theta}(aY \leq \theta \leq bY) = P_{\theta}\left( \frac{1}{a} \leq T = \frac{Y}{\theta} \leq \frac{1}{b} \right) = \int_{\frac{1}{a}}^{\frac{1}{b}}{nt^{n - 1}dt} = \left( \frac{1}{a} \right)^{n} - \left( \frac{1}{b} \right)^{n}$
-- 신뢰구간은 모수에 의존하지 않는다.

\(2\) 신뢰구간 $(Y + c,Y + d)$의 신뢰수준 :
$P_{\theta}(Y + c \leq \theta \leq Y + d) = P_{\theta}\left( 1 - \frac{d}{\theta} \leq T = \frac{Y}{\theta} \leq 1 - \frac{c}{\theta} \right) = \left( 1 - \frac{c}{\theta} \right)^{n} - \left( 1 - \frac{d}{\theta} \right)^{n}$
-- 신뢰구간은 모수에 의존한다.

【유의수준 $\alpha$ 가설검정과 $100(1 - \alpha)\%$ 신뢰구간은
동일하다.】

모집단 $f(x;\theta) \sim N(\mu,\sigma^{2})$에서 표본 크기 $n$인 확률표본
$\overline{X} = \left( X_{1},X_{2},\ldots,X_{n} \right)$ 추출하였다.
귀무가설 $H_{0}:\mu = \mu_{0}$, 대립가설 $H_{0}:\mu \neq \mu_{0}$ UMPT
기각역은
$RR = \{\overline{x};\left| \overline{x} - \mu_{0} \right| > z_{1 - \frac{\alpha}{2}}\frac{\sigma}{\sqrt{n}}\}$(유의수준
$\alpha$)이다. 그러므로 채택역은
$\left| \overline{x} - \mu_{0} \right| \leq z_{1 - \alpha/2}\frac{\sigma}{\sqrt{n}}$이다.
이것은 모수 $\theta$에 대한 $100(1 - \alpha)\%$ 신뢰구간과 동일하다.

$$P\left( \overline{x} - z_{1 - \frac{\alpha}{2}}\frac{\sigma}{\sqrt{n}} \leq \mu \leq \overline{x} + z_{1 - \frac{\alpha}{2}}\frac{\sigma}{\sqrt{n}} \right) = 1 - \alpha$$

결론적으로 $100(1 - \alpha)\%$ 신뢰구간 포함된 모수 값을 (유의수준
$\alpha$) 설정한 귀무가설은 채택하게 된다. 신뢰구간 밖의 모수를 설정한
귀무가설은 기각된다.

【신뢰구간】

귀무가설 $H_{0}:\theta = \theta_{0}$에 대한 유의수준 $\alpha$의 검정의
채택역을 $A(\theta_{0})$ 이라 하자. 모수 공간
$C\left( \overline{x} \right) = \{\theta_{0}:\overline{x} \in A(\theta_{0})\}$은
$(1 - \alpha)$ 신뢰구간이다.

【예제】 모집단 $f(x;\theta) \sim exponetial(\theta = \lambda)$에서 표본
크기 $n$인 확률표본
$\overline{X} = \left( X_{1},X_{2},\ldots,X_{n} \right)$ 추출하였다.
모수 $\lambda$(평균)에 대한 $100(1 - \alpha)\%$ 신뢰구간을 구하라.

귀무가설 : $H_{0}:\lambda = \lambda_{0}$ vs. 대립가설 :
$H_{0}:\lambda \neq \lambda_{0}$

우도함수 :
$L\left( \overline{x};\Omega \right) = \Pi\frac{1}{\lambda}\exp\left( - \frac{x_{i}}{\lambda} \right) = \frac{1}{\lambda^{n}}exp( - \frac{\sum x_{i}}{\lambda})$

우도비 :
$\lambda = \frac{L(\overset{\hat{}}{\Omega_{0}})}{L(\overset{\hat{}}{\Omega})} = \frac{\frac{1}{\lambda_{0}^{n}}exp( - \frac{\sum x_{i}}{\lambda_{0}})}{\frac{1}{(\sum x_{i}/n)^{n}}exp( - n)} = \left( \frac{\sum x_{i}}{n\lambda_{0}} \right)^{n}e^{n}e^{- \sum x_{i}/\lambda_{0}}$
(모수 $\lambda$의 MLE는 $\overset{¯}{x}$ 이기 때문이다)

기각역 :
${\lambda = \left( \frac{\sum x_{i}}{n\lambda_{0}} \right)}^{n}e^{n}e^{- \sum x_{i}/\lambda_{0}} \leq k \Leftrightarrow {\lambda = \left( \frac{\sum x_{i}}{\lambda_{0}} \right)}^{n}e^{- \frac{\sum x_{i}}{\lambda_{0}}} \leq k^{*}$,
$RR(\overline{x}) = \{\overline{x};\left( \frac{\sum x_{i}}{\lambda_{0}} \right)^{n}e^{- \frac{\sum x_{i}}{\lambda_{0}}} \leq c\}$

그러므로 채택역은
$A(\theta) = \{\overline{x};\left( \frac{\sum x_{i}}{\lambda} \right)^{n}e^{- \frac{\sum x_{i}}{\lambda}} \geq c\}$
이고 충분통계량 $\sum x_{i}$의 함수이므로 신뢰구간은 다음과 같다.
$C\left( \sum x_{i} \right) = \{\lambda;L\left( \sum x_{i} \right) < \lambda < U\left( \sum x_{i} \right)\}$

$\left( \frac{\sum x_{i}}{L\left( \sum x_{i} \right)} \right)^{n}e^{- \frac{\sum x_{i}}{L\left( \sum x_{i} \right)}}$,
$\left( \frac{\sum x_{i}}{U\left( \sum x_{i} \right)} \right)^{n}e^{- \frac{\sum x_{i}}{U\left( \sum x_{i} \right)}}$
=\> 만약
$a = \frac{\sum x_{i}}{L\left( \sum x_{i} \right)},b = \frac{\sum x_{i}}{U\left( \sum x_{i} \right)}$라
놓으면 $a^{n}e^{- a}$, $be^{- b}$

$\sum x_{i} \sim Gamma(n,\lambda)$ 이므로
$\frac{\sum x_{i}}{\lambda} \sim Gamma(n,1)$ 다음을 만족하는 $(a,b)$를
찾으면 $100(1 - \alpha)\%$ 신뢰구간이다.

$$P_{\lambda}\left( \frac{\sum x_{i}}{a} \leq \lambda \leq \frac{\sum x_{i}}{b} \right) = P_{\lambda}\left( b \leq \frac{\sum x_{i}}{\lambda} \leq a \right) = 1 - \alpha$$

【주축통계량】

통계량 $Q\left( \overline{x},\theta \right)$의 분포가 모수와 독립이면
이를 주축 pivotal 통계량이라 한다. 즉, $X \sim F(x)$이면
$Q\left( \overline{x},\theta \right)$ 확률밀도함수는 모수 $\theta$에
상관없이 동일하다. 일반적으로 주축 통계량은 충분 통계량의 함수이다.

  ----------------------------------------------- ---------------------- ----------------------------------
                   확률분포 형식                           타입                     주축 통계량

                  $$f(x - \mu)$$                           위치                $$\overline{x} - \mu$$

      $$\frac{1}{\sigma}f(\frac{x}{\sigma})$$              크기              $$\frac{\overline{x}}{S}$$

   $$\frac{1}{\sigma}f(\frac{x - \mu}{\sigma})$$        위치-크기         $$\frac{\overline{x} - \mu}{S}$$
  ----------------------------------------------- ---------------------- ----------------------------------

【예제】 모집단 $f(x;\theta) \sim exponetial(\theta = \lambda)$에서 표본
크기 $n$인 확률표본
$\overline{X} = \left( X_{1},X_{2},\ldots,X_{n} \right)$ 추출하였다.
모수 $\lambda$(평균)에 대한 $100(1 - \alpha)\%$ 신뢰구간을 구하라.

지수분포는 지수족이므로 $T = \sum x_{i}$는 충분 통계량이고
$T = \sum x_{i} \sim Gamma(n,\lambda))$이다.$\sum x_{i}/\lambda \sim Gamma(1,\lambda))$이
주축 통계량이다. 그러므로 $100(1 - \alpha)\%$ 신뢰구간은 다음을 만족하는
$(a,b)$이다.
$P_{\lambda}\left( a \leq \frac{\sum x_{i}}{\lambda} \leq b \right) = 1 - \alpha$.

【예제】
$f(x;\theta) \sim N\left( \theta = \mu,\sigma^{2} \right),where\sigma^{2}isknown$에서
표본 크기 $n$인 확률표본 $\left( X_{1},X_{2},\ldots,X_{n} \right)$
추출하였다. 모수 $\mu$(평균)에 대한 $100(1 - \alpha)\%$ 신뢰구간을
구하라.

정규분포는 위치-크기 모수 타입이므로 주축 통계량은
$\frac{\overline{x} - \mu}{S}$ 형태이다. 만약 분산 $\sigma^{2}$을 알고
있다면
$P\left( a \leq \frac{\overline{x} - \mu}{\frac{\sigma}{\sqrt{n}}} \sim N(0,1) \leq b \right) = 1 - \alpha$
=\>
$(\overline{x} - z_{1 - \frac{\alpha}{2}}\frac{\sigma}{\sqrt{n}},\overline{x} + z_{1 - \frac{\alpha}{2}}\frac{\sigma}{\sqrt{n}})$

만약 분산 $\sigma^{2}$을 모른다면
$P\left( a \leq \frac{\overline{x} - \mu}{\frac{s}{\sqrt{n}}} \sim t(n - 1) \leq b \right) = 1 - \alpha$

=\>
$(\overline{x} - t_{1 - \frac{\alpha}{2},n - 1}\frac{s}{\sqrt{n}},\overline{x} + t_{1 - \frac{\alpha}{2},n - 1}\frac{s}{\sqrt{n}})$

2정규분포 가정 모형

모집단 $f(x;\theta)$에서 확률표본
$\overline{X} = \left( X_{1},X_{2},\ldots,X_{n} \right)$의 제곱 변환
$X_{i}^{2}$의 확률분포함수에 대하여 살펴보자. 모집단
$f(x;\theta) \sim N(0,1)$이면 $X^{2}$의 분포는 $\chi^{2}(1)$을 따른다.

【cochran theorem】

모집단 $f(x;\theta) \sim N(\mu,\sigma^{2})$에서의 확률표본
$\overline{X} = \left( X_{1},X_{2},\ldots,X_{n} \right)$ 의 제곱 형태
(이차형식, quadratic form) $Q_{i}^{2},fori = 1,2,\ldots,k$ 확률변수와
${Q = \sum Q}_{i}^{2}$ 확률변수에 대하여 다음이 성립한다.

1)  $Q_{1},Q_{2},\ldots,Q_{k}$는 서로 독립이다.

<!-- -->

5)  $\frac{Q_{k}}{\sigma^{2}} \sim \chi^{2}(r_{k})$을 갖는다.

6)  $\frac{Q}{\sigma^{2}} \sim \chi^{2}(\sum r_{i})$을 갖는다.

【예제】 모집단 $f(x;\theta) \sim N\left( \mu,\sigma^{2} \right)$에서
확률표본 $\left( X_{1},X_{2},\ldots,X_{n} \right)$의 분산 $\sigma^{2}$의
MVUE 표본 분산
$S^{2} = \frac{\sum\left( x_{i} - \overset{¯}{x} \right)^{2}}{n - 1}$ 은
이차형식이다.
$(n - 1)S^{2} = \sum\left( x_{i} - \overset{¯}{x} \right)^{2} \Leftrightarrow \frac{(n - 1)S^{2}}{\sigma^{2}} = \frac{\sum\left( x_{i} - \overset{¯}{x} \right)^{2}}{\sigma^{2}} = \frac{\sum\left( x_{i} - \mu \right)^{2}}{\sigma^{2}} - \frac{\sum\left( \overset{¯}{x} - \mu \right)^{2}}{\sigma^{2}}$

$X_{i} \sim N\left( \mu,\sigma^{2} \right)$ 이므로
$Q_{i} = \frac{\left( X_{i} - \mu \right)^{2}}{\sigma^{2}} \sim \chi^{2}(1) = > \sum Q_{i} = \frac{\sum\left( X_{i} - \mu \right)^{2}}{\sigma^{2}} \sim \chi^{2}(n)$
이다.

$\overset{¯}{X} \sim N\left( \mu,\frac{\sigma^{2}}{n} \right)$
이므로![그림 1](media/image17.png){width="2.6694444444444443in"
height="2.582638888888889in"}
$Q_{1} = \frac{{n\left( \overset{¯}{X} - \mu \right)}^{2}}{\sigma^{2}} \sim \chi^{2}(1$)
이다. 그러므로 코크란 정리에 의해
$\frac{(n - 1)S^{2}}{\sigma^{2}} \sim \chi^{2}(n - 1)$이다.

분산분석

모델 : $X_{ij} = \mu_{i} + e_{ij};i = 1,2,\ldots,a,j = 1,2,\ldots,n$

가정 : $e_{ij} \sim (iid)N(0,\sigma^{2})$

모수 :
$\Omega = \{\left( \mu_{1},\mu_{2},\ldots,\mu_{k},\sigma^{2} \right); - \infty < \mu_{j} < \infty,0 < \sigma^{2} < \infty$

귀무가설 $H_{0}:\mu_{1} = \mu_{2} = \ldots = \mu_{k} = \mu$

귀무가설 모수 :
$\Omega_{0} = \{\left( \mu_{1},\mu_{2},\ldots,\mu_{k},\sigma^{2} \right); - \infty < \mu_{1} = \mu_{2} = \ldots = \mu_{k} = \mu < \infty,0 < \sigma^{2} < \infty$

【표본분산 분할】

분산분석 모형에서 표본분산
$S^{2} = \frac{1}{(an - 1)}\sum_{i}^{}{\sum_{j}^{}\left( x_{ij} - {\overset{¯}{x}}_{..} \right)^{2}}$을
이차형식으로 분할하라. 단,
${\overset{¯}{x}}_{..} = \frac{\sum_{i}^{}{\sum_{j}^{}x_{ij}}}{an},{\overset{¯}{x}}_{i.} = \frac{\sum_{j}^{}x_{ij}}{n}$,
${\overset{¯}{x}}_{.j} = \frac{\sum_{i}^{}x_{ij}}{a}$이다.

${(an - 1)S}^{2} = \sum_{i}^{}{\sum_{j}^{}\left( x_{ij} - {\overset{¯}{x}}_{..} \right)^{2}} = \sum_{i}^{}{\sum_{j}^{}\left( x_{ij} - {\overset{¯}{x}}_{i.} \right)^{2}} + \sum_{i}^{}{\sum_{j}^{}\left( {\overset{¯}{x}}_{i.} - {\overset{¯}{x}}_{..} \right)^{2}}$

$= \sum_{i}^{}{\sum_{j}^{}\left( x_{ij} - {\overset{¯}{x}}_{i.} \right)^{2}} + n\sum_{i}^{}{\left( {\overset{¯}{x}}_{i.} - {\overset{¯}{x}}_{..} \right)^{2} = Q_{1} + Q_{2}}$

${(an - 1)S}^{2} = \sum_{i}^{}{\sum_{j}^{}\left( x_{ij} - {\overset{¯}{x}}_{..} \right)^{2}} = \sum_{i}^{}{\sum_{j}^{}\left( x_{ij} - {\overset{¯}{x}}_{.j} \right)^{2}} + \sum_{i}^{}{\sum_{j}^{}\left( {\overset{¯}{x}}_{.j} - {\overset{¯}{x}}_{..} \right)^{2}}$

$= \sum_{i}^{}{\sum_{j}^{}\left( x_{ij} - {\overset{¯}{x}}_{.j} \right)^{2}} + a\sum_{j}^{}{\left( {\overset{¯}{x}}_{.j} - {\overset{¯}{x}}_{..} \right)^{2} = Q_{3} + Q_{4}}$

$${(an - 1)S}^{2} = a\sum_{j}^{}{\left( {\overset{¯}{x}}_{.j} - {\overset{¯}{x}}_{..} \right)^{2} +}n\sum_{i}^{}{\left( {\overset{¯}{x}}_{i.} - {\overset{¯}{x}}_{..} \right)^{2} +}\sum_{i}^{}{\sum_{j}^{}\left( x_{ij} - {\overset{¯}{x}}_{i.} - {\overset{¯}{x}}_{.j} + {\overset{¯}{x}}_{..} \right)^{2}} = Q_{2} + Q_{3} + Q_{4}$$

【우도비 검정】

분자 우도함수 :
$L\left( \Omega_{0} \right) = \left( \frac{1}{2\pi\sigma} \right)^{an/2}exp( - \frac{1}{2\sigma^{2}}\sum_{i}^{}{\sum_{j}^{}\left( x_{ij} - \mu \right)^{2}})$

$\mu$  MLE :
$\frac{\partial L\left( \Omega_{0} \right)}{\partial\mu} = 0 \Rightarrow \overset{\hat{}}{\mu} = \frac{1}{an}\sum_{i}^{}{\sum_{j}^{}x_{ij} = {\overset{¯}{x}}_{..}}$

$\sigma^{2}$  MLE :
$\frac{\partial L\left( \Omega_{0} \right)}{\partial\sigma^{2}} = 0 \Rightarrow \overset{\hat{}}{\sigma^{2}} = \frac{1}{an}\sum_{i}^{}{\sum_{j}^{}{{(x}_{ij} - {\overset{¯}{x}}_{..})^{2}} = v}$

분모 우도함수 :
$L(\Omega) = \left( \frac{1}{2\pi\sigma} \right)^{an/2}exp( - \frac{1}{2\sigma^{2}}\sum_{i}^{}{\sum_{j}^{}\left( x_{ij} - \mu_{i} \right)^{2}})$

$\mu_{i}$  MLE :
$\frac{\partial L(\Omega)}{\partial\mu_{i}} = 0 \Rightarrow \overset{\hat{}}{\mu_{i}} = \frac{1}{n}\sum_{j}^{}{x_{ij} = {\overset{¯}{x}}_{i.}}$

$\sigma^{2}$  MLE :
$\frac{\partial L(\Omega)}{\partial\sigma^{2}} = 0 \Rightarrow \overset{\hat{}}{\sigma^{2}} = \frac{1}{an}\sum_{i}^{}{\sum_{j}^{}{{(x}_{ij} - {\overset{¯}{x}}_{..})^{2}} = w}$

우도비 :
$\lambda = \frac{L\left( {\overset{\hat{}}{\Omega}}_{0} \right)}{L\left( \overset{\hat{}}{\Omega} \right)} = \left( \frac{\sum_{i}^{}{\sum_{j}^{}\left( x_{ij} - {\overset{¯}{x}}_{i.} \right)^{2}}}{\sum_{i}^{}{\sum_{j}^{}\left( x_{ij} - {\overset{¯}{x}}_{..} \right)^{2}}} \right)^{an/2}$.

가정 $e_{ij} \sim (iid)N(0,\sigma^{2})$에 의해
$X_{ij} \sim (iid)N(\mu_{i},\sigma^{2})$ 이다.

그러므로
$V = \frac{1}{an}\sum_{i}^{}{\sum_{j}^{}{{(x}_{ij} - {\overset{¯}{x}}_{..})^{2}} = \frac{Q}{an}}$
이고
$W = \frac{1}{an}\sum_{i}^{}{\sum_{j}^{}{{(x}_{ij} - {\overset{¯}{x}}_{i.})^{2}} = \frac{Q_{3}}{an}}$
이다.

우도비 :
$\lambda^{2/an} = \frac{Q_{3}}{Q_{3} + Q_{4}} = \frac{1}{1 + Q_{4}/Q_{3}}$

귀무가설 검정 유의수준 $\alpha$ :
$\alpha = P_{H_{0}}\left( \frac{1}{1 + \frac{Q_{4}}{Q_{3}}} \leq \lambda^{\frac{2}{an}} \right) = P_{H_{0}}\left( \frac{Q_{4}/(a - 1)}{Q_{3}/a(n - 1)} \leq c \right)$

$\frac{Q_{4}/(\sigma^{2}(a - 1))}{Q_{3}/(\sigma^{2}a(n - 1))} \sim F(a - 1,a(n - 1))$이다.

【ANOVA table】

+:--------:+:-----------------------------------------------------------------------------------------------:+:------------:+:------------------------------:+:-----------------------:+
| 변동     | 제곱합                                                                                          | 자유도       | 평균제곱합                     | F-통계량                |
+----------+-------------------------------------------------------------------------------------------------+--------------+--------------------------------+-------------------------+
| 집단간   | $$SSB = \sum_{i}^{}{\sum_{j}^{}\left( {\overline{x}}_{i} - {\overset{¯}{x}}_{..} \right)^{2}}$$ | $$a - 1$$    | $$MSB = \frac{SSB}{a - 1}$$    | $$F = \frac{MSB}{MSE}$$ |
| 변동     |                                                                                                 |              |                                |                         |
+----------+-------------------------------------------------------------------------------------------------+--------------+--------------------------------+                         |
| 오차변동 | $$SSE = \sum_{i}^{}{\sum_{j}^{}\left( x_{ij} - {\overline{x}}_{i} \right)^{2}}$$                | $$a(n - 1)$$ | $$MSE = \frac{SSE}{a(n - 1)}$$ |                         |
+----------+-------------------------------------------------------------------------------------------------+--------------+--------------------------------+-------------------------+
| 총변동   | $$SST = \sum_{i}^{}{\sum_{j}^{}\left( x_{ij} - {\overset{¯}{x}}_{..} \right)^{2}}$$             | $$an - 1$$   |                                |                         |
+----------+-------------------------------------------------------------------------------------------------+--------------+--------------------------------+-------------------------+

회귀분석

모델 : $Y_{i} = a + bx_{i} + e_{i};i = 1,2,\ldots,n$

가정 : $e_{i} \sim (iid)N(0,\sigma^{2})$

귀무가설 $H_{0}:\alpha = \beta = 0$![텍스트, 라인, 폰트, 스크린샷이(가)
표시된 사진 자동 생성된
설명](media/image18.png){width="2.775042650918635in"
height="2.033863735783027in"}

【OLS 최소자승추정법】

관측점들을 가장 대표하는 직선 (best fit)을 어떻게 구할 것인가? 데이터
$(x_{i},y_{i})$를 활용하여 점들에 가장 적합한 직선의 회귀계수 $(a,b)$를
추정하고 이를 이용하여 목표변수 추정값을
$\overset{\hat{}}{y_{i}} = \overset{\hat{}}{a} + \overset{\hat{}}{b}x_{i}$
fitted value 구한다. 오차항에 대한 추정값으로
 ${\overset{\hat{}}{e}}_{i} = y_{i} - \overset{\hat{}}{y_{i}}$을
사용하여 직선의 유의성을 검증한다.

$$\min_{(a,b)}{Q = \sum_{i}^{}\left( y_{i} - a - bx_{i} \right)}^{2}$$$$\frac{\partial Q}{\partial a} = - 2\sum\left( y_{i} - a - bx_{i} \right) = 0,\frac{\partial Q}{\partial b} = - 2\sum x_{i}\left( y_{i} - a - bx_{i} \right) = 0$$

$\overset{\hat{}}{b} = \frac{\sum\left( x_{i} - \overset{¯}{x} \right)\left( y_{i} - \overset{¯}{y} \right)}{\sum\left( x_{i} - \overset{¯}{x} \right)^{2}} = \frac{S_{XY}}{S_{XX}}$,
$\overset{\hat{}}{a} = \overset{¯}{y} - \overset{\hat{}}{b}\overset{¯}{x}$

【MLE】

$e_{i} \sim (iid)N(0,\sigma^{2})$ 이므로
$y_{i} \sim (iid)N(a + bx_{i},\sigma^{2})$ 이다.

우도함수 :
$L\left( \overline{x};a,b \right) = \left( \frac{1}{2\pi\sigma} \right)^{\frac{n}{2}}exp( - \frac{{\sum_{i}^{}\left( y_{i} - a - bx_{i} \right)}^{2}}{2\sigma^{2}}) \propto {\sum_{i}^{}\left( y_{i} - a - bx_{i} \right)}^{2}$우도함수를
최대화 하는 것이나
${\sum_{i}^{}\left( y_{i} - a - bx_{i} \right)}^{2}$을 최소화 하는 OLS와
동일하다.

【 OLS is blue 】 markov theorem

회귀계수에 대한 OLS 추정치는 BLUE(Best Linear Unbiased Estimator)이다.
즉 모든 선형, 불편 추정량 중 최소 분산(minimum variance)를 갖는다.
분포함수가 지수족이므로 MLE는 CSS이고 불편추정량이므로 Rao-Blackwell
정리에 의해 MVUE이다.

【 오차 분산 MVUE 추정량 】

$\overset{\hat{}}{\sigma^{2}} = \frac{1}{n - 2}\sum\left( y_{i} - \overset{\hat{}}{a} - \overset{\hat{}}{b}x_{i} \right)^{2} = MSE$.

【 $\overset{\hat{}}{b}$ 샘플링 분포 】

$\overset{\hat{}}{b} = \frac{\sum\left( x_{i} - \overset{¯}{x} \right)\left( y_{i} - \overset{¯}{y} \right)}{\sum\left( x_{i} - \overset{¯}{x} \right)^{2}}$
이므로
$k_{i} = \frac{\left( y_{i} - \overset{¯}{y} \right)}{\sum\left( x_{i} - \overset{¯}{x} \right)^{2}}$
이라 하면 $\overset{\hat{}}{b} = \sum k_{i}y_{i}$ 이다. 그리고
$\sum k_{i} = 0,\sum k_{i}x_{i} = 1,\sum k_{i}^{2} = \frac{1}{\sum\left( x_{i} - \overset{¯}{x} \right)^{2}}$이다.

$y_{i} \sim (iid)N(a + bx_{i},\sigma^{2})$ 이므로 선형 결합
$\overset{\hat{}}{b} = \sum k_{i}y_{i}$도 정규분포를 따른다.

$$E\left( \overset{\hat{}}{b} \right) = E\left( \sum k_{i}y_{i} \right) = \sum k_{i}{E(y}_{i}) = \sum k_{i}(a + bx_{i}) = b$$

$$V\left( \overset{\hat{}}{b} \right) = V\left( \sum k_{i}y_{i} \right) = \sum k_{i}^{2}{V(y}_{i}) = \sum k_{i}^{2}(\sigma^{2}) = \frac{\sigma^{2}}{\sum\left( x_{i} - \overset{¯}{x} \right)^{2}}$$

$\overset{\hat{}}{b} \sim N(b,\frac{\sigma^{2}}{\sum\left( x_{i} - \overset{¯}{x} \right)^{2}})$.
만약 $\sigma^{2}$을 MVUE 추정량
$\frac{(n - 2)\overset{\hat{}}{\sigma^{2}}}{\sigma^{2}} = \frac{\sum\left( y_{i} - \overset{\hat{}}{a} - \overset{\hat{}}{b}x_{i} \right)^{2}}{\sigma^{2}} \sim \chi^{2}(n - 2)$
성립한다.

그러므로
$\frac{\overset{\hat{}}{b} - b}{s\left( \overset{\hat{}}{b} \right)} \sim t(n - 2)$,
$s^{2}\left( \overset{\hat{}}{b} \right) = \frac{MSE}{\sum\left( x_{i} - \overset{¯}{x} \right)^{2}}$이다.

【기울기 $b$ 에 대한 $100(1 - \alpha)\%$ 신뢰구간 】

$(\overset{\hat{}}{b} - t_{1 - \frac{\alpha}{2},(n - 1)}s\left( \overset{\hat{}}{b} \right),\overset{\hat{}}{b} + t_{1 - \frac{\alpha}{2},(n - 1)}s\left( \overset{\hat{}}{b} \right)$,
$s^{2}\left( \overset{\hat{}}{b} \right) = \frac{MSE}{\sum\left( x_{i} - \overset{¯}{x} \right)^{2}}$.

【 절편 $a$ 에 대한 $100(1 - \alpha)\%$ 신뢰구간 】

$(\overset{\hat{}}{a} - t_{1 - \frac{\alpha}{2},(n - 1)}s\left( \overset{\hat{}}{a} \right),\overset{\hat{}}{a} + t_{1 - \frac{\alpha}{2},(n - 1)}s\left( \overset{\hat{}}{a} \right)$,
$s^{2}\left( \overset{\hat{}}{a} \right) = MSE(\frac{1}{n} + \frac{{\overset{¯}{x}}^{2}}{\sum\left( x_{i} - \overset{¯}{x} \right)^{2}}$)

저자 정보

1982\. 성균관대학교 통계학 학사

1985\. 성균관대학교 통계학 석사

1992\. 미국 North Carolina State University 통계학 박사

1993-1995. 전자통신연구원 선임연구원

1995-2026. 한남대학교 통계학과 교수
