권세혁교수 통계 사전 【차원축소】

s[[https://sites.google.com/view/wolfpack61]{.underline}](https://sites.google.com/view/wolfpack61)

Chapter 통계적 차원축소

1\. PCA: 분산 최대화와 재구성 관점

주성분분석 PCA는 고차원 변수 $X \in \mathbb{R}^{n \times p}$를 더 낮은
차원 $Z \in \mathbb{R}^{n \times k}(k \ll p)$로 선형 변환하여 데이터의
핵심 변동 구조를 보존하려는 차원축소 방법이다.

PCA는 원변수들의 선형결합으로 이루어진 새로운 좌표축을 구성하는
방법이며, 그 좌표축은 데이터의 분산을 가장 잘 설명하는 방향들로 정해지는
방식이다.

PCA의 결과는 행렬 형태로 $Z = XW$로 표현되는 구조이며, 여기서
$W \in \mathbb{R}^{p \times k}$는 주성분 적재행렬(loading matrix)이고
Z는 주성분 점수(score) 행렬이다.

분산 최대화 관점의 PCA

자료는 보통 중심화(centered)된 형태로 다루는 것이 기본이다. 각 변수의
평균을 뺀 중심화 행렬을 여전히 X로 표기하면, 공분산 행렬은
$S = \frac{1}{n - 1}X^{\top}X$으로 정의되는 형태이다.

첫 번째 주성분 방향 벡터 $w_{1} \in \mathbb{R}^{p}$는 $Xw_{1}$의 분산이
최대가 되도록 선택되는 벡터이다. 즉
$\max_{\parallel w_{1} \parallel = 1}Var(Xw_{1}) = \max_{\parallel w_{1} \parallel = 1}w_{1}^{\top}Sw_{1}$로
정의되는 최적화 문제의 해가 $w_{1}$인 구조이다. 여기서
$\parallel w_{1} \parallel = 1$ 제약은 축의 크기를 고정하여 문제를
유일하게 만드는 역할을 하는 제약이다. 이 최적화의 해는 S의 가장 큰
고유값 $\lambda_{1}$에 대응하는 고유벡터이며, $w_{1}$이 그 고유벡터가
되는 구조이다.

두 번째 주성분 $w_{2}$는 $\parallel w_{2} \parallel = 1$이고
$w_{2}^{\top}w_{1} = 0$을 만족하면서 $w_{2}^{\top}Sw_{2}$를 최대화하는
벡터로 정의되는 구조이다. 일반적으로 m번째 주성분 방향 $w_{m}$은 앞선
주성분들과 직교하면서 분산을 최대화하는 방향이며, 결과적으로
$w_{1},\ldots,w_{k}$는 S의 상위 k개 고유벡터로 구성되는 구조이다.

주성분 점수는
$z_{im} = x_{i}^{\top}w_{m},i = 1,\ldots,n,m = 1,\ldots,k$로 정의되는
형태이며, 이를 모으면 $Z = XW$가 되는 구조이다. 여기서
$W = \lbrack w_{1},\ldots,w_{k}\rbrack$이다. 각 주성분의 분산은 해당
고유값과 연결되며 $Var(Xw_{m}) = \lambda_{m}$로 해석되는 구조이다.
따라서 \\lambda_m은 m번째 주성분이 설명하는 변동의 크기를 나타내는
수량이다.

재구성(근사) 관점의 PCA

PCA는 분산을 최대화하는 축을 찾는 문제이면서 동시에 원자료를 저차원으로
가장 잘 근사하는 문제로도 해석되는 방법이다. k차원 표현을 이용하여
원자료 X를 $\widehat{X} = ZW^{\top} = XWW^{\top}$로 근사한다고 할 때,
PCA는 다음의 재구성 오차를 최소화하는 W를 제공하는 구조이다.

$\min_{W^{\top}W = I_{k}} \parallel X - XWW^{\top} \parallel_{F}^{2}$,
여기서 $\parallel \cdot \parallel_{F}$는 Frobenius 놈(norm)이며, 행렬
원소 제곱합의 제곱근인 형태이다. 이 관점에서 PCA는 p차원 공간에서
데이터를 k차원 부분공간으로 투영한 뒤 다시 원공간으로 되돌렸을 때의
오차가 최소가 되도록 하는 최적의 부분공간을 찾는 방법인 구조이다. 따라서
k가 커질수록 재구성 오차는 감소하며, k=p이면 재구성 오차가 0이 되는
구조이다.

표준화 여부와 공분산 PCA vs 상관 PCA

PCA는 거리와 분산 구조를 기반으로 하는 방법이므로, 변수의 스케일에 매우
민감한 방법이다. 예를 들어 어떤 변수는 단위가 매우 커서 분산이 크고,
다른 변수는 단위가 작아 분산이 작은 경우가 흔한 구조이다.

공분산 PCA는 중심화된 X로부터 공분산 S를 만들고 그 고유분해로 주성분을
구하는 방식이다. 이 경우 분산이 큰 변수의 방향이 주성분을 지배하기 쉬운
구조이다. 즉 큰 단위를 가진 변수의 변동이 ["]{dir="rtl"}중요한 신호"로
간주되어 주성분에 크게 반영되는 경향이 있는 구조이다.

상관 PCA는 각 변수를 표준화하여 평균 0, 분산 1로 만든 뒤 PCA를 수행하는
방식이다. 표준화된 자료를 X_s라 하면, 상관행렬은
$R = \frac{1}{n - 1}X_{s}^{\top}X_{s}$로 주어지는 형태이며, R의
고유벡터로 주성분 방향을 구하는 구조이다. 이 방식은 변수 스케일 차이를
제거하여, 모든 변수가 동일한 분산 척도에서 주성분에 기여하도록 만드는
방법인 구조이다.

공분산 PCA를 사용하는 것이 타당한 상황은 변수들이 동일 단위이거나 분산
차이 자체가 의미 있는 정보일 때인 구조이다. 상관 PCA를 사용하는 것이
타당한 상황은 단위가 서로 다르거나, 단위 차이가 분석 목적과 무관하며
["]{dir="rtl"}상관 구조" 자체를 보고자 할 때인 구조이다. 실무에서는 서로
단위가 다른 변수가 혼재하는 경우가 많으므로, 상관 PCA가 기본 선택이 되는
경우가 많은 구조이다.

차원 선택 k의 기준: 설명분산, 스크리 플롯, CV

PCA에서 핵심 실무 의사결정은 k, 즉 몇 개의 주성분을 남길지를 정하는
문제이다. 가장 널리 쓰이는 기준은 설명분산 비율이다. S의 고유값을
$\lambda_{1} \geq \lambda_{2} \geq \cdots \geq \lambda_{p}$라 하면,
m번째 주성분의 설명분산 비율은
$\text{PVE}_{m} = \frac{\lambda_{m}}{\sum_{j = 1}^{p}\lambda_{j}}$로
정의되는 형태이다.

누적 설명분산 비율은
$\text{CPVE}(k) = \frac{\sum_{m = 1}^{k}\lambda_{m}}{\sum_{j = 1}^{p}\lambda_{j}}$로
정의되는 형태이며, $\text{CPVE}(k)$가 예를 들어 0.80(80%)를 넘는 최소의
k를 선택하는 방식이 흔히 쓰이는 구조이다. 이 기준은 직관적이지만,
["]{dir="rtl"}설명분산이 크다"가 반드시 ["]{dir="rtl"}예측에 유리하다"를
의미하지는 않는 한계가 있는 기준이다.

스크리 플롯은 고유값 $\lambda_{m}$을 m에 따라 그린 그림에서 꺾이는
지점(elbow)을 찾아 k를 선택하는 방법이다. 고유값이 급격히 감소하다가
완만해지는 지점 이후의 성분은 추가적인 정보가 적고 잡음에 가까울 수
있다는 직관을 활용하는 방식이다. 이 방법은 시각적으로 이해가 쉽지만
꺾이는 지점의 판단이 주관적일 수 있는 한계가 있는 구조이다.

교차검증 CV 기반 선택은 다운스트림 목적을 반영하는 선택 기준이다. 예측이
목적이면, 각 k에 대해 $Z = XW_{k}$로 축소한 뒤 회귀나 분류 모델을
학습하고 검증 성능이 가장 좋은 k를 선택하는 방식이다.

재구성이 목적이면, 훈련 데이터에서 $W_{k}$를 추정한 뒤 검증 데이터
$X_{\text{val}}$에 대해
${\widehat{X}}_{\text{val}} = X_{\text{val}}W_{k}W_{k}^{\top}$를 만들고
재구성 오차
$\parallel X_{\text{val}} - {\widehat{X}}_{\text{val}} \parallel_{F}^{2}$가
작은 k를 선택하는 방식이다.

CV 기반 선택은 실용적이지만, 반드시 훈련 데이터에서만 $W_{k}$를 추정하고
검증 데이터에는 같은 변환을 적용해야 하며, 이를 어기면 데이터 누수가
발생하는 구조이다.

행렬 표현 $Z = XW$와 해석 요소

PCA 결과를 행렬로 정리하면 $Z = XW$라는 표현이 가장 핵심이다. W의 각 열
$w_{m}$은 m번째 주성분의 적재벡터이며, 각 성분의 크기와 부호는
원변수들이 그 주성분에 어떻게 기여하는지를 나타내는 구조이다.

Z의 각 열은 주성분 점수이며, 각 관측치가 해당 주성분 축에서 어디에
위치하는지를 나타내는 좌표인 구조이다. 따라서 해석은 보통 W를 통해
["]{dir="rtl"}축의 의미"를 설명하고, Z를 통해 ["]{dir="rtl"}관측치의
위치와 군집"을 설명하는 방식으로 이루어지는 구조이다.

결론적으로 PCA가 분산을 최대화하는 최적의 선형 축을 제공하는 동시에,
원자료를 저차원 부분공간으로 가장 잘 근사하는 최적화 해석을 갖는
방법이라는 점이다. 또한 표준화 여부는 분석 결과를 크게 바꾸는 핵심
선택이며, 차원 선택 k는 설명분산과 스크리 플롯의 직관을 활용하되 목적에
따라 교차검증 기반 선택으로 보완하는 것이 타당하다는 점으로 정리할 수
있다.

기하학적 해석

이 그림은 PCA가 고차원 데이터 X를 더 낮은 차원 Z로 바꾸는 과정을
기하적으로 보여주는 그림이다. 파란 점들은 원래 공간에서의 관측치들이며,
이 관측치들이 가장 많이 퍼져 있는 방향을 찾으면 그 방향이 첫 번째 주성분
$PC_{1}$이 되는 구조이다.

![붙여넣은 이미지.tiff](media/image1.tif){width="4.5in" height="3.0in"}

$PC_{1}$은 데이터 분산이 최대가 되는 축이므로, 점구름을 가장 잘
["]{dir="rtl"}펴서" 설명하는 방향이라는 의미이다. $PC_{2}$는 $PC_{1}$과
직교하면서 남아 있는 변동을 가장 크게 설명하는 두 번째 축이라는
의미이다.

점에서 아래의 평면으로 내려가는 점선은 각 관측치를 주성분 부분공간으로
직교 투영(projection)하는 과정을 나타내며, 투영된 위치들이 저차원 표현
Z가 되는 구조이다.

즉 $Z = XW$에서 W는 $PC_{1}$, $PC_{2}$ 방향 벡터를 모아 만든 행렬이고,
각 관측치는 그 축들 위에서의 좌표값으로 요약되는 구조이다. 그림의
["]{dir="rtl"}Reconstruction" 표시는 저차원 표현 Z로부터 다시 원공간으로
되돌려 근사 $\widehat{X} = ZW^{\top}$를 만드는 개념을 나타내며, 이때
원점과의 차이가 재구성 오차로 해석되는 구조이다.

요약하면, PCA는 분산이 큰 방향의 축을 선택해 그 축으로 투영하여 차원을
줄이고, 그 결과로 정보 손실은 최소화하면서도 데이터 구조를 간단한
좌표계로 표현하는 방법이라는 내용을 시각적으로 보여주는 그림이다.

2\. 요인분석(FA): 공통요인과 고유요인

요인분석은 다변량 관측변수 X의 공분산 구조를 소수의 잠재변수로
설명하려는 통계적 차원축소 방법이다. PCA가 관측자료의 분산을 최대한
보존하는 선형축을 구성하는 방법이라면, 요인분석은 관측변수들 사이의
상관을 만들어내는 공통 원인을 잠재요인으로 가정하고 그 구조를 추정하는
방법이다. 요인분석에서 핵심은 관측된 변동을 공통요인이 설명하는 부분과
변수마다 고유하게 남는 오차(특수요인)로 분해하는 관점이다.

요인모형의 기본 형태

관측치가 n개이고 변수가 p개인 자료를 $X \in \mathbb{R}^{n \times p}$로
두며, 한 관측치를 열벡터로 표현하면 $x \in \mathbb{R}^{p}$이다.
요인분석의 표준 모형은 다음과 같은 형태이다.

$x = \Lambda f + \varepsilon$, 여기서 $f \in \mathbb{R}^{q}$는 q차원의
잠재요인(공통요인)이고, $\Lambda \in \mathbb{R}^{p \times q}$는
적재행렬(loading matrix)이며, $\varepsilon \in \mathbb{R}^{p}$는
고유오차(특수요인 포함)이다. 보통 $q \ll p$를 가정하여 저차원 잠재구조로
상관을 설명하는 설정이다.

관측치 $i$에 대한 표현은
$x_{i} = \Lambda f_{i} + \varepsilon_{i},i = 1,\ldots,n$인 형태이다.
요인분석의 핵심 가정은 공통요인과 오차의 분산 구조에 관한 가정이다.
대표적으로 다음 가정이 사용되는 구조이다.

$\mathbb{E}(f) = 0,Cov(f) = I_{q}$,
그리고$\mathbb{E}(\varepsilon) = 0,Cov(\varepsilon) = \Psi Cov(f,\varepsilon) = 0$이다.
여기서 $\Psi$는 보통 대각행렬로 가정되며
$\Psi = diag(\psi_{1},\ldots,\psi_{p})$인 형태이다. 이 가정은 변수별
고유오차는 서로 상관이 없고 각 변수 고유의 분산만 가진다는 가정이다.

이때 관측변수의 공분산 행렬 $\Sigma = Cov(x)$는 다음처럼 분해되는
구조이다. $\Sigma = \Lambda\Lambda^{\top} + \Psi$. 이 식이 요인분석의
핵심 결과이다. $\Lambda\Lambda^{\top}$는 공통요인이 만들어내는 공분산
구조이고, $\Psi$는 변수별 고유분산을 모은 부분이다.

공통성, 고유성, 요인적재의 해석

요인분석에서는 각 변수 $x_{j}$의 분산이 공통요인이 설명하는 부분과
고유오차가 설명하는 부분으로 분해되는 구조이다. 위 분해에서 대각 원소를
보면
$Var(x_{j}) = \overset{q}{\sum_{m = 1}}\lambda_{jm}^{2} + \psi_{j}$인
형태이다. 여기서 $\lambda_{jm}$은 $\Lambda(j,m)$원소이다.

공통성(communality)은 변수 $x_{j}$의 분산 중 공통요인이 설명하는
비중이며 $h_{j}^{2} = \overset{q}{\sum_{m = 1}}\lambda_{jm}^{2}$로
정의되는 구조이다.

고유성(uniqueness)은 변수 $x_{j}$에만 남는 분산이며
$\psi_{j} = Var(\varepsilon_{j})$로 표현되는 구조이다.

요인적재 $\lambda_{jm}$는 요인 $f_{m}$이 변수 $x_{j}$에 미치는 영향의
크기를 나타내는 계수로 해석되는 경우가 많다. 다만 요인의 부호, 회전 등에
의해 적재행렬의 표현은 여러 형태로 나타날 수 있으므로, 해석은 보통 회전
이후의 패턴을 중심으로 이루어지는 구조이다.

PCA와의 차이: 목적, 가정, 오차모형

요인분석과 PCA는 모두 선형 결합을 이용한 차원축소로 보일 수 있으나,
목적과 모형 가정이 근본적으로 다른 방법이다.

첫째, 목적의 차이이다. PCA는 관측자료 X의 분산을 최대한 설명하는 축을
찾는 것이 목적이다. 요인분석은 공분산 구조를
$\Lambda\Lambda^{\top} + \Psi$로 설명하여 변수들 사이의 상관을
만들어내는 공통 원인을 추정하는 것이 목적이다. 따라서 PCA는 분산 보존과
재구성 정확도에 초점이 맞고, 요인분석은 공통요인에 의한 구조적 설명에
초점이 맞는 방식이다.

둘째, 가정의 차이이다. PCA는 확률모형이 없어도 정의되는 방법이다. 표본
공분산 행렬의 고유분해 또는 SVD로 계산되는 대수적 절차이다. 요인분석은
명시적 확률모형을 전제로 하는 방법이다. 특히 $Cov(\varepsilon) = \Psi$가
대각이라는 구조적 가정이 핵심이며, 이 가정이 성립할 때
$\Lambda\Lambda^{\top}$가 ["]{dir="rtl"}공통" 구조로 해석되는 방식이다.

셋째, 오차모형의 차이이다. PCA는 X를 저차원 부분공간으로 투영할 때의
재구성 오차를 최소화하는 성격이 강하며, 오차는 주로 근사 관점에서
등장하는 구조이다. 요인분석은 $\varepsilon$가 명시적으로 모형에
포함되며, 각 변수마다 고유오차 분산 $\psi_{j}$를 추정하는 구조이다. 즉
요인분석은 변수별로 ["]{dir="rtl"}설명되지 않는 분산"을 따로 모델링하는
방식이다.

이 차이는 고유값이 큰 방향을 단순히 남기는 것이 항상 좋은 요인구조를
의미하지 않는다는 점으로 연결되는 구조이다. PCA는 총분산을 기준으로 축을
뽑는 반면, 요인분석은 공통분산의 구조를 우선적으로 설명하려는 방식이기
때문이다.

식별 문제와 회전

요인분석에는 식별 문제가 존재하는 구조이다. 그 이유는 $\Lambda f$에서
$\Lambda$와 $f$를 동시에 바꾸어도 $\Lambda f$가 동일하게 유지되는 변환이
존재하기 때문이다. 예를 들어 $q \times q$ 직교행렬 T에 대해
$\Lambda f = (\Lambda T)(T^{\top}f)$가 성립하는 구조이다.

따라서 $\Lambda$ 자체는 유일하지 않으며, 같은
$\Sigma = \Lambda\Lambda^{\top} + \Psi$를 만드는 $\Lambda$가 여러 개
존재할 수 있다. 이 때문에 요인분석에서는 보통 $Cov(f) = I$ 같은 정규화와
더불어, 해석 가능한 구조를 얻기 위해 회전(rotation)을 사용하는 방식이다.

대표적인 회전 기준은 직교회전인 varimax가 있으며, 요인들 간 상관을
허용하는 사각회전(oblique rotation)도 사용되는 방식이다. 회전은 적합도를
바꾸기보다 해석의 단순성을 높이는 절차로 이해되는 구조이다.

추정과 요인 수 선택의 개요

요인분석의 추정은 대개 표본 공분산 S를 관측된 공분산으로 보고,
$\Sigma = \Lambda\Lambda^{\top} + \Psi$가 $S$에 가깝도록
$\Lambda,\Psi$를 추정하는 방식이다. 대표적인 추정 방법은 최대우도추정,
주축요인법(principal axis factoring) 등이 알려져 있는 구조이다.
최대우도추정에서는 정규성 가정 하에서 로그우도를 최대화하여
$\Lambda,\Psi$를 구하는 방식이다.

요인 수 q의 선택은 차원축소의 핵심 결정이다. 실무에서는 설명 목적과 해석
가능성을 함께 고려하며, 적합도 지표와 잔차 공분산의 크기, 요인해석의
안정성 등을 종합해 결정하는 방식이다. 요인 수가 너무 작으면 공통구조를
충분히 설명하지 못하고, 너무 크면 해석이 복잡해지고 과적합적 구조가 생길
수 있는 위험이 존재하는 구조이다.

요인점수와 주성분 점수

요인점수는 요인분석 모형 $x = \Lambda f + \varepsilon$에서 관측된
x로부터 잠재요인 f를 추정한 값이라는 의미를 갖는 점수이다. 주성분 점수는
PCA에서 정해진 주성분 방향 W로 자료를 투영하여 얻는 $z = x^{\top}w$의
값이라는 의미를 갖는 점수이다. 두 점수는 모두 관측치를 저차원 좌표로
표현한다는 점에서 형태는 유사하지만, 생성 원리와 계산 방식, 해석의
기준이 서로 다른 점수이다.

첫째, 주성분 점수는 W가 고유분해로 결정되면 $Z = XW$로 유일하게 계산되는
점수이다. 요인점수는 f가 관측되지 않으므로 어떤 규칙으로 f를 추정해야
하며, 보통 $\widehat{F} = XA$ㅊ형태의 선형추정을 사용하고 A의 선택에
따라 서로 다른 점수가 나오는 구조이다. 대표적으로 회귀법, Bartlett 방법,
Thomson 방법 등 서로 다른 산출 방식이 존재하는 구조이다.

둘째, 요인점수는 실제 분석 보고나 해석 단계에서 ["]{dir="rtl"}개별
관측치의 연속형 점수"로만 사용되는 것이 아니라, 적재(부하) 패턴을 이용해
관측변수들을 요인별로 배타적으로 묶고 그 그룹의 평균을 요인 수준의
대용치로 사용하는 방식이 함께 활용되는 구조이다.

즉 요인 m에 대해 적재값 $\lambda_{jm}$가 큰 변수들만 골라 하나의
묶음으로 두고, 다른 요인에는 포함시키지 않는 방식으로 요인별 지표를
구성한 뒤, 관측치 i에 대해 그 묶음에 속한 변수들의 표준화 값 평균을
$s_{im} = \frac{1}{|G_{m}|}\sum_{j \in G_{m}}x_{ij}^{*}$와 같이 계산하여
요인점수처럼 사용하는 접근이 존재하는 구조이다. 여기서 $G_{m}$은 요인
m에 배정된 변수 집합이고, $x_{ij}^{*}$는 표준화된 관측값인 형태이다.
경우에 따라 평균 대신 적재값을 가중치로 사용하여
$s_{im} = \frac{\sum_{j \in G_{m}}\lambda_{jm}x_{ij}^{*}}{\sum_{j \in G_{m}}|\lambda_{jm}|}$처럼
계산하는 방식도 사용되는 구조이다.

이 방식은 요인점수 추정치를 직접 산출하지 않더라도 해석 가능한 요인별
지표를 쉽게 만들 수 있다는 장점이 있는 구조이다. 다만 교차적재가 큰
변수들이 존재하면 배타적 그룹화가 임의적이 될 수 있고, 요인모형이
내포하는 $\Psi$나 추정오차를 충분히 반영하지 못할 수 있다는 한계가 있는
구조이다.

셋째, 목적과 해석이 다르다. 주성분 점수는 총분산을 가장 잘 설명하는
축에서의 좌표이며 재구성 관점에서 $\widehat{X} = ZW^{\top}$의 오차를
최소화하는 의미가 강한 점수이다. 요인점수는 공통요인의 수준을 나타내는
값으로 해석되며, 추정점수를 사용하든 부하 기반 그룹 평균을 사용하든
["]{dir="rtl"}공통구조를 대표하는 잠재특성의 정도"를 요약한다는 목적을
갖는 점수이다.

정리하면, 주성분 점수는 $Z = XW$로 유일하게 계산되는 투영 좌표인
점수이다. 요인점수는 잠재요인 f의 추정치로서 여러 산출 방식이 가능하며,
실무에서는 적재값을 기준으로 변수를 요인별로 배타적으로 묶어 그룹
평균이나 가중평균으로 요인 지표를 구성해 요인점수처럼 사용하는 방식도
일반적으로 활용되는 점수이다.

3\. 선형 판별 기반 축소: LDA는 [']{dir="rtl"}차원축소[']{dir="rtl"}인가?

선형판별분석 LDA는 분류 문제에서 집단 간 분리를 극대화하는 선형 변환을
찾는 방법이다. LDA는 결과적으로 원변수 $X \in \mathbb{R}^{n \times p}$를
더 낮은 차원 $Z \in \mathbb{R}^{n \times k}$로 바꾸는 형태를 가지므로
차원축소처럼 보이는 방법이다.

그러나 LDA는 비지도 차원축소인 PCA와 달리, 반응변수 y의 클래스 정보를
사용하여 ["]{dir="rtl"}분류에 유리한 방향"을 찾는 지도 학습 기반의
표현학습이라는 점에서 목적과 의미가 다른 방법이다. 따라서 LDA를
차원축소로 볼 수 있는지의 질문은 ["]{dir="rtl"}차원을 줄이는 연산이
존재하는가"가 아니라 ["]{dir="rtl"}무엇을 보존하기 위해 줄이는가"의
관점에서 답해야 하는 질문이다.

LDA의 핵심 목적과 변환 형태

클래스가 K개인 분류 문제에서, $y \in \{ 1,\ldots,K\}$이고 설명변수
벡터가 $x \in \mathbb{R}^{p}$인 상황을 고려한다. LDA는 각 클래스의
평균벡터를 $\mu_{k}$, 공통 공분산을 $\Sigma$로 가정하는 정규모형 기반
분류 규칙으로도 이해되지만, 여기서는 선형 변환 관점이 핵심이다.

LDA는 선형 투영 $z = W^{\top}x$의 형태로 p차원을 k차원으로 줄이는 변환을
구성하는 방법이다. 여기서 $W \in \mathbb{R}^{p \times k}$의 열벡터들이
판별축(discriminant directions)이며, 이 축 위에서 클래스들이 가장 잘
분리되도록 W를 선택하는 구조이다.

산포행렬과 ["]{dir="rtl"}분리 최대화" 기준

LDA의 차원축소는 클래스 내 변동을 작게, 클래스 간 변동을 크게 만드는
방향을 찾는 최적화 문제로 정리되는 구조이다. 전체 평균을 $\mu$라 하면,
클래스 내 산포행렬(within-class scatter)은
$S_{W} = \overset{K}{\sum_{k = 1}}\sum_{i \in C_{k}}(x_{i} - \mu_{k})(x_{i} - \mu_{k})^{\top}$로
정의되는 형태이다. 클래스 간 산포행렬(between-class scatter)은
$S_{B} = \overset{K}{\sum_{k = 1}}n_{k}(\mu_{k} - \mu)(\mu_{k} - \mu)^{\top}$로
정의되는 형태이다. 여기서 $n_{k}$는 k번째 클래스의 표본수이다.

한 개의 축 $w \in \mathbb{R}^{p}$로 투영할 때, LDA는 다음 비율을
최대화하는 방향을 찾는 구조이다.
$\max_{w \neq 0}\frac{w^{\top}S_{B}w}{w^{\top}S_{W}w}$. 이 목적함수는
투영된 공간에서 클래스 평균 간 거리를 키우되, 클래스 내 분산을 작게
만드는 방향을 선택한다는 의미이다.

다차원 축 $W = \lbrack w_{1},\ldots,w_{k}\rbrack$의 경우에도 유사하게
일반화되며, 해는 일반화 고유값 문제 $S_{B}w = \lambda S_{W}w$를 풀어
얻는 구조이다.

LDA가 줄일 수 있는 최대 차원

LDA는 원칙적으로 K개의 클래스를 (K-1)차원 공간에서 완전히 분리할 수 있는
정보만 갖는 구조이다. 이유는 S_B의 랭크가 최대 (K-1)이기 때문이다.
따라서 LDA로 얻을 수 있는 판별축의 수는 $k \leq \min(p,K - 1)$인
구조이다.

예를 들어 클래스가 3개이면 최대 2개의 판별축만 의미가 있는 구조이다. 이
성질 때문에 LDA는 ["]{dir="rtl"}항상 p에서 원하는 임의의 k로 줄이는
일반적 차원축소"라기보다, ["]{dir="rtl"}클래스 수가 허용하는 범위 안에서
분류에 필요한 좌표로 바꾸는 축소"라는 성격이 강한 구조이다.

PCA와의 차이와 차원축소로서의 위치

PCA는 반응변수 없이 데이터의 총분산을 가장 잘 보존하는 축을 찾는 비지도
방법이다. LDA는 반응변수 y를 이용하여 클래스 분리를 최대화하는 축을 찾는
지도 방법이다.

PCA의 변환은 $Z = XW_{\text{PCA}}$형태이며, $W_{\text{PCA}}$는 S의
고유벡터로 결정되는 구조이다. LDA의 변환은
$Z = XW_{\text{LDA}}$형태이지만, $W_{\text{LDA}}$는 $S_{W}^{- 1}S_{B}$의
구조를 반영하는 판별축이라는 점에서 완전히 다른 기준으로 정해지는
구조이다.

따라서 LDA를 차원축소라고 부를 수 있는 이유는 ["]{dir="rtl"}실제로
$z = W^{\top}x$ 형태의 저차원 표현을 만든다"는 점이다. 그러나 LDA를
PCA와 같은 의미의 차원축소로 부르기 어려운 이유는 ["]{dir="rtl"}보존
대상이 분산이 아니라 분류 정보"라는 점이다. 즉 LDA는 차원축소이되, 분류
목적의 지도 표현학습에 속하는 차원축소라는 결론이 자연스러운 구조이다.

수치적 문제와 정규화의 필요성

LDA에서 핵심 계산은 $S_{W}^{- 1}$ 또는 그에 준하는 선형시스템 풀이이다.
그런데 p가 크고 n이 작으면 $S_{W}$가 특이해질 수 있으며, 이 경우 고전적
LDA는 계산이 불가능하거나 불안정해지는 구조이다. 이는 고차원 분류에서
매우 흔한 상황이다.

이 문제를 해결하기 위해 정규화 LDA가 사용되며, 대표적으로
$S_{W}(\gamma) = (1 - \gamma)S_{W} + \gamma I$와 같이 클래스 내
산포행렬을 단위행렬 방향으로 수축시키는 방식이 사용되는 구조이다.

이는 릿지와 유사한 안정화 효과를 주며, 고차원에서의 판별축 추정을
가능하게 만드는 방식이다. 또한 실제 파이프라인에서는 먼저 PCA로 차원을
줄여 S_W의 특이성을 완화한 뒤 LDA를 적용하는 PCA+LDA 조합이 자주
사용되는 구조이다.

언제 LDA를 ["]{dir="rtl"}차원축소"로 사용하는가

LDA는 다음 상황에서 차원축소로 유용한 구조이다. 첫째, 클래스 구조가
분명하고 저차원 투영에서 분리도가 크게 개선되는 상황이다. 둘째, 시각화
목적에서 K-1차원의 판별공간을 이용해 클래스 간 분리를 보여주려는
상황이다.

셋째, 다운스트림 분류기의 입력으로 저차원 판별특징을 만들고자 하는
상황이다. 이때 LDA의 출력 Z는 분류에 최적화된 특징이므로, 단순한 분산
보존 기반 축소보다 분류 성능에 유리할 수 있는 구조이다.

정리

LDA는 클래스 정보를 이용하여 클래스 간 분산을 크게 하고 클래스 내 분산을
작게 만드는 선형 투영을 찾는 방법이다. 이 방법은 $z = W^{\top}x$ 형태의
저차원 표현을 제공하므로 차원축소의 기능을 수행하는 방법이다.

그러나 LDA는 비지도 차원축소가 아니라 지도 학습 기반의 판별
표현학습이며, 얻을 수 있는 축의 수가 최대 (K-1)로 제한된다는 점에서
PCA와 성격이 다른 차원축소이다. 따라서 LDA는
[']{dir="rtl"}차원축소인가[']{dir="rtl"}라는 질문에 대해, 분류 목적의
지도 차원축소라는 의미에서 차원축소로 분류되는 방법이라는 결론이 타당한
구조이다.
