---
title: "차원축소: 개념 및 필요성"
format: html
---


### Chapter 1. 고차원 데이터란?

#### 1\. 고차원 데이터 개념

회귀와 분류를 위한 대부분의 전통적 통계 기법은 관측치의 개수 n이 특징(변수)의 개수 p보다 훨씬 큰 저차원 상황을 전제로 설계된 방법이다.
이는 통계학의 역사 대부분에서 통계를 사용해야 했던 과학적 문제의 상당수가 저차원 문제였기 때문이기도 하다.

예를 들어 한 병원이 흉부 CT 영상을 이용해 폐암 여부를 예측하는 모형을
만든다고 하자. 한 장의 영상은 수많은 픽셀로 구성되며, 각 픽셀 값을
하나의 예측변수로 취급하면 예측변수의 수 p는 수십만에서 수백만 수준이 될
수 있다. 반면 라벨이 확정된 환자 영상은 비용과 판독 인력의 제약 때문에
수백 명 정도만 확보될 수도 있다. 따라서 $p \gg n$인 고차원 상황이 되며,
이 문제는 고차원 문제이다. 여기서 차원이라는 말은 p의 크기를 의미한다.

지난 20년 동안 새로운 기술은 금융, 마케팅, 의학처럼 다양한 분야에서
데이터가 수집되는 방식을 바꾸었다. 이제는 거의 무제한에 가까운 수의 특징
측정치를 수집하는 일이 흔해졌다(p가 매우 큰 상황이다). p가 극도로 커질
수 있는 반면, 관측치 수 n은 비용, 표본 확보 가능성, 또는 다른 제약
때문에 종종 제한된다. 예시는 다음과 같다.

제조업 설비 고장 예측의 예시이다. 한 공장에서는 설비에 부착된 센서로부터
초당 수백에서 수천 개의 신호(진동, 전류, 온도, 음향 등)를 연속적으로
수집하는 상황이다. 이 원시 신호로부터 시간 구간별 통계량, 주파수 영역
특징, 웨이블릿 계수, 상호상관 지표 등 파생변수를 생성하면 설비 한 대의
한 구간에 대해 특징 수 p가 수만에서 수십만 개까지 커질 수 있는 상황이다.
반면 실제 고장은 드물게 발생하므로 고장 라벨이 확정된 구간의 수 n은 수백
건 수준에 머물 수 있는 상황이다. 따라서 p가 매우 큰데 n은 제한되는
$p \gg n$의 고차원 문제로 정리되는 예시이다.

신용카드 이상거래 탐지 예시이다. 카드사는 한 거래에 대해 가맹점, 위치,
시간, 금액, 단말기 정보, 사용자 행동 로그, 과거 거래 패턴 파생변수 등을
만들면 변수 수 p가 수만 개까지 확장될 수 있다. 그러나 신규 서비스이거나
특정 유형의 사기(라벨)가 희귀하면, 해당 유형에 대해 라벨이 확정된 관측치
n은 수백\~수천 건으로 제한될 수 있다.

텍스트 기반 민원·상담 분류 예시이다. 콜센터나 민원 데이터를 분류하려고
할 때, 단어·형태소·n-gram·문서 임베딩 이전의 희소 표현을 사용하면 특징
수 p는 어휘 수에 비례해 수십만 이상이 될 수 있다. 반면 특정 분야(예:
특정 정책 민원)만 골라 라벨링한 데이터는 비용 때문에 수천 건 수준에 그칠
수 있다. 따라서 n이 제한된 상태에서 p가 매우 커지는 고차원 예시가 된다.

의료 웨어러블 시계열 예시이다. 심전도, PPG, 활동량, 수면 단계 같은 연속
신호에서 시간창별 통계량, 주파수 특징, 비선형 지표를 대량 생성하면
개인당 특징 수 p가 매우 커질 수 있다. 그러나 특정 질환의 확진
사례(라벨)를 가진 환자 수 n은 모집이 어려워 제한될 수 있다. 이 경우도
고차원 상황이 되는 예시이다.

이런 예시들의 공통점은 ["]{dir="rtl"}기술 발전으로 특징 생성은 거의
무제한이 되었지만, 라벨이 확정된 관측치 확보는 비용과 희귀성 때문에
제한된다"는 구조이며, 결과적으로 p\\gg n 문제가 자주 발생하는 상황이라는
점이다.

관측치보다 특징이 더 많은 데이터셋은 흔히 고차원(high-dimensional)이라고
부른다. 최소제곱 선형회귀 같은 고전적 접근은 이런 상황에 적절하지 않다.
고차원 데이터 분석에서 나타나는 많은 이슈는 이 책 앞부분에서 이미 논의한
것들이며, 그 이유는 n\>p인 경우에도 그런 이슈가 존재하기 때문이다.

대표적으로 편향--분산 트레이드오프와 과적합 위험이 있다. 이러한 이슈들은
언제나 관련이 있지만, 특징의 수가 관측치 수에 비해 매우 클 때 특히 더
중요해질 수 있다.

특징의 수 p가 관측치 수 n보다 큰 경우를 고차원 상황으로 정의한다. 다만
p가 n보다 약간 작더라도 아래에서 논의할 고려사항들은 여전히 적용되며,
지도학습을 수행할 때 항상 염두에 두는 것이 바람직하다.

#### 2\. 고차원에서 무엇이 잘못되는가?

$p > n$일 때 회귀와 분류를 위해 추가적인 주의와 특화된 기법이 필요함을
보이기 위해, 고차원 상황을 전제로 설계되지 않은 통계 기법을 적용했을 때
무엇이 잘못되는지를 살펴본다. 이를 위해 최소제곱 회귀를 살펴본다. 다만
동일한 개념은 로지스틱 회귀, 선형판별분석, 그 밖의 고전적 통계 접근에도
적용된다.

특징의 수 p가 관측치 수 n만큼 크거나 더 크면, 최소제곱은 수행할 수
없거나(추정해야 하는 모수의 수보다 방정식의 개수가 적으므로 모수를
추정하려면 모수에 대한 제약식이 필요함) 그런 방식으로 수행하는 것이
부적절하다. 이유는 단순하다. 특징과 반응 사이에 실제 관계가 있든 없든,
최소제곱은 잔차가 0이 되도록 하는 완벽 적합을 만들어내는 계수 추정치를
산출할 수 있기 때문이다.

예를 들어, 하나의 설명변수와 절편을 포함한 단순 선형회귀에서 표본 크기가
한 경우는 관측치가 20개이고 다른 경우는 관측치가 2개뿐인 상황을 비교해
보자.

관측치가 20개인 경우에는 n\>p이므로 최소제곱 회귀선이 모든 점을 정확히
통과할 필요가 없으며, 데이터 전체의 경향을 가장 잘 근사하는 방향으로
직선이 정해지는 상황이다. 이때는 잔차가 남는 것이 정상이며, 회귀선은
개별 관측치를 ["]{dir="rtl"}외우는" 것이 아니라 평균적 관계를 추정하는
역할을 하는 상황이다.

반면 관측치가 2개뿐이면 직선을 결정하는 데 필요한 정보가 최소한만
존재하므로, 두 점을 정확히 통과하는 직선이 항상 존재하고 최소제곱은 그
직선을 선택하게 되는 상황이다. 이때 훈련 데이터에서는 잔차가 0이 되어
완벽 적합이 발생하지만, 이는 두 점에 포함된 우연한 변동까지 그대로
따라간 결과이므로 일반화 성능이 매우 불안정해지는 상황이다.

따라서 훈련 데이터를 완벽히 맞출 수 있다는 사실은 좋은 모형의 증거가
아니며, 특히 $p > n$이거나 $p \approx n$인 상황에서는 단순한
선형모형조차 훈련 데이터에 과도하게 맞춰져 독립적인 데이터에서 성능이
급격히 나빠질 가능성이 커지는 상황이다.

최소제곱 회귀를 고차원 상황에 가깝게 적용할 때 훈련 적합도 지표가 얼마나
쉽게 오도될 수 있는지를 알아보자. 관측치 수를 n=20으로 고정한 뒤,
반응변수와 전혀 관련이 없는 예측변수를 1개부터 20개까지 점차 늘려가며
회귀모형을 적합하는 상황을 생각한다.

이때 변수를 추가할수록 모형의 자유도가 증가하므로, 최소제곱은 훈련
데이터에 대해 점점 더 유연하게 적합할 수 있게 된다. 그 결과 반응과
무관한 변수만 추가했음에도 불구하고 훈련 데이터에서의 $R^{2}$는 계속
증가하며 결국 1에 가까워지고, 훈련 MSE는 계속 감소하여 0에 가까워지는
현상이 나타나는 상황이다. 즉 훈련 데이터만 놓고 보면
["]{dir="rtl"}변수를 많이 넣을수록 더 좋은 모형"처럼 보이는 상황이
만들어진다.

그러나 독립적인 테스트셋에서 성능을 평가하면 정반대의 결과가 나타나는
상황이다. 무관한 변수를 계속 추가하면 계수 추정이 불필요하게 복잡해지고,
표본의 우연한 요동까지 설명하려는 방향으로 계수가 맞춰지면서 추정의
분산이 크게 증가하는 상황이 된다. 그 결과 테스트셋 MSE는 변수 수가
늘수록 오히려 커지며, 일반화 성능이 악화되는 현상이 나타나는 상황이다.
따라서 테스트 성능의 관점에서 보면 최선의 모형은 많아야 몇 개의 변수만
포함하는 단순한 모형인 경우가 많다는 결론으로 이어지는 상황이다.

이 예가 주는 핵심 메시지는 훈련 $R^{2}$나 훈련 MSE는 고차원 또는
고유연성 상황에서 거의 언제든지 좋아질 수 있으므로, 그것만으로 모형의
우수성을 판단하면 쉽게 잘못된 결론에 도달한다는 점이다. 따라서 변수가
많거나 모형이 유연한 상황일수록, 독립 테스트셋 성능 또는 교차검증 오차를
기준으로 모형을 평가하고 선택해야 한다는 결론이 되는 설명이다.

#### 3\. 고차원 데이터와 과적합

고차원 데이터와 과적합의 관계는 p가 커질수록 모형이 ["]{dir="rtl"}훈련
데이터를 맞추는 방법"을 너무 많이 갖게 되어, 신호뿐 아니라 잡음까지
학습하기 쉬워진다는 관계이다. 여기서 고차원은 보통 $p \approx n$ 또는
$p > n$ 인 상황을 의미하는 경우가 많다.

첫째, $p \approx n$ 또는 $p > n$이면 훈련 데이터 완벽 적합이 쉬워지는
구조이다

선형회귀 $y = X\beta + \varepsilon$에서 최소제곱은
$\widehat{\beta} = (X^{\top}X)^{- 1}X^{\top}y$ 형태로 정의되지만,
$p \geq n$이면 $X^{\top}X$가 특이해져 역행렬이 존재하지 않거나, 잔차를
0으로 만드는 해가 여러 개 존재할 수 있는 구조이다. 이때 훈련 오차(RSS,
훈련 MSE)는 0에 가깝게 만들 수 있지만, 이것은 훈련 데이터의 우연한
변동까지 설명한 결과이므로 일반화 성능이 나빠질 가능성이 커지는
구조이다.

둘째, 잡음 변수가 많아질수록 ["]{dir="rtl"}우연한 상관"이 늘어나는
구조이다 고차원에서는 실제로는 반응과 무관한 변수라도, 표본이 유한하면
어떤 변수는 우연히 반응과 상관이 있어 보일 수 있다. 변수가 많아질수록
이런 우연한 상관을 가진 변수가 나타날 확률이 커지고, 모형은 그 변수를
신호로 오인해 계수를 부여하게 되는 구조이다. 훈련에서는 성능이
좋아지지만 테스트에서는 재현되지 않아 성능이 떨어지는 전형적 과적합이
발생하는 구조이다.

셋째, 다중공선성과 분산 폭증이 과적합을 강화하는 구조이다 고차원에서는
변수 간 상관이 필연적으로 커지기 쉬우며 $X^{\top}X$의 조건수가 커지는
경향이 있다. 그러면
$Var(\widehat{\beta}) = \sigma^{2}(X^{\top}X)^{- 1}$에서 보듯 계수 추정
분산이 커지고, 작은 데이터 변화가 예측을 크게 흔드는 불안정성이 증가하는
구조이다. 이 불안정성은 테스트 성능 악화로 이어지기 쉬운 구조이다.

넷째, 편향--분산 관점에서 과적합은 ["]{dir="rtl"}분산이 너무 큰
상태"이다. 고차원에서 복잡한 모형을 쓰면 편향은 줄어들 수 있지만 분산이
크게 증가한다. 결과적으로 테스트 오차는
$\mathbb{E}\lbrack(y - \widehat{f}(x))^{2}\rbrack = \text{편향}^{2} + \text{분산} + \sigma^{2}$에서
분산 항이 커져 증가하는 구조이다. 고차원일수록 정규화나 차원축소가
필요한 이유가 여기에 있는 구조이다.

마지막으로, 해결 방향은 ["]{dir="rtl"}유효 차원"을 줄이는 것이다.
고차원에서 과적합을 줄이는 핵심은 모형을 덜 유연하게 만드는 것이다.
대표적으로 라쏘·릿지 같은 정규화, 변수 선택, PCA/주성분회귀 같은
차원축소, 또는 교차검증 기반의 튜닝이 사용되는 구조이다. 특히 평가를
훈련 적합도(R\^2, 훈련 MSE)가 아니라 교차검증/테스트 성능으로 해야
과적합을 피할 수 있는 구조이다.

정리하면, 고차원 데이터에서는 훈련 데이터를 잘 맞추는 것이 너무
쉬워지고, 우연한 패턴을 신호로 학습할 가능성이 커지며, 다중공선성으로
추정이 불안정해지기 때문에 과적합 위험이 급격히 커지는 구조이다.

#### 4\. 고차원에서의 회귀

전진 단계적 선택, 릿지 회귀, 라쏘, 주성분회귀는 p가 크고 n이 제한된
고차원 상황에서 회귀를 수행할 때 특히 유용한 방법이다. 이 방법들의
공통점은 최소제곱처럼 모든 변수를 자유롭게 사용하여 훈련 데이터를 최대한
맞추는 방식이 아니라, 변수 수를 줄이거나 계수를 수축시키거나 저차원
부분공간에서만 적합하도록 만들어 모형의 유연성을 의도적으로 낮춘다는
점이다. 이 ["]{dir="rtl"}유연성 감소"가 과적합을 줄이고 일반화 성능을
확보하는 핵심 장치가 된다.

라쏘의 성능을 설명하기 위해 다음과 같은 모의 상황을 생각한다. 훈련
관측치는 n=100개이며, 후보 특징의 수 p는 20, 50, 2,000으로 달라지는 세
가지 경우를 고려한다. 중요한 설정은 p개의 특징 중 실제로 반응과 관련된
신호 특징은 20개뿐이고, 나머지는 반응과 무관한 잡음 특징이라는 점이다.
이때 라쏘를 적합하고, 예측 성능은 훈련 데이터가 아닌 독립적인 테스트셋의
평균제곱오차 MSE로 평가한다고 하자.

이 실험에서 먼저 나타나는 현상은 p가 커질수록 테스트셋 오차가 증가하는
경향이다. 이유는 단순히 ["]{dir="rtl"}변수가 많아진다"가 아니라, 그 중
상당수가 신호가 아닌 잡음일 때 잡음이 학습 과정에 끼어들어 과적합을
유발하기 때문이다. 훈련셋에서는 우연한 상관 때문에 잡음 변수에도 계수가
배정될 수 있고, 이는 테스트셋에서는 재현되지 않아 오차 증가로 이어지는
구조이다.

두 번째로 중요한 현상은 최적의 정규화 강도 $\lambda$가 p에 따라
달라진다는 점이다. p=20처럼 변수 수가 비교적 작고 대부분이 신호에
가깝다면, 강한 수축이 필요하지 않아 작은 $\lambda$에서 좋은 성능이 나올
수 있다. 반면 p=50처럼 잡음 변수가 더 섞이기 시작하면, 불필요한 변수를
억제하기 위해 더 큰 $\lambda$가 필요해지는 경향이 있다. p가 더 커지면
커질수록, 즉 후보 변수가 폭증할수록 ["]{dir="rtl"}아무 제약 없이
학습하는 것"은 매우 위험해지므로 정규화의 역할이 더 중요해지는 구조이다.
라쏘에서는 $\lambda$가 커질수록 0이 아닌 계수의 수가 줄어드는 경향이
있으며, 이를 자유도로 해석하면 ["]{dir="rtl"}실제로 남겨서 쓰는 변수
개수"가 줄어드는 것으로 이해되는 구조이다.

세 번째로 p=2,000처럼 변수가 극단적으로 많고 그중 신호가 20개뿐인
상황에서는 라쏘가 정규화를 하더라도 성능이 전반적으로 좋지 않을 수 있다.
이는 표본 크기 n=100이 너무 작아, 거대한 후보 공간에서 신호 변수 20개를
안정적으로 찾아내기 어렵기 때문인 구조이다. 즉 정규화가 과적합을 줄이는
데 도움은 되지만, ["]{dir="rtl"}신호 대비 잡음이 압도적으로 큰
상황"에서는 정규화만으로 충분하지 않을 수 있다는 의미이다.

이 설명이 강조하는 핵심 요지는 세 가지로 정리된다. 첫째, 고차원에서는
정규화나 수축이 필수적인 안정화 장치라는 점이다. 둘째, 좋은 예측 성능을
위해서는 \\lambda 같은 튜닝 파라미터를 교차검증 등으로 적절히 선택해야
한다는 점이다. 셋째, 추가된 특징이 실제로 신호가 아니라 잡음이라면, 차원
p가 커질수록 테스트 오차가 증가하는 경향이 나타난다는 점이다.

마지막 세 번째 요지는 고차원 분석의 핵심 원리로서 차원의 저주라는
이름으로 요약된다. 변수를 많이 모으면 정보가 늘어 성능이 좋아질 것이라고
기대하기 쉽지만, 실제로는 ["]{dir="rtl"}유의미한 신호 특징을 추가할
때만" 성능이 개선되는 구조이다. 반대로 잡음 특징을 추가하면 모형이 훈련
데이터의 우연한 패턴까지 학습하게 되어 분산이 커지고, 그 결과 테스트셋
오차가 증가하는 구조가 된다. 따라서 대규모 특징 수집 기술은 문제에
관련된 신호를 충분히 포함하면 강력한 무기가 되지만, 관련 없는 특징이
대량으로 포함되면 오히려 성능을 악화시키는 양날의 검이라는 결론으로
정리되는 설명이다.

#### 5\. 고차원에서 결과 해석하기

고차원 상황에서 라쏘, 릿지 회귀 또는 다른 회귀 절차를 수행할 때에는,
얻어진 결과를 보고하는 방식에 특히 주의해야 한다. 고차원 상황에서
다중공선성 문제는 극단적이다. 모형에 포함된 어떤 변수든 모형 안의 다른
모든 변수들의 선형결합으로 표현될 수 있기 때문이다.

본질적으로 이는 결과를 진정으로 예측하는 변수가 정확히 무엇인지(있다면
무엇인지)를 결코 확실하게 알 수 없고, 회귀에 사용할
[']{dir="rtl"}최선의[']{dir="rtl"} 계수를 결코 식별할 수 없음을
의미한다. 우리가 할 수 있는 최선은 결과를 진정으로 예측하는 변수들과
상관된 변수들에 큰 회귀계수를 부여하는 정도를 기대하는 것이다.

예를 들어 50만 개의 SNP를 바탕으로 혈압을 예측하려고 하며, 전진 단계적
선택이 그중 17개의 SNP가 훈련 데이터에서 좋은 예측모형을 만든다고 알려
주었다고 하자. 이때 이 17개의 SNP가 모형에 포함되지 않은 다른 SNP들보다
혈압을 더 효과적으로 예측한다고 결론내리는 것은 옳지 않다.

선택된 모형만큼이나 혈압을 잘 예측하는 17개 SNP의 다른 조합이 많이
존재할 가능성이 크다. 독립 데이터셋을 얻어 그 데이터셋에서 전진 단계적
선택을 수행하면, 서로 다른 SNP 집합을 포함하는 모형을 얻을 가능성이
크며, 심지어 선택된 SNP들과 전혀 겹치지 않는 집합을 얻을 수도 있다.

그렇다고 해서 얻어진 모형의 가치가 사라지는 것은 아니다. 예를 들어 그
모형이 독립 환자 집합에서 혈압을 매우 효과적으로 예측하여 임상적으로
유용할 수도 있다. 그러나 얻어진 결과를 과장하지 않도록 주의해야 하며,
우리가 식별한 것은 혈압 예측을 위한 가능한 많은 모형 중 하나일 뿐이고,
독립 데이터셋에서 추가 검증이 필요함을 명확히 해야 한다.

또한 고차원 상황에서 오류나 적합도 측도를 보고할 때에도 특히 신중해야
한다. p\>n이면 잔차가 0인 쓸모없는 모형을 쉽게 얻을 수 있음을 이미
보았다. 따라서 고차원 상황에서 훈련 데이터에 대한 좋은 적합의 증거로
제곱오차합, p-값, $R^{2}$ 통계량, 또는 다른 전통적 훈련 적합도 지표를
절대로 사용해서는 안 된다.

예를 들어 $p > n$이면 $R^{2} = 1$인 모형을 쉽게 얻을 수 있다. 이 사실을
보고하면 통계적으로 타당하고 유용한 모형을 얻었다고 다른 사람들을 오도할
수 있지만, 실제로 이것은 설득력 있는 모형이라는 증거를 전혀 제공하지
않는다.

대신 독립 테스트셋에서의 결과 또는 교차검증 오차를 보고하는 것이
중요하다. 예를 들어 독립 테스트셋에서의 MSE나 $R^{2}$는 모형 적합의
타당한 척도이지만, 훈련셋에서의 MSE는 그렇지 않다.

### Chapter 2. 차원축소의 필요성

차원축소는 고차원 데이터에서 정보를 요약하여 더 안정적인 추정과 더 나은
일반화를 얻기 위한 방법론이다. 관측 데이터 행렬을
$X \in \mathbb{R}^{n \times p}$라 할 때, 차원축소는 p개의 원변수를
$k \ll p$개의 표현 $Z \in \mathbb{R}^{n \times k}$로 바꾸는 과정이다.

선형 차원축소에서는 보통 $Z = XW$형태로 나타나며, 여기서
$W \in \mathbb{R}^{p \times k}$는 투영 행렬이다. 이 절은 차원축소가
필요한 이유를 다중공선성, 차원의 저주, 계산 안정성, 그리고 시각화·노이즈
제거·일반화라는 네 가지 관점에서 설명한다.

#### 1\. 다중공선성: 왜 추정이 불안정해지는가

다중공선성은 설명변수들 사이의 강한 선형 의존성이 추정의 분산을 키우고
계수 해석을 불안정하게 만드는 현상이다. 선형회귀 모형이
$y = X\beta + \varepsilon$로 주어질 때, 최소제곱추정량은
$\widehat{\beta} = (X^{\top}X)^{- 1}X^{\top}y$로 정의되는 형태이다.

이때 $X^{\top}X$가 거의 특이행렬에 가까우면 $(X^{\top}X)^{- 1}$이 매우
큰 값을 가지게 되어, 표본의 작은 변화가 $\widehat{\beta}$의 큰 변화로
증폭되는 구조이다. 이는
$Var(\widehat{\beta}) = \sigma^{2}(X^{\top}X)^{- 1}$ 관계에서 직접
확인되는 사실이다.

즉 $X^{\top}X$의 역행렬이 커지는 방향으로 불안정해지는 순간, 추정량의
분산이 커지고 신뢰구간이 넓어지며 계수 부호가 쉽게 바뀌는 현상이
나타나는 구조이다.

다중공선성이 심한 경우, 예측 자체는 그럭저럭 유지되더라도
["]{dir="rtl"}어떤 변수가 얼마나 중요한가"라는 해석이 붕괴하는 경향이
강한 구조이다. 이는 $X\beta$는 유사하게 유지되지만 $\beta$의 분해가
유일하지 않거나 수치적으로 민감해지기 때문인 구조이다.

차원축소는 상관이 강한 변수들이 공유하는 변동을 몇 개의 축으로 묶어
$Z = XW$로 재표현하는 방식으로, 사실상 중복 정보를 제거하고 문제의 유효
차원을 낮추는 접근이다. 이 과정은 $(X^{\top}X)$의 불안정한 방향을
제거하거나 약화시켜, 추정량의 분산을 줄이는 방향으로 작동하는 구조이다.

#### 2\. 차원의 저주: 거리/밀도/표본크기 관점 직관

차원의 저주는 차원이 증가할수록 데이터가 희소해지고 거리와 밀도의 직관이
무너지며 필요한 표본크기가 급격히 증가하는 현상을 의미하는 개념이다.

첫째, 거리 관점에서 고차원에서는 점들 간 거리가 서로 비슷해지는 경향이
나타나는 구조이다. 예를 들어 성분들이 독립이고
$\mathbb{E}(X_{j}) = 0,Var(X_{j}) = 1$인 벡터
$X = (X_{1},\ldots,X_{p})$에 대해
$\parallel X \parallel^{2} = \overset{p}{\sum_{j = 1}}X_{j}^{2}$는
평균이 $p$이고 분산이 $2p$인 크기로 커지는 형태이다.

따라서 $\parallel X \parallel$는 대략 $\sqrt{p}$ 근처에 집중되는 경향이
강해지는 구조이다. 이 집중 현상은 서로 다른 점들 사이의 거리도
비슷해지게 하여, 최근접 이웃과 최원거리 이웃의 차이가 상대적으로
작아지는 문제로 이어지는 구조이다. 거리 기반 방법론에서
["]{dir="rtl"}가깝다"라는 개념이 약해지는 이유가 여기에 있는 구조이다.

둘째, 밀도 관점에서 고차원에서는 공간을 ["]{dir="rtl"}충분히 촘촘하게"
채우기 위해 필요한 표본수가 폭발하는 구조이다. 각 차원을 동일 간격으로
$m$등분하여 공간을 커버한다고 가정하면, 필요한 격자점 수는 $m^{p}$가
되는 형태이다.

$p$가 커질수록 표본 요구량이 기하급수적으로 증가하는 것이 차원의 저주의
핵심 특징이다. 이는 비모수 밀도추정이나 국소적 구조를 사용하는 방법론이
고차원에서 급격히 어려워지는 이유이다.

셋째, 기하학적 관점에서도 고차원에서는 ["]{dir="rtl"}중심부"가 사라지는
현상이 나타나는 구조이다. $\lbrack - 1,1\rbrack^{p}$ 하이퍼큐브의 부피가
$2^{p}$인 반면, 반지름 1 하이퍼스피어의 부피는
$V_{p} = \pi^{\frac{p}{2}}/\Gamma(p/2 + 1)$인 형태이다.

부피 비율 $\frac{V_{p}}{2^{p}}$는 $p$가 커질수록 빠르게 0에 가까워지는
구조이다. 즉 차원이 커질수록 구 형태의 ["]{dir="rtl"}중심" 영역이 전체
공간에서 차지하는 비중이 급격히 작아지는 구조이다. 차원축소는 데이터가
실제로는 저차원 구조에 가깝다는 가정 아래, 학습이 이루어지는 공간을
$k \ll p$로 줄여 희소성과 거리 붕괴 문제를 완화하는 전략이다.

#### 3\. 계산 안정성: 역행렬, 조건수, 수치오차와 정규화의 연결

계산 안정성은 통계적 추정과 학습이 수치적으로 얼마나 신뢰 가능하게
수행되는지를 의미하는 관점이다. 통계 모델링에서는 역행렬, 고유값 분해,
선형연립방정식 풀이와 같은 연산이 빈번히 등장하는 구조이다.

선형회귀의 $(X^{\top}X)^{- 1}$뿐 아니라, 공분산 행렬
$S = \frac{1}{n - 1}X^{\top}X$의 고유값 분해, 가우시안 모형에서
$\Sigma^{- 1},\log|\Sigma|$ 계산 등이 대표적 사례이다.

이때 행렬 $A$의 조건수
$\kappa(A) = \parallel A \parallel \parallel A^{- 1} \parallel$가 크면,
입력의 작은 오차가 출력에 크게 증폭되는 구조이다. 실데이터에는
측정오차와 반올림오차가 항상 존재하는 구조이므로, 조건수가 큰 문제는
본질적으로 불안정해지는 구조이다.

특히 고차원에서는 p가 n에 비해 커지기 쉬우며, $p \geq n$이면
$X^{\top}X$는 특이행렬이 되어 역행렬이 존재하지 않는 구조이다. 이 상황은
최소제곱추정이 정의되지 않거나 무수히 많은 해를 갖는 문제로 연결되는
구조이다.

정규화는 이러한 문제를 완화하는 대표적 방법이다. 예를 들어 릿지 회귀는
${\widehat{\beta}}_{\lambda} = (X^{\top}X + \lambda I)^{- 1}X^{\top}y$로
주어지는 형태이며, $\lambda I$를 더함으로써 고유값이 작은 방향을 부풀려
조건수를 개선하는 구조이다.

차원축소는 정규화와 동일한 목적을 다른 방식으로 달성하는 접근이다. 즉
불안정성을 유발하는 작은 고유값 방향을 제거하거나, $Z = XW$로 문제의
차원을 줄여 이후 연산을 더 작은 공간에서 수행하게 만들어 수치오차
민감도를 낮추는 구조이다.

#### 4\. 시각화·노이즈 제거·일반화: ["]{dir="rtl"}왜 줄이면 더 잘 맞나"

차원축소는 시각화와 탐색적 분석을 가능하게 하는 실용적 도구이기도 하다.
$p$가 큰 데이터는 원공간에서 구조를 보기 어려우므로, $k = 2or3$으로 줄인
표현 Z를 통해 군집, 이상치, 경향성을 확인하는 절차가 유효한 접근이다.
이는 모델 학습 이전에 데이터의 구조적 가설을 세우는 데 중요한 역할을
하는 방식이다.

또한 차원축소는 노이즈 제거의 방법이기도 하다. 관측 변수는 보통 신호와
잡음이 섞여 있는 형태이며, 잡음은 상대적으로 작은 분산 또는 불규칙한
방향으로 나타나는 경향이 있는 구조이다.

PCA 관점에서는 큰 고유값을 갖는 몇 개의 주성분이 주요 신호를 설명하고,
작은 고유값 방향은 잡음에 가까운 경우가 많다는 직관이 성립하는 구조이다.
따라서 상위 k개 성분만 남기면 데이터가 매끄러워지고 모델이 잡음을
따라가지 않게 되는 효과가 발생하는 구조이다.

일반화 성능 측면에서 ["]{dir="rtl"}차원을 줄였더니 오히려 성능이
좋아지는" 현상은 편향-분산 관점으로 설명되는 구조이다. 표현을 Z로 바꾸는
것은 모델의 입력 차원을 줄여 유효 복잡도를 낮추는 조치이다.

이는 분산을 줄이는 방향으로 작동하는 구조이며, 원데이터의 불필요한
변동을 제거하는 과정과 결합되면 테스트 성능이 향상될 수 있는 구조이다.
즉 차원축소는 정보 손실이라는 비용을 지불하지만, 그 대가로 추정의
안정성, 계산 가능성, 노이즈 강건성, 그리고 일반화 성능을 얻는
방법론이라는 결론으로 연결되는 절이다.
