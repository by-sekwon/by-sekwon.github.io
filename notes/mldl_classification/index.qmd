---
title: "AI·ML 방법론: 분류문제"
---

##### [머신·딥러닝 예측방법] 섹션 메인(AI·ML 분류문제 방법론)

이 섹션에서는 분류(classification) 를 “통계학적 분류모형의 확장”으로 이해하는 관점을 다룬다. 분류는 목표변수가 범주형(이진/다범주) 일 때 관측치 x로부터 클래스 y를 예측하는 문제이며, 핵심은 일반화 성능과 오분류 비용을 고려한 의사결정이다.

분류문제는 회귀와 달리 예측값 자체보다 확률 \Pr(Y=k\mid X=x) 또는 점수(score) 를 추정하고, 이를 임계값(threshold)이나 규칙(rule)로 변환해 최종 클래스를 결정하는 구조를 갖는다. 따라서 “잘 맞추는가?”뿐 아니라 “어떤 기준으로 나누는가?”(임계값, 비용, 불균형 데이터 대응)가 모델의 실전 성능을 좌우한다.

⸻

[분류문제: 정의]

분류는 목표변수 Y가 범주형일 때, 입력 X로부터 클래스(라벨)를 예측하는 문제다. 핵심은 단순히 “맞추기”가 아니라 \Pr(Y=k\mid X) 같은 확률 또는 점수를 추정하고, 임계값·비용·불균형 등을 반영해 최종 의사결정을 내리는 것이다. 이 절에서는 분류의 문제 설정, 결정경계, 확률예측과 라벨결정의 연결을 기본 개념으로 정리한다.

⸻

[예측분류–로지스틱회귀]

로지스틱 회귀는 이진 분류에서 가장 표준적인 확률모형으로, 선형예측자 \eta=\beta_0+\beta^\top x를 로짓(link)으로 연결해 \Pr(Y=1\mid X)를 직접 모델링한다. 계수는 오즈비(odds ratio)로 해석 가능해 설명력이 높고, 규제화(L1/L2)로 일반화 성능을 안정화할 수 있다. 이 절에서는 추정(최대우도), 해석, 임계값 선택까지 실전적 관점에서 다룬다.

⸻

[예측분류–판별분석]

판별분석(LDA/QDA)은 클래스별 분포 가정을 통해 분류규칙을 유도하는 방식으로, “생성모형(generative)” 관점에서 분류를 이해하게 해준다. 공분산 가정에 따라 선형(LDA) 또는 곡선(QDA) 결정경계가 만들어지며, 표본 수가 작거나 구조가 단순할 때 강점을 가진다. 이 절에서는 분포 가정과 결정경계의 형태, 로지스틱과의 관계를 연결해 설명한다.

⸻

[예측분류–ML 트리기반]

트리 기반 분류(CART)는 변수를 기준으로 데이터를 단계적으로 분할해 규칙 형태의 분류기를 만든다. 해석이 직관적이고 비선형·상호작용을 자연스럽게 반영하지만, 단일 트리는 과적합 위험이 있어 가지치기나 앙상블로 보완하는 것이 일반적이다. 이 절에서는 분할 기준(지니/엔트로피), 트리의 장단점, 실전 튜닝 포인트를 정리한다.

⸻

[머신러닝 kNN·SVM 이론]

kNN은 “가까운 이웃”의 다수결로 분류하는 대표적 비모수 방법이며, SVM은 마진을 최대화하는 결정경계를 통해 일반화 성능을 확보하는 강력한 분류기다. kNN은 거리·스케일링·차원의 영향을 크게 받고, SVM은 커널을 통해 비선형 경계까지 확장 가능하지만 하이퍼파라미터(C, \gamma) 선택이 중요하다. 이 절에서는 두 방법의 직관(거리 vs 마진)과 데이터 상황별 선택 기준을 중심으로 다룬다.

⸻

[분류모델 평가]

분류 성능 평가는 정확도 하나로 끝나지 않으며, 혼동행렬을 바탕으로 정밀도·재현율·F1을 함께 해석해야 한다. 확률예측의 품질은 ROC–AUC, PR–AUC, 캘리브레이션으로 점검하고, 클래스 불균형이나 오분류 비용이 큰 문제에서는 임계값과 지표 선택이 모델보다 더 중요해질 수 있다. 이 절에서는 목적에 맞는 평가체계 설계와 보고서 해석 틀을 제공한다.

⸻

[머신러닝 이진형 사례]

이진 분류 사례에서는 전처리(결측·스케일링), 학습/검증 분리, 불균형 처리, 임계값 조정까지 “끝까지” 수행하는 흐름이 중요하다. 단순 정확도보다 ROC/PR, 비용기반 의사결정, 오류 사례 분석을 통해 모델의 실제 활용 가능성을 평가한다. 이 절은 대표 이진 분류 파이프라인을 예제로 정리해 재현 가능하게 제시한다.

⸻

[머신러닝 k≥3 사례]

다범주 분류는 클래스 수가 늘면서 오류 구조가 복잡해지고, one-vs-rest, softmax 등 모델링 방식과 지표 선택이 달라진다. 특히 어떤 클래스끼리 혼동되는지(혼동행렬 패턴)를 해석해 특성공학·모델 개선 방향을 찾는 것이 핵심이다. 이 절에서는 다범주 데이터에서의 학습·평가·해석 포인트를 사례로 다룬다.

⸻

[딥러닝 분류 이론]

딥러닝 분류는 표현학습(representation learning)을 통해 특징을 자동으로 추출하고, 마지막 분류 헤드(시그모이드/소프트맥스)로 확률을 출력한다. 손실함수(이진/범주형 크로스엔트로피), 정규화(드롭아웃, 배치정규화), 데이터 증강 등 일반화 전략이 성능을 좌우한다. 이 절에서는 “왜 딥러닝이 분류에서 강한가”를 구조와 학습 관점에서 정리한다.

⸻

[딥러닝 분류(희소성공)사례]

희소성공(positive가 매우 적은) 문제는 정확도가 높아도 실무적으로 쓸모없을 수 있어, PR–AUC·재현율 중심의 목표 설정과 비용민감 학습이 필요하다. 언더/오버샘플링, class weight, focal loss, 임계값 최적화 같은 전략을 비교하며, “성공을 더 잡는” 방향의 모델링을 설계한다. 이 절에서는 불균형 분류의 대표 난제를 딥러닝 파이프라인으로 해결하는 예를 제시한다.

⸻

[딥러닝 분류(이미지)사례]

이미지 분류는 CNN을 통해 국소 패턴을 계층적으로 학습하고, 전이학습(pretrained backbone)으로 적은 데이터에서도 성능을 확보하는 것이 일반적이다. 데이터 증강과 규제화, 학습률 스케줄링 같은 학습 전략이 결과를 크게 좌우하며, 오분류 이미지 분석이 모델 개선의 핵심 단서가 된다. 이 절에서는 전이학습 기반 이미지 분류의 표준 절차를 사례로 정리한다.

⸻

[딥러닝 분류(텍스트)사례]

텍스트 분류는 토큰화→임베딩→시퀀스 인코더(RNN/CNN/Transformer) 또는 평균 임베딩 같은 단순 모델로 문서 표현을 만든 뒤 분류한다. 데이터 규모와 목적에 따라 전처리(불용어, 길이 제한), 사전학습 임베딩/언어모형 활용 여부가 성능과 비용을 결정한다. 이 절에서는 간단한 베이스라인부터 모델 확장까지, 텍스트 분류의 실전 설계를 사례로 다룬다.