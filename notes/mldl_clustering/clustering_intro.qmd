---
title: "군집・비지도학습: 개념 및 군집개수 결정"
format: html
---

### Chapter 1. 군집・비지도학습 개념

비지도학습은 정답 레이블 y 가 주어지지 않은 상태에서 데이터 X 자체가
가진 구조를 찾아내는 학습 패러다임이다. 지도학습이 ["]{dir="rtl"}입력
X로부터 출력 y를 예측"하는 문제라면, 비지도학습은 ["]{dir="rtl"}입력 X의
분포와 기하학적 형태를 이해"하는 문제이다.

이때 [']{dir="rtl"}정답이 없다[']{dir="rtl"}는 말은 임의적이거나
주관적이라는 뜻이 아니라, 문제 자체가 기술적·탐색적 성격을 갖는다는
뜻이다. 따라서 비지도학습의 결과는 어떤 단일한 정답이라기보다, 데이터에
대한 하나의 구조적 제안이다.

비지도학습의 대표적 과업은 (i) 군집화(clustering), (ii)
차원축소(dimensionality reduction), (iii) 밀도추정(density estimation),
(iv) 이상치 탐지(anomaly detection) 로 정리된다.

이 중 군집화는 관측치들을 몇 개의 집단으로 묶어 ["]{dir="rtl"}서로
비슷한 것끼리 묶고, 다른 것은 분리"하는 것을 목표로 한다. 차원축소는
고차원 자료를 저차원 표현으로 바꾸어 시각화·요약·노이즈 제거·후속 분석을
돕는 역할을 한다.

밀도추정은 p(x) 자체를 추정하거나 그 형태를 이해하는 문제이며, 이상치
탐지는 분포의 저밀도 영역에 놓인 관측치 혹은 정상 패턴과 다른 관측치를
찾는 문제이다. 이들 과업은 서로 분리된 것이 아니라 실제로는 서로 깊게
연결되어 있다. 예를 들어, 모델 기반 군집(GMM) 은 밀도추정 관점에서
이해할 수 있으며, PCA 후 군집은 차원축소와 군집이 결합된 전형적
파이프라인이다.

군집의 핵심 질문은 다음과 같이 정리된다.

① 유사성(similarity)을 어떻게 정의할 것인가의 문제이다.

군집은 ["]{dir="rtl"}비슷함"의 형식화에 달려 있으며, 이 비슷함은 대개
거리(distance) 또는 유사도(similarity)로 정의된다. 어떤 거리를 쓰는가에
따라 군집 결과가 달라지는 것은 자연스러운 현상이다.

② 군집의 목표함수(objective)를 무엇으로 둘 것인가의 문제이다.

예를 들어 k-means는 군집 내 제곱거리합을 최소화하는 반면, 계층적 군집은
병합 기준(linkage)에 따라 다른 목적을 암묵적으로 채택한다. 모델 기반
군집은 우도를 최대화한다.

③ 군집 수 K를 어떻게 결정할 것인가의 문제이다.

K는 데이터가 ["]{dir="rtl"}본질적으로 몇 개의 집단으로 나뉜다"는
형이상학적 질문이 아니라, 분석 목적과 단순성-설명력의 균형에 의해
정해지는 선택 문제이다. 이 때문에 Elbow, Silhouette, 정보 기준(AIC/BIC)
등이 사용된다.

④ 군집 결과를 어떻게 해석하고 검증할 것인가의 문제이다.

지도학습처럼 정답과의 오차로 평가할 수 없으므로, 내부 타당도(internal
validity), 안정성(stability), 외부 정보가 있을 때의 외부 타당도(external
validity) 등을 조합하여 평가한다.

이제 군집화 문제를 수학적으로 표현해 보자. 관측치가 n개이고 각 관측치가
p차원 벡터일 때,
$X = \{ x_{1},x_{2},\ldots,x_{n}\},x_{i} \in \mathbb{R}^{p}$이다.
군집화의 결과는 각 관측치에 군집 라벨 $z_{i} \in \{ 1,\ldots,K\}$를
부여하는 것이며, $z = (z_{1},\ldots,z_{n})$로 나타낼 수 있다.

여기서 중요한 점은 z가 관측되지 않았고, 군집 알고리즘이 z를
["]{dir="rtl"}추정"한다는 사실이다. 다만 이
[']{dir="rtl"}추정[']{dir="rtl"}은 지도학습의 정답 회귀·분류와 달리,
정의된 목적함수/가정/거리 하에서 얻어진 최적화 결과이다. 따라서 군집은
통계적으로 다음과 같은 관점을 동시에 갖는다.

기하학적 관점: 데이터 공간에서 거리나 각도에 의해 가까운 점을 묶는
문제이다.

분산 분해 관점: 전체 변동(total variation)을 군집 내 변동(within)과 군집
간 변동(between)으로 나누어 해석하는 문제이다.

확률모형 관점: 데이터가 혼합분포(mixture)에서 생성되었다고 보고
잠재집단(latent class)을 추정하는 문제이다.

### Chapter 2. 군집의 통계적 정의

군집화(clustering)는 관측치 집합 $X = \{ x_{1},\ldots,x_{n}\}$를 K개의
집단으로 분할하는 문제이다. 각 관측치 $x_{i} \in \mathbb{R}^{p}$에 대해
군집 라벨 $z_{i} \in \{ 1,\ldots,K\}$를 부여하며, 군집 k는
$C_{k} = \{ i:z_{i} = k\}$로 정의된다.

군집화는 지도학습처럼 정답 라벨이 존재하지 않으므로,
["]{dir="rtl"}올바른 분류"라는 기준 대신 유사성의 정의와 변동의 분해를
통해 통계적으로 정당화된다. 즉, 군집은 (i) 서로 가까운 관측치를 묶는다는
기하학적 기준과 (ii) 군집 내 변동을 작게, 군집 간 변동을 크게 만든다는
분산 기준을 동시에 포함하는 개념이다.

군집의 통계적 정의는 ["]{dir="rtl"}데이터를 K개의 부분집합으로 분할하여,
군집 내 변동(WSS)을 작게 하고 군집 간 변동(BSS)을 크게 만드는 구조를
찾는 것"이다. 이때 그 출발점은 거리(또는 유사도)의 선택이며, 거리의
선택은 곧 군집 결과의 의미를 규정하는 분석자의 가정이다.

#### 1\. 거리 기반 유사성의 정의

군집의 출발점은 ["]{dir="rtl"}비슷함"을 수학적으로 표현하는 일이다. 이를
위해 가장 흔히 사용하는 도구가 거리(distance) 또는
비유사도(dissimilarity) 이다. 일반적으로 거리함수 $d( \cdot , \cdot )$는
다음 성질을 만족하는 것이 이상적이다.

- 비음수성: $d(x,y) \geq 0$ 이다.
- 동일성: $d(x,y) = 0 \Leftrightarrow x = y$ 이다.
- 대칭성: $d(x,y) = d(y,x)$ 이다.
- 삼각부등식: $d(x,z) \leq d(x,y) + d(y,z)$ 이다.

이 네 조건을 만족하면 d는 척도(metric) 이다. 다만 군집 실무에서는 반드시
metric일 필요는 없고, 목적에 맞는 비유사도를 쓰는 것이 더 중요하다.

##### 유클리드 거리(Euclidean distance)

연속형 변수에서 가장 표준적으로 쓰는 거리는 유클리드 거리이다.
$d_{E}(x_{i},x_{j}) = \sqrt{\overset{p}{\sum_{m = 1}}(x_{im} - x_{jm})^{2}}$.
유클리드 거리는 좌표공간에서의 직선거리이며, k-means의 기본 가정과 가장
잘 맞는다.

그러나 변수 스케일이 다르면 큰 분산을 가진 변수가 거리를 지배하는 문제가
발생한다. 따라서 단위가 다른 변수가 섞인 자료에서는 표준화(예:
z-score)가 사실상 필수이다.

##### 맨해튼 거리(Manhattan distance)

절댓값 합으로 정의되는 거리이다.
$d_{M}(x_{i},x_{j}) = \overset{p}{\sum_{m = 1}}|x_{im} - x_{jm}|$.맨해튼
거리는 좌표축 방향 이동거리이며, 이상치에 대해 유클리드 거리보다 덜
민감한 경우가 많다.

특히 고차원에서 유클리드 거리의 제곱항이 큰 차이를 과장할 수 있는데,
이때 맨해튼 거리가 더 안정적인 대안이 되기도 한다.

##### 민코프스키 거리(Minkowski distance)

유클리드와 맨해튼을 포함하는 일반화된 거리이다.
$d_{q}(x_{i},x_{j}) = \left( \overset{p}{\sum_{m = 1}}|x_{im} - x_{jm}|^{q} \right)^{\frac{1}{q}},q \geq 1$.
q=2이면 유클리드 거리, q=1이면 맨해튼 거리이다. q가 커질수록 큰 차이를
더 강조하는 거리이다.

##### 코사인 거리(Cosine distance)

텍스트 임베딩, 고차원 희소벡터에서 자주 쓰는 유사도 기반 척도이다.
코사인 유사도는 두 벡터의 각도를 이용한다.
$\cos(x_{i},x_{j}) = \frac{x_{i}^{\top}x_{j}}{\parallel x_{i} \parallel \parallel x_{j} \parallel}$.
거리로 쓰려면 보통 $d_{C}(x_{i},x_{j}) = 1 - \cos(x_{i},x_{j})$로
정의한다.

코사인 거리는 크기(magnitude)보다 방향(direction)에 민감하므로, 문서
길이 차이 같은 스케일 요인을 덜 받는다.

##### 마할라노비스 거리(Mahalanobis distance)

변수들 간 상관과 스케일을 동시에 고려하는 거리이다. 공분산행렬을
$\Sigma$라 하면
$d_{Mah}(x_{i},x_{j}) = \sqrt{(x_{i} - x_{j})^{\top}\Sigma^{- 1}(x_{i} - x_{j})}$이다.
유클리드 거리가 ["]{dir="rtl"}구형(spherical)" 등거리 곡면을 가지는 데
비해, 마할라노비스 거리는 공분산 구조에 따라
["]{dir="rtl"}타원형(elliptical)" 등거리 곡면을 가진다.

다만 $\Sigma^{- 1}$ 추정이 불안정한 고차원(p가 크고 n이 작음)에서는
정규화나 차원축소가 필요하다.

#### 2\. 군집의 분할과 지시변수 표현

군집 분할은 지시변수(indicator)로도 표현된다. $h_{ik} \in \{ 0,1\}$를
$h_{ik} = \{\begin{matrix}
1, & z_{i} = k \\
0, & \text{otherwise}
\end{matrix}$로 두면, 각 관측치는 정확히 하나의 군집에 속하므로
$\overset{K}{\sum_{k = 1}}h_{ik} = 1(i = 1,\ldots,n)$이다.

군집 k의 크기는 $n_{k} = \overset{n}{\sum_{i = 1}}h_{ik}$이며, 군집
평균(centroid)은
$\mu_{k} = \frac{1}{n_{k}}\sum_{i:z_{i} = k}x_{i} = \frac{1}{n_{k}}\overset{n}{\sum_{i = 1}}h_{ik}x_{i}$로
정의된다. 이 $\mu_{k}$는 k-means에서 군집을 대표하는 중심으로 사용된다.

#### 3\. Within / Between variance의 통계적 의미

군집이 ["]{dir="rtl"}좋다"는 것은 흔히 다음 두 조건을 의미한다.

같은 군집 내부에서는 관측치가 서로 가깝다(응집도가 크다)

서로 다른 군집 사이에서는 중심이 멀다(분리도가 크다)

이를 통계적으로 표현하는 대표적 방식이 총변동의 분해이다. 전체 평균을
$\overline{x} = \frac{1}{n}\overset{n}{\sum_{i = 1}}x_{i}$라 하자.
그러면 전체 제곱합(Total SS)은
$TSS = \overset{n}{\sum_{i = 1}} \parallel x_{i} - \overline{x} \parallel^{2}$로
정의된다. 여기서 $\parallel \cdot \parallel$는 보통 유클리드 노름이다.

군집 내 제곱합(Within SS)은
$WSS = \overset{K}{\sum_{k = 1}}\sum_{i:z_{i} = k} \parallel x_{i} - \mu_{k} \parallel^{2}$로
정의된다. 이는 각 군집 내부의 산포를 합친 값이며, 작을수록 군집이
조밀하다고 해석한다.

군집 간 제곱합(Between SS)은
$BSS = \overset{K}{\sum_{k = 1}}n_{k} \parallel \mu_{k} - \overline{x} \parallel^{2}$로
정의된다. 이는 군집 중심들이 전체 평균으로부터 얼마나 떨어져 있는지를
군집 크기 $n_{k}$로 가중하여 합친 값이며, 클수록 군집들이 서로 분리되어
있다고 해석한다.

이 세 가지는 다음 관계를 만족한다. $TSS = WSS + BSS$이다. 이는
분산분해의 다변량 버전이며, 군집 평균으로의 제곱거리 분해를 통해
증명된다.

즉, 각 관측치에 대해
$x_{i} - \overline{x} = (x_{i} - \mu_{z_{i}}) + (\mu_{z_{i}} - \overline{x})$로
분해하고 양변의 제곱노름을 합하면 교차항이 0이 되는 구조가 나타나며, 그
결과가 위 등식이다. 이때 교차항이 0이 되는 이유는 각 군집에서
$\sum_{i:z_{i} = k}(x_{i} - \mu_{k}) = 0$ 이기 때문이다.

이 분해는 군집의 목표를 명확히 한다. $TSS$는 데이터가 주어지면 고정된
값이므로, $WSS$를 최소화하는 것은 동시에 $BSS$를 최대화하는 것과
동치이다. 따라서 k-means는 다음 최적화 문제로 정리된다.
$\min_{z_{1},\ldots,z_{n}}\overset{K}{\sum_{k = 1}}\sum_{i:z_{i} = k} \parallel x_{i} - \mu_{k} \parallel^{2}$이며,
이는 ["]{dir="rtl"}군집 내 변동을 최소화"한다는 통계적 정의에 해당한다.

#### 4\. 군집 품질에 대한 해석적 기준

군집은 정답이 없으므로, 군집 품질은 목적함수 값뿐 아니라 해석 가능성까지
포함한다. 통계적 관점에서 기본적인 품질 판단은 다음과 같이 정리된다.

① 응집도(cohesion) 는 WSS와 관련되며, 군집 내부 산포가 작은지로 판단하는
기준이다.

② 분리도(separation) 는 군집 중심 간 거리 또는 BSS와 관련되며, 군집 간
구분이 뚜렷한지로 판단하는 기준이다.

③ 안정성(stability) 는 표본을 약간 바꾸거나(부트스트랩, subsampling)
초기값을 바꾸었을 때 결과가 크게 흔들리지 않는지를 보는 기준이다.

④ 해석가능성(interpretability) 는 군집별 특성이 명확하게 요약되고,
도메인 맥락에서 설득력이 있는지로 판단하는 기준이다.

여기서 특히 중요한 결론은 다음과 같다. 군집은 $WSS$를 극단적으로
줄이려면 K=n으로 잡으면 되므로, 단순히 $WSS$를 최소화하는 것이 목표가
아니다. 군집은 항상 단순성(적은 K) 과 적합도(작은 WSS, 큰 BSS) 사이에서
균형을 선택하는 문제이다. 이 균형의 선택이 바로 군집 수 결정 문제로
이어진다.

### Chapter 3. 군집 개수 결정

군집 수 K의 결정은 군집 분석에서 가장 중요한 선택 문제이다.
지도학습에서는 정답 y가 존재하므로 예측오차를 최소화하는 K를 논할 수
있으나, 군집은 정답이 없으므로 ["]{dir="rtl"}진짜 K"를 찾는 문제가
아니다.

군집 수 결정은 데이터의 변동을 얼마나 세분화하여 설명할 것인지, 즉
단순성(parsimony) 과 적합도(fit) 사이의 균형을 선택하는 문제이다. 이
절에서는 전통적 군집에서 널리 쓰이는 Elbow 방법과 Silhouette 방법,
그리고 모델 기반 군집에서 사용되는 정보 기준(AIC/BIC)을 체계적으로
정리한다.

Elbow는 WSS 감소의 꺾임으로 K를 선택하는 방법이다. Silhouette는 군집 내
응집도와 군집 간 분리도를 동시에 반영하여 평균 실루엣 점수가 큰 K를
선호한다.

정보 기준(AIC/BIC)은 모델 기반 군집에서 로그우도에 복잡도 패널티를
부여하여 K를 선택하는 방법이다. 이들 방법은 서로 보완적이며, 군집은
정답을 찾는 문제가 아니라 구조를 제안하는 문제라는 관점에서 K 역시
["]{dir="rtl"}근거를 갖춘 선택"으로 이해되어야 한다.

#### 1\. Elbow 방법

Elbow 방법은 K가 증가함에 따라 군집 내 변동이 감소하는 양상을 이용하여
["]{dir="rtl"}감소 폭이 급격히 둔화되는 지점"을 K로 선택하는 방법이다.
k-means를 기준으로 설명하면, K개 군집에서의 군집 내 제곱합은
$WSS(K) = \overset{K}{\sum_{k = 1}}\sum_{i:z_{i} = k} \parallel x_{i} - \mu_{k} \parallel^{2}$이다.

K가 커질수록 각 군집이 더 잘게 쪼개지므로 $WSS(K)$는 단조감소한다.
극단적으로 K=n이면 각 점이 하나의 군집이 되어 $WSS(n) = 0$이 된다.
따라서 $WSS(K)$의 절대값만으로는 K를 결정할 수 없으며, 감소 곡선의
["]{dir="rtl"}형태"를 본다.

##### Elbow의 해석

K에 대한 $WSS(K)$ 그래프에서, 초기에 K를 늘리면 군집 내 변동이 크게
줄어들지만 어느 시점 이후부터는 감소 폭이 작아진다. 이때 꺾이는
지점(elbow)은 ["]{dir="rtl"}군집을 더 늘려도 추가 설명력이 크지 않다"는
의미로 해석된다. 즉 Elbow는 다음과 같은 직관을 형식화한 것이다.

- 작은 K: 군집이 거칠어 WSS가 크다.
- 큰 K: 군집이 과도하게 세분화되어 WSS가 작지만 복잡하다.
- 적절한 K: WSS 감소의 효율이 급격히 떨어지기 시작하는 지점이다.

##### Elbow의 한계

Elbow는 시각적 판단에 의존하는 경우가 많아, 꺾임이 불명확하면 결론이
애매해진다. 또한 데이터가 연속적 스펙트럼 구조를 가지면(뚜렷한 군집이
없으면) WSS 곡선이 완만하게 감소하여 뚜렷한 elbow가 나타나지 않을 수
있다. 따라서 Elbow는 단독 기준이라기보다 다른 지표와 함께 사용되는 것이
바람직하다.

##### 표준화와 거리의 영향

Elbow는 거리 기반 목적함수(WSS)에 의존하므로, 표준화 여부와 거리 척도
선택이 \\mathrm{WSS}(K) 곡선 자체를 바꾼다. 따라서 Elbow를 적용할 때는
["]{dir="rtl"}내가 어떤 거리에서 군집을 보고 있는가"를 먼저 고정해야
한다.

#### 2\. Silhouette 방법

Silhouette 방법은 ["]{dir="rtl"}군집 내 응집도(cohesion)"와
["]{dir="rtl"}군집 간 분리도(separation)"를 동시에 반영하는 내부 타당도
지표이다. 각 관측치 i에 대해 다음을 정의한다.

- a(i): 관측치 i가 속한 군집 내부에서 다른 점들과의 평균거리이다.
$a(i) = \frac{1}{|C_{z_{i}}| - 1}\sum_{\begin{array}{r}
j \in C_{z_{i}} \\
j \neq i
\end{array}}d(x_{i},x_{j})$
- b(i): 관측치 i가 속하지 않은 다른 군집들 중, 평균거리가 가장 작은
군집까지의 평균거리이다.
$b(i) = \min_{l \neq z_{i}}\frac{1}{|C_{l}|}\sum_{j \in C_{l}}d(x_{i},x_{j})$

이때 실루엣 계수(silhouette coefficient)는
$s(i) = \frac{b(i) - a(i)}{\max\{ a(i),b(i)\}}$로 정의된다.

##### 값의 범위와 의미

실루엣 값은 \[-1,1\] 범위를 가지며 해석은 다음과 같다.

- $s(i) \approx 1$ 이다: $a(i) \ll b(i)$이므로, 자기 군집 내부에 잘 묶여 있고 다른 군집과도 잘 분리되어 있다.
- $s(i) \approx 0$ 이다: $a(i) \approx b(i)$이므로, 군집 경계에 놓여 있다.
- $s(i) < 0$ 이다: $a(i) > b(i)$이므로, 다른 군집이 더 가까운
["]{dir="rtl"}오배정 가능성"이 있다.

전체 군집 품질은 평균 실루엣 점수로 요약한다.
$S(K) = \frac{1}{n}\overset{n}{\sum_{i = 1}}s(i)$. 그리고 S(K)가 최대가
되는 K를 선택 후보로 삼는다.

##### Silhouette의 장점

Silhouette는 WSS처럼 단순히 ["]{dir="rtl"}군집 내 변동"만 보는 것이
아니라, ["]{dir="rtl"}다른 군집과의 분리"까지 포함한다는 점에서 균형
잡힌 지표이다. 또한 점 단위로 $s(i)$를 볼 수 있어, 어떤 점들이 경계에
있거나 불안정한지를 진단할 수 있다.

##### Silhouette의 한계

Silhouette는 거리 계산에 기반하므로, 거리 척도 선택과 표준화 여부에
민감하다. 또한 비구형 군집이나 밀도 차이가 큰 군집에서는 점들 간
평균거리 기반 요약이 군집 구조를 충분히 반영하지 못할 수 있다. 큰
n에서는 모든 점 쌍 거리 계산이 부담이 될 수 있어 근사적 계산을 쓰기도
한다.

#### 3\. 정보 기준(AIC/BIC)과 군집 수 결정

정보 기준은 모델 기반 군집에서 K를 결정하는 대표적 방법이다. 모델 기반
군집에서는 데이터가 K개의 성분분포의 혼합으로 생성된다고 가정하고, K에
따라 모형의 복잡도(추정 모수 개수)가 달라진다.

이때 단순히 우도(likelihood)를 최대화하면 K가 커질수록 유리해지므로,
우도에 복잡도 패널티를 부과한 기준이 필요하다. 그 대표가 AIC와 BIC이다.

##### 로그우도와 모수 개수

GMM에서 K가 주어졌을 때 최대우도추정치 \\hat{\\Theta}\_K로 계산한
로그우도는
$\ell({\widehat{\Theta}}_{K}) = \overset{n}{\sum_{i = 1}}\log\left( \overset{K}{\sum_{k = 1}}{\widehat{\pi}}_{k}\mathcal{N}(x_{i} \mid {\widehat{\mu}}_{k},{\widehat{\Sigma}}_{k}) \right)$이다.
또한 모수의 개수 $m_{K}$는 K, 차원 p, 공분산 구조(자유/대각/공유)에 따라
달라진다.

예를 들어 공분산이 군집별 자유(full)인 경우, 평균은 군집당 p개이므로
Kp개이다. 공분산은 군집당 $p(p + 1)/2$개이므로
$K \cdot p(p + 1)/2$개이다. 그리고 혼합계수 $\pi_{k}$는 합이 1이므로
K-1개이다. 따라서 $m_{K} = (K - 1) + Kp + K\frac{p(p + 1)}{2}$이다. 대각
공분산이면 공분산 모수는 군집당 p개로 줄어든다.

##### AIC

AIC는 다음과 같이 정의된다.
$AIC(K) = - 2\ell({\widehat{\Theta}}_{K}) + 2m_{K}$. AIC는
적합도(로그우도)를 좋게 하되(첫 항), 모수 개수가 많아지는 것을
패널티(둘째 항)로 억제한다. AIC는 상대적으로 복잡한 모형을 선택하는
경향이 있으며, 예측 관점에서 유리한 경우가 있다.

##### BIC

BIC는 다음과 같이 정의된다.
$BIC(K) = - 2\ell({\widehat{\Theta}}_{K}) + m_{K}\log n$. BIC는 n이
커질수록 패널티가 강해지므로, AIC보다 단순한 모형을 선택하는 경향이
있다. 군집 수 선택에서는 BIC가 널리 사용되며, 특히 혼합모형에서
["]{dir="rtl"}군집 수를 과도하게 키우는" 문제를 억제하는 데 유리하다.

##### 정보 기준의 해석

AIC/BIC는 절대값 자체보다 ["]{dir="rtl"}K에 따른 비교"가 핵심이며, 값이
작은 K가 선호된다.
$\widehat{K} = \arg\min_{K}AIC(K),\widehat{K} = \arg\min_{K}BIC(K)$이다.
다만 혼합모형은 국소해 문제가 있으므로, 각 K마다 EM을 여러 초기값에서
실행하고 가장 큰 로그우도를 사용하는 것이 기본이다.

#### 4\. 실무적 결론: 단일 기준이 아닌 ["]{dir="rtl"}근거의 조합"이다

군집 수 결정은 보통 하나의 지표로 끝나지 않는다. 다음과 같은 조합이
일반적이다. k-means/거리 기반 군집에서는 Elbow로 ["]{dir="rtl"}감소 효율의 변화"를 보고, Silhouette로 ["]{dir="rtl"}응집-분리 균형"을 확인한다.

모델 기반 군집에서는 BIC(AIC)로 ["]{dir="rtl"}적합도-복잡도 균형"을
평가하고, 소속확률(책임도) 분포로 군집 경계의 불확실성을 점검한다.

최종 선택은 분석 목적(해석 단위, 정책 타깃 수, 운영 가능 군집 수)과
일관되어야 한다. 즉, 군집 수 K는 자연상수처럼 ["]{dir="rtl"}발견"되는 값이 아니라, 데이터와 목적에 근거하여 설득력 있게 선택되는 값이다.
