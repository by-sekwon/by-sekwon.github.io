---
title: "군집・비지도학습: 방법론"
format: html
---

### Chapter 2. 전통적 군집모형

전통적 군집모형은 데이터의 잠재 확률모형을 명시적으로 가정하기보다,
거리(또는 비유사도) 와 최적화 기준에 기반하여 관측치를 분할하는
방법이다. 여기서는 대표적인 거리 기반 군집인 k-means와 계층적 군집을
중심으로 정리하고, 군집 결과를 좌우하는 거리 척도의 선택 원리를 함께
서술한다.

전통적 군집모형의 공통점은 (i) 유사성의 정의가 알고리즘의 본질을
결정한다는 점이며, (ii) 군집의 결과가 하나의 ["]{dir="rtl"}정답"이
아니라 선택된 기준(거리, linkage, 표준화)에 대한 구조적 제안이라는
점이다.

전통적 군집모형은 거리 기반 최적화 또는 병합 규칙에 의해 군집을 구성하는
방법이다. k-means는 군집 내 제곱거리합을 최소화하는 분산 기반 방법이며,
계층적 군집은 덴드로그램을 통해 계층적 구조를 제공하는 방법이다.

이들 방법에서 핵심은 거리 척도와 표준화의 선택이며, 이는 곧 군집이
의미하는 ["]{dir="rtl"}유사성"의 정의이다. 따라서 전통적 군집 결과는
정답이 아니라, 선택된 거리와 규칙 하에서 도출된 구조적 제안이다.

#### 1\. k-means 군집

k-means는 K개의 군집을 가정하고, 각 군집의 중심(centroid)으로부터의 군집
내 제곱거리합(WSS) 을 최소화하는 방법이다. 데이터
$X = \{ x_{1},\ldots,x_{n}\},x_{i} \in \mathbb{R}^{p}$에 대해 군집 할당
$z_{i} \in \{ 1,\ldots,K\}$와 군집 중심 $\mu_{k}$를 사용하면 목적함수는
다음과 같이 정의된다.

$\min_{z_{1},\ldots,z_{n},\mu_{1},\ldots,\mu_{K}}\overset{K}{\sum_{k = 1}}\sum_{i:z_{i} = k} \parallel x_{i} - \mu_{k} \parallel^{2}$.
여기서 $\parallel \cdot \parallel$는 보통 유클리드 노름이다. k-means는
$TSS = WSS + BSS$분해에 의해 $WSS$를 줄이는 것이 곧 $BSS$를 키우는 것과
연결된다는 점에서, 통계적 해석이 명확한 알고리즘이다.

##### 최적해의 구조: 군집 중심은 평균이다

군집 할당 z가 주어졌다고 하자. 이때 군집 k의 목적함수 부분은
$\sum_{i:z_{i} = k} \parallel x_{i} - \mu_{k} \parallel^{2}$이며, 이를
$\mu_{k}$에 대해 최소화하는 해는 군집 내 평균
$\mu_{k} = \frac{1}{n_{k}}\sum_{i:z_{i} = k}x_{i}$이다.

즉 k-means에서 중심은 ["]{dir="rtl"}대표값"으로서 통계적 평균이며, 이는
제곱오차 손실의 최적 대표값이 평균이라는 성질과 동일하다.

##### Lloyd 알고리즘(표준 k-means 절차)

k-means의 최적화는 z와 $\mu$를 동시에 찾는 비볼록(non-convex) 문제이므로
전역최적(global optimum)이 보장되지 않는다. 대신 다음 두 단계를 반복하는
방식으로 국소해(local minimum)를 찾는다.

- 할당(Assignment) 단계: 현재 중심 $\mu_{1},\ldots,\mu_{K}$가 주어지면 각 관측치는 가장 가까운 중심에 배정된다.
$z_{i} \leftarrow \arg\min_{k \in \{ 1,\ldots,K\}} \parallel x_{i} - \mu_{k} \parallel^{2}$
- 갱신(Update) 단계: 현재 할당 z가 주어지면 각 중심은 해당 군집의 평균으로
갱신된다. $\mu_{k} \leftarrow \frac{1}{n_{k}}\sum_{i:z_{i} = k}x_{i}$. 이 반복은 매 단계에서 목적함수 $WSS$를 감소시키므로 유한번 내 수렴한다.
다만 수렴이 곧 전역최적을 의미하지는 않으며, 초기값에 따라 결과가 달라질
수 있다.

##### 초기값과 k-means++의 의미

초기 중심을 무작위로 잡으면 좋지 않은 국소해에 빠질 수 있다. 이를
완화하기 위해 k-means++ 초기화는 ["]{dir="rtl"}이미 선택된 중심에서 멀리
떨어진 점"을 중심으로 뽑아 초기 중심들의 분산을 확보한다. 실무적으로는
(i) k-means++ 사용, (ii) 여러 번 재시작(n_init)을 통해 가장 작은 WSS
해를 선택하는 방식이 안정적이다.

##### k-means의 통계적 한계

k-means는 다음의 구조적 가정을 강하게 포함한다.

- 구형(spherical) 군집 가정이다. 유클리드 제곱거리 최소화는 등거리 곡면이
구형이므로, 군집이 타원형이거나 길게 늘어진 형태이면 부적절할 수 있다.
- 동일 분산 및 유사 크기 군집에 유리하다. 큰 군집이 작은 군집을 흡수하는
경향이 나타날 수 있다.
- 이상치(outlier)에 민감하다. 평균은 이상치에 약하므로 중심이 끌려가며
군집이 왜곡된다.
- 스케일에 민감하다. 표준화 여부가 결과를 결정하는 경우가 많다.

이 한계는 모델 기반 군집(GMM)이나 밀도 기반 군집(DBSCAN)으로 확장되는
이유가 된다.

#### 2\. 계층적 군집(Hierarchical clustering)

계층적 군집은 군집의 개수를 미리 고정하지 않고, 데이터 간 유사성을
기반으로 계층적 구조를 구성하는 방법이다.

대표적으로 병합적(agglomerative) 방식이 널리 쓰이며, 이는 처음에 각
관측치를 하나의 군집으로 두고 점차 병합하여 하나의 군집으로 만드는
절차이다. 그 결과는 덴드로그램(dendrogram)이라는 나무 구조로 표현된다.
덴드로그램을 특정 높이에서 자르면 그 높이에 해당하는 군집 개수 K가
결정된다.

##### 병합적 계층군집의 알고리즘

초기 상태에서 n개의 군집이 있고(각 관측치가 하나의 군집), 단계 t마다
가장 가까운 두 군집 A,B를 선택해 병합한다. 이때 ["]{dir="rtl"}군집 간
거리"를 정의하는 방식이 linkage 이다. 병합이 n-1번 수행되면 최종적으로
하나의 군집이 된다.

##### 주요 linkage의 정의와 성질

군집 A,B의 원소를 각각 $x \in A,y \in B$라 하자. 점 간 거리 d(x,y)가
주어졌을 때 linkage는 다음과 같이 정의된다.

- Single linkage(최단거리 연결): $D(A,B) = \min_{x \in A,y \in B}d(x,y)$, 군집 사이의 최소 거리로 병합하며, 사슬처럼 이어지는 chaining 현상이
나타나기 쉽다.
- Complete linkage(최장거리 연결):
$D(A,B) = \max_{x \in A,y \in B}d(x,y)$, 군집 간 최대거리를 기준으로
병합하며, 군집이 비교적 조밀하고 타이트하게 형성되는 경향이 있다.
- Average linkage(평균거리 연결):
$D(A,B) = \frac{1}{|A||B|}\sum_{x \in A}\sum_{y \in B}d(x,y)$, single과
complete의 중간 성격이며, 전반적으로 안정적인 결과를 주는 경우가 많다.
- Ward linkage(분산 증가 최소화): Ward는 거리 자체보다
["]{dir="rtl"}병합했을 때의 WSS 증가량"을 기준으로 한다. 즉,
$\Delta(A,B) = WSS(A \cup B) - WSS(A) - WSS(B)$가 가장 작은 두 군집을
병합한다. Ward는 k-means의 분산 최소화 철학과 유사하며, 유클리드 거리와
결합될 때 특히 해석이 깔끔하다.

##### 계층군집의 장단점

계층적 군집의 장점은 (i) 덴드로그램으로 군집 구조를 시각적으로 해석할 수
있고, (ii) K를 사전에 정하지 않고 사후에 선택할 수 있다는 점이다. 반면
단점은 (i) 계산량이 커서 대규모 데이터에서 부담이 크고, (ii) 한 번
병합된 결정이 이후 단계에서 되돌릴 수 없는 탐욕적(greedy) 구조라는
점이다. 즉 초기에 잘못된 병합이 발생하면 이후 결과가 연쇄적으로 영향을
받는다.

#### 3\. 거리 척도의 선택 원리

전통적 군집에서 거리 척도는 단순한 계산 도구가 아니라,
["]{dir="rtl"}비슷함"에 대한 통계적 가정 그 자체이다. 거리 척도의 선택은
다음 세 요소에 의해 결정된다.

- 자료의 척도(scale)와 분포이다.
- 변수 간 상관 구조이다.
- 분석 목적(형태 vs 방향 vs 순서) 이다.

##### 연속형 변수: 유클리드, 마할라노비스, 민코프스키

유클리드 거리는 변수 간 독립성과 동일 스케일을 암묵적으로 전제한다.
스케일이 다르면 표준화가 필요하다.

마할라노비스 거리는 공분산 $\Sigma$를 반영하므로 상관이 큰 변수들로 인한
["]{dir="rtl"}중복 정보"를 완화한다.
$d_{Mah}(x_{i},x_{j}) = \sqrt{(x_{i} - x_{j})^{\top}\Sigma^{- 1}(x_{i} - x_{j})}$.
다만 $\Sigma^{- 1}$ 추정이 불안정하면 오히려 노이즈가 커지므로,
차원축소(PCA)나 정규화(regularization)가 병행되는 것이 일반적이다.

민코프스키 거리 d_q는 q로 민감도를 조절하는 일반화이다.

##### 방향성이 중요한 경우: 코사인 거리

텍스트 벡터나 임베딩에서는 벡터 크기보다 방향이 유사성에 중요할 때가
많다. 이때 코사인 거리
$d_{C}(x_{i},x_{j}) = 1 - \frac{x_{i}^{\top}x_{j}}{\parallel x_{i} \parallel \parallel x_{j} \parallel}$를
사용하면, 크기 차이에 덜 민감한 군집이 형성된다.

##### 범주형/이진형 변수: 해밍, 자카드

이진 벡터에서 해밍거리는 불일치의 개수를 센다.

자카드 유사도는 ["]{dir="rtl"}둘 다 1인 경우"를 강조하고
["]{dir="rtl"}둘 다 0인 경우"를 덜 중요하게 본다. 희소 이진 데이터(예:
구매 여부, 클릭 여부)에서 유용하다.

##### 표준화의 통계적 의미

표준화는 단순한 전처리가 아니라 ["]{dir="rtl"}각 변수를 동등한 중요도로
보겠다"는 선언이다. 표준화(z-score)는 각 변수를 평균 0, 분산 1로 맞춘다.
이를 통해 거리 계산에서 분산이 큰 변수의 영향력이 과도해지는 현상을
줄인다. 반대로 원자료 스케일을 유지하는 것은 ["]{dir="rtl"}단위 자체가
의미가 있다"는 가정에 해당한다. 즉 표준화 여부는 군집 결과의 해석을
바꾸는 중요한 선택이다.

### Chapter 3. 모델기반 군집

모델 기반 군집(model-based clustering)은 군집을 ["]{dir="rtl"}거리로
묶는 규칙"으로 보지 않고, 데이터가 어떤 확률모형으로부터 생성되었다는
가정 하에 잠재집단(latent class) 을 추정하는 문제로 정의하는 방법이다.

즉 관측치 $x_i$ 는 군집 라벨 $z_i$ 에 의해 조건부분포가 달라지며, 전체 분포는
여러 분포의 혼합(mixture) 으로 표현된다고 보는 관점이다. 이 관점의
장점은 (i) 군집이 확률적 의미를 가지며, (ii) 군집 소속을
["]{dir="rtl"}딱 잘라" 배정하는 hard clustering 대신 소속확률을 제공하는
soft clustering이 가능하고, (iii) 정보기준(AIC/BIC) 등 통계적 모형 선택
도구로 군집 수 K를 정당화할 수 있다는 점이다.

모델 기반 군집은 데이터가 혼합분포에서 생성된다는 가정 하에 잠재집단을
추정하는 방법이다. Gaussian Mixture Model은 정규분포의 혼합으로 전체
분포를 표현하며, EM 알고리즘은 잠재변수의 사후확률(책임도)을 이용해
혼합계수·평균·공분산을 반복적으로 갱신하여 최대우도해를 찾는 절차이다.
이 접근은 군집을 ["]{dir="rtl"}거리 규칙"이 아니라 ["]{dir="rtl"}확률적
생성모형"으로 해석하게 해준다는 점에서 통계적 의미가 분명한 방법이다.

#### 1\. Gaussian Mixture Model(GMM)

Gaussian Mixture Model은 데이터가 K개의 가우시안(정규) 성분분포의
혼합으로 생성된다고 가정하는 모형이다. 관측치
$x_{i} \in \mathbb{R}^{p}$에 대해 혼합분포는 다음과 같이 정의된다.

$p(x_{i}) = \overset{K}{\sum_{k = 1}}\pi_{k}\mathcal{N}(x_{i} \mid \mu_{k},\Sigma_{k})$.
여기서 $\pi_{k}$는 혼합계수(mixing proportion)이며
$\pi_{k} \geq 0,\overset{K}{\sum_{k = 1}}\pi_{k} = 1$을 만족한다.
$\mu_{k} \in \mathbb{R}^{p}$는 k번째 군집의 평균벡터,
$\Sigma_{k} \in \mathbb{R}^{p \times p}$는 공분산행렬이다.

##### 잠재변수 표현

군집 라벨 $z_{i} \in \{ 1,\ldots,K\}$를 잠재변수로 도입하면 생성모형은
다음과 같이 표현된다.
$P(z_{i} = k) = \pi_{k},x_{i} \mid (z_{i} = k) \sim \mathcal{N}(\mu_{k},\Sigma_{k})$

즉, 먼저 군집이 $\pi$에 따라 선택되고, 선택된 군집의 정규분포에서
$x_{i}$가 생성된다고 보는 모형이다. 이때 $z_{i}$는 관측되지 않으므로
추정의 핵심이 된다.

##### k-means와의 연결

k-means는 ["]{dir="rtl"}유클리드 제곱거리" 기준의 분할이며, GMM은
["]{dir="rtl"}정규 혼합의 우도" 기준의 분할이다. 특히 다음의 제한을 두면
GMM은 k-means와 매우 가까워진다.

- 모든 군집이 동일한 구형 공분산을 가진다고 가정한다.
$\Sigma_{k} = \sigma^{2}I_{p}(k = 1,\ldots,K)$
- 혼합계수 $\pi_{k}$가 크기 비례로 추정된다.

이때 $\sigma^{2}$가 작아질수록(분포가 뾰족해질수록) 각 점은 가장 가까운
평균 $\mu_{k}$의 성분에 사실상 배정되며, 군집 중심은 평균으로 추정된다.
따라서 k-means는 ["]{dir="rtl"}구형·동분산 가정의 GMM을 hard하게 근사한
방법"으로 이해할 수 있다. 반대로 GMM은 k-means를 타원형(elliptical)
군집까지 확장한 일반화로 해석할 수 있다.

##### 공분산 구조에 따른 군집 형태

$\Sigma_{k}$의 형태에 따라 군집이 표현할 수 있는 모양이 달라진다.

- $\Sigma_{k} = \sigma^{2}I$ : 구형, 동일 크기 군집이다.
- $\Sigma_{k} = \sigma_{k}^{2}I$ : 구형이지만 군집별 크기(분산) 차이를
허용하는 모형이다.
- $\Sigma_{k} = \Sigma$ : 타원형이지만 모든 군집이 동일한 타원 모양을
공유하는 모형이다.
- $\Sigma_{k}$ 자유 : 군집별로 서로 다른 타원형을 허용하는 가장 유연한
모형이다.

유연할수록 적합도는 좋아지나, 추정해야 할 모수가 급격히 늘어나 과적합
위험이 커진다. 특히 p가 큰 경우 $\Sigma_{k}$의 추정이 불안정해지므로
정규화나 차원축소를 병행하는 것이 일반적이다.

#### 2\. 우도함수와 최대우도추정

관측 데이터 $X = \{ x_{1},\ldots,x_{n}\}$에 대한 모수
$\Theta = \{\pi_{k},\mu_{k},\Sigma_{k}\}_{k = 1}^{K}$의 우도는
$L(\Theta) = \overset{n}{\prod_{i = 1}}\overset{K}{\sum_{k = 1}}\pi_{k}\mathcal{N}(x_{i} \mid \mu_{k},\Sigma_{k})$이며
로그우도는
$\ell(\Theta) = \overset{n}{\sum_{i = 1}}\log\left( \overset{K}{\sum_{k = 1}}\pi_{k}\mathcal{N}(x_{i} \mid \mu_{k},\Sigma_{k}) \right)$이다.
혼합모형의 특징은 로그 안에 합이 존재한다는 점이며, 이 때문에
$\ell(\Theta)$를 직접 미분하여 닫힌형태 해를 얻기가 어렵다. 이 문제를
해결하는 대표적 방법이 EM(Expectation--Maximization) 알고리즘이다.

#### 3\. EM 알고리즘

EM 알고리즘은 관측되지 않은 잠재변수 $Z = \{ z_{i}\}$를 도입하여,
["]{dir="rtl"}완전자료(complete data)"의 로그우도를 반복적으로
최적화하는 방법이다. 핵심 아이디어는 다음과 같다.

관측자료만 보면 최적화가 어렵다(로그-합 구조 때문이다). 그러나 $z_{i}$까지 관측된다고 가정하면(완전자료), 최적화가 매우 쉬워진다.

따라서 현재 모수로 $z_{i}$의 기대값(사후확률)을 계산(E-step)하고, 그
기대값을 가중치로 사용하여 모수를 갱신(M-step)한다.

##### 완전자료 로그우도

지시변수 $z_{ik} = \mathbf{1}(z_{i} = k)$를 사용하면 완전자료의
로그우도는
$\ell_{c}(\Theta) = \overset{n}{\sum_{i = 1}}\overset{K}{\sum_{k = 1}}z_{ik}(\log\pi_{k} + \log\mathcal{N}(x_{i} \mid \mu_{k},\Sigma_{k}))$이다.
이 식은 $z_{ik}$가 주어지면 $\pi_{k},\mu_{k},\Sigma_{k}$에 대해 분리되어
최적화가 가능하다.

##### E-step: 책임도(responsibility) 계산

현재 모수 $\Theta^{(t)}$하에서 관측치 $x_{i}$가 군집 k에 속할 사후확률을
계산한다.
$\gamma_{ik}: = P(z_{i} = k \mid x_{i},\Theta^{(t)}) = \frac{\pi_{k}^{(t)}\mathcal{N}(x_{i} \mid \mu_{k}^{(t)},\Sigma_{k}^{(t)})}{\sum_{j = 1}^{K}\pi_{j}^{(t)}\mathcal{N}(x_{i} \mid \mu_{j}^{(t)},\Sigma_{j}^{(t)})}$.

$\gamma_{ik}$는 ["]{dir="rtl"}관측치 i를 군집 k가 얼마나 책임지는가"라는
의미에서 책임도라 부른다. 이 값이 soft clustering의 핵심 결과이며, hard
clustering은 $\arg\max_{k}\gamma_{ik}$로 얻어진다.

##### M-step: 모수 갱신

E-step에서 얻은 \\gamma\_{ik}를 z\_{ik}의 기대값으로 두고, 기대 완전자료
로그우도를 최대화하는 방식으로 모수를 갱신한다. 우선 유효표본크기를
$N_{k} = \overset{n}{\sum_{i = 1}}\gamma_{ik}$로 정의하면, 갱신식은
다음과 같다.

- 혼합계수: $\pi_{k}^{(t + 1)} = \frac{N_{k}}{n}$
- 평균:
$\mu_{k}^{(t + 1)} = \frac{1}{N_{k}}\overset{n}{\sum_{i = 1}}\gamma_{ik}x_{i}$
- 공분산:
$\Sigma_{k}^{(t + 1)} = \frac{1}{N_{k}}\overset{n}{\sum_{i = 1}}\gamma_{ik}(x_{i} - \mu_{k}^{(t + 1)})(x_{i} - \mu_{k}^{(t + 1)})^{\top}$

이 갱신은 ["]{dir="rtl"}가중 평균/가중 공분산" 형태이며, 가중치가 바로
책임도 $\gamma_{ik}$이다.

##### 수렴 성질과 주의점

EM 알고리즘은 반복할수록 관측자료 로그우도 $\ell(\Theta)$를 감소시키지
않는 성질(단조증가)을 가진다. 즉,
$\ell(\Theta^{(t + 1)}) \geq \ell(\Theta^{(t)})$이다. 그러나 이는
전역최적을 보장하지 않으며, 초기값에 따라 국소최적에 수렴할 수 있다.
따라서 여러 초기값으로 반복하여 가장 큰 로그우도를 선택하는 방식이
권장된다.

또한 혼합모형에서는 다음과 같은 수치적 문제가 발생할 수 있다.

특이해(singularity) 문제이다. 어떤 성분이 한 점에 과도하게 집중되면
$\Sigma_{k} \rightarrow 0$로 가면서 우도가 발산할 수 있다. 이는 공분산에
작은 $\epsilon I$를 더하는 정규화나 최소 공분산 제약으로 완화된다.

고차원 불안정성 문제이다. p가 크면 $\Sigma_{k}$ 추정이 불안정해진다.
이때 PCA로 차원을 줄인 뒤 GMM을 적용하거나, 대각 공분산(diagonal
covariance) 가정 등 단순화를 적용하는 것이 일반적이다.

라벨 스위칭(label switching) 문제이다. 혼합모형은 성분의 순서가
본질적으로 의미가 없으므로(군집 1과 2의 이름을 바꿔도 같은 모형이다)
해석 시 ["]{dir="rtl"}군집 번호 자체"에 의미를 부여하지 않는 것이
원칙이다.

#### 4\. 모델 기반 군집의 해석

GMM의 군집 결과는 단순한 분할이 아니라, 각 점에 대한 소속확률
$\gamma_{ik}$를 제공한다는 점에서 해석이 풍부하다. 예를 들어
$\max_{k}\gamma_{ik}$가 작으면 해당 점은 어느 군집에도 강하게 속하지
않는 경계점일 가능성이 크다.

또한 $\Sigma_{k}$는 군집의 모양과 방향성을 의미하므로,
["]{dir="rtl"}군집이 어떤 축으로 퍼져 있는가"를 통계적으로 설명할 수
있다. 이는 k-means처럼 구형 군집만 가정하는 방법보다 현실 데이터를 더 잘
반영하는 경우가 많다.

### Chapter 3. 표현기반 군집

표현 기반 군집(representation-based clustering)은 원자료 공간에서 곧바로
군집을 수행하기보다, 데이터를 더 ["]{dir="rtl"}군집하기 좋은"
표현공간으로 변환한 뒤 군집을 수행하는 접근이다.

실제 데이터는 고차원, 강한 상관, 잡음, 비선형 구조를 동시에 포함하는
경우가 많으며, 이때 단순 거리 기반 군집은 거리의 의미가
약해지거나(차원의 저주), 특정 변수의 스케일·상관에 의해 결과가 왜곡되기
쉽다.

표현 기반 군집은 이러한 문제를 완화하기 위해 (i) 차원축소 및 특징추출로
핵심 구조를 요약하고, (ii) 그 표현 위에서 k-means, GMM, 계층군집 등을
수행하여 군집의 안정성과 해석가능성을 높이는 방법이다. 이 절에서는 PCA
기반 파이프라인과 딥러닝 기반 deep clustering의 개념을 정리한다.

표현 기반 군집은 원공간에서 직접 군집하기 어려운 경우, 차원축소 또는
특징학습을 통해 군집에 유리한 표현공간을 만든 뒤 군집을 수행하는
방법이다.

PCA+clustering은 가장 표준적인 선형 표현 기반 방법이며, deep
clustering은 신경망으로 비선형 임베딩을 학습하면서 군집 목적을 함께
최적화하는 확장된 개념이다. 이 절의 핵심은 군집이 ["]{dir="rtl"}알고리즘
선택"만의 문제가 아니라, 표현공간을 어떻게 설계하느냐의 문제라는 점이다.

#### 1\. PCA + clustering

##### PCA의 목적과 군집과의 연결

주성분분석(PCA)은 p차원 데이터를 분산이 큰 방향으로 정렬하여, 소수의
선형결합으로 데이터를 요약하는 차원축소 방법이다. 평균을 제거한 데이터
행렬을 $X_{c} \in \mathbb{R}^{n \times p}$라 하면, PCA는 다음 최적화
문제로 정의된다.

$\max_{\parallel w \parallel = 1}Var(X_{c}w)$. 즉 1번째 주성분 방향
$w_{1}$는 투영점수의 분산을 최대화하는 단위벡터이다. 이를 확장하면 상위
q개 주성분으로 구성된 행렬
$W_{q} = \lbrack w_{1},\ldots,w_{q}\rbrack \in \mathbb{R}^{p \times q}$에
대해 저차원 표현은 $Z = X_{c}W_{q} \in \mathbb{R}^{n \times q}$로
주어진다. 여기서 Z가 바로 군집에 투입되는
["]{dir="rtl"}표현(representation)"이다.

군집 관점에서 PCA의 핵심 역할은 다음 두 가지이다.

- 잡음 제거 및 거리 안정화이다. 고차원에서 유클리드 거리는 차이가 균질해져
구분력이 약해지는 경향이 있으며, 잡음 차원이 많을수록 이 현상이
강해진다. PCA로 중요한 분산 방향만 남기면 거리가 더 의미 있게 작동한다.
- 상관 구조 정리이다. 상관이 큰 변수들이 중복정보를 제공하면 거리가 특정
방향으로 과도하게 왜곡될 수 있다. PCA는 서로 직교하는 축으로 변환하여
중복성을 줄인다.

따라서 PCA+clustering은 ["]{dir="rtl"}차원축소로 표현공간을 정리한 뒤,
그 공간에서 거리 기반 또는 모델 기반 군집을 수행하는 표준적
파이프라인"이다.

##### 절차: 전형적 파이프라인

PCA+clustering은 다음 순서로 수행하는 것이 일반적이다.

① 전처리 및 표준화이다. 변수 단위가 다르면 표준화(z-score)가 필요하다.
이는 각 변수의 분산을 동일하게 두겠다는 가정이다.

② PCA 적합 및 차원 q 선택이다. 설명분산비 또는 누적 설명분산 기준으로
q를 정한다.

③ 저차원 점수 Z 산출이다. $Z = X_{c}W_{q}$이다.

④ 군집 수행이다. Z에 대해 k-means, GMM, 계층군집 등을 적용한다.

⑤ 해석이다. 군집 결과를 원변수로 되돌려 군집별 평균·비율·특징을
요약한다.

이 과정에서 ["]{dir="rtl"}PCA에서의 q"와 ["]{dir="rtl"}군집에서의 K"는
서로 다른 선택 문제이다. q는 표현의 복잡도를, K는 군집 구조의 복잡도를
조절한다.

##### PCA 공간에서의 거리 해석

PCA는 선형변환이므로, 원공간의 유클리드 거리와 PCA 공간의 유클리드
거리는 일반적으로 동일하지 않다. 다만 모든 주성분을 사용하면(즉 q=p)
직교변환이므로 거리가 보존된다.

하지만 q\<p로 축소하면 일부 방향 정보를 버리므로 거리도 근사적으로만
유지된다. 이때 중요한 점은 ["]{dir="rtl"}버린 방향이 주로 잡음이라면
오히려 거리 기반 군집이 좋아질 수 있다"는 사실이다.

따라서 PCA+clustering은 단순한 정보 손실이 아니라, 군집에 유리한
신호대잡음비(signal-to-noise ratio)를 높이는 전략이다.

##### k-means와 PCA의 관계

PCA와 k-means는 서로 다른 목적함수를 갖지만, 둘 다
["]{dir="rtl"}제곱거리와 분산"의 구조를 공유한다. 특히 k-means는 군집 내
제곱거리합을 최소화하며, PCA는 투영 후 재구성 오차를 최소화하는 성질을
가진다.

따라서 분산 구조가 강하게 존재하는 데이터에서는 PCA 축 위에서 군집이 더
뚜렷해지는 경우가 많다. 그러나 데이터 구조가 비선형이면 PCA만으로는 군집
구조가 선명해지지 않을 수도 있다.

#### 2\. Deep clustering의 개념

딥러닝 기반 deep clustering은 신경망을 사용해 데이터의 표현을
학습하면서, 그 표현에서 군집이 잘 분리되도록 만드는 접근이다. 이는
["]{dir="rtl"}좋은 표현이 주어지면 군집은 쉬워진다"는 명제를 전제로
한다.

전통적 방식(PCA+clustering)은 표현 학습(PCA)과 군집을 분리하여
수행하지만, deep clustering은 두 단계를 결합하거나 번갈아 최적화한다는
점에서 차이가 있다.

##### 기본 구성: 표현 함수와 군집 목적

딥러닝은 입력 x를 저차원 임베딩 z로 변환하는 함수
$f_{\theta}( \cdot )$를 학습한다.
$z_{i} = f_{\theta}(x_{i}) \in \mathbb{R}^{q}$. 여기서 $\theta$는 신경망
파라미터이다. deep clustering은 이 임베딩 $z_{i}$가
["]{dir="rtl"}군집하기 좋은 공간"이 되도록 $\theta$를 학습한다. 즉,
목표는 단순히 재구성이나 분류가 아니라 군집 구조가 드러나는 표현을
만드는 것이다.

##### 대표적 접근 1: 오토인코더 + 군집

가장 직관적인 방식은 오토인코더(autoencoder)를 이용하는 방식이다.
오토인코더는 인코더 $f_{\theta}$와 디코더 $f_{\theta}$로 구성되며,
$z_{i} = f_{\theta}(x_{i}),{\widehat{x}}_{i} = g_{\phi}(z_{i})$로
재구성한다. 재구성 손실은 보통
$L_{\text{rec}} = \overset{n}{\sum_{i = 1}} \parallel x_{i} - {\widehat{x}}_{i} \parallel^{2}$이다.
여기서 $z_{i}$는 PCA의 점수와 유사한 역할을 하되, 비선형 함수를 통해 더
유연한 표현을 학습한다.

이후 z_i에 대해 k-means를 수행하는 방법은 ["]{dir="rtl"}오토인코더로
표현 학습 → 임베딩에서 군집"이라는 2단계 접근이다. 그러나 deep
clustering이라 부르는 핵심은 ["]{dir="rtl"}재구성만 잘하는 임베딩"이
아니라 ["]{dir="rtl"}군집이 잘 되도록 임베딩을 조정"하는 것에 있다.

따라서 다음과 같은 결합 목적이 등장한다.
$L = L_{\text{rec}} + \lambda L_{\text{clust}}$. 여기서
$L_{\text{clust}}$는 임베딩에서 군집이 조밀·분리되도록 유도하는
손실이며, $\lambda$는 두 목표의 균형을 조절하는 계수이다.

##### 대표적 접근 2: 자기학습(self-training) 기반 soft assignment

딥러닝 기반 군집에서는 hard assignment 대신 soft assignment를 이용하는
경우가 많다. 임베딩 z_i와 군집 중심 $m_{k}$가 있을 때, 예를 들어
Student-t 분포 형태의 soft assignment를
$q_{ik} \propto \left( 1 + \frac{\parallel z_{i} - m_{k} \parallel^{2}}{\alpha} \right)^{- \frac{\alpha + 1}{2}}$로
정의하고, 이를 정규화하여 $\sum_{k}q_{ik} = 1$이 되게 한다.

이후 $q_{ik}$로부터 더 ["]{dir="rtl"}날카로운" 목표분포 $p_{ik}$를
만들고(예: 큰 값을 강조하는 방식), q가 p를 따라가도록 KL divergence를
최소화한다.
$L_{\text{clust}} = \overset{n}{\sum_{i = 1}}\overset{K}{\sum_{k = 1}}p_{ik}\log\frac{p_{ik}}{q_{ik}}$이라는
형태가 자주 사용된다.

이 방식의 의미는 ["]{dir="rtl"}현재 임베딩에서 군집이 될 듯한 구조를
만든 뒤, 그 구조가 더 선명해지도록 임베딩을 다시 학습"하는 반복
구조이다.

##### deep clustering의 장점과 위험

deep clustering의 장점은 다음과 같다.

- 고차원·비선형 구조에서 PCA보다 더 강력한 표현을 학습할 수 있다.
- 이미지, 텍스트 등 복잡한 데이터에서 ["]{dir="rtl"}군집 가능한 특징"을
자동으로 추출할 수 있다.
- soft assignment를 통해 경계점의 불확실성까지 표현할 수 있다.

반면 위험도 분명하다.

- 퇴화(degenerate) 해의 위험이다. 모든 점이 하나의 군집으로 몰리거나,
임베딩이 상수로 붕괴(collapse)하는 현상이 발생할 수 있다.
- 하이퍼파라미터 의존성이 크다. 임베딩 차원 q, 손실 가중치 $\lambda$, 초기
중심, 학습률 등에 따라 결과가 크게 달라질 수 있다.
- 해석가능성이 낮아질 수 있다. PCA는 선형결합으로 설명되지만, 딥 임베딩은
설명이 어려운 경우가 많다.

따라서 deep clustering은 ["]{dir="rtl"}군집 성능"만이 아니라
["]{dir="rtl"}안정성과 해석가능성"까지 함께 고려하여 사용해야 한다.

