---
title: "수학의 기초 3. 벡터"
format: html
---
<br>

### <span style="color:green">chapter 1. 선형대수 개념</span>

##### 1. 선형대수 정의
선형대수(Linear Algebra)는 벡터 공간과 벡터 간의 관계를 탐구하며,
벡터와 행렬을 활용한 수학적 표현과 계산을 다루는 수학 분야이다. 주요
구성 요소로는 벡터, 행렬, 스칼라가 있으며, 핵심 개념으로는 선형 변환,
고유값과 고유벡터, 내적(inner product), 외적(cross product), 그리고 행렬
분해 등이 포함된다. 

통계학에서 선형대수는 데이터를 벡터와 행렬로 표현하여 복잡한 수치
계산을 단순화하고 효율적으로 수행할 수 있게 해 준다. 특히, 고차원
데이터의 계산 및 변환을 가능하게 하며, 이는 데이터 분석의 구조 이해와
차원 축소에 핵심적인 역할을 한다. 또한, 선형대수는 머신러닝과 통계
모델링의 기초를 이루며, 회귀 분석, 주성분 분석(PCA), 군집 분석 등 다양한 기법에서 필수적으로 활용된다. 

##### 2. 선형대수와 선형변환 
선형대수는 선형적인 관계를 다루는 학문으로, 모든 연산과 변환이 선형성을 만족해야 한다. 따라서 일반적인 함수는 직접적으로 주요 개념으로 다루어지지 않지만, 선형 변환이라는 개념이 함수의 특수한 형태로 다뤄진다. 선형변환은 벡터 공간의 구조를 유지하며, 행렬로 표현될 수 있어 선형대수의 핵심 요소 중 하나로 자리 잡고 있다. 

##### 함수 $y=f(x)$

두 집합 사이의 관계로, 각 입력값(정의역 domain)에 대해 <u>정확히
하나의</u> 출력값(공역 range)을 대응시키는 규칙이다. 
함수는 <span
class="math inline"><em>f</em> : <em>X</em> → <em>Y</em></span>로
표기되며, <span class="math inline"><em>X</em></span>는 정의역, <span
class="math inline"><em>Y</em></span>는 공역입니다. 
함수값이 0인 <span
class="math inline"><em>f</em>(<em>x</em>) = 0</span>를 방정식이라 하고
이를 만족하는 <span class="math inline"><em>x</em></span>를 방정식의
해(root, solution)라고 한다.

![](/images/함수3.png){fig-align="center"}

##### 선형함수
선형함수는 입력 변수와 출력 변수 사이의 관계를 직선으로 나타내는 함수입니다. 일반적으로 다음과 같은 형태로 표현되며 다음 두 성질을 갖는다. <span
class="math inline"><em>f</em>(<em>x</em>) = <em>a</em> + <em>b</em><em>x</em></span>,
<span class="math inline"><em>a</em>:</span> 절편, <span
class="math inline"><em>b</em>:</span> 기울기 

&nbsp; 가법성 additivity: <span
class="math inline"><em>f</em>(<em>x</em> + <em>y</em>) = <em>f</em>(<em>x</em>) + <em>f</em>(<em>y</em>)</span> 

&nbsp; 동차성 homogeniety: <span
class="math inline"><em>f</em>(<em>c</em><em>x</em>) = <em>c</em><em>f</em>(<em>x</em>)</span>,
<span class="math inline"><em>c</em></span>는 상수 

##### 선형변환 

선형변환은 벡터 공간에서 벡터를 다른 벡터로 변환하는 함수의 특수한
형태이다. 이 변환은 **선형성(linearity)**이라는 두 가지 성질을 만족해야
한다. <span class="math inline">$\underset{¯}{u},\underset{¯}{v}$</span>
동일 차원의 벡터에 대하여 함수 <span
class="math inline"><em>T</em></span>가 다음 조건을 만족하면
선형변환이다. 

&nbsp; 덧셈에 대한 선형성: <span class="math inline">$T(\underset{¯}{u} +
\underset{¯}{v}) = T(\underset{¯}{u}) + T(\underset{¯}{v})$</span> 

&nbsp; 스칼라 곱에 대한 선형성: <span
class="math inline">$T(c\underset{¯}{u}) =
cT(\underset{¯}{u})$</span> 
, <span class="math inline">$\underset{¯}{x} = \left(

![](/images/선형변환.png){fig-align="center" width="40%"}


### <span style="color:green">chapter 2. 벡터</span>

##### 1. 벡터정의 
벡터는 정렬된 유한한 숫자 목록으로 일반적으로 정사각형 또는 곡선 괄호로 둘러싸인 수직 형태의 배열로 작성된다. 수평 배열의 행벡터와 구별하기 위하여 열벡터라고 하기도 한다. 


<span class="math inline">$\left( \begin{array}{r}
1 \\
- 2 \\
0
\end{array} \right)$</span>, <span class="math inline">$\left\lbrack
\begin{array}{r}
1 \\
- 2 \\
0
\end{array} \right\rbrack$</span>

벡터를 행으로 사용할 때는 쉼표로
구분되고 괄호로 둘러싸인 숫자로 쓴다. <span class="math inline">$\left(
\begin{array}{r}
1, - 2,0
\end{array} \right)$</span> 

배열의 값을 벡터의 원소 element 라 하고 원소의 개수를 벡터의 크기(차원 demension)라고 한다. 위 벡터는 크기가 3 이고 세 번째 원소는 0 이다. n 크기의 벡터는 n-벡터라고 불리고 1벡터는 숫자와 같은 것으로 간주한다. 즉, 우리는 1-벡터 [ 13 ]와 숫자 13을 구별하지 않으며 숫자는 스칼라 scalar 라 한다. 벡터의 각 원소는 스칼라이고 원소가 실수인 <span
class="math inline"><em>a</em><sub><em>i</em></sub> ∈ <em>R</em><sup><em>n</em></sup></span>
벡터를 실수 벡터라 한다. 

##### 2. 벡터 기호 
n-벡터를 나타내기 위해 <span
class="math inline">${\underset{¯}{a}}_{n}$</span>(구별이 가능한 경우
알파벳 <span class="math inline"><em>a</em></span>를 벡터로 표현) 기호를
사용한다. <span
class="math inline"><em>a</em><sub><em>n</em></sub></span>벡터 의 i-번째
요소는 <span
class="math inline"><em>a</em><sub><em>i</em></sub></span>로 표시되며,
여기서 첨자 i는 벡터의 크기인 1에서 <span
class="math inline"><em>n</em></span>까지 정의되는 정수 인덱스이다. 
두 벡터 <span
class="math inline"><em>a</em><sub><em>n</em></sub>, <em>b</em><sub><em>n</em></sub></span>가
동일하다는 것은 (1)크기(차수)도 <span
class="math inline"><em>n</em></span> 동일하고 (2) 각 대응 원소가 동일
<span
class="math inline"><em>a</em><sub><em>i</em></sub> = <em>b</em><sub><em>i</em></sub></span>함을
의미한다. 

##### 3, 특수한 벡터 

##### \(1) 영벡터 zero vector 
모든 원소가 0인 벡터이며 <span
class="math inline">0<sub><em>n</em></sub></span>으로 표현된다.
일반적으로 모든 0 벡터는 0으로 표시되며, 숫자 0을 나타내는데 사용되는
것과 동일한 기호이다. 다른 크기의 제로 벡터를 나타내기 위해 모두 같은
기호 0을 사용하므로 기호 0은 문맥에 따라 다른 것을 의미할 수 있기 때문에
컴퓨터에서는 이를 과부하라 한다. 

##### \(2) 단위벡터 unit vector

(표준) 단위 벡터는 1인 하나의 원소를 제외한 모든 요소가 0과 같은
벡터이다. i-번째 단위 벡터(n 크기)는 i-번째 원소만 1을 가진<img
src="media/image5.png" style="width:1.79646in;height:1.42013in"
alt="붙여넣은 동영상.png" /> 단위 벡터이며, <span
class="math inline"><em>e</em><sub><em>i</em></sub></span>로 표현한다.
이렇게 되면 크기를 나타내는 첨자와 1인 원소 위치를 나타내는 첨자가 구별이 되지 않는 모호성을 갖는다. 


##### \(3) 일벡터 ones vector 

모든 원소가 1인 n-벡터이며 <span
class="math inline">1<sub><em>n</em></sub></span>로 표현한다. 우리는
또한 벡터의 크기가 문맥에서 결정될 수 있다면 1을 쓴다. 

##### 4. 벡터 개념 

##### \(1) 위치 location 

2차원 공간, 즉 평면의 위치를 나타내는 데 사용될 수 있다. 3-벡터는 3차원(3-D) 공간에서 어떤 지점의 위치나 위치를 나타내는 데 사용된다. 벡터의 원소는 위치의 좌표를 제공한다. 벡터는 주어진 시간에 평면이나 3차원 공간에서 움직이는 지점의 속도나 가속도를 나타내는 데 사용될 수
있다. 

![](/images/벡터개념위치.png){fig-align="center" width="40%"}

##### \(2) 희소성 

많은 원소가 0이면 희소하다고 한다. 그것의 희소성 패턴은 0이 아닌 항목의 인덱스 집합이다. <span class="math inline"><em>n</em></span>-벡터
<span class="math inline"><em>a</em><sub><em>n</em></sub></span>의 0이
아닌 항목의 수는 <span
class="math inline"><em>n</em><em>n</em><em>z</em>(<em>a</em><sub><em>n</em></sub>)</span>로
표시한다다. 단위벡터는 0이 아닌 항목이 하나만 있기 있고 0 벡터는 0이
아닌 항목이 없기 때문에 희소한 벡터이다. 

##### \(3) 이미지

3차원 벡터는 빨간색, 녹색 및 파란색(R-G-B) 강도 값(0에서 1 사이)을 제공하는 항목을 통해 색상을 나타낸다. 벡터(0,0,0)는 검은색을 나타내고, 벡터(0, 1, 0)는 밝은 순수한 녹색을 나타내며, 벡터(1, 0.5, 0.5)는 분홍색을 나타낸다. 


### <span style="color:green">chapter 3. 벡터 연산과 크기 </span>

##### 1. 벡터 연산 

##### \(1) 벡터 합

두 벡터를 합을 구한다는 것은 (1) 차수가 동일한 두 벡터의 (2) 동일 위치의 원소를 합하여 하나의 벡터를 계산한다는 것을 의미한다. 차도 동일하다. 

<span class="math inline">$\left\lbrack \begin{array}{r}
1 \\
- 2 \\
0
\end{array} \right\rbrack + \left\lbrack \begin{array}{r}
1 \\
2 \\
3
\end{array} \right\rbrack = \left\lbrack \begin{array}{r}
2 \\
0 \\
3
\end{array} \right\rbrack$</span>, <span
class="math inline">$\left\lbrack \begin{array}{r}
1 \\
- 2 \\
0
\end{array} \right\rbrack - \left\lbrack \begin{array}{r}
1 \\
2 \\
3
\end{array} \right\rbrack = \left\lbrack \begin{array}{r}
0 \\
- 4 \\
- 3
\end{array} \right\rbrack$</span> 

##### 성질 
- 차수가 동일한 벡터 <span
class="math inline"><em>a</em>, <em>b</em>, <em>c</em></span>에 대하여
다음이 성립한다. 
- 교환법칙 : <span
class="math inline"><em>a</em> + <em>b</em> = <em>b</em> + <em>a</em></span> 
- 교환법칙 : <span
class="math inline">(<em>a</em> + <em>b</em>) + <em>c</em> = <em>a</em> + (<em>b</em> + <em>c</em>)</span> 
영벡터를 더하거나 빼도 영향을 받지 않는다. <span
class="math inline"><em>a</em> ± 0 = <em>a</em></span> 
벡터에서 자체 벡터를 빼면 영벡터가 된다. <span
class="math inline"><em>a</em> − <em>a</em> = 0</span> 

##### \(2) 스칼라-벡터 곱

벡터에 스칼라(즉, 숫자)를 곱하는 스칼라-벡터 곱셈은 벡터의 모든 요소에 스칼라를 곱하여 수행한다. 일반적으로 스칼라를 왼쪽, 벡터를 오른쪽에 적지만 순서를 바꾸어 사용해도 되고 계산 결과는 동일하다. 

<span class="math inline">$a = \left\lbrack \begin{array}{r}
1 \\
- 2 \\
0
\end{array} \right\rbrack$</span>이면 <span class="math inline">$3a = a3
= \left\lbrack \begin{array}{r}
3 \\
- 6 \\
0
\end{array} \right\rbrack$</span> 

##### 성질 
벡터 <span class="math inline"><em>a</em></span>, 스칼라 <span
class="math inline"><em>c</em>, <em>k</em></span>에 대하여 다음이
성립한다. 
교환법칙 : <span
class="math inline"><em>k</em><em>a</em> = <em>a</em><em>k</em></span> 
배분법칙 : <span
class="math inline">(<em>c</em> + <em>k</em>)<em>a</em> = <em>c</em><em>a</em> + <em>k</em><em>a</em></span> 

##### \(3) 선형 결합 linear combination

차수 <span class="math inline"><em>n</em></span>-벡터 <span
class="math inline"><em>a</em><sub>1</sub>, <em>a</em><sub>2</sub>, ..., <em>a</em><sub><em>m</em></sub></span>,
스칼라 <span
class="math inline"><em>k</em><sub>1</sub>, <em>k</em><sub>2</sub>, ..., <em>k</em><sub><em>m</em></sub></span>에
대하여 다음 <span class="math inline"><em>n</em></span>-벡터를 벡터
<span
class="math inline"><em>a</em><sub>1</sub>, <em>a</em><sub>2</sub>, ..., <em>a</em><sub><em>m</em></sub></span>의
선형결합이라 하고 스칼라 <span
class="math inline"><em>k</em><sub>1</sub>, <em>k</em><sub>2</sub>, ..., <em>k</em><sub><em>m</em></sub></span>는
선형결합의 계수라 한다. 

- <span
class="math display"><em>k</em><sub>1</sub><em>a</em><sub>1</sub> + <em>k</em><sub>2</sub><em>a</em><sub>2</sub> + ... + <em>k</em><sub><em>m</em></sub><em>a</em><sub><em>m</em></sub></span> 

- <span
class="math inline"><em>k</em><sub>1</sub> = <em>k</em><sub>2</sub> = ... = <em>k</em><sub><em>m</em></sub> = 1</span>이면,
선형결합은 벡터 합이다. 
<span class="math inline">$k_{1} = k_{2} = ... = k_{m} =
\frac{1}{m}$</span>이면, 선형결합은 벡터 평균이다. 
- <span
class="math inline"><em>k</em><sub>1</sub> + <em>k</em><sub>2</sub> + ... + <em>k</em><sub><em>m</em></sub> = 1</span>이면, 선형결합은 affine 결합이라 하고 모든 계수가 양수인 경우 선형결합을 가중평균이라 한다. 


##### \(4) 내적 inner product 

두 벡터 간의 관계를 정의하고 벡터의 길이와 각도 등의 개념을 도입하는
중요한 연산이다. 차수(<span class="math inline"><em>m</em></span>)가
동일한 두 벡터 (<span
class="math inline"><em>u</em>, <em>v</em></span>)의 내적 곱은 다음과
같이 정의하고 결과는 <u>스칼라</u>이다. 
<span class="math display">$$u^{T}v = \lbrack
u_{1},u_{2},...,u_{m}\rbrack\left\lbrack \begin{array}{r}
v_{1} \\
v_{2} \\
... \\
v_{m}
\end{array} \right\rbrack = u_{1}v_{1} + u_{2}v_{2} + ... + u_{m}v_{m} =
\overset{m}{\sum_{i = 1}}u_{i}v_{i}$$</span> 
단, <span
class="math inline"><em>u</em><sup><em>T</em></sup></span>는 <span
class="math inline"><em>u</em></span>의 전치 transpose라 하고 열벡터를
행벡터로 변환한 것이다. 
【예제】 
<table>
<colgroup>
<col style="width: 100%" />
</colgroup>
<tbody>
<tr>
<td><span class="math display">$$\lbrack 1,3,5\rbrack^{T}\left\lbrack
\begin{array}{r}
0 \\
- 1 \\
1
\end{array} \right\rbrack = (1)(0) + (3)( - 1) + (5)(1) =
2$$</span></td>
</tr>
</tbody>
</table>

##### 내적 성질 
- unit 벡터 : <span
class="math inline"><em>e</em><sub><em>i</em></sub><em>v</em> = <em>v</em><sub><em>i</em></sub></span> 

- 벡터 합 : <span class="math inline">$1_{m}^{T}v = \overset{m}{\sum_{i
= 1}}v_{i}$</span> 

- 벡터 평균 : <span class="math inline">$avg(v) = (1/n)1_{m}^{T}v =
(1/n)\overset{m}{\sum_{i = 1}}v_{i}$</span> 

- 벡터 제곱합 : <span class="math inline">$v^{T}v = v_{1}^{2} +
v_{2}^{2} + ... + v_{m}^{2} = \overset{m}{\sum_{i =
1}}v_{i}^{2}$</span> 

##### Cauchy–Schwarz inequality 

차수 동일한 두 벡터의 내적 inner product에 대하여 다음이 성립한다. 
<span
class="math display"> ∥ <em>a</em><sup><em>T</em></sup><em>b</em> ∥  ≤  ∥ <em>a</em> ∥  ∥ <em>b</em>∥</span> 
<span class="math display">$$|\overset{n}{\sum_{i}}a_{i}b_{i}| \leq
(\sum a_{i}^{2})^{\frac{1}{2}}(\sum
b_{i}^{2})^{\frac{1}{2}}$$</span> 


##### \(5) 외적 cross product 

주로 3차원 공간에서 두 벡터로부터 새로운 벡터를 생성하는 연산입니다. 이 연산의 결과는 두 벡터에 모두 수직인 벡터이며, 크기는 두 벡터가 이루는 평행사변형의 면적에 해당합니다. 

##### 외적 정의 

벡터 <span class="math inline">$\underset{¯}{a} =
(a_{1},a_{2},a_{3})$</span>와 벡터 <span
class="math inline">$\underset{¯}{b} = (b_{1},b_{2},b_{3})$</span>의
외적 <span class="math inline">$\underset{¯}{a}
\times \underset{¯}{b}$</span>는 다음과 같이 계산한다. 

- <span class="math inline"><em>x</em></span> 성분: <span
class="math inline"><em>a</em><sub>2</sub><em>b</em><sub>3</sub> − <em>a</em><sub>3</sub><em>b</em><sub>2</sub></span> 

- <span class="math inline"><em>y</em></span> 성분: <span
class="math inline"><em>a</em><sub>3</sub><em>b</em><sub>1</sub> − <em>a</em><sub>1</sub><em>b</em><sub>3</sub></span> 
- <span class="math inline"><em>z</em></span> 성분: <span
class="math inline"><em>a</em><sub>1</sub><em>b</em><sub>2</sub> − <em>a</em><sub>2</sub><em>b</em><sub>1</sub></span> 

![](/images/외적.png){fig-align="center" width="40%"}

【예제】 벡터 <span class="math inline">$\underset{¯}{a} =
(2,3,4)$</span>와 벡터 <span class="math inline">$\underset{¯}{b} =
(5,6,7)$</span>의 외적은 <span class="math inline">$\underset{¯}{c} =
\underset{¯}{a} \times \underset{¯}{b} = ( - 3,6, - 3)$</span> 이다. 
외적은 벡터 <span
class="math inline">$\underset{¯}{a},\underset{¯}{b}$</span>와
수직(<span class="math inline">${\underset{¯}{c}}^{T}\underset{¯}{a} =
0$</span>, <span
class="math inline">${\underset{¯}{c}}^{T}\underset{¯}{b} =
0$</span>)이며 외적의 크기(놈 norm)는 두 벡터가 이루는 평행사면형
면적이다. 

##### 2. 선형함수 

##### 선형함수 정의
<span
class="math inline"><em>f</em> : <em>R</em><sup><em>n</em></sup> → <em>R</em></span>는
크기 n-벡터를 실수(스칼라)로 매핑하는 함수이다. 함수 <span
class="math inline"><em>f</em>(<em>x</em>)</span>의 <span
class="math inline"><em>x</em><sub>1</sub>, <em>x</em><sub>2</sub>, ..., <em>x</em><sub><em>n</em></sub></span>은
함수 <span class="math inline"><em>f</em></span>의 인수 argument라 하고
결과 값 스칼라는 함수 값이다. <span
class="math inline"><em>f</em>(<em>x</em>) = <em>f</em>(<em>x</em><sub>1</sub>, <em>x</em><sub>2</sub>, ..., <em>x</em><sub><em>n</em></sub>)</span> 

【예제】 
$$
f: \mathbb{R}^4 \to \mathbb{R}, \quad f(x) = x_1 - x_2 + x_4^2
$$


차수 n-벡터 <span class="math inline"><em>a</em>, <em>x</em></span>에
대하여 내적 함수 <span
class="math inline"><em>f</em>(<em>x</em>) = <em>a</em><sup><em>T</em></sup><em>x</em> = <em>s</em><em>c</em><em>a</em><em>l</em><em>a</em><em>r</em></span>는
선형함수일 때 다음이 성립한다. 단, <span
class="math inline"><em>α</em>, <em>β</em></span>는 스칼라, <span
class="math inline">(<em>x</em>, <em>y</em>)</span>는 n-벡터이다. <span
class="math inline"><em>f</em>(<em>α</em><em>x</em> + <em>β</em><em>y</em>) = <em>α</em><em>f</em>(<em>x</em>) + <em>β</em><em>f</em>(<em>y</em>)</span> 

##### 선형함수 조건 
다음 조건을 만족하는 <span
class="math inline"><em>f</em> : <em>R</em><sup><em>n</em></sup> → <em>R</em></span>
는 선형함수이다. 단, <span class="math inline"><em>α</em></span>는
스칼라, <span class="math inline">(<em>x</em>, <em>y</em>)</span>는
n-벡터이다. 

- Homogeniety : <span
class="math inline"><em>f</em>(<em>α</em><em>x</em>) = <em>α</em><em>f</em>(<em>x</em>)</span> 
- Additivity : <span
class="math inline"><em>f</em>(<em>x</em> + <em>y</em>) = <em>f</em>(<em>x</em>) + <em>f</em>(<em>y</em>)</span> 

##### \(1) 절편 Affine 함수

선형 함수에 상수 항을 추가한 형태의 함수이다. 이는 선형 변환과 평행
이동을 결합한 함수로, 다음과 같은 수식으로 표현된다. 
n-벡터, <span class="math inline"><em>x</em></span>에 대하여 다음
<span class="math inline"><em>f</em></span>는 절편 함수이다. 단, <span
class="math inline"><em>a</em></span>는 n-벡터, <span
class="math inline"><em>k</em></span>는 스칼라이다. <span
class="math inline"><em>f</em>(<em>a</em><sup><em>T</em></sup><em>x</em> + <em>k</em>) = <em>a</em><sup><em>T</em></sup><em>f</em>(<em>x</em>) + <em>k</em></span> 

【예제】 
<table>
<colgroup>
<col style="width: 100%" />
</colgroup>
<tbody>
<tr>
<td><span
class="math inline"><em>f</em>(<em>x</em>) = 7 − 2<em>x</em><sub>1</sub> + 3<em>x</em><sub>2</sub> − <em>x</em><sub>3</sub></span>,
<span class="math inline">$k = 7,a = \left\lbrack \begin{array}{r}
- 2 \\
3 \\
- 1
\end{array} \right\rbrack$</span></td>
</tr>
</tbody>
</table>

##### \(2) 선형함수의 내적 표현 

<span class="math inline"><em>e</em><sub><em>i</em></sub></span>
단위벡터, <span
class="math inline"><em>x</em><sub><em>n</em></sub></span> 차수 n-벡터,
<span class="math inline"><em>f</em></span> 선형함수라 하면, 
<span class="math display">$$\begin{matrix}
f(x) = f(x_{1}e_{1} + x_{2}e_{2} + ... + x_{n}e_{n}) \\
= x_{1}f(e_{1}) + x_{2}f(e_{2}) + ... + x_{n}f(e_{n}) \\
= a^{T}x,wherea^{T} = \lbrack
f(e_{1}),f(e_{2}),...,f(e_{n})\rbrack
\end{matrix}$$</span> 


##### \(3) 사례 : sag 처짐 (단위: mm) 

하중벡터 <span class="math inline">$w = \left( \begin{array}{r}
w_{1} \\
w_{2} \\
w_{3}
\end{array} \right)$</span>(단위:톤), 변형 compliance 민감도 벡터 <span
class="math inline">$c = \left( \begin{array}{r}
c_{1} \\
c_{2} \\
c_{3}
\end{array} \right)$</span>(단위:mm/톤)이라면 교량 처짐 sag은 <span
class="math inline"><em>s</em> = <em>c</em><sup><em>T</em></sup><em>w</em></span>
(하중 가중합)이다. 

![](/images/하중.png){fig-align="center" width="40%"}

##### \(4) 테일러 근사 Taylor 함수 

<span
class="math inline"><em>f</em> : <em>R</em><sup><em>n</em></sup> → <em>R</em></span>이
1차 미분이 가능하다고 하면 <span
class="math inline"><em>n</em></span>-벡터 함수 <span
class="math inline"><em>f</em>(<em>x</em>)</span>의 근사값은 다음과 같이
구한다. 이를 1차 테일러 근사라 한다. 단, n-벡터 <span
class="math inline"><em>z</em></span>는 n-벡터 <span
class="math inline"><em>x</em></span>와 가까운 값이다. 
<span class="math display">$$\widehat{f}(x) = f(z) + \frac{\partial
f}{\partial x_{1}}(z)(x_{1} - z_{1}) + ... + \frac{\partial f}{\partial
x_{n}}(z)(x_{n} - z_{n})$$</span> 

【예제】 
<table>
<colgroup>
<col style="width: 100%" />
</colgroup>
<tbody>
<tr>
<td>함수 <span
class="math inline"><em>f</em> : <em>R</em><sup>2</sup> → <em>R</em></span>을
<span
class="math inline"><em>f</em>(<em>x</em>) = <em>x</em><sub>1</sub> + <em>e</em><em>x</em><em>p</em>(<em>x</em><sub>2</sub> − <em>x</em><sub>1</sub>)</span>라
하자. 이 함수는 선형함수는 아니다. 이를 선형함수로 근사하는 것을 테일러
근사라 한다. <span class="math inline"><em>z</em> = (1, 2)</span>라
하면, 
<span class="math display">$$\triangledown f(z) = \left\lbrack
\begin{array}{r}
1 - exp(z_{2} - z_{1}) \\
exp(z_{2} - z_{1})
\end{array} \right\rbrack|_{z_{1} = 1,z_{2} = 2} = ( -
1.72,2.72)$$</span> 
그러므로 <span class="math inline"><em>z</em> = (1, 2)</span>에서
<span class="math inline"><em>f</em>(<em>x</em>)</span>의 테일러
근사값은 
<span class="math inline">$\widehat{f}(x) = 3.718 + \left\lbrack
\begin{array}{r}
- 1.72 \\
2.72
\end{array} \right\rbrack^{T}(\left\lbrack \begin{array}{r}
x_{1} \\
x_{2}
\end{array} \right\rbrack - \left\lbrack \begin{array}{r}
1 \\
2
\end{array} \right\rbrack)$</span>이다. </td>
</tr>
</tbody>
</table>

##### \(5) 회귀모형 

차원 2-예측(설명, 독립) 벡터 <span class="math inline">$x =
\left\lbrack \begin{array}{r}
x_{1} \\
x_{2}
\end{array} \right\rbrack$</span>, 회귀계수 벡터 <span
class="math inline">$b = \left\lbrack \begin{array}{r}
b_{1} \\
b_{2}
\end{array} \right\rbrack$</span>, 그리고 <span
class="math inline"><em>a</em></span>을 절편 스칼라라 하면 회귀모형은
다음과 같다. 
<span class="math inline">$\widehat{y} = \left\lbrack
\begin{array}{r}
1 \\
x
\end{array} \right\rbrack^{T}\left\lbrack \begin{array}{r}
a \\
b
\end{array} \right\rbrack = {\overset{˜}{x}}^{T}\overset{˜}{b}$</span>
OLS 추정치 : <span class="math inline">$\widehat{\overset{˜}{b}} =
({\overset{˜}{x}}^{T}\overset{˜}{x})^{-
1}{\overset{˜}{x}}^{T}y$</span> 

##### 3. 벡터놈 norm 

##### \(1) 정의

벡터의 유클리디안 놈, <span
class="math inline"> ∥ <em>x</em>∥</span>은 벡터의 크기에 대한 척도로
다음과 같이 구한다. 놈은 벡터의 원점에서의 거리이다. 
<span class="math display">$$\parallel x \parallel = \sqrt{x_{1}^{2}
+ x_{2}^{2} + ... + x_{n}^{2}} = \sqrt{x^{T}x}$$</span> 

【예제】 
<table>
<colgroup>
<col style="width: 100%" />
</colgroup>
<tbody>
<tr>
<td><span class="math inline">$\parallel \left\lbrack \begin{array}{r}
0 \\
- 1 \\
1
\end{array} \right\rbrack \parallel = \sqrt{2}$</span>, <span
class="math inline">$\parallel \left\lbrack \begin{array}{r}
- 1 \\
2
\end{array} \right\rbrack \parallel = \sqrt{5}$</span></td>
</tr>
</tbody>
</table>

##### 성질 

- 비음수 동차성: <span
class="math inline"> ∥ <em>β</em><em>x</em> ∥  = |<em>β</em>| ∥ <em>x</em>∥</span>,
where <span class="math inline"><em>β</em></span>는 스칼라 

- 삼각 부등식: <span
class="math inline"> ∥ <em>x</em> + <em>y</em> ∥  ≤  ∥ <em>x</em> ∥ + ∥ <em>y</em>∥</span> 

- 비음수: <span class="math inline"> ∥ <em>x</em> ∥  ≥ 0</span> 

##### \(2) 놈의 종류 

- L1 norm : <span class="math inline">$L_{1} =
\overset{n}{\sum_{i}}|x_{i}|$</span> 절대값의 합으로 맨하튼
Manhattan 놈이라고도 한다. 지도의 거리 측정에 사용된다. 
- L2 norm : <span class="math inline">$L_{2} =
(\overset{n}{\sum_{i}}x_{i}^{2})^{\frac{1}{2}}$</span> 제곱합의
제곱근으로 유클리디안 놈이라 한다. 통계학에서 가장 많이 사용된다.
회귀계수 추정치를 구하는 최소제곱추정치 구할 때 사용된다. 

![](/images/놈.png){fig-align="center" width="60%"}


```python
#행렬 정의
import numpy as np
A=np.array([[1,2,3], [4,5,7],[8,9,10]])
#L1 norm Mahattan
la.norm(A,axis=1,ord=1)

```
【결과】 array([ 6., 16., 27.])

```python
#L2 norm Euclidean
la.norm(A,axis=1,ord=2)

```
【결과】 array([ 3.74165739,  9.48683298, 15.65247584])

##### \(3) 평균 제곱근 RMS root mean square value 

데이터 크기를 정량화하는데 사용되며 데이터의 평균적인 크기를
나타낸다. <span class="math inline">$rms(x) = \frac{\parallel x
\parallel}{\sqrt{n}} = \sqrt{\frac{1}{n}\sum x_{i}^{2}}$</span> 

##### \(4) 두 벡터의 합의 놈 

<span class="math display">$$\parallel x + y \parallel =
\sqrt{\parallel x \parallel^{2} + 2x^{T}y + \parallel y
\parallel^{2}}$$</span> 

##### \(5) Chebyshev inequality

차수 n-벡터 <span class="math inline"><em>x</em></span>, <span
class="math inline"><em>x</em><sub><em>i</em></sub><sup>2</sup> ≥ <em>a</em><sup>2</sup></span>을
만족하는 원소 개수를 <span class="math inline"><em>k</em></span>라 하면,
<span
class="math inline"> ∥ <em>x</em>∥<sup>2</sup> = <em>x</em><sub>1</sub><sup>2</sup> + ... + <em>x</em><sub>2</sub><sup>2</sup> ≥ <em>k</em><em>a</em><sup>2</sup></span>이다.
<span class="math inline"><em>k</em> ≤ <em>n</em></span>이므로 <span
class="math inline">$n \leq \frac{\parallel x
\parallel}{a^{2}}$</span>이다. 즉, 벡터의 어떠한 원소도 그 벡터의 놈보다크지 않다.

<span class="math inline">$\frac{k}{n} \leq
(\frac{rms(x)}{a})^{2}$</span>. 왼쪽 항은 벡터의 성분 중 절대값이 최소한
<span class="math inline"><em>a</em></span>이상인 성분의 비율을
나타낸다. 오른쪽 항은 <span class="math inline"><em>a</em></span>와
<span
class="math inline"><em>r</em><em>m</em><em>s</em>(<em>x</em>)</span>의
비율의 제곱에 대한 역수이다. 예를 들어, 벡터의 성분 중 1/25 = 4% 이상은 RMS 값의 5배를 초과할 수 없다는 것을 의미한다. 


### <span style="color:green">chapter 4. 벡터간 거리 </span>

##### 1. 유클리디안 거리 

##### \(1) 정의 

차수가 동일한 두 벡터(<span
class="math inline"><em>a</em>, <em>b</em></span>)의 놈을 유클리디안
거리로 정의한다. 
<span
class="math display"><em>d</em><em>i</em><em>s</em><em>t</em>(<em>a</em>, <em>b</em>) =  ∥ <em>a</em> − <em>b</em> ∥  =  ∥ <em>b</em> − <em>a</em>∥</span> 
<span class="math display">$$||a - b|| = \sqrt{(a_{1} - b_{1})^{2} +
(a_{2} - b_{2})^{2} + ... + (a_{n} - b_{n})^{2}}$$</span> 
두 벡터의 Root Mean Square 편차 = <span
class="math inline">$\frac{\parallel x - y
\parallel}{\sqrt{n}}$</span> 

【예제】 
<table>
<colgroup>
<col style="width: 100%" />
</colgroup>
<tbody>
<tr>
<td><span class="math display">$$a = \left\lbrack \begin{array}{r}
0 \\
- 1 \\
1
\end{array} \right\rbrack,b = \left\lbrack \begin{array}{r}
1 \\
- 2 \\
1
\end{array} \right\rbrack,c = \left\lbrack \begin{array}{r}
1 \\
0 \\
3
\end{array} \right\rbrack$$</span> 
<span class="math display">$$dist(a,b) = \sqrt{2},dist(b,c) =
2.8284$$</span> </td>


```python
#행렬 정의
import numpy as np
a=np.array([[0],[-1],[1]])
b=np.array([[1],[-2],[1]])
c=np.array([[1],[0],[3]])
#거리 계산
np.linalg.norm(a-b),np.linalg.norm(b-c)
```
【결과】 (np.float64(1.4142135623730951), np.float64(2.8284271247461903))

##### \(2) 활용 
- feature distance: <span
class="math inline"> ∥ <em>x</em> − <em>y</em>∥</span> 차수가 동일한 두
벡터의 거리를 개체의 유사성 척도로 사용한다. 

- Nearest neighbor: <span
class="math inline"> ∥ <em>x</em> − <em>z</em><sub><em>i</em></sub>∥</span>
두 개체 간의 거리를 이용하여 유사한 개체를 군집으로 묶는다. k-means 알고리즘 

- RMS prediction error: <span
class="math inline"><em>r</em><em>m</em><em>s</em>(<em>y</em> − <em>ŷ</em>)</span>
관측치와 예측치의 거리를 예측의 정확도 척도로 사용한다. </li>
</ol>

```python
# 감성분석
import numpy as np
from sklearn.neighbors import KNeighborsClassifier
from sklearn.feature_extraction.text import TfidfVectorizer
# 데이터 준비
texts = ["I love this product", "This is terrible", "Absolutely fantastic", "Not good at all"]
labels = [1, 0, 1, 0]  # 1: 긍정, 0: 부정
# TF-IDF 벡터화
vectorizer = TfidfVectorizer()
X = vectorizer.fit_transform(texts)
# KNN 모델
knn = KNeighborsClassifier(n_neighbors=1, metric='euclidean')
knn.fit(X, labels)
# 새로운 리뷰 분류
new_text = ["I hate this product"]
new_vector = vectorizer.transform(new_text)
prediction = knn.predict(new_vector)
print(f"Prediction: {'Positive' if prediction[0] == 1 else 'Negative'}")
```
【결과】 Prediction: Positive

##### \(3) 삼각 부등식 

차수가 동일한 n벡터 <span
class="math inline"><em>a</em>, <em>b</em>, <em>c</em></span>에 대하여
다음이 발생한다. 

##### \(4) triangle 부등식 

<span
class="math inline"> ∥ <em>a</em> + <em>b</em>∥<sup>2</sup> ≤ ( ∥ <em>a</em> ∥ + ∥ <em>b</em> ∥ )<sup>2</sup></span>

##### \(5) 맨해튼 거리 

<span class="math display">$$d(\mathbf{a},\mathbf{b}) =
\overset{n}{\sum_{i = 1}}|a_{i} - b_{i}|$$</span> 
맨해튼 거리는 벡터 간의 축을 따라 이동한 거리의 합으로 이는 그리드
기반 공간에서 이동하는 경우에 적합하다. 맨해튼 거리라는 이름은 도로망이
격자 형태로 이루어진 맨해튼 도시 구조에서 유래되었다. 자동차나 사람이
이동할 때 대각선으로 이동하지 못하고 도로를 따라 움직이는 경우에
적합하다. 예: 두 위치 간 최단 이동 거리 계산. 

![](/images/맨하튼거리.png){fig-align="center" width="40%"}


##### 2. 유클리디안 거리와 통계 

##### \(1) de-meanded 벡터 

【reall】 치수 n-벡터 <span
class="math inline"><em>x</em><sub><em>n</em></sub></span>, 평균은 <span
class="math inline">$avg(x) = (1_{n}^{T}x)/n = (instat) =
\overline{x}$</span> 

【정의】 <span class="math inline">$\overset{˜}{x} = x -
avg(x)1_{n}$</span> : 벡터의 각 원소를 평균을 뺀 벡터 

【성질】 <span class="math inline">$avg(\overset{˜}{x}) = 0$</span> 

- 통계 분석: 데이터의 평균을 제거함으로써 분산이나 공분산과 같은 통계적 특성을 더 명확하게 분석할 수 있다. 
- 주성분 분석(PCA): 데이터의 분산을 분석하기 전에 데이터를 중심에 맞추기 위해 사용된다. 

- 회귀 분석: 회귀 분석에서 독립 변수와 종속 변수의 평균을 제거하여 상수항 없이 회귀 모델을 구축할 수 있다. 

##### \(2) 표준편차 standard deviation 

<span class="math display">$$std(x) = \sqrt{\frac{(x_{1} -
avg(x))^{2} + (x_{2} - avg(x))^{2} + ... + (x_{n} -
avg(x))^{2})}{n}}$$</span> 
<span class="math display">$$std(x) = \frac{\parallel x - (1^{T}x/n)1
\parallel}{\sqrt{n}}$$</span> 
【응용】 투자에서 평균은 일정기간 평균 수익율, 표준편차는 위험 척도이다. 

##### \(3) 표준편차 성질 

- 상수를 더해도 표준편차는 동일하다. <span
class="math inline"><em>s</em><em>t</em><em>d</em>(<em>x</em> + <em>a</em>1) = <em>s</em><em>t</em><em>d</em>(<em>x</em>)</span> 
- 스칼라(상수) 곱 : <span
class="math inline"><em>s</em><em>t</em><em>d</em>(<em>k</em><em>x</em>) = |<em>k</em>|<em>s</em><em>t</em><em>d</em>(<em>x</em>)</span> 

##### \(4) 평균, RMS, STD 관계 

<span
class="math inline"><em>s</em><em>t</em><em>d</em>(<em>x</em>)<sup>2</sup> = <em>r</em><em>m</em><em>s</em>(<em>x</em>)<sup>2</sup> − <em>a</em><em>v</em><em>g</em>(<em>x</em>)<sup>2</sup></span> 
(in stat) <span
class="math inline"><em>s</em><em>t</em><em>d</em>(<em>x</em>)<sup>2</sup> = <em>v</em><em>a</em><em>r</em>(<em>x</em>)</span>
분산 

##### \(5) 표준편차와 Chebychev 부등식 

만약 차원 <span class="math inline"><em>n</em></span>-벡터에서 <span
class="math inline">|<em>x</em><sub><em>i</em></sub> − <em>a</em><em>v</em><em>g</em>(<em>x</em>)| ≥ <em>a</em></span>을
만족하는 원소 개수를 <span class="math inline"><em>k</em></span>라 하면
<span class="math inline">$\frac{k}{n} \leq
(\frac{std(x)}{a})^{2}$</span>이다. 벡터 <span
class="math inline"><em>x</em></span> 평균으로부터 <span
class="math inline"><em>k</em></span> 표준편차 이내에 있는 성분 비율은
최소 <span
class="math inline">1 − 1/<em>k</em><sup>2</sup></span>이다. 

$$
P(|X - \mu| > k\sigma) \leq 1 -\frac{1}{k^{2}}
$$ 
예를 들어, 일정 기간 투자 평균 수익률은 8%이고, 리스크(표준편차)는
3%입니다. 체비셰프의 부등식에 따르면, 손실을 기록한 기간의 비율(즉, 0%
이하인 기간, 16% 이상인 기간)은 최대 (3/8)^2 = 14.1%이다. 

##### \(6) 실증적 규칙 

<span
class="math display"><em>P</em>(|<em>X</em> − <em>μ</em>| ≤ <em>k</em><em>σ</em>)</span> 
<ul>
<li><span class="math inline"><em>k</em> = 1</span>, 데이터의 68.3%가
<span
class="math inline">(<em>μ</em> − <em>σ</em>, <em>μ</em> + <em>σ</em>)</span>
내에 있음 </li>
<li><span class="math inline"><em>k</em> = 2</span>, 데이터의 95.4%,
<span class="math inline"><em>k</em> = 3</span>, 데이터의 99.9% </li>
</ul>

![](/images/실증적규칙.png){fig-align="center" width="60%"}

##### 3. 거리와 개체 군집화 
##### \(1) 개념
<span class="math inline"><em>N</em></span>개의 차수 <span
class="math inline"><em>n</em></span>-벡터 <span
class="math inline">(<em>x</em><sub>1</sub>, <em>x</em><sub>2</sub>, ..., <em>x</em><sub><em>N</em></sub>)</span>에
대하여 각 벡터(개체) 쌍 사이의 거리로 측정하여 서로 가까운 클러스터 또는
클러스터로 묶는 작업을 다룬다. 클러스터링의 목표는 가능한 경우 벡터들을
<span class="math inline"><em>k</em></span>개의 클러스터 또는 클러스터로
묶거나 나누어, 각 클러스터 내의 벡터들이 서로 가깝도록 하는 것이다.
클러스터링은 벡터들이 객체의 특징을 나타낼 때 널리 사용된다. 다음은
<span class="math inline"><em>n</em> = 2</span>(군집변수 2개), <span
class="math inline"><em>k</em> = 3</span>으로 클러스터링 한
사례이다. 

![](/images/클러스터링.png){fig-align="center" width="60%"}

##### \(2) 클러스터 할당 

<span class="math inline"><em>N</em></span>개 개체, <span
class="math inline"><em>x</em><sub><em>i</em></sub></span>를 개체(<span
class="math inline"><em>i</em> = 1, 2, ..., <em>N</em></span>), <span
class="math inline"><em>c</em><sub><em>i</em></sub></span>는 <span
class="math inline"><em>i</em></span>-개체가 할당된 클러스터이고 (<span
class="math inline"><em>j</em> = 1, 2, ...<em>k</em></span>), <span
class="math inline"><em>G</em><sub><em>j</em></sub></span>을 <span
class="math inline"><em>j</em></span>-클러스터에 속한 개체의 집합이라
하자. 
<span
class="math display"><em>G</em><sub><em>j</em></sub> = {<em>i</em>|<em>c</em><sub><em>i</em></sub> = <em>j</em>}</span> 
클러스터을 대표하는 차원 <span
class="math inline"><em>n</em></span>-벡터를 <span
class="math inline"><em>z</em><sub>1</sub>, <em>z</em><sub>2</sub>, ..., <em>z</em><sub><em>k</em></sub></span>라
하자. <span class="math inline"><em>i</em></span>-개체가 <span
class="math inline"><em>j</em> = <em>c</em><sub><em>i</em></sub></span>에
있다면 <span
class="math inline"> ∥ <em>x</em><sub><em>i</em></sub> − <em>z</em><sub><em>c</em><sub><em>i</em></sub></sub>∥</span>은
모든 클러스터 중 가장 가까워야 한다. 

##### \(3) 클러스터 목적 

<span
class="math inline"><em>J</em><sup><em>c</em><em>l</em><em>u</em><em>s</em><em>t</em></sup> = ( ∥ <em>x</em><sub>1</sub> − <em>z</em><sub><em>c</em><sub>1</sub></sub> ∥ + ∥ <em>x</em><sub>2</sub> − <em>z</em><sub><em>c</em><sub>2</sub></sub> ∥ +...+ ∥ <em>x</em><sub><em>N</em></sub> − <em>z</em><sub><em>c</em><sub><em>N</em></sub></sub> ∥ )/<em>N</em></span>
함수를 최소화 하는 <span
class="math inline"><em>z</em><sub><em>c</em><sub>1</sub></sub>, <em>z</em><sub><em>c</em><sub>2</sub></sub>, ..., <em>z</em><sub><em>c</em><sub><em>N</em></sub></sub></span>을
구한다. 

##### \(4) 최적 클러스링

목적함수 <span
class="math inline"><em>J</em><sup><em>c</em><em>l</em><em>u</em><em>s</em><em>t</em></sup></span>을
최소화 하는 <span
class="math inline"><em>z</em><sub><em>c</em><sub>1</sub></sub>, <em>z</em><sub><em>c</em><sub>2</sub></sub>, ..., <em>z</em><sub><em>c</em><sub><em>N</em></sub></sub></span>을
찾는 것은 개체 수가 많고 차원 개수가 커지면 계산 회수가 기하 급수적으로
늘어나 불가능하다. 그러므로 최적 대신 차선 sub-optimal 방법으로 대표
벡터를 고정화 하는 k-평균 방법을 사용한다.

![](/images/최적클러스터링.png){fig-align="center" width="60%"}


##### 4. k-means 알고리즘 

##### \(1) 개념 

클러스터 할당과 클러스터 대표자를 선택하여 <span
class="math inline"><em>J</em><sup><em>c</em><em>l</em><em>u</em><em>s</em><em>t</em></sup></span>를
최소화하는 문제를 해결할 수 있을 것처럼 보이나 두 가지 선택은 순환적입니다. 즉, 각각의 선택이 다른 하나에 의존한다. 클러스터 대표자를 선택하고 클러스터 할당을 선택하는 것을 반복하는 것이 벡터 집합을 클러스터링하는 데 있어서 유명한 k-means 알고리즘이다. k-means 알고리즘은
1957년에 Stuart Lloyd와 독립적으로 Hugo Steinhaus에 의해 처음 제안되어
때때로 Lloyd 알고리즘이라고도 불린다. <span
dir="rtl">‘</span>k-means<span dir="rtl">’</span>라는 이름은
1960년대부터 사용되었다. 

##### \(2) k-평균 알고리즘 

<span class="math inline"><em>N</em></span>개 개체를 <span
class="math inline"><em>k</em></span>개 클러스터으로 분류한다고
가정하자. <span
class="math inline"><em>z</em><sub>1</sub>, <em>z</em><sub>2</sub>, ..., <em>z</em><sub><em>k</em></sub></span>을
각 클러스터의 대표 벡터라 하자. k-평균 알고리즘은 다음 작업을 반복
실행한다. 

&nbsp; 1. 대표 벡터를 결정하고 각 개체를 가장 가까운 대표 벡터의
클러스터으로 분류한다. 

&nbsp; 2. 클러스터에 할당된 개체의 중심점(평균 벡터)을 대표 벡터로
설정한다.

&nbsp; 3. 수렴 조건 만족 때까지 위의 작업을 반복한다. 

##### \(3)이슈사항 

##### 타이 브레이커 

두 개 이상의 클러스터과 최소 거리인 개체는 클러스터 할당을 하지
않는다. 그러므로 이 개체는 다음 단계에서 대표 벡터 결정에는 활용되지
않는다. 

##### 수렴 조건 

개체의 클러스터 이동이 더 이상 발생하지 않으면 대표 벡터는 움직이지
않음을 의미하므로 클러스터링 결과는 동일해진다. 

##### k-평균 알고리즘은 직관적이다. 

목표함수 <span
class="math inline"><em>J</em><sup><em>c</em><em>l</em><em>u</em><em>s</em><em>t</em></sup></span>을
최적화 하지 못하지만 반복을 통하여 줄여 나가게 된다. 

##### 대표벡터 해석 

&nbsp; 각 <span class="math inline"><em>N</em></span>개의 회사마다 총
자본화, 분기별 수익 및 위험, 거래량, 손익, 배당금 등과 같은 금융 및 사업 속성을 구성 요소로 하는 n-벡터을 이용하여 k-평균 클러스터링 결과 얻은 대표벡터를 이용하여 클러스터(군집)에 이름을 부여한다. 기업연수, 기업종류, 매출액 등 군집변수로 사용하지 않은 특성 벡터를 이용하여 개체군의 이름을 부여하고 해석한다.

##### 클러스터 <span class="math inline"><em>k</em></span> 결정 

k 의 결정은 다소 주관적이고
시행착오 방법을 사용한다. <span
class="math inline">(<em>k</em>, <em>J</em><sup><em>c</em><em>l</em><em>u</em><em>s</em><em>t</em></sup>)</span>을
이용하여 Elbow Method 팔꿈치 기법을 사용한다. 군집 개수가 증가할수록
<span
class="math inline"><em>J</em><sup><em>c</em><em>l</em><em>u</em><em>s</em><em>t</em></sup></span>는
감소하게 되지만, 이 감소율이 꺾이는 지점을 찾아내는 방법이다. 

##### 고정 대표 벡터 분할하기 
만약 <span class="math inline"><em>j</em></span> 클러스터을 대표하는
벡터 <span
class="math inline"><em>z</em><sub>1</sub>, <em>z</em><sub>2</sub>, ..., <em>z</em><sub><em>j</em></sub></span>르
고정하면 모든 개체 <span
class="math inline"><em>x</em><sub>1</sub>, <em>x</em><sub>2</sub>, ..., <em>x</em><sub><em>N</em></sub></span>을
최적 클러스터으로 분류 문제는 다음과 같다. 
<span
class="math display"> ∥ <em>x</em><sub><em>i</em></sub> − <em>z</em><sub><em>c</em><sub><em>i</em></sub></sub> ∥  = <em>m</em><em>i</em><em>n</em><sub><em>j</em> = 1, 2, ..., <em>k</em></sub> ∥ <em>x</em><sub><em>i</em></sub> − <em>z</em><sub><em>j</em></sub>∥</span> 
고정 대표 벡터를 활용하면 최적 클러스터링 문제는 다음과 같이 sub 최적
문제로 변환된다. 각 <span class="math inline"><em>N</em></span>개 개체에
최적 <span class="math inline"><em>j</em></span>-클러스터(거리가 가장
가까운 클러스터)을 결정하는 개별적 문제와 동일하다. 
<blockquote>
<span
class="math display"><em>J</em><sup><em>c</em><em>l</em><em>u</em><em>s</em><em>t</em></sup> = <em>m</em><em>i</em><em>n</em><sub><em>j</em> = 1, 2, ..., <em>k</em></sub> ∥ <em>x</em><sub>1</sub> − <em>z</em><sub><em>j</em></sub> ∥ +... + <em>m</em><em>i</em><em>n</em><sub><em>j</em> = 1, 2, ..., <em>k</em></sub> ∥ <em>x</em><sub><em>N</em></sub> − <em>z</em><sub><em>j</em></sub> ∥ )/<em>N</em></span> 
</blockquote>
고정 벡터를 group(or cluster) centroid라 한다. 

##### \(4) 사례 

```python
# 60000(train 훈련)/10000(test 테스트), 28x28
import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.datasets import mnist
# MNIST 데이터셋 로드 및 훈련데이터, 테스트데이터 분할 
(x_train, y_train), (x_test, y_test) = mnist.load_data()
# 데이터 형태 출력
print(f"x_train shape: {x_train.shape}")
print(f"y_train shape: {y_train.shape}")
print(f"x_test shape: {x_test.shape}")
print(f"y_test shape: {y_test.shape}")
# 첫 10개 샘플 이미지와 레이블 시각화
num_samples = 10
plt.figure(figsize=(10, 1))
for i in range(num_samples):
    plt.subplot(1, num_samples, i+1)
    plt.imshow(x_train[i], cmap='gray')
    plt.title(y_train[i])
    plt.axis('off')
plt.show()
```

![](/images/클러스터링예제.png){fig-align="center"}

```python
# 훈련 데이터 클러스트링, 첫 20개 군집결과
# 이미지 데이터를 2차원 배열로 변환
x_train2 = x_train.reshape((x_train.shape[0], -1))
x_test2 = x_test.reshape((x_test.shape[0], -1))
# 데이터 정규화
x_train2 = x_train2 / 255.0
x_test2 = x_test2 / 255.0
# k-means 모델 생성 및 학습
kmeans = KMeans(n_clusters=10, random_state=42)
kmeans.fit(x_train2)
# 클러스터 할당 결과
y_kmeans = kmeans.predict(x_train2)
# 첫 20개 분류결과 이미지와 레이블 시각화
num_samples = 20
plt.figure(figsize=(10, 1))
for i in range(num_samples):
    plt.subplot(1, num_samples, i+1)
    plt.imshow(x_train[i], cmap='gray')
    plt.title(y_kmeans[i])
    plt.axis('off')
plt.show()
```

![](/images/클러스터링예제2.png){fig-align="center"}

```python
#클러스터 3 평균벡터 출력 클러스터 대표 이미지
plt.figure(figsize=(10, 1))
plt.imshow((x_train[0]+x_train[7]+x_train[17])/3, cmap='gray')
plt.title('cluster 3')
plt.axis('off')
plt.show()
```
![](/images/클러스터링예제3.png){fig-align="center" width="60%"}

##### 5. 벡터의 각도
##### \(1) 코사인 유사도 

벡터의 코사인 유사도(Cosine Similarity)는 두 벡터 간의 방향적 유사성을 측정하는 지표로, 벡터 간의 각도 <span class="math inline"><em>θ</em></span>의 코사인 값을 이용하여
계산된다. 
<span class="math display">$$\text{Cosine Similarity} = \cos(\theta)
= \frac{\mathbf{A} \cdot \mathbf{B}}{\parallel \mathbf{A} \parallel
\parallel \mathbf{B} \parallel}$$</span> 
코사인 유사도와 유클리드 거리의 차이는 다음과 같다. 
<ul>
<li>코사인 유사도는 두 벡터의 방향에 집중하며, 벡터 크기의 차이를
무시한다. </li>
<li>유클리드 거리는 두 벡터 사이의 실제 거리(크기 차이 포함)를
측정한다. </li>
<li>예를 들어, 텍스트 데이터에서 코사인 유사도는 문서 간의 내용적 유사성을 비교하는 데 유리하며, 추천 시스템, 정보 검색, 클러스터링 등에서 널리 사용된다. </li>
</ul>

【예제】 
<table>
<tr>
<td>벡터 <span
class="math inline"><strong>A</strong> = [1, 2, 3], <strong>B</strong> = [4, 5, 6]</span>이 주어졌을 때

1. 벡터 내적: 
<span
class="math display"><strong>A</strong> ⋅ <strong>B</strong> = 1 ⋅ 4 + 2 ⋅ 5 + 3 ⋅ 6 = 32</span> 

2. 벡터 크기: 
<span class="math display">$$\parallel \mathbf{A} \parallel =
\sqrt{1^{2} + 2^{2} + 3^{2}} = \sqrt{14}, \parallel \mathbf{B} \parallel
= \sqrt{4^{2} + 5^{2} + 6^{2}} = \sqrt{77}$$</span> 

3. 코사인 유사도:  <span class="math display">$$\cos(\theta) = \frac{\mathbf{A} \cdot \mathbf{B}}{\parallel \mathbf{A} \parallel \parallel \mathbf{B} \parallel} = \frac{32}{\sqrt{14} \cdot \sqrt{77}} \approx
0.975$$</span> </td>
</tr>
</tbody>
</table>

##### \(2) 코사인 유사도의 특징
코사인 유사도 값의 범위는 [-1, 1]이고 다음의 특징을 갖는다. 

- <span class="math inline">cos (<em>θ</em>) = 1</span>: 두 벡터가
완전히 같은 방향 
- <span class="math inline">cos (<em>θ</em>) = 0</span>: 두 벡터가
직교(Orthogonal, 90도) 
- <span class="math inline">cos (<em>θ</em>) = −1</span>: 두 벡터가
완전히 반대 방향. 

코사인 유사도는 벡터의 크기가 아닌 방향만 고려되므로 벡터를
정규화하지 않고도 비교할 수 있다. 고차원 벡터에도 적용 가능하여 텍스트
데이터, 사용자 선호도 등 고차원 데이터에서 벡터 간 유사성 측정에 많이
사용된다. 
각도 종류 
<ol type="1">
<li>각도가 <span
class="math inline"><em>θ</em> = 90<sup><em>o</em></sup> = <em>π</em>/2</span>이면
두 벡터는 직교 orthogonal 한다. </li>
<li>각도가 <span
class="math inline"><em>θ</em> = 0<sup><em>o</em></sup></span>이면 두
벡터는 정렬 aligned 되어 있다. </li>
<li>각도가 <span
class="math inline"><em>θ</em> = 180<sup><em>o</em></sup> = <em>π</em></span>이면
두 벡터는 역정렬 anti-aligned 되어 있다. </li>
<li>각도가 <span
class="math inline"><em>θ</em> &gt; 90<sup><em>o</em></sup> = <em>π</em>/2</span>이면
두 벡터의 각은 둔각 obtuse, <span
class="math inline"><em>θ</em> &lt; 90<sup><em>o</em></sup> = <em>π</em>/2</span>이면
두 벡터의 각은 예각 acute 이다. </li>
</ol>

![](/images/각도.png){fig-align="center" width="60%"}

##### 두 벡터 합의 놈과 각도 
<span
class="math display"> ∥ <em>x</em> + <em>y</em>∥<sup>2</sup> =  ∥ <em>x</em>∥<sup>2</sup> + 2 ∥ <em>x</em> ∥  ∥ <em>y</em> ∥ <em>c</em><em>o</em><em>s</em>(<em>θ</em>)+ ∥ <em>y</em>∥<sup>2</sup></span> 
만약 <span
class="math inline"><em>θ</em> = 90<sup><em>o</em></sup> = <em>π</em>/2</span>이면
<span
class="math inline"> ∥ <em>x</em> + <em>y</em>∥<sup>2</sup> =  ∥ <em>x</em>∥<sup>2</sup>+ ∥ <em>y</em>∥<sup>2</sup></span>
(피타고라스 정리) 

##### 6. 상관계수 

##### \(1) 상관계수 정의 

만약 <span class="math inline">$\overset{˜}{a} = a -
avg(a)1,\overset{˜}{b} = b - avg(b)1$</span>이면, 상관계수(correlation
coefficient) <span class="math inline"><em>ρ</em></span>는 다음과 같이
정의된다. 
<span class="math inline">$\rho =
\frac{{\overset{˜}{a}}^{T}\overset{˜}{b}}{\parallel \overset{˜}{a}
\parallel \parallel \overset{˜}{b} \parallel}$</span> ⇔ <span
class="math inline">$\rho =
(\frac{\overset{˜}{a}}{std(a)})^{T}(\frac{\overset{˜}{b}}{std(b)})/n$</span> 

- <span class="math inline">$cov(a,b) =
{\overset{˜}{a}}^{T}\overset{˜}{b}/n$</span>: 두 벡터의 공분산 
- <span
class="math inline"><em>v</em><em>a</em><em>r</em>(<em>a</em>) = <em>s</em><em>t</em><em>d</em>(<em>a</em>)<sup>2</sup></span>:
벡터의 분산 
상관계수와 공분산 관계: <span
class="math inline"><em>c</em><em>o</em><em>v</em>(<em>a</em>, <em>b</em>) = <em>ρ</em><em>s</em><em>t</em><em>d</em>(<em>a</em>)<em>s</em><em>t</em><em>d</em>(<em>b</em>)</span> 
- <span class="math inline"><em>ρ</em> = ±1</span> (완전 상관) : 두
벡터가 (역)정렬되어 있음 
- <span class="math inline"><em>ρ</em> = 0</span> (독립) : 두 벡터가
직교되어 있음. <span
class="math inline"><em>c</em><em>o</em><em>v</em>(<em>a</em>, <em>b</em>) = 0</span> 

##### \(2) 두 벡터 합의 분산 

<span
class="math display"><em>v</em><em>a</em><em>r</em>(<em>a</em> + <em>b</em>) = <em>v</em><em>a</em><em>r</em>(<em>a</em>) + 2<em>c</em><em>o</em><em>v</em>(<em>a</em>, <em>b</em>) + <em>v</em><em>a</em><em>r</em>(<em>b</em>)</span> 

<span
class="math display"><em>v</em><em>a</em><em>r</em>(<em>a</em> + <em>b</em>) = <em>v</em><em>a</em><em>r</em>(<em>a</em>) + 2<em>ρ</em><em>s</em><em>t</em><em>d</em>(<em>a</em>)<em>s</em><em>t</em><em>d</em>(<em>b</em>) + <em>v</em><em>a</em><em>r</em>(<em>b</em>)</span> 
<ul>
<li>만약 <span class="math inline"><em>ρ</em> = 0</span>이면, <span
class="math inline"><em>v</em><em>a</em><em>r</em>(<em>a</em> + <em>b</em>) = <em>v</em><em>a</em><em>r</em>(<em>a</em>) + <em>v</em><em>a</em><em>r</em>(<em>b</em>)</span> </li>
<li>만약 <span class="math inline"><em>ρ</em> = 1</span>이면, <span
class="math inline"><em>v</em><em>a</em><em>r</em>(<em>a</em> + <em>b</em>) = (<em>s</em><em>t</em><em>d</em>(<em>a</em>) + <em>s</em><em>t</em><em>d</em>(<em>b</em>))<sup>2</sup></span> </li>
<li>만약 <span class="math inline"><em>ρ</em> = −1</span>이면, <span
class="math inline"><em>v</em><em>a</em><em>r</em>(<em>a</em> + <em>b</em>) = (<em>s</em><em>t</em><em>d</em>(<em>a</em>) − <em>s</em><em>t</em><em>d</em>(<em>b</em>))<sup>2</sup></span> </li>
</ul>

##### \(3) 헤징 hedging 투자 
  두 개 회사 주가 벡터 <span
class="math inline">(<em>a</em>, <em>b</em>)</span>의 평균은<span
class="math inline"><em>μ</em></span>, 표준편차(위험) <span
class="math inline"><em>σ</em></span>이고 상관계수는 <span
class="math inline"><em>ρ</em></span>이다. 각각 50% 투자, <span
class="math inline">$c = \frac{(a + b)}{2}$</span>의 평균 수익율과
표준편차은 다음과 같다. 
<ul>
<li>  평균 : <span class="math inline">$avg(\frac{a + b}{2}) =
\mu$</span> </li>
<li>  표준편차 : <span class="math inline">$std(c) = \sigma\sqrt{(1 +
\rho)/2}$</span> </li>
<li>  상관계수 <span class="math inline"><em>ρ</em> = 0</span>이면
(독립) 표준편차는 <span
class="math inline">$\frac{1}{\sqrt{2}}$</span>만큼 줄어든다. </li>
<li>  완벽한 상관관계가 있는 경우에만 표준편차는 동일하다. </li>
</ul>


### <span style="color:green">chapter 5. 선형독립 </span>

##### 1. 선형독립 정의 

##### \(1) 선형 종속 linear dependence

  <span class="math inline"><em>k</em> ≥ 2</span>개의 크기 n-벡터 <span
class="math inline"><em>x</em><sub>1</sub>, <em>x</em><sub>2</sub>, ..., <em>x</em><sub><em>k</em></sub></span>가
다음을 만족하면 선형종속이라 한다. 만약 <span
class="math inline"><em>a</em><sub>1</sub><em>x</em><sub>1</sub> + <em>a</em><sub>2</sub><em>x</em><sub>2</sub> + ... + <em>a</em><sub><em>k</em></sub><em>x</em><sub><em>k</em></sub> = 0</span>을
만족하는 <span
class="math inline"><em>a</em><sub><em>i</em></sub></span>가 적어도
하나는 0이 아니다. 
  선형독립이면 적어도 하나의 <span
class="math inline"><em>a</em><sub><em>i</em></sub></span>는 0이
아니므로 벡터 <span
class="math inline"><em>x</em><sub><em>i</em></sub></span> 다음과 같이
다른 벡터의 선형함수로 표현될 수 있다. 
  <span class="math display">$$x_{k} = \frac{- a_{1}}{a_{i}}x_{1} + ...
+ \frac{- a_{i - 1}}{a_{i}}x_{i - 1} + \frac{- a_{i + 1}}{a_{i}}x_{i +
1} + ... + \frac{- a_{k}}{a_{i}}x_{k}$$</span> 
  【예제】 
<table>
<colgroup>
<col style="width: 100%" />
</colgroup>
<tbody>
<tr>
<td style="text-align: left;"><blockquote>
  <span class="math inline">$x_{1} = \left\lbrack \begin{array}{r}
0 \\
- 1 \\
1
\end{array} \right\rbrack,x_{2} = \left\lbrack \begin{array}{r}
1 \\
- 2 \\
1
\end{array} \right\rbrack,x_{3} = \left\lbrack \begin{array}{r}
1 \\
0 \\
- 1
\end{array} \right\rbrack$</span> ⬄<span
class="math inline">−2<em>x</em><sub>1</sub> + <em>x</em><sub>2</sub> − <em>x</em><sub>3</sub> = 0</span> 
</blockquote></td>
</tr>
</tbody>
</table>

##### \(2) 선형 독립 linear independence 
  만약 <span
class="math inline"><em>a</em><sub>1</sub><em>x</em><sub>1</sub> + <em>a</em><sub>2</sub><em>x</em><sub>2</sub> + ... + <em>a</em><sub><em>k</em></sub><em>x</em><sub><em>k</em></sub> = 0</span>이
모든 <span
class="math inline"><em>a</em><sub><em>k</em></sub> = 0</span>일 때만
만족한다면, n-벡터 <span
class="math inline"><em>x</em><sub>1</sub>, <em>x</em><sub>2</sub>, ..., <em>x</em><sub><em>k</em></sub></span>을
선형독립이라 한다. 
  【예제】 
<table>
<colgroup>
<col style="width: 100%" />
</colgroup>
<tbody>
<tr>
<td style="text-align: left;"><blockquote>
  <span class="math display">$$x_{1} = \left\lbrack \begin{array}{r}
1 \\
0 \\
0
\end{array} \right\rbrack,x_{2} = \left\lbrack \begin{array}{r}
0 \\
- 1 \\
1
\end{array} \right\rbrack,x_{3} = \left\lbrack \begin{array}{r}
- 1 \\
1 \\
1
\end{array} \right\rbrack$$</span> 
</blockquote></td>
</tr>
</tbody>
</table>

##### \(3) 선형독립 벡터의 선형결합 

선형독립인 <span
class="math inline"><em>x</em><sub>1</sub>, <em>x</em><sub>2</sub>, ..., <em>x</em><sub><em>k</em></sub></span>의
선형결합의 모든 계수(<span
class="math inline"><em>a</em><sub><em>k</em></sub></span>)는 유일하다.
선형결합 <span
class="math inline"><em>x</em> = <em>a</em><sub>1</sub><em>x</em><sub>1</sub> + <em>a</em><sub>2</sub><em>x</em><sub>2</sub> + ... + <em>a</em><sub><em>k</em></sub><em>x</em><sub><em>k</em></sub></span> 
  증명 
  다른 계수를 <span
class="math inline"><em>b</em><sub><em>k</em></sub></span>라 하자. <span
class="math inline"><em>x</em> = <em>b</em><sub>1</sub><em>x</em><sub>1</sub> + <em>b</em><sub>2</sub><em>x</em><sub>2</sub> + ... + <em>b</em><sub><em>k</em></sub><em>x</em><sub><em>k</em></sub></span> 
  <span
class="math inline">0 = (<em>a</em><sub>1</sub> − <em>b</em><sub>1</sub>)<em>x</em><sub>1</sub> + (<em>a</em><sub>2</sub> − <em>b</em><sub>2</sub>)<em>x</em><sub>2</sub> + ... + (<em>a</em><sub><em>k</em></sub> − <em>b</em><sub><em>k</em></sub>)<em>x</em><sub><em>k</em></sub></span>이다. 
  <span
class="math inline"><em>x</em><sub>1</sub>, <em>x</em><sub>2</sub>, ..., <em>x</em><sub><em>k</em></sub></span>가
선형독립이므로 모든 <span
class="math inline">(<em>a</em><sub><em>i</em></sub> − <em>b</em><sub><em>i</em></sub>) = 0</span>
만족한다. 

##### 2. 기저 

##### \(1) 기저 개념 

벡터 공간은 다양한 차원의 벡터로 이루어진 공간이며, 그 공간 안의 벡터들을 다른 벡터들의 선형 조합으로 표현할 수 있다. 이때, 특정 벡터 공간의 기저 basis 는 그 공간 안의 모든 벡터들을 생성할 수 있는 최소한의 독립적인 벡터들의 집합이다. 
예를 들어, 2차원 공간에서의 기저는 일반적으로 (1,0)과 (0,1)이다. 이 두 벡터는 선형 독립이며, 이들의 모든 선형 조합으로 2차원 평면 상의 어떤 점이든 표현할 수 있다. 따라서 (1,0)과 (0,1)은 2차원 공간의 기저입니다. 단, 벡터 공간의 기저는 유일하지 않다. 
크기 2인 벡터의 기저 벡터는 <span
class="math inline"><em>k</em> = 2</span>개이다. 위의 그림에서 <span
class="math inline"><em>a</em><sub>3</sub></span>벡터는<img
src="media/image20.png" style="width:3.92546in;height:3.97194in"
alt="붙여넣은 동영상.png" /> <span
class="math inline">(<em>a</em><sub>1</sub>, <em>a</em><sub>2</sub>)</span>(기저벡터)의 선형 결합으로 만들 수 있다. 

![](/images/벡터기저.png){fig-align="center" width="60%"}

##### \(2) 기저 정의 
n개의 선형독립인 크기 n-벡터를 기저 basis 라 한다. 즉, n-벡터 <span
class="math inline">(<em>x</em><sub>1</sub>, <em>x</em><sub>2</sub>, ..., <em>x</em><sub><em>n</em></sub>)</span>가
기저이면, 모든 크기 n-벡터는 <span
class="math inline">(<em>x</em><sub>1</sub>, <em>x</em><sub>2</sub>, ..., <em>x</em><sub><em>n</em></sub>)</span>의
선형 결합으로 표현할 수 있다. 

【증명】 (n+1)개 차원 n-벡터 <span
class="math inline">(<em>x</em><sub>1</sub>, <em>x</em><sub>2</sub>, ..., <em>x</em><sub><em>n</em></sub>, <em>y</em>)</span>개가
있다고 가정하자. 단,<span
class="math inline">(<em>x</em><sub>1</sub>, <em>x</em><sub>2</sub>, ..., <em>x</em><sub><em>n</em></sub>)</span>
선형독립이며 기저이다. 이들 벡터는 선형독립(차원개수 n보다 벡터 개수가
(n+1)로 크다)이므로 다음을 만족하는 모든 <span
class="math inline"><em>a</em><sub><em>i</em></sub></span>가 0은 아니다.
<span
class="math inline"><em>a</em><sub>1</sub><em>x</em><sub>1</sub> + <em>a</em><sub>2</sub><em>x</em><sub>2</sub> + ... + <em>a</em><sub><em>n</em></sub><em>x</em><sub><em>n</em></sub> + <em>a</em><sub><em>n</em> + 1</sub><em>y</em> = 0</span> 

만약 <span
class="math inline"><em>a</em><sub><em>n</em> + 1</sub> = 0</span>이면,
<span
class="math inline"><em>a</em><sub>1</sub><em>x</em><sub>1</sub> + <em>a</em><sub>2</sub><em>x</em><sub>2</sub> + ... + <em>a</em><sub><em>n</em></sub><em>x</em><sub><em>n</em></sub> = 0</span>을
만족하는 모든 <span
class="math inline"><em>a</em><sub><em>i</em></sub> = 0</span>이다.
왜냐하면 <span
class="math inline">(<em>x</em><sub>1</sub>, <em>x</em><sub>2</sub>, ..., <em>x</em><sub><em>n</em></sub>)</span>
선형독립이기 때문이다.(모순) 

##### 3. 직교정규 

##### \(1) 정의
만약 <span
class="math inline"> ∥ <em>x</em><sub><em>i</em></sub> ∥  = 1</span>이고
<span
class="math inline"><em>x</em><sub><em>i</em></sub><sup><em>T</em></sup><em>x</em><sub><em>j</em></sub> = 0<em>f</em><em>o</em><em>r</em><em>i</em> ≠ <em>j</em></span>
(두 벡터 <span
class="math inline">(<em>x</em><sub><em>i</em></sub>, <em>x</em><sub><em>j</em></sub>)</span>는
직교)이면, <span
class="math inline">(<em>x</em><sub>1</sub>, <em>x</em><sub>2</sub>, ..., <em>x</em><sub><em>k</em></sub>)</span>
벡터 집합은 직교 정규 orthonormal 벡터라고 한다. 
  직교정규성은 선형종속, 선형독립처럼 <u>집합의 속성</u>이지 개별
벡터의 속성은 아니다. 

###### \(2) 예제 
  n개의 단위벡터는 직교정규 벡터이다. 
  직교정규벡터 <span class="math inline">$\left\lbrack \begin{array}{r}
- 1 \\
0 \\
0
\end{array} \right\rbrack,\frac{1}{\sqrt{2}}\left\lbrack
\begin{array}{r}
0 \\
1 \\
1
\end{array} \right\rbrack,\frac{1}{\sqrt{2}}\left\lbrack
\begin{array}{r}
0 \\
- 1 \\
1
\end{array} \right\rbrack$</span> 
  직교정규 벡터는 선형독립이다. </li>

##### \(3)직교정규 성질 

<ol type="1">
<li>  벡터 <span class="math inline"><em>x</em></span>가 직교정규벡터
선형결합이면 <span
class="math inline"><em>x</em> = <em>a</em><sub>1</sub><em>x</em><sub>1</sub> + <em>a</em><sub>2</sub><em>x</em><sub>2</sub> + ... + <em>a</em><sub><em>k</em></sub><em>x</em><sub><em>k</em></sub></span>
내적을 이용하여 다음을 얻으므로 내적을 이용하여 계수를 얻을 수
있다.<br />
<span
class="math inline"><em>x</em><sub><em>i</em></sub><sup><em>T</em></sup><em>x</em> = <em>x</em><sub><em>i</em></sub><sup><em>T</em></sup>(<em>a</em><sub>1</sub><em>x</em><sub>1</sub> + <em>a</em><sub>2</sub><em>x</em><sub>2</sub> + ... + <em>a</em><sub><em>k</em></sub><em>x</em><sub><em>k</em></sub>) = <em>a</em><sub><em>i</em></sub></span> </li>
</ol>
<ol start="2" type="1">
<li>  벡터 <span
class="math inline">(<em>x</em><sub>1</sub>, <em>x</em><sub>2</sub>, ..., <em>x</em><sub><em>k</em></sub>)</span>가
직교정규 (선형독립이고 기저임) 벡터이면 <span
class="math inline"><em>x</em> = (<em>x</em><sub>1</sub><sup><em>T</em></sup><em>x</em>)<em>x</em><sub>1</sub> + (<em>x</em><sub>2</sub><sup><em>T</em></sup><em>x</em>)<em>x</em><sub>2</sub> + ... + (<em>x</em><sub><em>k</em></sub><sup><em>T</em></sup><em>x</em>)<em>x</em><sub><em>k</em></sub></span>이
성립한다. </li>
</ol>
  벡터 (1, 2, 3)을 직교정규 벡터의 선형결합으로 표현하자. 
  <span class="math display">$$\left\lbrack \begin{array}{r}
1 \\
2 \\
3
\end{array} \right\rbrack = 1\left\lbrack \begin{array}{r}
1 \\
0 \\
0
\end{array} \right\rbrack + 2\left\lbrack \begin{array}{r}
0 \\
1 \\
0
\end{array} \right\rbrack + 3\left\lbrack \begin{array}{r}
0 \\
0 \\
1
\end{array} \right\rbrack$$</span> 
<ul>
<li>  <span class="math inline">$\lbrack - 100\rbrack\left\lbrack
\begin{array}{r}
1 \\
2 \\
3
\end{array} \right\rbrack = - 1$</span>, <span
class="math inline">$\frac{1}{\sqrt{2}}\lbrack 011\rbrack\left\lbrack
\begin{array}{r}
1 \\
2 \\
3
\end{array} \right\rbrack = \frac{5}{\sqrt{2}}$</span>, <span
class="math inline">$\frac{1}{\sqrt{2}}\lbrack 0 - 11\rbrack\left\lbrack
\begin{array}{r}
1 \\
2 \\
3
\end{array} \right\rbrack = \frac{1}{\sqrt{2}}$</span> </li>
</ul>
  <span class="math display">$$\left\lbrack \begin{array}{r}
1 \\
2 \\
3
\end{array} \right\rbrack = - 1\left\lbrack \begin{array}{r}
- 1 \\
0 \\
0
\end{array} \right\rbrack + \frac{5}{2}\left\lbrack \begin{array}{r}
0 \\
1 \\
1
\end{array} \right\rbrack + \frac{1}{2}\left\lbrack \begin{array}{r}
0 \\
- 1 \\
1
\end{array} \right\rbrack$$</span> 

##### 4. Gram-Schmidt 알고리즘 

##### \(1) 개념 
n-벡터 <span
class="math inline"><em>x</em><sub>1</sub>, <em>x</em><sub>2</sub>, ..., <em>x</em><sub><em>k</em></sub></span>가
선형 독립인지 여부를 결정할 수 있는 알고리즘으로 수학자 Jørgen Pedersen
Gram과 Erhard Schmidt의 이름을 따서 명명되었다. 
  만약 벡터들이 선형 독립이라면, Gram–Schmidt 알고리즘은 다음과 같은
속성을 가진 직교정규 벡터 <span
class="math inline"><em>q</em><sub>1</sub>, <em>q</em><sub>2</sub>, ..., <em>q</em><sub><em>k</em></sub></span>
을 생성한다. 
<ol type="1">
<li>  각 <span
class="math inline"><em>i</em> = 1, 2, ..., <em>k</em></span>에서 <span
class="math inline"><em>x</em><sub><em>i</em></sub></span>는 <span
class="math inline"><em>q</em><sub>1</sub>, <em>q</em><sub>2</sub>, ..., <em>q</em><sub><em>i</em></sub></span>의
선형결합이다. </li>
</ol>
<ol start="3" type="1">
<li>  각 <span
class="math inline"><em>i</em> = 1, 2, ..., <em>k</em></span>에서 <span
class="math inline"><em>q</em><sub><em>i</em></sub></span>는 <span
class="math inline"><em>x</em><sub>1</sub>, <em>x</em><sub>2</sub>, ..., <em>x</em><sub><em>i</em></sub></span>의
선형결합이다. </li>
<li>  만약 <span
class="math inline"><em>x</em><sub>1</sub>, <em>x</em><sub>2</sub>, ..., <em>x</em><sub><em>i</em> − 1</sub></span>
선형독립이나 <span
class="math inline"><em>x</em><sub>1</sub>, <em>x</em><sub>2</sub>, ..., <em>x</em><sub><em>i</em></sub></span>는
선형종속이면 멈춘다. </li>
</ol>
<ol start="31" type="1">
<li>  알고리즘 </li>
</ol>
  주어진 n-벡터 <span
class="math inline"><em>x</em><sub>1</sub>, <em>x</em><sub>2</sub>, ..., <em>x</em><sub><em>k</em></sub></span>,
<span class="math inline"><em>i</em> = 1, 2, ..., <em>k</em></span>일
때 
<ol type="1">
<li>  직교화 : <span class="math inline">${\overset{˜}{q}}_{i} = x_{i}
- (q_{1}^{T}x_{i})q_{1} - ... - (q_{i - 1}^{T}x_{i})q_{i -
1}$</span> </li>
<li>  선형종속 검증 : 만약 <span
class="math inline">${\overset{˜}{q}}_{i} = 0$</span>이면,
멈춘다. </li>
<li>  정규화 : <span class="math inline">$q_{i} =
\frac{{\overset{˜}{q}}_{i}}{\parallel q_{i} \parallel}$</span>. </li>
</ol>
  이렇게 얻은 <span
class="math inline"><em>q</em><sub>1</sub>, <em>q</em><sub>2</sub>, ..., <em>q</em><sub><em>i</em></sub></span>는
직교정규 벡터이다. 알고리즘 적용 중 중간에 중단되면 기저젝터가
아니다. 

##### \(2) Gram-Schmidt 알고리즘 예제
  <span
class="math inline"><em>x</em><sub>1</sub> = (−1, 1, −1, 1), <em>x</em><sub>2</sub> = (−1, 3, −1, 3), <em>x</em><sub>3</sub> = (1, 3, 5, 7)</span>
에 대하여 Gram–Schmidt 알고리즘을 적용하자. 

###### i=1 
  <span class="math inline">$\parallel {\overset{˜}{q}}_{1} \parallel =
2$</span>이므로 <span class="math inline">$q_{1} =
\frac{{\overset{˜}{q}}_{1}}{\parallel {\overset{˜}{q}}_{1} \parallel} =
\left\lbrack \begin{array}{r}
- 1/2 \\
1/2 \\
- 1/2 \\
1/2
\end{array} \right\rbrack$</span>이다. 

###### i=2 
  <span
class="math inline"><em>q</em><sub>1</sub><sup><em>T</em></sup><em>x</em><sub>2</sub> = 4</span>이므로
<span class="math inline">${\overset{˜}{q}}_{2} = x_{2} -
(q_{1}^{T}x_{2})q_{1} = \left\lbrack \begin{array}{r}
1 \\
1 \\
1 \\
1
\end{array} \right\rbrack$</span>이고 <span
class="math inline">$\parallel {\overset{˜}{q}}_{2} \parallel =
2$</span>이다. 그러므로 <span class="math inline">$q_{2} =
\frac{{\overset{˜}{q}}_{2}}{\parallel {\overset{˜}{q}}_{2} \parallel} =
\left\lbrack \begin{array}{r}
1/2 \\
1/2 \\
1/2 \\
1/2
\end{array} \right\rbrack$</span>. 

###### i=3
  <span
class="math inline"><em>q</em><sub>1</sub><sup><em>T</em></sup><em>x</em><sub>3</sub> = 2, <em>q</em><sub>2</sub><sup><em>T</em></sup><em>x</em><sub>3</sub> = 8</span>이므로
<span class="math inline">${\overset{˜}{q}}_{3} = x_{3} -
(q_{1}^{T}x_{3})q_{1} - (q_{2}^{T}x_{3})q_{2} = \left\lbrack
\begin{array}{r}
- 2 \\
- 2 \\
2 \\
2
\end{array} \right\rbrack$</span>이고 <span
class="math inline">$\parallel {\overset{˜}{q}}_{3} \parallel =
4$</span>이다. 그러므로 <span class="math inline">$q_{3} =
\frac{{\overset{˜}{q}}_{3}}{\parallel {\overset{˜}{q}}_{3} \parallel} =
\left\lbrack \begin{array}{r}
- 1/2 \\
- 1/2 \\
1/2 \\
1/2
\end{array} \right\rbrack$</span>. 



```python
# Gram-Schmidt 알고리즘
import numpy as np

def gram_schmidt(A):
    # Get the number of rows (n) and columns (k) in A
    n, k = A.shape
    # Initialize matrix Q with zeros, same shape as A
    Q = np.zeros((n, k))
    
    for j in range(k):
        # Start with the current column vector of A
        v = A[:, j]
        for i in range(j):
            # Subtract the projection of v onto the ith orthonormal vector
            v -= np.dot(Q[:, i], A[:, j]) * Q[:, i]
        
        # Normalize the vector
        Q[:, j] = v / np.linalg.norm(v)
    return Q
# Example usage
A = np.array([[-1,-1,1],
              [1,3,3],
              [-1,-1,5],
              [1,3,7]], dtype=float)

gram_schmidt(A)

```
【결과】 array([[-0.5,  0.5, -0.5],
       [ 0.5,  0.5, -0.5],
       [-0.5,  0.5,  0.5],
       [ 0.5,  0.5,  0.5]])