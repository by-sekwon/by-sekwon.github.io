[
  {
    "objectID": "cardnews/index.html",
    "href": "cardnews/index.html",
    "title": "📰 카드뉴스 모음",
    "section": "",
    "text": "통계를 주제로 한 다양한 카드뉴스를 주제별로 모았습니다.\n클릭하면 자세한 내용을 보실 수 있어요 😊\n\n\n\n\n\n #### 고령화 속도 65세 이상 비율이 역대 최고!\n자세히 보기 ▶\n\n\n #### 출산율 감소 합계출산율 0.72명의 의미는?\n자세히 보기 ▶\n\n\n\n\n\n\n\n\n #### 청년 고용률 청년 고용의 구조적 문제\n자세히 보기 ▶\n\n\n #### 전공별 격차 전공 따라 취업률이 이렇게나!\n자세히 보기 ▶"
  },
  {
    "objectID": "cardnews/index.html#인구와-사회",
    "href": "cardnews/index.html#인구와-사회",
    "title": "📰 카드뉴스 모음",
    "section": "",
    "text": "#### 고령화 속도 65세 이상 비율이 역대 최고!\n자세히 보기 ▶\n\n\n #### 출산율 감소 합계출산율 0.72명의 의미는?\n자세히 보기 ▶"
  },
  {
    "objectID": "cardnews/index.html#청년과-고용",
    "href": "cardnews/index.html#청년과-고용",
    "title": "📰 카드뉴스 모음",
    "section": "",
    "text": "#### 청년 고용률 청년 고용의 구조적 문제\n자세히 보기 ▶\n\n\n #### 전공별 격차 전공 따라 취업률이 이렇게나!\n자세히 보기 ▶"
  },
  {
    "objectID": "cardnews/news001.html",
    "href": "cardnews/news001.html",
    "title": "고령화 속도",
    "section": "",
    "text": "👵 고령화 속도, 세계 최고\n\n\n\n고령화\n\n\n\n2024년 한국 65세 이상 인구: 18.4%\n고령사회(14%) 기준 이미 초과\n초고령사회(20%) 진입도 눈앞\n\n\n📌 정책은 얼마나 따라가고 있을까?"
  },
  {
    "objectID": "cardnews/news003.html",
    "href": "cardnews/news003.html",
    "title": "청년 고용률",
    "section": "",
    "text": "청년 고용률\n\n\n2023년 기준, 15~29세 청년 고용률은 45.4%\n코로나 이후 회복 중이지만 여전히 회복세는 느림.\n\n\n\n\n청년층 일자리 중 단기 계약직, 인턴 비중이 높음\n\n정규직 진입률은 10년 전보다 낮아진 상태\n\n\n🔍 “취업했다”는 통계 뒤에 숨은 고용의 질 문제를 들여다봐야 합니다.\n\n\n\n\n\n\n일자리 찾기 → 계약직 → 재취업 준비 → 불안정 반복\n이직까지 평균 1.5년 소요\n\n\n\n\n\n\n\nNote\n\n\n\n📌 통계청 자료 기준: 2023 경제활동인구조사 청년 부문\n\n\n\n\n\n\n\n청년 고용률 수치만 보는 게 아니라\n고용의 질, 지속성, 업종 다양성을 함께 살펴야 합니다."
  },
  {
    "objectID": "cardnews/news003.html#정규직-비중은-여전히-낮아",
    "href": "cardnews/news003.html#정규직-비중은-여전히-낮아",
    "title": "청년 고용률",
    "section": "",
    "text": "청년층 일자리 중 단기 계약직, 인턴 비중이 높음\n\n정규직 진입률은 10년 전보다 낮아진 상태\n\n\n🔍 “취업했다”는 통계 뒤에 숨은 고용의 질 문제를 들여다봐야 합니다."
  },
  {
    "objectID": "cardnews/news003.html#반복되는-패턴",
    "href": "cardnews/news003.html#반복되는-패턴",
    "title": "청년 고용률",
    "section": "",
    "text": "일자리 찾기 → 계약직 → 재취업 준비 → 불안정 반복\n이직까지 평균 1.5년 소요\n\n\n\n\n\n\n\nNote\n\n\n\n📌 통계청 자료 기준: 2023 경제활동인구조사 청년 부문"
  },
  {
    "objectID": "cardnews/news003.html#정책이-나아가야-할-방향은",
    "href": "cardnews/news003.html#정책이-나아가야-할-방향은",
    "title": "청년 고용률",
    "section": "",
    "text": "청년 고용률 수치만 보는 게 아니라\n고용의 질, 지속성, 업종 다양성을 함께 살펴야 합니다."
  },
  {
    "objectID": "notes/survey/survey_intro.html",
    "href": "notes/survey/survey_intro.html",
    "title": "조사방법론. 1.개요",
    "section": "",
    "text": "chapter 1. 개요\n\n1. 조사란?\n조사는 특정 집단, 즉 표본으로부터 정보를 수집하여, 그 집단이 속한 더 큰 모집단의 특성을 수치적으로 설명하고자 하는 체계적인 방법이다. 조사를 통해 얻어진 통계는 특정 요소 집합에 대한 관찰 결과를 요약한 수치적 표현이며, 이는 크게 두 가지 유형으로 구분된다. 이러한 통계는 사회의 다양한 모집단이 가진 특성이나 경험을 이해하고 설명하는 데 중요한 도구로 활용된다.\n첫째, 기술통계는 모집단 내 다양한 특성의 수준과 분포를 설명한다. 예를 들어, 사람들의 평균 교육 연수, 병원에 있는 총 환자 수, 대통령을 지지하는 사람들의 비율 등이 이에 해당한다.\n둘째, 분석통계는 두 개 이상의 변수 간 관계를 측정한다. 예를 들어, 소득 수준과 교육 연수 간의 관계를 설명하는 회귀계수, 혹은 지난 1년 동안 읽은 책의 수와 교육 수준 간의 상관관계 등이 이에 속한다.\n조사는 사회과학에서 사회의 작동 방식이나 행동 이론을 검증하는 데 가장 널리 사용되는 방법 중 하나이며, 다양한 형태로 수행된다. 본 강의에서는 그중 특정 유형의 조사를 중심으로 다룰 예정이며, 정보는 주로 사람들에게 질문을 통해 수집된다.\n정보 수집 방식은 조사자가 직접 질문하고 응답을 기록하거나, 응답자가 스스로 질문을 읽거나 들은 뒤 답을 작성하는 방식으로 이루어진다. 일반적으로 정보는 모집단 전체가 아닌, 기술된 모집단의 일부인 표본으로부터 수집된다.\n조사방법론은 조사 과정에서 발생하는 다양한 오류의 원인을 분석하고, 조사 결과로 얻어진 수치가 가능한 한 정확하게 모집단을 반영하도록 하기 위한 연구 분야이다. 여기서 ’오류’란 원하는 결과에서 벗어난 편차나 이탈을 의미하며, 통계적 오류는 단순한 실수가 아닌, 모집단의 실제 값과 조사로 얻은 추정값 사이의 차이를 설명하기 위해 사용된다.\n\n\n2. 조사목적\n조사는 특정 현상을 이해하고 분석하기 위해 체계적으로 데이터를 수집하고 해석하는 활동으로, 다양한 목적에 따라 설계된다. 조사 과정에서 가장 중요한 두 가지 질문은 “무엇을 발견할 것인가?“와 “가장 효과적인 방법은 무엇인가?“이다. 이러한 질문을 바탕으로, 대부분의 조사는 다음 세 가지 주요 목적을 중심으로 수행된다.\n첫째, 탐구는 관심 있는 집단이나 현상을 보다 깊이 이해하기 위한 예비적 조사로, 잘 알려지지 않은 영역이나 새로운 주제를 다룰 때 활용된다. 이는 조사 설계와 연구 방향 설정의 기초를 마련하며, 예를 들어 새로운 사회적 트렌드나 특정 인구 집단의 행동 패턴을 파악하기 위한 사전 연구가 이에 해당한다. 탐구적 조사는 주로 인터뷰나 포커스 그룹과 같은 정성적 방법을 사용하여 초기 데이터를 수집한다.\n둘째, 서술은 조사 대상 집단의 특성을 수치적 또는 질적 데이터로 기술하는 데 목적을 둔다. 이는 대상 집단의 상태를 명확히 규명하고, 그 결과를 일반화할 수 있는 기반을 제공한다. 예를 들어 실업률, 인구 구조, 산업 동향과 같은 국가 통계나 시장 조사 등이 이에 해당한다. 서술적 조사에서는 데이터의 품질과 일반화 가능성이 핵심이며, 이를 위해 엄격한 표본추출과 신뢰도 검증이 요구된다.\n셋째, 설명은 특정 현상의 원인과 결과를 실증적으로 밝히는 것을 목표로 한다. 이는 변수 간의 관계를 탐구하거나, 특정 행동이나 태도의 원인을 설명하기 위해 설계된다. 예를 들어 “왜 노년층은 보수적인 정치 성향을 보이는가?“와 같은 질문에 답하기 위해 설문조사와 통계 분석을 통해 인과 관계를 추론하는 방식이 이에 해당한다. 설명적 조사는 대체로 정량적 데이터를 바탕으로 가설 검정과 통계 분석을 수행한다.\n조사의 목적을 명확히 하기 위해서는 누구를 대상으로, 어떤 방법으로, 어떤 내용을 조사할 것인지가 분명히 정의되어야 한다. 이를 위해 조사 대상 집단을 규정하고 적절한 표본 프레임을 설정한 뒤, 수집하고자 하는 정보에 기반하여 설문 항목을 신중히 설계해야 한다.\n조사 목적은 연구의 가설 설정과 직결되며, 이 가설을 바탕으로 설문지가 구성된다. 단, 설문조사 결과를 분석한 후에 가설을 설정하거나, 조사의 목적을 설정할 때부터 결론을 미리 정하는 것은 바람직하지 않다. 이러한 방식은 조사 결과를 왜곡시킬 가능성이 있으므로 피해야 한다.\n조사 목적이 분명히 설정된 이후에는 이해관계자 또는 관련 전문가로 구성된 포커스 그룹이나 컨센서스 패널을 통해 설문지를 검토하고 개선할 필요가 있다. 또한, 유사한 주제를 다룬 기존 조사 문헌을 분석함으로써 설문지의 완성도를 높이는 것이 바람직하다.\n\n\n\nchapter 2. 조사에서의 추론과 오류\n조사는 설계 단계에서 출발하여 실행 과정을 거쳐, 궁극적으로 모집단의 통계적 특성을 설명하는 데 목적을 둔다. 조사의 출발점은 응답자가 제시하는 답변이며, 이 답변을 바탕으로 응답자의 개별적인 특성을 추론하게 된다. 이후 이러한 개인 수준의 정보는 통계적 계산을 통해 표본의 특성으로 통합되며, 다시 이를 기반으로 전체 모집단의 특성을 추론하는 과정으로 이어진다.\n즉, 조사는 응답자의 답변에서 시작하여 개인의 특성을 도출하고, 이를 표본 수준으로 확장한 뒤, 다시 모집단 수준으로 일반화하는 일련의 추론 과정을 포함한다. 이 과정에서 두 가지 핵심 조건이 충족되어야 한다.\n첫째, 응답자가 제공한 답변이 실제로 그 사람의 특성을 정확하게 반영해야 한다.\n둘째, 조사에 참여한 표본이 모집단 전체의 특성을 대표할 수 있어야 한다.\n이 두 조건 중 하나라도 충족되지 않으면 오류가 발생할 수 있다. 여기서 말하는 오류는 단순한 실수가 아니라, 의도한 결과와 실제 결과 사이의 편차를 의미한다. 예를 들어, 측정 오류는 질문에 대한 응답이 실제 측정하고자 하는 속성과 일치하지 않을 때 발생하며, 비관찰 오류는 표본으로부터 추정한 통계량이 모집단의 실제 값과 차이를 보일 때 나타난다.\n조사방법론은 이러한 오류를 체계적으로 분류하고 분석하며, 오류를 최소화하기 위해 조사 설계, 표본추출, 자료 수집 등 모든 단계에서 신중한 계획이 요구된다.\n\n1. 조사 주기(조사 설계 관점)\n조사를 바라보는 데에는 두 가지 주요 관점이 있다. 하나는 설계 관점이며, 다른 하나는 품질 관점이다. 설계 관점에서는 조사 설계를 추상적인 아이디어를 구체적인 실행 단계로 전환하는 과정으로 이해한다. 반면, 품질 관점에서는 조사 설계가 조사 통계에 영향을 미치는 다양한 오류의 근원으로 작용할 수 있음을 강조한다.\n조사는 설계 단계에서 출발하여 실행 단계로 이어진다. 적절한 설계 없이는 신뢰할 수 있는 조사 결과를 얻기 어렵다. 설계에서 실행으로 초점이 이동함에 따라 조사 작업은 추상적인 구상에서 실제적인 실행으로 전환된다. 이후 조사 결과를 해석하고 모집단에 대한 추론을 수행하는 과정에서는 다시 추상적인 수준의 사고가 요구된다.\n조사의 핵심은 측정 차원과 표현 차원이라는 두 가지 틀을 통해 설명할 수 있다. 측정 차원은 표본 내 관찰 단위에서 수집되는 데이터, 즉 “무엇에 관한 조사인가?“에 해당하며, 표현 차원은 조사에서 다루는 모집단, 즉 “누구에 관한 조사인가?“에 초점을 둔다.\n\n측정 과정\n조사의 측정 과정은 먼저 조사에서 측정하고자 하는 개념이나 구성 요소를 정의하는 것으로 시작된다. 이를 바탕으로 구체적인 측정 도구와 질문이 설계되고, 응답자는 이에 대한 답변을 제공한다. 수집된 응답은 검토 및 편집 과정을 통해 오류나 불일치가 수정되며, 정제된 데이터를 기반으로 통계가 산출된다.\n\n\n표현 과정\n표현 과정은 조사 대상이 되는 모집단을 명확히 정의하는 것으로 시작된다. 이후, 해당 모집단의 특정 부분을 대상으로 하는 표본 프레임이 설정되고, 이로부터 표본이 추출된다. 표본으로 선정된 응답자가 실제로 조사에 참여하게 되며, 조사 이후에는 필요에 따라 보정 작업이 이루어진다. 이렇게 수집된 자료는 전체 모집단을 대표하는 통계로 일반화된다.\n\n\n\n(1) 구성 요소 constructs\n구성 요소는 연구자가 조사에서 얻고자 하는 정보의 내용을 의미한다. 예를 들어, 고용 통계 조사는 특정 월의 근로자 수나 일자리 개수를 측정하고자 하며, 교육 성취도 평가는 학생들의 지식을 평가하는 데 목적이 있다. 전국 범죄 피해 조사는 지난 1년 동안 발생한 범죄 피해 사건의 수를 파악하려는 조사이다. 이처럼 구성 요소는 조사 목적에 따라 다양하지만, 종종 추상적이며 정확하게 측정하기 어려운 특성을 지닌다.\n예를 들어, 범죄 피해자의 정체성을 명확히 정의하는 것은 경우에 따라 모호할 수 있다. 공공장소에 낙서가 그려진 경우, 피해자를 누구로 간주할 수 있는가? 특정 수준의 범죄가 실제로 처벌의 대상이 되는 기준은 어디에 있는가? 이러한 질문들은 단순한 서술 수준의 개념을 실제 측정 가능한 항목으로 전환하는 과정에서 발생한다.\n구성 요소의 추상성은 주제에 따라 달라질 수 있다. 예를 들어, 소비자 신뢰도 조사는 개인의 재정 상태에 대한 단기적인 낙관적 태도를 측정하는데, 이는 매우 주관적이며 사람마다 인식의 차이가 크다. 반면, 전국 약물 사용 및 건강 조사는 지난달의 맥주 소비량처럼 비교적 구체적이고 관찰 가능한 행동을 측정한다. 이 경우에는 맥주로 간주되는 음료의 범위를 어떻게 정의할 것인지와 같은 실질적인 문제를 해결하는 것이 중요하다. 이처럼 소비자 신뢰도는 맥주 소비량에 비해 훨씬 더 추상적인 구성 요소라 할 수 있다.\n\n\n(2) 측정 measurement\n측정은 구성 요소보다 한층 더 구체적인 개념이다. 조사에서 ’측정’은 특정 구성 요소에 대한 정보를 수집하는 방법을 의미한다. 조사의 측정 방식은 매우 다양하며, 조사 주제에 따라 물리적 측정, 행동 관찰, 또는 질문을 통한 정보 수집 등 여러 형태로 나타난다.\n예를 들어, 유독물질 오염에 관한 조사에서는 표본 가구의 마당에서 흙 샘플을 채취할 수 있고, 건강 조사에서는 혈압을 측정할 수 있으며, 교통 조사에서는 센서를 활용해 차량 흐름을 전자적으로 기록할 수 있다. 한편, 많은 조사는 응답자에게 질문을 던져 그들의 인식이나 행동에 대한 정보를 수집하는 방식으로 이루어진다. 예를 들어, “지난 6개월 동안 본인이 범죄라고 생각한 사건과 관련해 경찰에 신고한 적이 있습니까?“와 같은 질문이 포함될 수 있다.\n측정 과정에서 가장 중요한 과제는 측정하고자 하는 구성 요소를 충실히 반영할 수 있는 질문을 설계하는 것이다. 질문이 부정확하거나 모호할 경우, 수집된 정보는 실제 구성 요소를 제대로 대변하지 못할 수 있다.\n이러한 질문은 전화 인터뷰나 대면 조사 방식으로 제시될 수 있으며, 종이 설문지나 컴퓨터를 이용한 자가 응답 방식으로도 제공될 수 있다. 경우에 따라서는 조사자가 직접 관찰을 통해 정보를 수집해야 하는 상황도 있다.\n\n\n(3) 응답 response\n조사에서 생성된 데이터는 조사 측정을 통해 수집된 정보에서 비롯되며, 응답의 성격은 사용된 측정 방법에 따라 달라진다. 질문이 측정 도구로 사용되는 경우, 응답자는 기억을 되살리거나 주관적 판단을 통해 답을 생성하거나, 기록을 참고하거나, 때로는 타인의 도움을 받아 응답할 수 있다.\n예를 들어, 소비자 신뢰도 조사에서는 “앞으로 1년 후 본인과 가족의 경제적 상황이 더 나아질 것 같은지, 더 나빠질 것 같은지, 아니면 비슷할 것 같은지”와 같은 질문이 제시된다. 반면, 고용 통계 조사에서는 고용주가 직원 기록을 확인하여 특정 주간의 비관리직 직원 수를 보고하는 방식으로 이루어진다.\n측정 방식에 따라 응답 형식도 달라진다. 어떤 경우에는 선택지가 제공되어 응답자가 해당 범주 중 하나를 선택하면 되며, 다른 경우에는 질문만 주어지고 응답자가 자신의 언어로 자유롭게 답을 작성해야 하는 경우도 있다.\n\n\n(4) 편집된 응답 edited response\n일부 데이터 수집 방식에서는 초기 측정 데이터를 다음 단계로 넘기기 전에 사전 검토 과정을 거친다. 컴퓨터를 이용한 조사에서는 정량적 응답에 대해 범위 검사를 수행하여, 허용된 한계를 벗어난 답변을 자동으로 감지하고, 후속 질문을 통해 응답의 정확성을 확인한다. 예를 들어, 출생 연도를 묻는 질문에 1890년 이전의 숫자가 입력되었거나, 한 응답자가 자신의 나이를 14세라고 답하면서 동시에 다섯 명의 자녀가 있다고 응답한 경우, 이러한 불일치를 확인하고 수정을 유도하는 후속 질문이 제시된다.\n종이 설문지의 경우에는 조사자가 설문지를 수기로 검토하여, 읽기 어려운 답변이나 누락된 항목을 찾아 보완하는 작업이 이루어진다. 이는 현장 조사자가 수행하는 1차적인 오류 점검 단계라 할 수 있다.\n모든 응답자의 답변이 수집된 이후에도 데이터에 대한 추가적인 편집 과정이 진행될 수 있다. 이 과정에서는 전체 응답 분포를 검토하고, 비정상적인 응답 패턴이나 불일치 사례를 탐지하여 이상치를 식별한다. 경우에 따라 특정 설문지나 응답자의 응답을 보다 면밀히 검토해야 할 수도 있다.\n이러한 데이터의 검토와 편집 과정은 조사 결과의 신뢰성과 정확성을 확보하기 위한 핵심적인 절차로 간주된다.\n\n\n(5) 조사 대상 모집단 target population\n조사 대상 모집단은 조사의 대상이 되는 단위들의 집합을 의미한다. 예를 들어, 가구 조사의 경우 성인을 조사 대상 모집단으로 정의할 수 있다. 그러나 이와 같은 정의는 몇 가지 세부 사항이 명확히 설정되지 않으면 해석의 여지를 남기게 된다. 조사 시점을 특정하지 않거나, 전통적인 가구 형태에 속하지 않는 사람들(예: 노숙인, 시설 거주자 등)을 포함할지 여부를 명시하지 않는다면 모집단의 범위는 모호해질 수 있다. 또한, 최근 성인이 된 사람들을 포함할 것인지, 국내 거주 상태를 어떤 기준으로 판단할 것인지 등이 명확히 정의되지 않으면, 모집단의 일관성과 재현 가능성에 문제가 발생할 수 있다.\n조사 대상 모집단은 일반적으로 유한한 규모의 개인들로 구성되며, 이들은 조사의 분석 대상이 된다. 예를 들어, 경제활동인구조사는 만 15세 이상이면서 현재 군 복무를 하지 않고, 병원·교도소·기숙사와 같은 시설이 아닌 일반 주거지에 거주하는 사람들을 모집단으로 정의한다. 모집단의 시간적 범위는 보통 특정 월이나 주로 고정되며, 이 시점에 표본으로 선정된 사람이 해당 거주지에 실제로 거주하고 있어야 한다.\n\n\n(6) 표본 프레임 모집단 frame population\n표본 프레임 모집단은 조사 표본에 선택될 가능성이 있는 대상 모집단의 구성원 집합을 의미한다. 단순한 경우에는 표본 프레임이 대상 모집단의 모든 단위(예: 개인, 고용주 등) 목록으로 구성된다. 그러나 현실에서는 표본 프레임이 대상 모집단과 완전히 일치하지 않거나, 일부 단위만을 포함하는 경우도 많다.\n예를 들어, 소비자 신뢰도 조사의 대상 모집단은 성인 가구이며, 이때 전화번호 목록이 표본 프레임으로 사용될 수 있다. 이는 각 사람을 자신이 속한 가구의 전화번호와 연결할 수 있다는 전제에 기반한다. 그러나 실제로는 전화가 없는 가구도 존재하며, 하나의 가구가 여러 개의 전화번호를 보유하고 있는 경우도 있어, 이러한 전제가 항상 성립하지 않으며 표본 프레임의 복잡성이 발생할 수 있다.\n건강 조사의 경우, 행정구역별 거주지 목록이 표본 프레임으로 활용된다. 이 목록은 각 주택 단위를 특정 시군구와 연결시키며, 일반적으로 15세 이상의 성인이 거주하는 주택을 조사 대상으로 설정한다. 그러나 고정된 거주지가 없는 사람, 혹은 복수의 거주지를 가진 사람과 같이 표본 프레임에서 다루기 어려운 사례도 존재한다.\n\n\n(7) 표본 sample\n표본은 표본프레임에서 선택된다. 이 표본은 측정을 통해 데이터를 수집할 대상 그룹이다. 표본은 표본 프레임의 매우 작은 부분만을 차지한다.\n\n\n(8) 응답자 respondents\n대부분의 조사에서는 선택된 표본 사례를 모두 성공적으로 측정하기 어렵다. 조사에 성공적으로 응답한 사례는 ‘응답자’로 분류되며, 반대로 전혀 응답하지 않은 사례는 ‘무응답자’ 또는 ’단위 무응답’으로 간주된다.\n그러나 어떤 사례를 응답자로 분류할지, 무응답자로 간주할지를 판단하는 일은 종종 명확하지 않다. 일부 응답자는 조사에서 요구되는 정보 중 일부만을 제공하는 경우가 있으며, 이 경우 응답 상태의 구분이 애매해질 수 있다. 이러한 사례에 대해 데이터를 구축할 때는, 불완전한 응답을 포함할지 아니면 해당 응답자를 분석 파일에서 제외할지를 결정해야 한다.\n한편, ‘항목 결측’ 또는 ’항목 무응답’이라는 용어는 응답자가 전체적으로는 조사에 참여했지만, 특정 질문에 대한 응답이 누락된 경우를 지칭한다. 즉, 단위 무응답이 전체 사례에 대한 응답 실패를 의미한다면, 항목 무응답은 일부 질문에 대한 응답 누락을 의미한다.\n\n\n(9) 조사 후 조정 (Postsurvey Adjustments)\n모든 응답자가 데이터를 제공하고 해당 데이터 기록이 완성된 이후에도, 조사로부터 도출된 추정치의 품질을 향상시키기 위해 추가적인 절차가 진행되는 경우가 많다. 이는 무응답 문제나, 표본 프레임과 실제 대상 모집단 간의 불일치와 같은 커버리지 문제로 인해, 응답자 기반 통계가 전체 모집단의 통계와 차이를 보일 수 있기 때문이다.\n이러한 차이를 이해하기 위해, 조사 단계에서는 서로 다른 하위 집단에 대한 단위 무응답 패턴을 분석한다. 예를 들어, 도시 지역의 응답률이 농촌 지역보다 낮은 경우, 도시 거주자가 표본에서 과소 대표되고 있을 가능성을 시사한다. 이와 유사하게, 표본 프레임 자체에 포함되지 않은 단위 유형에 대한 정보를 분석하면, 모집단 내 특정 유형이 아예 조사 대상에서 누락되었음을 확인할 수 있다.\n이후 과정에서, 과소 대표된 집단의 영향을 보정하기 위해 가중치를 조정함으로써 전체 추정치를 개선할 수 있다. 또한, 응답하지 않은 항목은 ’결측 대체(imputation)’라 불리는 절차를 통해 추정된 값으로 대체할 수 있다.\n이러한 가중 조정과 결측 대체 방법은 다양하게 존재하며, 모두 조사 후 조정(post-survey adjustment)에 해당하는 절차로 분류된다.\n\n\n2. 조사 주기(조사 품질 관점)\n조사 방법론에서 흔히 사용하는 품질 개념을 추가적으로 표시한 타원형을 볼 수 있다. 이 개념들은 조사 과정의 연속적인 단계 사이에서 발생하는 불일치를 나타내며, 대부분 \"오류\" 라는 단어를 포함하고 있다. 조사 설계자의 역할은 설계 및 추정 선택을 통해 이러한 오류를 최소화하여 조사 통계의 품질을 높이는 것이다.\n\n\n\n\n\n\n\\(\\mu_{i}\\): 모집단에서 \\(i\\) 번째 사람의 참 값\n\\(Y_{i}\\): \\(i\\) 번째 표본 사람의 측정값\n\\(y_{i}\\): \\(i\\) 번째 표본 측정 응답값(조사 질문에 대한 응답)\n\\(y_{ip}\\): 편집 및 추가 처리 과정을 거친 \\(i\\) 번째 표본 데이터값\n\n결론적으로, 측정하려는 기본 목표 속성은 \\(\\mu_{i}\\)이지만, 실제로는 측정 오류로 인해 목표에서 벗어난 불완전한 지표 \\(y_{ip}\\)를 사용한다.\n\n\n(1) 타당성 validity\n구성 요소의 타당성 validity은 측정값이 근본적인 구성 요소와 관련된 정도를 나타낸다. 반면, 타당하지 않음은 타당성이 달성되지 않은 정도를 설명하는 데 사용되는 용어다. 통계적으로, 타당성의 개념은 개별 응답자 수준에서 적용된다. 이는 구성 요소가 관찰되지 않거나 쉽게 관찰될 수 없는 경우에도, 모집단에서 \\(i\\) 번째 사람과 관련된 일부 값을 가지며, 이는 전통적으로 \\(\\mu_{i}\\)(구성 요소의 참값)로 표시된다. 특정 측정값 \\(Y_{i}\\)의 결과는 \\(\\mu_{i}\\)가 아닌 \\(Y_{i} = \\mu_{i} + \\varepsilon_{i}\\)가 된다.\n이 식에서 \\(\\varepsilon_{i}\\)는 진짜 값에서의 편차를 나타내며, 타당성 개념의 기초를 형성한다. 또한, 측정 타당성을 이해하려면 한 번의 측정이 아닌 여러 번의 반복적인 측정을 고려해야 한다. 같은 측정이 \\(i\\) 번째 사람에게 여러 번 적용될 경우 각 결과는 달라질 수 있으며, 이를 고려해 식이 \\(Y_{it} = \\mu_{i} + \\varepsilon_{it}\\)로 확장된다. 여기서 \\(t\\)는 측정 시도의 번호를 나타낸다. 결국, 타당성은 측정값과 참값 간의 상관 관계로 정의된다. 이는 측정값 \\(Y\\)와 구성 요소 \\(\\mu\\)간의 관계가 높을수록 타당성이 높은 것으로 간주된다.\n\n\n(2) 측정 오류 measurement error\n측정 오류는 측정값이 실제 값, 즉 참값에서 벗어나는 현상을 의미한다. 예를 들어, 국민건강조사에서 “코카인을 한 번이라도 사용한 적이 있습니까?“라는 질문이 있다고 가정해보자. 연구에 따르면, 응답자가 부정적으로 인식하는 행동에 대해서는 과소 보고하는 경향이 있다. 따라서 실제로는 “예”라고 응답해야 하는 사람이 자신의 약물 사용 사실이 드러나는 것을 우려해 “아니요”라고 응답할 수 있다. 이처럼 특정 질문에 대한 응답이 반복적으로 왜곡된다면, 응답값의 평균과 모집단의 실제 평균 사이에 차이가 발생하게 된다.\n통계적으로는 특정 응답자 i에 대해, 측정값과 참값 사이의 체계적 편차를 \\((y_₁ - Y_₁)\\) 로 나타낼 수 있다. 이러한 차이가 일정한 방향으로 발생하면 이를 응답 편향이라고 한다. 예를 들어, 응답자가 자신의 약물 사용이나 범죄 피해 경험을 일관되게 과소 보고하는 경우가 이에 해당한다. 이 편향은 조사 결과가 실제보다 낮거나 높게 나타나는 경향을 유발한다.\n한편, 응답 행동의 불안정성은 또 다른 유형의 오류를 야기할 수 있다. 예를 들어, “현재 사업 환경이 1년 전보다 나아졌습니까, 아니면 나빠졌습니까?“와 같은 질문에 대해 응답자는 질문의 내용뿐 아니라 앞선 질문의 맥락이나 측정 환경의 자극 등 다양한 요소에 영향을 받아 응답할 수 있다. 이러한 자극은 예측하기 어려우며, 동일한 질문을 여러 차례 반복해도 응답이 일관되지 않을 수 있다. 다시 말해, 기대값 \\(E(y_i)\\) 가 참값 \\(Y_\\) 와 일치하지 않을 수 있다.\n응답 분산은 동일한 사람에게 동일한 질문을 여러 번 했을 때 매번 다른 응답이 나타나는 현상을 의미한다. 이는 응답 편향과 구별되며, 보통 신뢰도가 낮은 측정에서 발생한다. 설문 조사에서는 이러한 응답 분산이 추정값의 불안정성을 높이는 주요 원인 중 하나로 간주된다.\n\n\n(3) 처리 오류 processing error\n데이터가 수집된 이후, 추정 단계에 들어가기 전까지 발생할 수 있는 오류에는 여러 가지가 있다. 대표적인 예로 편집 오류와 코딩 오류가 있다.\n편집 단계에서는 응답값의 신뢰성과 일관성을 검토하면서, 명백히 이상해 보이는 값을 누락 처리하거나 수정할 수 있다. 예를 들어, 한 응답자가 “매일 여러 차례 폭행을 당했다”고 보고한 경우, 이는 직관적으로 믿기 어려운 보고로 간주되어 자동 편집 규칙에 따라 결측 처리될 수 있다. 그러나 만약 해당 응답자가 술집의 보안요원이라는 추가 정보가 제공된다면, 응답 내용이 실제 상황을 반영하고 있을 가능성이 높아진다. 이처럼 측정하려는 구성 요소의 맥락에 따라 응답을 수정하거나 유지할지 여부를 판단하는 과정에서 처리 오류가 발생할 수 있다.\n또한, 코딩 단계에서도 오류가 발생할 수 있다. 텍스트 응답을 분류할 때, 동일한 응답이라 하더라도 코딩하는 사람에 따라 다르게 해석될 수 있다. 이러한 차이는 결과의 변동성을 유발하며, 코딩 시스템의 구조나 코더 간 일관성 부족에서 기인한다. 이를 흔히 코딩 분산이라 부른다. 특히 훈련이 부족한 코더는 모호한 언어나 응답자의 설명을 일관되게 해석하지 못해 잘못된 범주로 분류할 수 있으며, 이는 코딩 편향을 초래하게 된다.\n결국, 편집과 코딩 단계 모두에서 발생하는 오류는 측정 이후 데이터 품질에 영향을 미치며, 추정 단계에서의 통계적 결과에도 왜곡을 가져올 수 있다.\n통계적 표기법으로 표현하면, 예를 들어 소득과 같은 변수를 고려할 때, 처리 효과는 제공된 응답과 편집된 응답 간의 차이로 정의될 수 있다. \\(y_{i}\\)는 조사 질문에 대한 응답을, \\(y_{ip}\\)는 편집된 응답을 나타낸다. 따라서, 처리 편차는 \\((y_{ip} - y_{i})\\)로 나타낼 수 있다.\n\n\n(4) 포함오류 coverage error\n포함 오류는 모집단과 표본 프레임 간의 차이에서 발생한다. 예를 들어, 표본 프레임이 모집단의 일부를 포함하지 못한 경우를 포함 부족(undercoverage)이라고 하며, 반대로 표본 프레임에 모집단에 속하지 않는 요소가 포함된 경우는 과잉 포함(overcoverage)이라고 한다.\n통계적으로 볼 때, 표본 평균에서 발생하는 포함 편향은 두 가지 요소에 의해 결정된다. 첫째는 표본 프레임에 포함되지 않은 모집단 구성원의 비율이고, 둘째는 프레임에 포함된 구성원과 포함되지 않은 구성원 간의 특성 차이이다. 즉, 포함되지 않은 비율이 높고, 포함된 구성원과 포함되지 않은 구성원 간에 측정하고자 하는 변수의 평균 차이가 클수록 포함 편향은 커지게 된다.\n\\({\\overline{Y}}_{C} - \\overline{Y} = \\frac{U}{N}({\\overline{Y}}_{C} - {\\overline{Y}}_{U})\\), 여기서 \\(\\overline{Y}\\)는 목표 모집단 전체의 평균, \\({\\overline{Y}}_{C}\\)는 표본 프레임에 포함된 모집단의 평균, \\({\\overline{Y}}_{U}\\)는 표본 프레임 밖 모집단의 평균을 나타낸다. \\(N\\)은 목표 모집단의 총 구성원 수, \\(C\\)는 표본 프레임에 포함된 적격 구성원의 총수, 그리고 \\(U\\)는 표본 프레임에 포함되지 않은 적격 구성원의 총수이다.\n예를 들어, 전화 조사를 통해 가구의 평균 교육 연수를 측정한다고 가정하자. 전화가 없는 가구는 표본에서 제외되므로 이들의 평균 교육 연수는 낮아질 가능성이 있다. 전화가 있는 가구의 평균 교육 연수가 14.3년이고, 전화가 없는 가구의 평균 교육 연수가 11.2년이라면, 전체 모집단 평균에 대한 편향은 다음과 같이 계산될 수 있다(단, 전화 없는 가구 비율을 5%라 가정하자).\n\\({\\overline{Y}}_{C} - \\overline{Y} = 0.05(14.3 - 11.2) = 0.16\\text{년}\\).\n즉, 전화가 없는 가구를 포함하지 않은 표본 프레임은 모집단 평균보다 약 0.16년 더 높은 평균 교육 연수를 보여줄 것이다. 결론적으로, 표본 프레임의 포괄 오류는 표본 평균 추정값에 영향을 미치며, 이는 모집단 평균이 아닌 표본 프레임 평균을 반영하게 된다.\n\n\n(5) 표본 오류 sampling err0r\n표본 설문조사에서는 비용과 시간의 제약으로 인해, 표본 프레임 내 모든 구성원을 조사하는 것이 현실적으로 어렵다. 따라서 전체 중 일부만을 선택하여 조사하고, 나머지는 제외하는 방식이 일반적으로 채택된다. 이러한 선택적 측정으로 인해 발생하는 통계적 차이를 표본 오류라고 한다.\n표본 오류는 크게 두 가지 유형으로 구분된다. - 표본 편향(sampling bias)은 표본 프레임의 일부 구성원이 표본으로 선택될 기회를 갖지 못하거나, 선택될 가능성이 상대적으로 낮을 때 발생한다. 예를 들어, 특정 표본 설계가 체계적으로 일부 그룹을 항상 제외하도록 구성되어 있다면, 그 결과로 도출된 통계치는 실제 프레임 모집단의 통계와 차이를 보일 수 있다. - 표본 분산(sampling variance)은 동일한 방법으로 표본을 반복 추출할 경우, 각 표본이 서로 다른 응답을 포함하게 되어 조사 통계가 반복마다 달라질 수 있는 현상을 의미한다. 이는 표본 선택이 확률적일 때 자연스럽게 발생하는 오차로, 표본의 수나 분포에 따라 크기가 달라질 수 있다. 표본 선택의 여러 가능성을 개념적으로 반복한 결과, 표본 분산의 개념이 만들어진다. 표본 분산은 동일한 설계에서 얻어진 여러 표본 평균이 얼마나 변동하는지를 나타낸다.\n\n\\({\\overline{Y}}_{s}\\): 특정 표본 추출의 표본 평균, 표본 \\(s;s = 1,2,\\ldots,S\\)\n\\({\\overline{Y}}_{C}\\): 표본 프레임에서의 모든 요소의 총평균\n\\(\\overline{Y}_s = \\frac{\\sum_{i=1}^{n_s} y_{si}}{n_s},\\ \\overline{Y}_C = \\frac{\\sum_{i=1}^{C} Y_i}{C}\\)\n표본 분산: \\(\\frac{\\sum_{s = 1}^{S}({\\overline{Y}}_{s} - {\\overline{Y}}_{C})^{2}}{S}\\)\n\n\n\n(6) 응답률 오류 nonresponse error\n모든 표본 구성원을 설문조사에서 완전히 측정하는 것은 현실적으로 어렵다. 특히 사람을 대상으로 하는 조사에서는 이러한 상황이 자주 발생한다. 이로 인해 발생하는 오류를 응답률 오류라고 하며, 이는 실제 응답한 사람들의 통계 값이 전체 표본을 기준으로 했을 때의 통계 값과 다를 때 나타난다.\n예를 들어, 수행평가 당일 결석한 학생들이 수학적 또는 언어적 능력이 낮은 경향이 있다면, 이들이 측정에서 제외됨으로써 전체 수행평가 점수가 과대평가될 수 있다. 즉, 응답자의 평균 점수가 전체 표본의 진정한 평균보다 체계적으로 높아지는 결과가 나타난다. 이러한 오류는 응답률이 낮을수록 그 영향이 커지며, 조사 결과의 왜곡 가능성도 더욱 심각해질 수 있다.\n응답률 편향: \\({\\overline{y}}_{r} - {\\overline{y}}_{s} = \\frac{m_{s}}{n_{s}}({\\overline{y}}_{r} - {\\overline{y}}_{m})\\)\n\n\\({\\overline{y}}_{s}\\): 선택된 특정 표본의 전체 평균,\n\\({\\overline{y}}_{r}\\): 𝑠번째 표본의 응답자 평균, \\({\\overline{y}}_{m}\\): 𝑠번째 표본의 비응답자 평균\n\n𝑛𝑠: 𝑠번째 표본의 총 구성원 수, 𝑟𝑠: 𝑠번째 표본의 응답자 수, 𝑚𝑠: 𝑠번째 표본의 비응답자 수\n따라서 표본 평균에 대한 응답률 편향은 응답률(데이터가 수집되지 않은 표본 구성원의 비율)과 응답자와 비응답자 평균 간의 차이의 곱으로 나타난다. 이는 높은 응답률만으로는 반드시 품질 지표가 아님을 나타낸다. 응답률이 높은 설문조사에서도 비응답자가 조사 변수에서 매우 독특할 경우, 높은 응답률 편향이 나타날 수 있다. 이 문제를 방지하는 가장 좋은 방법은 높은 응답률을 유지하여 응답률 편향의 위험을 줄이는 것이다.\n\n\n(7) 보정 오류 adjustment error\n조사에서 발생하는 비관측 오류를 줄이기 위한 마지막 단계는 조사 후 보정이다. 이 보정은 표본 추정치를 개선하기 위해 시행되며, 포함 오류, 표본 오류, 무응답 오류와 같은 오류를 줄이는 것을 목표로 하며, 보정 과정은 개별 응답에 대한 수정 단계와 유사한 역할을 한다.\n보정은 대상 모집단 또는 표본 프레임에 대한 정보와 응답률 데이터를 활용하여 과소 대표된 표본 사례에 더 큰 가중치를 부여함으로써 데이터의 균형을 맞춘다. 예를 들어, A지역의 응답률이 85%인 경우 해당 지역 응답자에게 \\(w_{i} = 1/0.85\\)의 가중치를 부여하여 특정 응답자의 영향을 평균 계산에서 확대한다.\n조정된 평균은 이러한 가중치를 적용해 계산되며, 조정된 표본 평균(\\(\\overline{y}nw = \\frac{\\sum_{i = 1}^{r}w_{i}y_{si}}{\\sum_{i = 1}^{r}w_{i}}\\))과 모집단 평균과 간의 차이는 \\(({\\overline{y}}_{nw} - \\overline{Y})\\) 로 나타난다. 이러한 보정은 통계적 편향을 줄이는 데 기여하지만, 경우에 따라 오류를 오히려 증가시킬 수도 있으므로 설계와 실행 단계에서 세심한 주의가 필요하다.\n\n\n\nchapter 3. 목표모집단, 표본프레임, 포함오류\n표본 설문조사는 명확히 정의된 모집단을 설명하거나, 그로부터 통계적 추론을 도출하는 과정이다. 이때 모집단을 구성하는 기본 단위는 ‘요소’ 또는 ’조사단위’로 불리며, 이 요소들이 전체 모집단을 형성한다. 예를 들어, 가구 조사의 경우 요소는 개별 가구원일 수 있으며, 학교 조사의 경우에는 학생이, 비즈니스 조사의 경우에는 사업체나 시설이 요소가 된다. 하나의 설문조사 내에서도 다양한 유형의 요소가 존재할 수 있다. 가구 조사의 경우, 조사 대상은 사람일 수도 있지만, 주거 단위나 거주 커뮤니티 등과 같은 더 넓은 단위로 확장될 수도 있다.\n설문조사에서 모집단 정의는 조사 설계와 결과 해석의 출발점이자 핵심이다. 모집단을 어떻게 정의하느냐에 따라 데이터의 대표성과 신뢰성이 결정되며, 이는 추론의 정확도와 직결된다. 다음은 설문조사에서 모집단 정의가 중요한 이유를 설명하는 네 가지 측면이다.\n\n설문의 주요 목적\n설문조사의 핵심 목적은 특정 모집단의 특성을 통계적으로 설명하거나, 그 모집단에 대해 일반화된 결론을 도출하는 것이다. 모집단이 명확히 정의되지 않으면, 조사 결과의 정확성과 대표성이 저하될 수 있다. 모집단 정의는 조사 결과가 어떤 집단을 설명하고 있는지를 분명히 밝히는 역할을 한다.\n\n\n조사의 설계 및 표본 추출\n설문조사는 모집단으로부터 표본을 추출하고, 이를 통해 모집단 전체에 대한 추정치를 산출한다. 모집단 정의가 불명확하면 표본 추출 과정에 왜곡이 생기고, 결과적으로 표본이 모집단을 제대로 대표하지 못하게 된다. 따라서 모집단 정의는 표본 설계의 출발점으로서 반드시 선행되어야 한다.\n\n\n다양한 단위와 요소 처리\n하나의 조사에서도 사람, 가구, 기업 등 다양한 요소들이 존재할 수 있다. 이러한 요소들의 범위와 성격이 명확히 정의되지 않으면, 데이터 해석 과정에서 혼란이 발생할 수 있으며, 잘못된 결론으로 이어질 위험도 존재한다.\n\n\n다른 연구와의 차별성\n실험 연구와 같이 자극과 반응 간의 인과관계를 규명하는 데 초점을 맞춘 연구에서는 모집단 정의가 상대적으로 부차적인 요소일 수 있다. 반면, 설문조사는 모집단 전체의 특성을 설명하고 해석하는 것을 궁극적인 목표로 삼기 때문에, 모집단 정의는 조사 전 과정에서 가장 중요한 요소 중 하나로 간주된다.\n\n\n1. 모집단과 프레임\n목표 모집단(target population)은 조사 결과를 일반화하고자 하는, 즉 표본 통계를 통해 추론하고자 하는 요소들의 집합을 의미한다. 목표 모집단은 다음과 같은 조건을 충족해야 한다:\n\n크기가 유한해야 한다. 이론적으로라도 개별 요소들을 셀 수 있어야 한다.\n시간적으로 정의되어야 한다. 특정 시점 또는 시기 내에서 존재하는 집단이어야 한다.\n관찰 가능해야 한다. 즉, 실제로 접근하여 조사할 수 있어야 한다.\n\n목표 모집단을 정의할 때는 두 가지 요소를 명확히 해야 한다. 첫째, 어떤 단위를 모집단의 요소로 간주할 것인가(예: 사람, 가구, 시설). 둘째, 어떤 시간적 범위를 설정할 것인가. 예를 들어, 가구 조사의 경우 조사 대상은 일반적으로 경제활동인구에 해당하는 만 15세 이상의 성인으로, 주택 단위에서 거주하는 사람들을 포함한다. 이때 ’가구’는 집, 아파트, 이동식 주택, 방 그룹 또는 독립된 방처럼 거주를 목적으로 마련된 공간을 포함한다.\n모든 사람이 반드시 성인이거나 주택에 거주하는 것은 아니므로(예: 교도소 수감자, 군부대 거주자, 장기 요양시설 입소자 등), 이들이 목표 모집단에 포함될지 여부는 조사의 목적과 범위에 따라 달라질 수 있다. 어떤 조사는 특정 지역(예: 특광역시)으로 모집단을 한정하기도 하며, 어떤 조사는 시설 거주자를 포함하기도 한다.\n모집단은 고정된 것이 아니라 시간에 따라 변할 수 있기 때문에, 조사 시점 역시 모집단 정의의 중요한 요소이다. 예를 들어, 가구 조사가 며칠 혹은 몇 주에 걸쳐 이루어질 경우, 그 조사 기간 중 해당 주택에 거주하는 사람이 모집단에 포함된다. 실무에서는 첫 번째 접촉 시점에서 거주자가 누구인지를 기준으로 모집단을 “고정”하는 방식을 자주 사용한다.\n그러나 실제로 조사 데이터를 수집할 때는, 조사 방법 자체가 모집단을 더 좁은 범위로 제한하는 경우가 많다. 예를 들어, 목표 모집단이 ’대한민국에 거주하는 만 18세 이상 성인’이라 하더라도, 전화 조사는 실제로 전화번호를 가진 사람들만 조사할 수 있다. 이처럼 실질적으로 조사가 이루어진 집단을 조사 모집단(survey population)이라 하며, 이는 원래 목표 모집단과 다를 수 있다.\n조사를 설계하기 위해서는 표본 프레임(sample frame)이 필요하다. 표본 프레임은 모집단 요소를 식별할 수 있는 자료 집합으로, 요소들의 명부(예: 회원 명단, 주소록)나 요소가 존재하는 지역·시설·시점의 목록일 수 있다. 예를 들어, 특정 전문 협회 회원 명단, 특정 지역 내 사업체 목록 등이 이에 해당한다.\n그러나 실제 조사에서는 단일한 표본 프레임이 존재하지 않는 경우도 많다. 예를 들어, 서울 지역 모든 초중고등학생의 명단이나, 교정시설 수감자 전체 명단은 현실적으로 존재하지 않거나 접근이 어렵다. 이 경우 조사자는 두 가지 선택지를 고려할 수 있다.\n\n표본 프레임에 맞게 목표 모집단을 재정의한다.\n원래 모집단을 유지하되, 포함 오류(coverage error)의 가능성을 인정한다.\n\n예를 들어, 전화 조사를 설계할 때 목표 모집단이 미국의 모든 성인이라 하더라도, 실제 표본 프레임은 전화번호를 보유한 성인으로 제한될 수 있다. 모집단을 전화가 있는 성인으로 재정의하는 것은 모집단과 프레임 간의 불일치를 해결하는 방식이지만, 원래 모집단을 그대로 유지할 경우에는 전화가 없는 성인이 누락되어 포함 오류가 발생한다.\n마지막으로, 표본 프레임 없이 조사를 수행해야 하는 경우도 있다. 이때는 확률 표집이 어려우므로, 눈덩이 표집(snowball sampling)이나 특정 지역을 설정한 현장 조사와 같은 비확률 표집(nonprobability sampling) 방식이 활용되기도 한다.\n\n\n2. 표본프레임의 포함 이슈\n조사에서 연구자들에게 중요한 통계적 관심은 표본 프레임(표본을 선택하기 위해 사용 가능한 자료)이 실제로 목표 모집단을 얼마나 잘 포함하고 있는가이다. 표본 프레임과 목표 모집단 간의 일치는 포함, 미포함(목표 모집단에 포함되어야 하지만 표본 프레임에 포함되지 않거나 포함될 수 없는 요소), 비적격(목표 모집단에 속하지 않지만 표본 프레임에 포함된 단위) 단위를 포함한 세 가지 잠재적 결과를 초래할 수 있다.\n표본 프레임이 완벽하다는 것은 프레임 요소와 목표 모집단 요소 간에 일대일 매핑이 있다는 것을 의미한다. 실제로는 완벽한 프레임이 존재하지 않으며, 일대일 매핑을 방해하는 문제가 항상 발생한다.\n프레임의 적합성을 논의할 때 포함 오류와 비적격 단위 문제이외 프레임 단위가 존재하고 목표 모집단의 요소와 매핑되지만 그 매핑이 고유하지 않을 때 발생하는 문제도 논의 되어야 한다. ”중복”은 여러 프레임 단위가 목표 모집단의 단일 요소에 매핑될 때 사용된다.\n”군집화”는 여러 목표 모집단 요소가 동일한 단일 프레임 요소와 연결될 때 사용되는 용어다. 표본 크기(요소 수로 측정)는 선택된 군집에 따라 클 수도 있고 작을 수도 있다. 또한 여러 프레임 단위가 여러 목표 모집단 요소와 매핑되는 경우(다대다 매핑)도 있다.\n\n\n(1) 미포함 undercoverage\n\n미포함 정의\n미포함는 조사 통계에서 특정 모집단 부분이 포함되지 않아 발생하는 오류를 뜻한다. 예를 들어, 전화 가구 조사는 모든 가구의 사람들을 대상으로 하지만, 전화 프레임에는 전화가 없는 가구가 포함되지 않아 미포함이 발생한다. 세계 여러 국가에서 전화 사용이 지속적인 비용을 요구하기 때문에 경제적으로 어려운 계층이 비율적으로 더 많이 제외된다. 또한, 휴대전화가 고정전화 서비스를 대체하는 국가에서는 젊은 사람들이 새로운 기술을 더 빨리 수용하기 때문에 고정전화 기반 프레임에서 제외될 가능성이 높다.\n\n\n미포함 문제의 원인\n미포함 문제는 표본 프레임 생성 과정에 따라 발생한다. 이 과정은 조사 설계에 의해 통제될 수도 있고, 외부 출처에서 프레임을 얻을 때는 조사 외부 요인에 의해 결정될 수도 있다. 예를 들어, 가구 조사의 경우, 조사 표본은 초기 지역 목록(시군구 등)에서 시작하여 시군구 내의 주택 목록으로 세분화되고, 최종적으로 가구 내 거주자 목록으로 연결된다. 이러한 샘플링 과정은 지역 샘플로 불린다.\n\n\n문제의 수준\n\n지리적 경계: 도로, 강, 철도 등 물리적 경계는 상대적으로 쉽게 구별되지만, 자연 경계선(산등성이, 능선 등)은 해석에 따라 차이가 발생할 수 있다. 경계 해석 오류로 인해 특정 가구가 목록에서 누락될 가능성이 있다.\n가구 정의: 가구는 독립된 입구를 갖춘 물리적 구조로 정의되지만, 추가가구나 숨겨진 입구가 있는 경우 누락될 가능성이 있다.\n특수 사례: 공동체 생활(공동 주방 사용 등)이나 제도적 시설(교도소, 병원 등)의 경우, 거주 단위를 정의하고 포함 여부를 결정하는 규칙이 필요하다.\n\n\n\n주민 등록 규칙의 문제\n조사에서 거주자는 일반적으로 ”일반 거주” 규칙에 따라 정의된다. 이 규칙은 거주 단위에서 통상적으로 거주하는 사람을 포함하도록 한다. 하지만, 여행하는 직업(트럭 운전사, 항공 조종사 등)을 가진 사람들의 경우 거주지 정의가 모호할 수 있다. 또한, 부모와 떨어져 살거나 복잡한 가족 구조를 가진 아동도 미포함이 발생할 수 있다.\n\n\n사업체 조사에서의 Undercoverage\n사업체 조사는 사업체의 생성, 병합, 종료로 인해 미포함이 발생할 가능성이 높다. 특히 대규모 또는 소규모 사업체는 표본 프레임에 포함되지 않을 수 있다. 새로운 사업체는 행정적 기록의 지연으로 프레임에서 누락되거나, 복잡한 사업체 구조는 데이터 정리 과정에서 오류를 일으킬 수 있다.\n\n\n\n(2) 부적격 단위 ineligible units\n표본 프레임에는 때로 목표 모집단에 속하지 않는 요소들이 포함될 수 있다. 예를 들어, 전화번호 프레임에는 작업 또는 비거주 전화번호가 많이 포함될 수 있는데, 이는 가구 모집단을 대상으로 하는 프레임의 사용을 복잡하게 만든다. 지역 확률 조사에서는 종종 지도 자료에 목표 지리적 영역 외부의 단위가 포함될 수 있다. 조사원이 주택 단위를 나열하기 위해 표본 영역을 방문할 때, 때때로 점유되지 않았거나 주택 단위로 보이는 사업장 구조물을 포함시킬 수 있다.\n조사원이 주택 단위에서 가구 구성원의 목록을 작성할 때, 응답자가 생각하는 ”가구”의 개념과 조사에서 요구하는 정의가 다를 수 있다. 예를 들어, 집을 떠나 학교에 다니는 학생의 부모는 여전히 그들을 가구의 일부로 여길 수 있지만, 대부분의 조사에서는 이들을 대학생으로 분류하여 별도로 다룬다. 또한, 응답자는 같은 주택 내에서 방을 임대해 거주하는 사람이나 친족 관계가 없는 사람들을 가구 구성원으로 포함하지 않을 가능성이 높다. 이는 조사 결과에서 특정 가구 유형이나 가족 구성원의 불균형을 초래할 수 있다.\n프레임에서 선택 시작 전에 외부 단위가 식별되면 적은 비용으로 제거될 수 있다. 외부 단위의 비율이 소수라면 표본 크기를 줄이는 것과 같은 스크리닝 단계에서 이를 식별하고 표본에서 제외할 수 있다. 외부 단위의 발생률이 대략적으로라도 사전에 알려진 경우, 일부 외부 단위를 스크리닝할 것을 예상하며 추가 단위를 선택할 수 있다. 예를 들어, 전국 전화번호 명부 리스트의 약 15%가 더 이상 존재하지 않는 번호임을 알고 있는 경우, 100개의 전화 가구 표본을 얻기 위해 디렉토리에서 100/(1 - 0.15) = 118개의 항목을 선택할 수 있으며, 그 중 18개가 유효하지 않는 번호일 것이다.\n\n\n(3) 프레임 요소 내에서 목표 모집단 요소의 클러스터링\n프레임에서 모집단으로, 또는 모집단에서 프레임으로의 다중 매핑(클러스터링 또는 중복)은 표본 선택에서 문제를 일으킬 수 있다. 전화번호부를 표본 프레임으로 사용해 전화 가구에 거주하는 성인(목표 모집단)을 표본으로 삼는 경우 전화번호부에 나열된 가구에는 하나의 프레임 요소(전화번호)로 여러 성인이 포함될 수 있다.\n\n클러스터링 문제의 예\n홍길동 가족(홀길동, 홍길동 아내, 홀길동 부모)은 같은 가구에 살며 동일 전화번호를 공유한다. 이 전화번호는 표본 프레임 요소로 사용되며, 홀길동 가족 모두가 동일한 프레임 요소와 연결됩니다. 그러나 이들은 목표 모집단의 4 요소를 구성한다.\n\n\n클러스터링 문제 해결 방법\n클러스터링 문제를 해결하는 한 가지 방법은 선택된 프레임 요소(예: 전화번호)에 속한 모든 자격 요소(목표 모집단 요소)를 포함하는 것이다. 이러한 설계에서는 클러스터 내의 모든 요소에 동일한 선택 확률이 적용된다.\n\n\n클러스터링 문제의 중요성\n클러스터링은 종종 클러스터를 부분적으로 표본화 하게 되는 중요한 문제를 제기한다.\n\n첫째, 일부 경우 클러스터의 모든 요소에서 성공적으로 정보를 수집하기 어려울 수 있다. 예를 들어, 전화 가구 조사에서는 한 가구에서 여러 번 인터뷰를 시도하면 무응답 비율이 증가하는 경우가 있다.\n둘째, 인터뷰가 여러 시간대에 걸쳐 진행되어야 할 경우 초기 응답자가 나중 응답자에게 설문 내용을 논의하면서 답변에 영향을 줄 수 있다.\n셋째, 클러스터 크기가 다를 경우 표본 크기 통제가 어려워질 수 있다.\n\n\n\n클러스터 크기와 표본 왜곡\n큰 클러스터의 요소는 작은 클러스터 요소에 비해 선택될 확률이 낮다. 예를 들어, 전화번호가 표본으로 선택되었을 때 두 명의 자격 요소를 포함한 가구에서는 한 사람이 선택될 확률이 50%인 반면, 네 명의 자격 요소를 포함한 가구에서는 각각 25%의 확률을 갖게 된다. 이러한 샘플링의 결과로 소규모 가구의 구성원이 목표 모집단에 비해 과대표될 가능성이 있다. 예를 들어, 범죄 피해 조사에서 소규모 가구의 구성원이 범죄 피해를 입을 확률이 더 높은 경우, 클러스터 크기와 변수 간의 관계로 인해 표본 결과는 편향된 추정치를 제공할 수 있다.\n\n\n해결 방법\n이러한 편향을 제거하기 위해 분석 과정에서 보상 조치를 취해야 한다. 클러스터 내 자격 요소 수에 따라 가중치를 적용해 표본 추정치를 수정할 수 있다.\n\n\n\n(4) 표본 프레임에서 목표 모집단 요소의 중복\n프레임과 목표 모집단 사이의 또 다른 중복 매핑 유형은 ”중복”이다. 중복은 단일 목표 모집단 요소가 여러 프레임 요소와 연관된 경우를 의미한다. 전화 설문조사 예를 들어, 단일 전화 가구가 전화번호부에 여러 번 나열되는 경우가 있다. 홍길동 목표 모집단 구성원은 전화번호 두 개를 등록하여 두 개의 프레임 요소와 연관되어 있다고 하자. 이러한 프레임 문제는 클러스터링과 유사하다. 여러 프레임 단위를 가진 목표 모집단 요소는 선택될 확률이 높아져 모집단에 비해 과대 대표된다. 중복과 관심 변수 간의 상관관계가 있는 경우, 설문조사 추정치는 편향될 수 있다. 문제는 중복 여부와 중복과 조사 변수 간의 상관관계가 종종 알려지지 않는다는 점이다.\n중복으로 인한 편향 문제는 다양한 방식으로 해결할 수 있다. 첫 번째 방법은 표본 선택 전에 프레임에서 중복 항목을 제거하는 것이다. 예를 들어, 전자 전화번호부를 정렬하여 동일한 번호의 중복 항목을 삭제하는 방식이다. 두 번째 방법은 표본 선택 과정이나 데이터 수집 중에 중복된 프레임 단위를 식별하는 것이다. 이 경우, 간단한 규칙을 적용하여 중복 항목을 처리할 수 있다. 예를 들어, 디렉토리에 여러 항목이 있을 경우, 첫 번째 항목만 유효하다고 간주하는 규칙을 사용할 수 있다. 데이터 수집 중에는 조사원이 가구에 여러 디렉토리 항목이 있는지 확인한 뒤, 확인된 중복 항목 중 첫 번째 항목만 선택하고 나머지는 ”외부 단위”로 분류하여 제외할 수 있다. 이러한 접근 방식은 중복으로 인한 표본 편향을 줄이는 데 효과적이다.\n또 다른 해결책은 가중치를 부여하는 방법이다. 클러스터링의 경우와 유사하게, 중복된 프레임 요소의 개수를 기반으로 역수를 사용하여 가중치를 계산한다. 예를 들어, 한 전화 가구가 두 개의 전화선을 보유하고 있으며 디렉토리에 총 세 개의 항목(한 개 번호는 홍길동 처의 이름으로 중복 등록)이 등재되어 있다면, 이 가구는 표본 내에서 가중치 1/3을 받게 된다. 반면, 무작위 숫자 다이얼(RDD) 프레임에서는 해당 가구가 가중치 1/2을 받게 된다. 이러한 가중치 계산은 표본 내의 중복 문제를 보정하여 통계적 편향을 최소화하는 데 기여한다.\n\n표본 프레임과 목표 모집단 요소 간의 복잡한 매핑\n다수의 프레임 단위가 여러 모집단 요소에 매핑될 가능성도 있습니다. 예를 들어, 성인을 대상으로 한 전화 가구 조사에서는 디렉토리에 여러 항목이 포함된 여러 성인이 있는 가구를 만날 수 있습니다. 이러한 다대다 매핑 문제는 클러스터링과 중복의 조합입니다.\n예를 들어, 홍길동 가구는 두 개의 전화번호 프레임 요소를 가지고 있으며, 이는 두 개의 표본 프레임 요소에 매핑된 세 개의 목표 모집단 요소를 나타낼 수 있습니다. 이 문제에 대한 일반적인 해결책은 조사 결과에 가중치를 부여하여 두 문제를 동시에 처리하는 것입니다. 개별 수준 통계를 위한 보정 가중치는 가구의 성인(또는 적격자) 수를 해당 가구의 프레임 항목 수로 나눈 값으로 정의됩니다. 예를 들어, 홍길동 가구의 구성원에게 부여되는 가중치는 1/2이 됩니다.\n\n\n\n3. 목표모집단과 표본프레임 이슈\n\n\n(1) 가구와 개인\n가구를 대상으로 한 일반적인 표본 프레임에는 지역 프레임(인구조사 구역 또는 카운티와 같은 지역 단위 목록), 전화번호, 전화목록, 그리고 우편목록이 있다. 지역 프레임은 지리적 단위를 기반으로 하기 때문에, 사람이 해당 지역에 속한다는 것을 거주 연결 규칙을 통해 연관지어야 한다. 이러한 프레임은 개인을 표본으로 선택할 때 여러 단계를 요구한다. 먼저 지역 단위의 일부를 선택한 후, 해당 구역의 주소 목록을 작성해야 한다. 우수한 지도나 항공사진이 있는 경우, 이 프레임은 이론적으로 주거지의 완전한 범위를 제공할 수 있다. 그러나 선택된 지역 단위 내 주거지 목록에 일부 누락된 단위가 존재할 경우, 프레임은 불포함 오류를 겪게 된다. 한 사람이 두 개 이상의 거주지를 가지고 있는 경우에는 중복 문제가 발생하며, 여러 사람이 동일한 거주지에 거주하는 경우에는 개인을 표본으로 선택할 때 클러스터링 문제가 발생한다.\n또 다른 가구 모집단 프레임은 주택 내 유선전화 번호를 기반으로 한 프레임이다. 일부 가구는 여러 개의 전화번호를 보유하고 있어 과포함 문제가 발생할 수 있다. 이 프레임에는 사용되지 않는 전화번호와 비주거용 번호가 포함되어 있기 때문에, 이를 개인 수준의 표본으로 활용하려면 제거해야 한다.\n주거용 전화번호 목록 프레임은 전화번호 프레임보다 범위가 좁지만, 비작동 번호와 비주거용 번호가 대부분 제거되어 있어 가구 조사에서는 더 효율적이다. 이 목록은 상업적 기업이 전자 및 인쇄된 전화번호 디렉토리에서 얻으며, 대량 발송업자와 조사기관에 판매한다. 그러나 상당수의 주거용 번호가 디렉토리에 포함되지 않으며, 특히 도시 지역 거주자나 일시적인 거주자의 번호가 빠질 수 있다. 같은 번호가 여러 이름으로 등재되는 경우도 많아 중복 문제가 발생하기도 한다.\n웹 설문조사에 대한 관심이 높아지면서, 이메일 주소를 기반으로 한 가구 모집단 프레임 개발에 주목이 쏠리고 있다. 그러나 이메일 프레임은 전체 가구 모집단을 충분히 포함하지 못하며, 한 사람이 여러 이메일 주소를 보유하거나 여러 사람이 하나의 이메일을 공유하는 등의 이유로 중복 및 클러스터링 문제가 존재한다.\n모바일 또는 휴대전화는 많은 국가에서 유선 전화를 빠르게 대체하고 있다. 예를 들어, 핀란드에서는 1990년대 중반부터 유선 전화 가입자가 감소하고 휴대전화 가입자가 급격히 증가하였다. 이는 기존의 유선 전화 기반 프레임에서 휴대전화 번호가 누락됨에 따라, 프레임 손실이 발생했음을 의미한다. 특히 젊은 세대 중 독립적인 가구를 처음 형성하는 층에서 이러한 손실이 두드러졌다.\n더욱이 휴대전화는 유선전화와 달리 한 사람과 직접 연결되며, 전체 가구 단위와 연결되지 않는다. 따라서 전화 설문조사는 휴대전화 번호를 표본으로 사용할 수밖에 없으며, 이는 프레임과 표본 단위가 가구에서 개인으로 분리되는 것을 요구하게 된다. 현재로서는 유선전화와 휴대전화 번호의 병용에서 비롯된 클러스터링과 중복 문제 등 여러 프레임 관련 이슈가 해결되지 않은 상태이다.\n\n\n(2) 고객, 고용주 또는 조직 구성원\n표본 프레임은 전자 파일 또는 인쇄물 형식으로 구성된 개인 기록일 수 있으며, 이러한 시스템은 주기적인 업데이트 지연으로 인해 불포함 오류가 발생하거나, 조직을 이미 떠난 인물이 빠르게 제거되지 않아 부적격 요소를 포함할 수 있다. 예를 들어, 마지막 거래가 오래전에 이루어진 고객이 여전히 목록에 남아 있는 경우가 있으며, 계약직 직원처럼 조직과의 소속이 모호한 경우도 존재한다. 고객 기반 프레임에서는 거래 단위별로 동일한 고객이 여러 차례 기록되어 중복이 발생할 수 있으며, 이때 설문조사 연구자는 목표 모집단이 ’사람’인지, ’거래’인지, 혹은 둘 다인지를 신중히 판단해야 한다.\n설문조사 연구자는 대체 가능한 프레임을 평가할 때, 해당 목록이 어떤 방식으로 생성되고 수정되는지 파악해야 하며, 예컨대 급여 목록이나 보안 시스템 기록이 특정 직원 집단을 포함하거나 제외할 가능성도 함께 검토해야 한다. 특히 월급제와 주급제의 차이, 임시 결근, 장기 병가 등은 프레임의 포괄성과 대표성을 더욱 복잡하게 만들 수 있다. 따라서 각 설문조사에서는 프레임에 포함될 대상의 기준과 선택 절차를 명확히 정의하고, 그 적절성을 면밀히 검토하는 과정이 반드시 필요하다.\n\n\n(3) 기관\n기관을 대상으로 한 표본 프레임은 일반적으로 단위 목록으로 구성되며, 이 중 기업체는 아마도 설문조사에서 가장 빈번하게 설정되는 목표 모집단일 것이다. 기업 모집단은 고유한 프레임 문제를 수반한다.\n첫째, 기업 모집단의 중요한 특성 중 하나는 규모의 차이가 매우 크다는 점이다. 예를 들어, 소프트웨어 판매업체를 조사할 경우, 연매출이 매우 큰 NC소프트와 소규모 소매점을 모두 프레임에 포함해야 한다. 많은 기업 설문조사는 산업 내 전체 고용 규모나 매출과 같은 크기 관련 변수를 측정하기 때문에, 프레임의 포괄성 문제는 일반적으로 가장 작은 기업보다 가장 큰 기업을 포함하는 데 더 많은 주의를 기울이게 된다.\n둘째, 기업 모집단은 매우 역동적이다. 소규모 기업은 빠르게 설립되거나 폐업되며, 대규모 기업은 다른 회사를 인수하거나 합병하기도 하고, 반대로 분할되기도 한다. 따라서 프레임 모집단은 새로운 기업을 포함하고, 더 이상 존재하지 않는 기업을 제거함으로써 그 포괄성을 유지하기 위해 지속적인 업데이트가 필요하다.\n셋째, 기업 모집단은 법적으로 정의된 실체와 물리적 위치 간의 구분을 내포한다. 예를 들어, 다지점 또는 다국적 기업은 전 세계에 여러 개의 사업장을 운영하지만 본사는 하나뿐이다. 이에 따라 설문조사는 ‘기업’(법적 실체)을 대상으로 할 수도 있고, ‘시설’(물리적 단위)을 대상으로 할 수도 있다. 일부 기업은 물리적 위치 없이 운영되기도 하며(예: 재택 근무 기반의 컨설팅 회사), 반대로 여러 기업이 하나의 물리적 위치를 공유하기도 한다. 이러한 구조는 표본 프레임 설계 시 조사 단위의 정의를 더욱 중요하게 만든다.\n\n\n(4) 사건\n설문조사는 사건 모집단을 대상으로 하며, 여기에 포함되는 사건의 예로는 서비스나 제품 구매, 결혼, 임신, 출생, 실직, 우울증 사례, 범죄 피해 등이 있다. 이러한 사건에 대한 설문조사는 일반적으로 사람들을 대상으로 한 프레임에서 시작되며, 일부 사람들은 여러 사건을 경험하면서 사건 요소 간의 집단을 형성하게 된다.\n사건 표본추출의 또 다른 접근 방식은 시간 단위를 프레임으로 사용하는 것이다. 예를 들어, 동물원 방문 사례를 조사할 때 방문 시간을 기준으로 프레임을 구성하고, 일정한 시간 간격(예를 들어 5분 블록)을 선택하여 해당 시간에 방문한 사람들을 대상으로 질문하는 방식이 사용될 수 있다.\n일부 시간 사용 설문조사는 무작위로 선택된 시점에 전자 호출기를 통해 신호음을 발생시키는 방식을 사용한다. 신호가 울리면, 응답자는 그 시점에 자신이 무엇을 하고 있었는지를 보고하도록 되어 있다. 예를 들어, 사무실에서 일하고 있었는지, 텔레비전을 시청하고 있었는지, 혹은 쇼핑을 하고 있었는지를 기록하게 된다.\n사건을 대상으로 하는 설문조사는 경우에 따라 여러 모집단을 동시에 포함할 수 있다. 이러한 조사는 사건 자체에 대한 통계뿐만 아니라 그 사건을 경험한 사람들에 대한 통계에도 관심을 가진다. 이처럼 목적이 이중적인 경우, 표본 설계 과정에서 클러스터링과 중복과 같은 문제가 발생할 수 있다. 예를 들어, 가족에 의한 자동차 구매 사건을 조사하는 경우, 사건 요소는 구매 행위이지만, 사건을 경험한 사람으로는 법적 소유자, 모든 가족 구성원, 또는 차량을 운전할 가능성이 있는 사람 등 다양한 해석이 가능하다.\n\n\n(5) 희귀 모집단\n희귀 모집단은 연구자가 관심을 갖는 소규모 대상 집단을 지칭하는 용어로, 이들이 희귀하다고 판단되는 이유는 절대적인 규모보다는 사용 가능한 프레임 내에서의 상대적 크기 때문이다. 예를 들어, 전체 인구가 5천만 명이고 이 중 100만 명이 노인 복지 혜택을 받고 있다면, 이는 전체 인구의 약 2%에 해당하므로 희귀 모집단으로 간주될 수 있다. 이러한 모집단을 조사 대상으로 설정할 경우, 적절한 표본 프레임을 식별하는 데 여러 가지 어려움이 따른다.\n희귀 모집단을 위한 표본 프레임을 구축하는 방식에는 크게 두 가지 접근이 있다. 첫째는 희귀 모집단에 속하는 요소들의 목록을 직접 구성하는 방법이다. 예를 들어, 복지 수급자의 목록을 복지 사무소의 행정기록에서 얻을 수 있다. 다만 이러한 자료는 종종 기밀로 취급되거나, 단일 목록이 전체 모집단을 포괄하지 못하는 경우가 많아 여러 출처의 목록을 조합해야 할 수도 있다.\n둘째는 보다 일반적인 모집단 프레임을 설정하고, 그 안에서 희귀 모집단에 해당하는 요소들을 선별하는 방식이다. 예를 들어, 일반 가구 모집단을 대상으로 하여 그 안에서 복지 수급 가구를 찾아내는 방식이 여기에 해당한다. 만약 희귀 모집단의 모든 구성원이 더 큰 프레임 모집단의 하위 집합으로 포함된다면, 희귀 모집단에 대한 완전한 포괄이 가능하다.\n\n\n4. 포함 오류\n불포함은 해결하기 어려운 문제이며, 설문조사에서 중요한 포함 오류의 원인이 될 수 있다. 포함 오류는 표본 통계나 설문조사에서 도출된 추정치의 특성에 영향을 미친다. 하나의 통계는 포함 오류로 인해 크게 왜곡될 수 있는 반면, 동일한 설문조사에서 얻어진 다른 통계는 같은 오류에 거의 영향을 받지 않을 수도 있다. 설문조사 방법론에서는 불포함, 중복, 클러스터링 등을 포함 오류를 유발하는 표본 프레임의 구조적 문제로 본다. 포함 오류란 이러한 문제들이 조사 결과에 미치는 영향을 지칭하는 개념이다.\n\n포함 오류: \\({\\overline{Y}}_{C} - \\overline{Y} = \\frac{U}{N}({\\overline{Y}}_{C} - {\\overline{Y}}_{U})\\), 여기서 \\(\\overline{Y}\\)는 목표 모집단 전체의 평균, \\({\\overline{Y}}_{C}\\)는 표본 프레임에 포함된 모집단의 평균, \\({\\overline{Y}}_{U}\\)는 표본 프레임 밖 모집단의 평균을 나타낸다. \\(N\\)은 목표 모집단의 총 구성원 수, \\(C\\)는 표본 프레임에 포함된 적격 구성원의 총수, 그리고 \\(U\\)는 표본 프레임에 포함되지 않은 적격 구성원의 총수이다.\n\n따라서 프레임에 포함되지 않은 \\((N - c)\\) 개의 단위로 인해 발생하는 오류는, 전체 모집단에서 포함되지 않은 비율과 포함된 단위와 포함되지 않은 단위 간의 평균 차이에 따라 결정된다. 설문조사는 표본 크기와 무관하게 포함된 단위의 평균 \\({\\overline{Y}}_{C}\\) 만을 추정할 수 있다. 이때, 포함되지 않은 단위 U의 규모가 크거나, 포함된 단위와 포함되지 않은 단위 간의 특성 차이가 클수록 편향, 즉 포함 오류의 크기는 커지게 된다.\n포함되지 않은 비율은 모집단의 하위 계층에 따라 달라질 수 있다. 전체 모집단에서의 불포함 비율보다 특정 하위 그룹에서의 불포함 비율이 더 높을 수도 있다. 또한 포함 오류는 포함된 단위와 포함되지 않은 단위 간의 추정치 차이에 따라 결정되므로, 동일한 적격 단위 하위 계층을 기준으로 계산된 통계라 하더라도 각 통계별로 포함 오류의 정도는 다를 수 있다."
  },
  {
    "objectID": "notes/math/derivate_integral.html",
    "href": "notes/math/derivate_integral.html",
    "title": "수학의 기초 2. 미분과 적분",
    "section": "",
    "text": "chapter 1. 미분\n세상에는 움직이지 않는 것이 없다. 우리가 함께 움직이고 있어 느끼지 못할 뿐, 지구는 자전 속도로 약 시속 1,660km, 공전 속도로 약 시속 10.75만 km의 속도로 계속 움직이고 있다. 코페르니쿠스가 제안한 지동설은 천체 관측을 통해 갈릴레오에 의해 뒷받침되었으며, 갈릴레오는 망원경 관찰을 통해 목성의 위성과 금성의 위상 변화를 발견함으로써 지동설을 지지했다. 또한, 행성의 운동 궤도가 완벽한 원이 아니라 타원이라는 사실을 밝혀내어 지동설을 더욱 확고히 한 케플러가 있었다. 이들과 같은 시대를 살았던 뉴턴은 “왜 달은 하늘에 떠 있는 반면, 사과는 땅으로 떨어질까?”라는 질문을 통해 만유인력의 법칙을 정립하고 중력과 중력 가속도를 발견했다.\n한편, 미분은 “변화”로 정의된다. 기존 상태와 변화된 상태는 다르며, 미분은 바로 그 상태 변화에 관심을 둔다. 거리의 변화는 속도, 속도의 변화는 가속도, 그리고 비용과 효용의 변화는 한계비용과 한계효용으로 나타난다. 미분은 함수의 한 점에서 접선의 기울기를 구하는 과정이며, 이를 미분 differentiation 이라 한다. 다항함수, 로그함수, 지수함수, 삼각함수 등 대부분의 함수는 미분가능하며, 특히 통계학에서 자주 사용하는 함수들은 모두 미분가능한 함수로 이루어져 있다.\n미분은 주어진 점에서 접선의 기울기를 계산할 수 있도록 하며 통계학에서는 최적화 문제, 회귀분석, 확률 밀도 함수의 특성 분석 등 다양한 분야에서 활용된다.\n\n1. 평균변화율 average rate of change\n구간 \\(a \\leq x \\leq b\\)에서 함수 \\(f\\)의 평균 변화량으로 \\(\\frac{rise}{run} = \\frac{\\Delta y}{\\Delta x} = \\frac{f(b) - f(b)}{b - a}\\)이다.\n\n\n\n\n\n\\(a \\leq x \\leq b\\) 구간에서 단위당 평균적으로 함수의 변화량을 측정한 것이다. 평균변화량은 고속도로 구간단속에 이용된다. 미분은 지점 과속 단속에 이용된다.\n\n\n\n\n\n측정 1 : 구간단속 시작, 종료 지점에서 과속여부 측정 (미분 응용)\n측정 2 : 예를 들어 구간 거리가 6km라 하자. 2분만에 구간을 통과했다면 평균속도는 \\(\\frac{6 - 0}{2 - 0} = 3km/min.\\)분당 3km를 달렸으므로 시간당 180km를 달렸으니 과속이 되는 것입니다. (평균변화량)\n\n\n2. 미분 정의\n함수 \\(f(x)\\)의 임의의 점 \\(x = a\\)에서의 미분값 \\(f'(a)\\)는 다음과 같이 정의된다. \\(f'(a) = \\lim_{h \\rightarrow 0}\\frac{f(a + h) - f(a)}{h}\\)\n\\(\\frac{f(a + h) - f(a)}{h}\\)는 Fermat’s Difference Quotient로 불리며, 점 \\(a\\)에서의 평균 변화율을 나타낸다.\n극한이 존재하면 \\(f'(a)\\)는 \\(x = a\\)에서의 접선의 기울기로 해석할 수 있다.\n미분 가능성\n  \\(f'(a)\\)가 존재하면, 점 \\(x = a\\)에서 함수 \\(f(x)\\)는 미분 가능하다고 한다. 함수 \\(f(x)\\)가 정의역 전체에서 미분 가능하, 함수 f(x)는 미분 가능 함수이다.\n미분의 기하학적 해석\n  미분값 \\(f'(a)\\)는 곡선 \\(y = f(x)\\)의 점 \\(x = a\\)에서의 접선의 기울기를 의미한다. \\(h\\)가 0으로 가까워질수록 평균 변화율은 접선의 기울기에 점점 가까워진다.\n미분 가능성과 연속성\n  함수 \\(f(x)\\)가 점 \\(x = a\\)에서 미분 가능하면 \\(f(x)\\)는 반드시 그 점에서 연속이다. 하지만, 연속이라고 해서 항상 미분 가능한 것은 아니다. 예를 들어, 절대값 함수 가능하지 않다.\n\n\n\n\n\n\n\n3. 미분 규칙\n상수 함수의 미분\n\\[\\frac{d}{dx}\\lbrack c\\rbrack = 0\\]\n거듭제곱 함수의 미분\n\\[\\frac{d}{dx}\\lbrack x^{n}\\rbrack = nx^{n - 1}, f(x) = x^{n}(n \\in \\mathbb{R})\\]\n【예제】 \\(f(x) = 2\\sqrt{x}\\) 을 미분하시오.\n\\[f'(x) = 2(\\frac{1}{2})x^{1/2 - 1} = x^{- 1/2} = \\frac{1}{\\sqrt{x}}\\]\n상수배의 미분\n\\[\\frac{d}{dx}\\lbrack c \\cdot f(x)\\rbrack = c \\cdot \\frac{d}{dx}\\lbrack f(x)\\rbrack\\]\n합/차의 미분\n\\[\\frac{d}{dx}\\lbrack f(x) \\pm g(x)\\rbrack = \\frac{d}{dx}\\lbrack f(x)\\rbrack \\pm \\frac{d}{dx}\\lbrack g(x)\\rbrack\\]\n곱의 미분\n\\[\\frac{d}{dx}\\lbrack f(x) \\cdot g(x)\\rbrack = f'(x) \\cdot g(x) + f(x) \\cdot g'(x)\\]\n나눗셈의 미분\n\\(\\frac{d}{dx}\\left\\lbrack \\frac{f(x)}{g(x)} \\right\\rbrack = \\frac{f'(x) \\cdot g(x) - f(x) \\cdot g'(x)}{\\lbrack g(x)\\rbrack^{2}}\\), \\(g(x) \\neq 0\\)\n체인룰 chain rule 연쇄규칙\n\\[\\frac{d}{dx}\\lbrack f(g(x))\\rbrack = f'(g(x)) \\cdot g'(x)\\]\n【예제】 \\(f(x) = 2\\sqrt{3x^{2} - 1}\\)을 미분하시오.\n\n\n\n\n\n  바깥부분 미분하고 안쪽 부분 그대로 적는다.\n  \\(2*(1/2){\\sqrt{(3x^{2} - 1)}}^{- 1/2}\\) 그리고 안쪽부분을 미분한다.\n\\[f'(x) = {\\sqrt{(3x^{2} - 1)}}^{- 1/2}6x = \\frac{6x}{\\sqrt{3x^{2} - 1}}\\]\n로그함수 미분\n\\[\\frac{d}{dx}\\lbrack\\log_{a}(x)\\rbrack = \\frac{1}{x\\ln(a)},x &gt; 0\\]\n\\[\\frac{d}{dx}\\lbrack\\ln(x)\\rbrack = \\frac{1}{x},x &gt; 0\\]\n【예제】 \\(f(x) = ln(x^{2} - 1)\\)을 미분하시오.\n  연쇄법칙 적용 : \\(f'(x) = \\frac{1}{x^{2} - 1}2x\\)\n지수함수 미분\n\\[\\frac{d}{dx}\\lbrack a^{x}\\rbrack = a^{x}\\ln(a)\\]\n\\[\\frac{d}{dx}\\lbrack e^{x}\\rbrack = e^{x}\\]\nimport sympy as sp\n\n# 변수와 함수를 정의\nx = sp.Symbol('x')\nf = 5*(x**2 - 2*x)**2\n\n# 함수 입력을 파싱하여 미분\nfunc = sp.sympify(f)\nderivative = sp.diff(func, x)\n\n\n4. 미분 응용\n\n\n(1) 최대, 최소\n\n\n1차 미분정리\n함수 f(x) 가 일정 구간 (a, b) 안의 모든 점에서 미분 가능하고, 구간 내 임의의 점 c 에서 1차 미분이 0이면, f(x) 함수는 c 점에서 지역 최대값이나 최소값을 갖는다. 이는 페르마의 정리에 Fermat’s 해당하며, 극대값 또는 극소값이 존재하는 필수 조건을 설명한다.\n함수 f(x) 가 c 에서 미분 가능하다면,\n극값이 c 에서 존재하면,\n반드시 \\(f'(c) = 0\\)이어야 한다.\n다만, \\(f'(c) = 0\\)이라고 해서 반드시 극값이 존재하는 것은 아니며, 이는 필요조건일 뿐 충분조건은 아니다. 극값의 존재를 확실히 판단하려면 2차 도함수 테스트나 첫 도함수의 부호 변화를 추가로 고려해야 한다.\n\n\n증가 함수와 감소 함수\n함수 f(x)가 구간 \\(I\\)에서 정의되어 있을 때,\n\\(x_{1} &lt; x_{2} \\Longrightarrow f(x_{1}) \\leq f(x_{2})\\)이면 구간 \\(I\\)에서 증가 함수이다.\n\\(x_{1} &lt; x_{2} \\Longrightarrow f(x_{1}) \\geq f(x_{2})\\) 이면 구간 \\(I\\)에서 감소 함수이다.\n\n\n1차 미분과 증가·감소 함수의 관계\n함수 f(x) 가 구간 \\(I\\)에서 미분 가능하다면,\n    - \\(f'(x) &gt; 0\\)이면, f(x) 는 구간 \\(I\\)에서 엄격히 증가한다\n    - \\(f'(x) &lt; 0\\)이면, f(x) 는 구간 \\(I\\)에서 엄격히 감소한다.\n\n\n오목성 concavity 정의\n함수 f(x) 의 기울기가 감소하는 경우 \\(f''(x) &lt; 0\\),\n    - 함수 f(x) 는 concave down (오목 아래)이다.\n    - 그래프가 아래로 휘어진 모양을 갖는다.\n함수 f(x) 의 기울기가 증가하는 경우 \\(f''(x) &gt; 0\\),\n    - 함수 f(x) 는 concave up (오목 위)이다.\n    - 그래프가 위로 휘어진 모양을 갖는다.\n\n\n\n\n\n\n\n변곡점 inflexion point 정의\n함수 \\(f(x)\\)의 오목성이 변하는 점이 있을 때, 이 점을 변곡점이라고 한다. 즉, \\(f(x)\\)가 \\(f’’(x) &gt; 0\\)에서 \\(f’’(x) &lt; 0\\)로 바뀌거나 \\(f’’(x) &lt; 0\\)에서 \\(f’’(x) &gt; 0\\)로 바뀌는 점이 변곡점이다.”\n1차 미분과 2차 미분을 이용한 최대, 최소 판단\n주어진 \\(f'(c) = 0\\)에서, \\(f''(c)\\)를 확인한다.\n  - \\(f''(c) &gt; 0\\) 이면 \\(x = c\\)에서 (지역) 최소값\n  - \\(f''(c) &lt; 0\\) 이면 \\(x = c\\)에서 (지역) 최대값\n  - \\(f''(c) = 0\\) 이고 \\(f''(x)\\) 부호가 바뀌면 \\(x = c\\)에서 변곡점\n\n\n(2) 통계학 응용\n단순 회귀모형 \\[y_{i} = \\beta_{0} + \\beta_{1}x_{i} + \\epsilon_{i},i = 1,2,\\ldots,n\\]\nOLS 추정치 \\[\\text{Minimize:}S(\\beta_{0},\\beta_{1}) = \\overset{n}{\\sum_{i = 1}}(y_{i} - \\beta_{0} - \\beta_{1}x_{i})^{2}\\]\n오차 제곱합   \\(S(\\beta_{0},\\beta_{1})\\)을 \\(\\beta_{0}\\)와 \\(\\beta_{1}\\)에 대해 편미분한 뒤 0으로 설정하여 최소값(OLS)을 찾는다.\n정규방정식\n\\[\\frac{\\partial S}{\\partial\\beta_{0}} = - 2\\overset{n}{\\sum_{i = 1}}(y_{i} - \\beta_{0} - \\beta_{1}x_{i}) = 0\\]\n\\[\\overset{n}{\\sum_{i = 1}}y_{i} = n\\beta_{0} + \\beta_{1}\\overset{n}{\\sum_{i = 1}}x_{i}\\]\n\\[\\frac{\\partial S}{\\partial\\beta_{1}} = - 2\\overset{n}{\\sum_{i = 1}}x_{i}(y_{i} - \\beta_{0} - \\beta_{1}x_{i}) = 0\\]\n\\[\\overset{n}{\\sum_{i = 1}}x_{i}y_{i} = \\beta_{0}\\overset{n}{\\sum_{i = 1}}x_{i} + \\beta_{1}\\overset{n}{\\sum_{i = 1}}x_{i}^{2}\\]\n두 식을 함께 사용하여 \\(\\beta_{0}\\)와 \\(\\beta_{1}\\)를 계산한다.\n\\[\\beta_{1} = \\frac{\\sum_{i = 1}^{n}(x_{i} - \\overline{x})(y_{i} - \\overline{y})}{\\sum_{i = 1}^{n}(x_{i} - \\overline{x})^{2}} = \\frac{\\text{Cov}(x,y)}{\\text{Var}(x)}\\]\n\\[\\beta_{0} = \\overline{y} - \\beta_{1}\\overline{x}\\]\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.optimize import curve_fit\n\n# 가상의 데이터 생성 n=20\nnp.random.seed(0)\nx_data = np.linspace(-5, 5, 20)\ny_data = 2 * x_data**3 - 3 * x_data**2 + 4 * x_data + 10 + np.random.normal(0, 10, 20)\n\n# 직선 적합 함수\ndef linear(x, a, b):\n    return a * x + b\n# 2차 함수 적합 함수\ndef quadratic(x, a, b, c):\n    return a * x**2 + b * x + c\n# 3차 함수 적합 함수\ndef cubic(x, a, b, c, d):\n    return a * x**3 + b * x**2 + c * x + d\n# 최소자승법을 이용한 직선, 2차, 3차 적합\nparams_linear, _ = curve_fit(linear, x_data, y_data)\nparams_quadratic, _ = curve_fit(quadratic, x_data, y_data)\nparams_cubic, _ = curve_fit(cubic, x_data, y_data)\n\n# 적합된 함수의 값을 계산\ny_fit_linear = linear(x_data, *params_linear)\ny_fit_quadratic = quadratic(x_data, *params_quadratic)\ny_fit_cubic = cubic(x_data, *params_cubic)\n\n# Residual Sum of Squares 계산\nrss_linear = np.sum((y_data - y_fit_linear) ** 2)\nrss_quadratic = np.sum((y_data - y_fit_quadratic) ** 2)\nrss_cubic = np.sum((y_data - y_fit_cubic) ** 2)\n\n# 최소자승법을 이용한 직선, 2차, 3차 적합\nparams_linear, _ = curve_fit(linear, x_data, y_data)\nparams_quadratic, _ = curve_fit(quadratic, x_data, y_data)\nparams_cubic, _ = curve_fit(cubic, x_data, y_data)\n\n# 적합된 함수의 값을 계산\ny_fit_linear = linear(x_data, *params_linear)\ny_fit_quadratic = quadratic(x_data, *params_quadratic)\ny_fit_cubic = cubic(x_data, *params_cubic)\n\n# Residual Sum of Squares 계산\nrss_linear = np.sum((y_data - y_fit_linear) ** 2)\nrss_quadratic = np.sum((y_data - y_fit_quadratic) ** 2)\nrss_cubic = np.sum((y_data - y_fit_cubic) ** 2)\n\n# 그래프 그리기\nplt.figure(figsize=(10, 6))\nplt.scatter(x_data, y_data, label='data', color='black')\nplt.plot(x_data, y_fit_linear, label='Linear fit (y = {:.2f}x + {:.2f}) : RSS={:.2f}'.format(params_linear[0], params_linear[1],rss_linear), color='blue')\nplt.plot(x_data, y_fit_quadratic, label='Quardratic fit (y = {:.2f}x^2 + {:.2f}x + {:.2f}) : RSS={:.2f}'.format(params_quadratic[0], params_quadratic[1], params_quadratic[2],rss_quadratic), color='green')\nplt.plot(x_data, y_fit_cubic, label='Cubic fit (y = {:.2f}x^3 + {:.2f}x^2 + {:.2f}x + {:.2f}) : RSS={:.2f}'.format(params_cubic[0], params_cubic[1], params_cubic[2], params_cubic[3],rss_cubic), color='red')\nplt.axhline(0, color='grey', lw=0.5, ls='--')\nplt.axvline(0, color='grey', lw=0.5, ls='--')\nplt.title('fit by OLS')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.legend()\nplt.grid()\nplt.show()\n\n\n\n\n\n\n\n(3) 한계효용체감의 법칙\n한계효용 marginal utility은 재화가 증가 혹은 감소함에 따라 주관적으로 매겨지는 경제적 효용(혹은 가치)의 관계에 대한 개념으로 합리적인 경제에서 인간 행동은 자신에게 가장 시급한 욕구를 충족하는 일을 가장 먼저 하거나 가치를 두는 특성이 있다. 따라서 어떤 사람이 재화나 용역을 이용하여 효용을 얻고자 할 때 주관적으로 판단되는 욕망 충족의 정도인 효용의 가치가 높은 것부터 낮은 것 쪽으로 추구한다. 재화나 용역의 한계효용은 그 재화나 용역을 사용하는 것을 증가하거나 감소함에 따라 변화한 가치의 양을 상정한 것인데 이런 변화에서 추가의 1단위 즉 경계인 단위에서의 재화나 용역의 효용을 한계효용이라고 한다.[위키피디아]\n\n\n\n\n\n총효용 total utility 은 주어진 기간 동안 소비된 특정 상품의 모든 단위에서 얻은 총만족입니다. 한계효용 marginal utility 마지막 소비량에서 상품 소비의 1단위 변화로 인해 발생하는 총 효용의 변화이다. 더 많은 단위의 상품을 구매하면 한계 효용은 감소하기 시작하지만 총 효용은 계속해서 감소 비율이 줄어든다. 한계효용이 0가 되는 포화점 satiety에 이르렀을 때 이 지점에서의 총효용은 최대가 된다. 이 지점에서 소비가 더 증가하면 한계 효용은 음수가 되고 총 효용은 감소하기 시작한다.\n\n\n(4) Cobb-Douglas 생산함수\n\\(Q = f(K,L) = AL^{\\alpha}K^{\\beta}\\), \\(Q\\)= 생산, \\(K\\)=자본, \\(L\\)=노동, \\(A,\\alpha,\\beta\\)는 모수이다. \\(K,L\\)에 대하여 각각 편미분 하면 다음과 같다.\n  - 양변에 로그를 취한다. \\(ln(Q) = lnA + \\alpha lnL + \\beta lnK\\)\n  - \\(\\frac{\\partial(lnQ)}{\\partial L} = \\alpha\\) : 한계 노동 생산량\n  - \\(\\frac{\\partial(lnQ)}{\\partial K} = \\beta\\) : 한계 자본 생산량\n\n\n\n\nchapter 2. 적분\n고대 수학자들은 직선으로 이루어진 도형의 면적을 비교적 쉽게 계산할 수 있었습니다. 사각형, 삼각형, 평행사변형, 사다리꼴과 같은 도형은 밑변과 높이를 활용한 간단한 공식을 통해 면적을 구할 수 있었기 때문입니다. 그러나 곡선이 포함된 도형의 면적을 계산하는 문제는 훨씬 더 복잡한 도전 과제였습니다.\n곡선이 포함된 도형의 면적을 구하기 위해 현대 수학에서는 적분이라는 개념이 도입되었습니다. 이는 고대 그리스의 수학자 아르키메데스가 처음으로 탐구한 주제 중 하나였습니다. 아르키메데스는 곡선 아래의 면적을 구하기 위해 곡선을 아주 작은 직사각형들로 나누고, 그 면적을 합산하여 근사값을 구하는 방식을 사용했습니다. 이 과정은 시간이 지나며 점점 더 체계적으로 발전하였고, 마침내 미적분학으로 이어졌습니다.\n아이작 뉴턴과 고트프리트 라이프니츠는 아르키메데스의 아이디어를 발전시켜 적분과 미분이라는 두 가지 핵심 개념을 정립하였고, 이를 통해 곡선 아래의 면적을 정확히 계산할 수 있는 도구를 완성했습니다. 오늘날 우리가 사용하는 적분법은 이들의 연구에 기반을 두고 있으며, 곡선의 면적뿐만 아니라 물리학, 공학, 경제학 등 다양한 분야에서 중요한 역할을 하고 있습니다.\n적분은 통계학에서 확률 계산, 기대값, 분산, 베이지안 추론 등 다양한 개념과 도구에 중요한 역할을 합니다. 확률을 곡선 아래 면적으로 해석하는데서부터 시작해, 통계적 추론의 기초를 형성하는 데 적분이 필수적입니다. 이러한 적분 개념은 통계학 이론뿐만 아니라 데이터 분석, 머신러닝, 신뢰구간 계산 등 실무적인 응용에서도 널리 사용됩니다.\n\n1. 부정 적분\n함수 F(x) 가 주어진 함수 f(x) 에 대해 정의역의 모든 점에서 \\(F'(x) = f(x)\\)를 만족한다면, F(x) 를 f(x) 의 역-미분 anti-derivative 또는 원시함수 primitive function 합니다. 이는 적분이 미분의 역연산임을 의미합니다.\n적분이 미분의 역연산이라는 사실을 처음 체계적으로 증명하고 이를 수학적으로 정립한 사람들은 아이작 뉴턴(Isaac Newton)과 고트프리트 라이프니츠(Gottfried Wilhelm Leibniz)입니다. 이들은 독립적으로 미적분학의 기본 개념을 발전시켰으며, 이 과정에서 적분과 미분의 관계를 설명한 미적분학의 기본정리를 도출했습니다.\n\n\n정적분과 미분의 관계\n특정 구간에서의 정적분은 미분을 통해 함수의 값을 복원할 수 있습니다. 예를 들어, 함수 f(x) 에 대해 다음과 같은 정적분이 있을 때,\n\\(F(x) = \\int_{a}^{x}f(t)dt\\). 이를 x 에 대해 미분하면 \\(\\frac{d}{dx}F(x) = f(x)\\)\n즉, 적분을 통해 구한 누적 변화량을 다시 미분하면, 원래의 함수로 돌아갑니다.\n적분과 미분은 서로 반대되는 과정처럼 보이지만, 실제로는 상호보완적입니다. 적분은 함수의 누적적인 변화(예: 곡선 아래의 면적)를 측정하며, 미분은 순간적인 변화(예: 기울기)를 측정합니다.\n\n\n2. 정적분\n\n\n(1) 정적분 개념\n정적분(면적)은 부정적분(역-미분 함수)과는 다른 접근 방식에서 출발합니다. 그러나 이 두 개념은 17세기에 뉴턴(Newton)과 라이프니츠(Leibniz)에 의해 서로 밀접하게 연결되었고, 이를 통합하여 적분(integral)이라고 명명하였습니다.\n우선, 정적분의 개념을 살펴보겠습니다. 구간 [a, b]에서 함수 f(x) 아래의 면적을 어떻게 구할 수 있을까요? 이를 위해 구간 [a, b]를 여러 작은 구간으로 나눈 다음, 각 구간에서 직사각형의 면적을 계산하여 합산하는 방법을 생각할 수 있습니다. 이러한 직사각형의 면적 합은 점점 더 작은 구간으로 나눌수록 실제 면적에 근사하게 됩니다.\n\n\n\n\n\n함수와 x-축 사이에 형성된 이 면적은 정적분이라 하며, 이는 구간 [a, b]에서 함수 f(x)와 x-축 사이의 공간에 해당합니다. 직사각형을 이용해 근사한 면적은 실제 면적보다 클 수도 있고 작을 수도 있습니다. 하지만 구간을 점점 더 세분화하면, 이 근사값은 실제 정적분 값에 수렴하게 됩니다.\n정적분은 함수의 곡선 아래의 면적을 계산하는 방법으로 출발했지만, 부정적분(역-미분 함수)과의 연결을 통해 더욱 강력한 수학적 도구로 발전하였습니다.\n\n\n(2) 정적분과 부정적분의 관계\n함수 f(x) 가 구간 [a, b]에서 연속일 때:\n\n부정적분(역-미분 함수): 함수 F(x) 가 f(x) 의 부정적분이라면 \\(F'(x) = f(x)\\)\n정적분(구간의 면적): 함수 f(x) 의 정적분은 구간 [a, b]에서 f(x) 와 x -축 사이의 면적을 나타냅니다. \\(\\int_{a}^{b}f(x)dx\\)\n뉴턴-라이프니츠 정리: 부정적분과 정적분은 다음과 같이 연결됩니다. \\(\\int_{a}^{b}f(x)dx = F(b) - F(a)\\)\n여기서 F(x) 는 f(x) 의 부정적분입니다.\n\n이 정리는 정적분(구간에서의 면적 계산)이 부정적분(역-미분 함수)을 사용하여 계산될 수 있음을 보여줍니다.\n\n\n(3) 정적분 규칙\n\n\n특정 점에서의 확률\n\\[\\int_{a}^{a}f(x)dx = 0\\]\n이는 구간의 길이가 0 일 때, 정적분의 결과가 항상 0 임을 나타냅니다(통계적으로: 연속 확률변수에서 특정 점에서의 확률은 0 이다).\n\n\n구간 순서 반대\n\\[\\int_{a}^{b}f(x)dx = - \\int_{b}^{a}f(x)dx\\]\n구간의 순서를 바꾸면 정적분의 부호가 반대가 됩니다.\n\n\n상수 배율\n\\[\\int_{a}^{b}c \\cdot f(x)dx = c\\int_{a}^{b}f(x)dx(\\text{c is constant})\\]\n적분 내부에 상수가 곱해져 있을 경우, 상수를 적분 기호 밖으로 꺼낼 수 있습니다.\n\n\n합과 차\n\\[\\int_{a}^{b}\\left( f(x) \\pm g(x) \\right)dx = \\int_{a}^{b}f(x)dx \\pm \\int_{a}^{b}g(x)dx\\]\n적분은 덧셈과 뺄셈 연산에 대해 분배법칙을 따릅니다.\n\n\nDomination Rule\n만약 \\(f(x) \\geq 0\\)가 구간 [a, b]에서 항상 성립하면\n\\(\\int_{a}^{b}f(x)dx \\geq 0\\) 이다. 통계적으로 확률변수의 분포 함수는 항상 0 이상 이므로, 확률값은 항상 0 이상이다.\n\n\n부등식 관계\n만약 \\(f(x) \\leq g(x)\\)가 구간 [a, b]에서 항상 성립하면\n\\(\\int_{a}^{b}f(x)dx \\leq \\int_{a}^{b}g(x)dx\\) 이다.\n\n\n구간 쪼개기\n\\[\\int_{a}^{c}f(x)dx + \\int_{c}^{b}f(x)dx = \\int_{a}^{b}f(x)dx\\]\n적분 구간을 나누어 계산할 수 있습니다.\n\n\n확률밀도함수 전체 구간\n\\(\\int_{- \\infty}^{\\infty}f(x)dx = 1\\). 확률밀도함수(PDF)는 전체 구간에서의 적분, 확률의 총합이 1 임을 나타냅니다.\n\n\n지수함수 적분\n\\[\\int a^{x}dx = \\frac{a^{x}}{\\ln a} + C(a &gt; 0,a \\neq 1)\\]\n\\[\\int e^{x}dx = e^{x} + C\\]\n\n\n로그함수 적분\n\\[\\int\\log_{a}(x)dx = \\frac{1}{\\ln(a)}\\left( x\\ln(x) - x \\right) + C\\]\n\\[\\int\\ln(x)dx = x\\ln(x) - x + C\\]\n\n\n특수한 적분\n\\[\\int\\frac{1}{x} = ln|x| + C\\]\n\n\n치환적분\n함수 g(x) 가 x 에 대한 미분가능한 함수이고, f(u) 가 u = g(x) 에 대한 함수라고 가정하겠습니다.\n\\[\\int f(g(x)) \\cdot g'(x)dx = \\int f(u)du\\]\n\\[u = g(x) , du = g'(x)dx\\]\n【사례】 \\(\\int x \\cdot e^{x^{2}}dx = \\frac{1}{2}e^{x^{2}} + C\\)\n\\(u = x^{2}\\)로 치환하면, \\(du = 2xdx\\). 따라서 \\(xdx = \\frac{1}{2}du\\)\n\\(\\int x \\cdot e^{x^{2}}dx = \\int e^{u} \\cdot \\frac{1}{2}du = \\frac{1}{2}\\int e^{u}du\\)=\\(\\frac{1}{2}\\int e^{u}du = \\frac{1}{2}e^{u} + C\\)\n\\(u = x^{2}\\) 이므로 \\(\\int x \\cdot e^{x^{2}}dx = \\frac{1}{2}e^{x^{2}} + C\\) 이다.\n\n\n부분적분\n함수 u(x) 와 v(x) 가 미분 가능할 때, 다음 공식이 성립합니다:\n\\[\\int udv = uv - \\int vdu\\]\n\n\\(u\\): 미분할 함수 (\\(u \\rightarrow du\\))\n\\(dv\\): 적분할 함수 (\\(dv \\rightarrow v\\))\n\n【사례】 \\(\\int xe^{x}dx\\)\n1) 함수 선택: \\(u = x,dv = e^{x}dx\\)\n2) 미분 및 적분: \\(u \\rightarrow du = dx\\),\\(dv \\rightarrow v = e^{x}\\)\n3) 부분적분 공식 적용: \\(\\int xe^{x}dx = uv - \\int vdu\\)\n\\(= xe^{x} - \\int e^{x}dx\\)\\(= xe^{x} - e^{x} + C\\).\n【사례】 \\(\\int_{0}^{1}x^{2} + \\sqrt{x}dx\\) 구하시오.\n\\(f(x) = x^{2} + \\sqrt{x}\\)이므로 \\(F(x) = \\frac{1}{3}x^{3} + \\frac{2}{3}x^{\\frac{3}{2}}\\)\n\\(F(1) = 1\\), \\(F(0) = 0\\)이므로 1이다.\n\\[\\int_{0}^{1}x^{2} + \\sqrt{x}dx = \\frac{1}{3}x^{3} + \\frac{2}{3}x^{\\frac{3}{2}}\\rbrack_{0}^{1} = 1 - 0 = 1\\]\n#부정적분\nfrom sympy import *\nx=Symbol('x')\nintegrate(x**2+x**(0.5), x)\n\\[ x^3/3 + 0.66667x^{1.5}\n\\]\n#정적분\nfrom scipy.integrate import quad\ndef integrand(x):\n   return x**2+x**(0.5)\nquad(integrand,0, 1)\n【결과】 첫번째 값은 적분값이고 두 번째는 적분 값을 얼마나 근사하게 계산하였는지 값이다. 완벽한 값이면 0이어야 하나 출력된 값은 0.0(14개)11…이다. root는 실제 근이다. (1.0, 1.1102230246251565e-15)\n【사례】 표준 정규확률분포함수\\(\\int_{0}^{\\infty}\\frac{1}{\\sqrt{2\\pi}}e^{- \\frac{x^{2}}{2}}dx\\) 구하시오.\n#부정적분\nfrom sympy import *\nimport numpy as np\nx=Symbol('x')\nintegrate(1/(2*np.pi)**0.5*exp(-x**2/2), x)\n\\[\n0.199471140200716 \\sqrt{2} \\sqrt{\\pi} \\, \\mathrm{erf}\\left( \\frac{\\sqrt{2}x}{2} \\right)\n\\]\nimport numpy as np\n#정적분\nfrom scipy.integrate import quad\ndef integrand(x):\n   return 1/(2*np.pi)**0.5*exp(-x**2/2)\nquad(integrand,0,np.inf)\n【결과】 (0.49999999999999983, 5.08909572547112e-09)\n\n\n(4) 표적분 tabular integral\n표 적분은 부분적분을 용이하게 한다. 미분 부분 \\(f(x)\\)는 미분하면서 차수가 용이해야 하고, 적분함수 \\(g(x)\\)는 용이하게 적분할 수 있어야 한다.\n(방법1) 미분 부분이 0이 될 때까지 미분과 적분을 반복 시행한다.\n\\[\\int_{a}^{b}udv = (1)*(a) - (2)*(b) + (3)(c)...\\rbrack_{a}^{b}\\]\n(방법2)한 번만 미분하고 \\(\\int_{a}^{b}udv = (1)*(a) - \\int_{a}^{b}(2)*(b)dx\\)\n\n\n\n\n\n【예제】 \\(\\int_{0}^{\\infty}xe^{- x}dx\\) 표 적분하시오.\n\n\n\n\n\n(방법1) \\(x( - e^{- x}) - e^{- x}\\rbrack_{0}^{\\infty} = 1\\)\n(방법2) \\(x( - e^{- x})\\rbrack_{0}^{\\infty} - \\int_{0}^{\\infty} - e^{- x} = 1\\)으로 계산한다.\n【예제】 \\(\\int_{1}^{2}ln(x)dx\\) 표 적분하시오.\n\n\n\n\n\n\\[\\int_{1}^{2}ln(x)dx = ln(x)x\\rbrack_{1}^{2} - \\int_{0}^{1}1dx = 2ln(2) - ln(1) - x\\rbrack_{0}^{1} = 0.386\\]\n\n\n3. 적분 응용\n연속형 확률분포의 확률밀도함수\n\n\n\n\n\n연속형 확률변수 X 의 확률밀도함수 f(x) 는 특정 구간에서 확률을 계산하는 데 사용됩니다. 이때 확률은 적분을 통해 구합니다:\n\\[P(a \\leq X \\leq b) = \\int_{a}^{b}f(x)dx\\]\nf(x) 는 음수가 아니며, 전체 구간에서의 적분값은 항상 1이 됩니다:\n\\[\\int_{- \\infty}^{\\infty}f(x)dx = 1\\]\n【예제】 정규분포 N(0, 1) 에서 \\(P( - 1 \\leq Z \\leq 1) = \\int_{- 1}^{1}\\phi(z)dz\\) 이다. 여기서 \\(\\phi(z) = \\frac{1}{\\sqrt{2\\pi}}e^{- z^{2}/2}\\)는 표준정규분포의 확률밀도함수입니다.\n누적확률분포함수 cumulative probability density fuction\n\n\n\n\n\n기대값\n  연속형 확률변수 X 의 확률밀도함수 f(x)라 하면 기대값은 \\(E(X) = \\int xf(x)dx\\) 이다.\n적분과 백분위값\n  백분위값 percentile은 확률분포에서 특정 비율의 누적 확률을 기준으로 하는 값입니다. P번째 백분위값은 확률변수 X의 값 \\(X_{P}\\)로, 확률변수가 \\(X_{P}\\)이하일 확률이 \\(\\frac{P}{100}\\)이 되는 값입니다.\n\\[F(x_{P}) = \\int_{- \\infty}^{x_{P}}f(x)dx = \\frac{P}{100}\\]\n  - \\(f(x)\\): 확률밀도함수(PDF)\n  - \\(F(x)\\): 누적분포함수(CDF)\n  - \\(x_{P}\\): \\(P\\)번째 백분위값"
  },
  {
    "objectID": "notes/math/function.html",
    "href": "notes/math/function.html",
    "title": "수학의 기초 1. 함수",
    "section": "",
    "text": "chapter 1. 기초\n\n1. 함수와 통계학\n함수는 통계학에서 데이터를 설명하고 모델링하는 수단이며, 이론적 개념을 수학적으로 표현하는 핵심 도구이다. 데이터 간의 관계를 나타내고, 확률분포, 추정, 검정 등 다양한 통계 기법에서 필수적인 역할을 수행한다.\n통계함수\n  통계함수는 독립변수(\\(x\\))와 종속변수(\\(y\\)) 데이터 간 관계를 설명한다. \\(y = f(x) + e\\)로 표현되며 \\(e\\)는 오차항이다.\n확률밀도함수\n  확률밀도함수 \\(p(x)\\)는 확률변수의 확률이 함수값이다.\n기대값\n  확률변수의 평균적인 값이다. \\(E(X) = \\sum xp(x)\\)\n\n\n2. 함수와 시리즈\n시리즈는 복잡한 함수를 단순한 다항식으로 근사하거나, 함수의 특성을 분석하는 데 사용된다. 시리즈는 유한하거나 무한한 항들로 이루어진 수열의 합으로 정의된다.\n유한 시리즈: \\(S_{n} = {\\sum_{i = 1}^{n}}a_{k}\\)\n무한 시리즈: \\(S_{\\infty} = {\\sum_{i = 1}^{\\infty}}a_{k}\\)\n이항시리즈 binomial series\n\\[(a + b)^{n} = a^{n} + \\binom{n}{1}a^{n - 1}b + ... + \\binom{n}{n - 1}ab^{n - 1} + b^{n}\\]\n특수한 경우\n\\[\\frac{1}{(1 + x)^{2}} = - 1 + 2x - 3x^{2} + 4x^{3} - ...\\]\n\\[\\frac{1}{1 + x} = 1 - x + x^{2} - x^{3} + x^{4} - ...\\]\n지수시리즈 exponential series\n\\[e^{x} = 1 + x + \\frac{x^{2}}{2!} + \\frac{x^{3}}{3!} + ...\\]\n\\[e^{x} = lim_{n \\rightarrow}^{\\infty}(1 + \\frac{x}{n})^{n}\\]\n\\[ln(1 + x) = x - \\frac{{}^{2}}{2} + \\frac{x^{3}}{3} - \\frac{x^{4}}{4} + ..., - 1 &lt; x &lt; 1\\]\n산술시리즈 arithmetic series\n\\[S_{n} = a + (a + d) + (a + 2d) + \\cdots + \\lbrack a + (n - 1)d\\rbrack\\]\n      - \\(a\\): 첫 번째 항, \\(d\\): 공차(항 사이의 일정한 차이), \\(n\\): 항의 개수\n\\[S_{n} = \\frac{n}{2}\\lbrack 2a + (n - 1)d\\rbrack\\]\n기하시리즈 geometric series\n\\[S_{n} = a + ar + ar^{2} + \\cdots + ar^{n - 1}\\]\n     - \\(a\\): 첫 번째 항, \\(r\\): 공비(항 사이의 일정한 차이)\n\\[S_{n} = \\frac{a(1 - r^{n})}{1 - r},r \\neq 1\\]\n무한 기하시리즈: \\(S_{n} = \\frac{a}{1 - r}, - 1 &lt; r &lt; 1\\)\n\n\n3. 통계학 주요상수\n지수 exponent \\(e\\)\n  자연로그 함수의 밑으로 정의되며, 무한 급수로 표현된다.\n  \\(e \\approx 2.71828182845904\\ldots\\)(무리수)\n  통계학의 주요 확률분포함수(정규분포, 포아송분포)의 항이다.\n자연상수 \\(ln2\\)\n\\[\\ln 2 \\approx 0.69314718056\\ldots \\text{(무리수)}\\]\n  정보 이론: 1비트의 정보. 이진수 체계와 로그 연산.\n황금비 \\(\\phi \\approx 1.61803398874989\\ldots\\)\n  \\(a/b = (a + b)/a\\)를 만족하는 비율 \\(\\phi = \\frac{1 + \\sqrt{5}}{2}\\)\n오일러상수\n  조화급수와 자연로그의 차이로 정의된다.\n\\[\\gamma = \\lim_{n \\rightarrow \\infty}\\left( \\overset{n}{\\sum_{k = 1}}\\frac{1}{k} - \\ln n \\right) \\approx 0.577215664901532\\ldots\\]\n\n\n기호\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n소문자\nα\nβ\nγ\nδ\nε\nζ\nη\nθ\n\n\n대문자\nΑ\nΒ\nΓ\nΔ\nΕ\nΖ\nΗ\nΘ\n\n\n발음\nalpha\nbeta\ngamma\ndelta\nepsilon\nzeta\neta\ntheta\n\n\n소문자\nι\nκ\nχ\nλ\nμ\nν\nξ\nο\n\n\n대문자\nΙ\nΚ\nΧ\nΛ\nΜ\nΝ\nΞ\nΟ\n\n\n발음\niota\nkappa\nchi\nlambda\nmu\nnu\nxi(ksi)\nomicron\n\n\n소문자\nπ\nρ\nσ\nτ\nυ\nϕ\nψ\nω\n\n\n대문자\nΠ\nΡ\nΣ\nΤ\nΥ\nΦ\nΨ\nΩ\n\n\n발음\npi\nrho\nsigma\ntau\nupsilon\nphi\npsi\nomega\n\n\n\n\n\n\n\nchapter 2. 좌표와 직선방정식\n\n1. 이차원평면과 데카르트 좌표\n\n\n\n\n\n이차원 평면에서 모든 점은 숫자 좌표로 coordinate 표현할 수 있으며, 점들의 집합으로 이루어진 선이나 곡선은 좌표방정식으로 나타낼 수 있다. 이를 위해 이차원 평면에는 두 개의 직선이 설정된다.. 수평선인 \\(x\\)-축 axis과 수직선인 \\(y\\)-축이다. 이 두 직선은 원점에서 직각으로 교차하며, 원점은 두 축의 기준점이 된다.\n원점을 기준으로 \\(x\\)-축에서 \\(a\\)만큼, \\(y\\)-축에서 \\(b\\)만큼 떨어진 점의 좌표는 \\((a,b)\\)로 표기된다. 이러한 표기 방식은 데카르트 좌표라고 한다. 여기서 \\(a\\)와 \\(b\\)는 각각 \\(x\\)-좌표와 \\(y\\)-좌표를 나타내며, 이 값들은 모두 실수 값으로 구성된다.\n데카르트 Cartesian 좌표계는 이차원 평면에서 점의 위치를 명확하고 직관적으로 나타내는 데 사용되며, 수학적 분석 및 응용의 기초가 된다. 이를 활용하면 점, 선, 곡선, 그리고 다양한 기하학적 형태를 방정식으로 표현하고, 이를 통해 여러 문제를 해결할 수 있다.\n\n\n2. 직선과 증가\n\n\n직선\n두 점을 가장 짧은 거리로 연결하는 선을 직선이라고 한다. 직선은 두 점 사이의 최단 경로로 정의되며, 그 위에는 무수히 많은 점이 존재한다. 좌표평면에서 직선은 중요한 기하학적 구조로, 점과 점 사이의 관계를 나타내는 기본 도구이다.\n\n\n증가량 (Increment)\n\n\n\n\n\n좌표평면에서 두 점 \\((x_{1},y_{1})\\)과 \\((x_{2},y_{2})\\)의 이동을 고려할 때, x-좌표와 y-좌표의 변화량을 각각 증가량이라고 한다.\nx-좌표의 증가량: \\(\\Delta x = x_{2} - x_{1}\\)\ny-좌표의 증가량: \\(\\Delta y = y_{2} - y_{1}\\)\n증가량의 부호와 크기는 두 점의 좌표 차이에 의해 결정되며, x-좌표나 y-좌표의 변화 방향을 나타낸다.\n\n\n기울기 slope\n증가량은 두 점을 지나는 직선의 기울기를 계산하는 데 활용된다. 기울기 m은 두 점 사이의 x-좌표의 증가량에 대한 y-좌표의 증가량의 비율로 정의되며, 다음과 같은 식으로 표현된다:\n\\[m = \\frac{\\Delta y}{\\Delta x} = \\frac{y_{2} - y_{1}}{x_{2} - x_{1}},\\Delta x \\neq 0\\]\n    - \\(m &gt; 0\\): 직선이 오른쪽으로 올라간다.\n    - \\(m &lt; 0\\): 직선이 오른쪽으로 내려간다.\n    - \\(m = 0\\): 직선이 수평이다.\n    - \\(m\\)이 정의되지 않음 (\\(\\Delta x = 0\\)): 직선이 수직이다.\n\n\n수평 parallel과 수직 perpendicular\n두 직선 \\(L_{1}\\)과 \\(L_{2}\\)의 기울기가 동일하면, 즉 \\(m_{1} = m_{2}\\)이면 두 직선은 서로 평행 하다고 한다. 이 경우, 두 직선은 교차하지 않으며, 동일한 방향으로 뻗어 있다.\n두 직선 \\(L_{1}\\)과 \\(L_{3}\\)의 기울기의 곱이 -1이면, 즉 \\(m_{1} \\cdot m_{2} = - 1\\)이면 두 직선은 서로 수직하다고 한다. 이는 두 직선이 교차할 때 \\(90^{\\circ}\\)의 각을 이루는 경우이다.\n\n\n3. 직선 방정식 linear equation\n직선 방정식은 직선 위의 모든 점의 좌표를 만족하며, 직선 이외의 점의 좌표에서는 만족하지 않는 방정식이다. 좌표평면에서 직선은 절편 intercept과 기울기 slope를 이용해 다음과 같은 일반적인 형태로 표현된다. \\(y = bx + a\\)\n\n\\(b\\): 직선의 기울기, \\(a\\): y-축과 교차하는 절편\n\n\n\n직선 구성요소\n기울기 \\(b\\)는 직선이 얼마나 가파르게 증가하거나 감소하는지를 나타내며, x-좌표의 변화량에 대한 y-좌표의 변화량의 비율로 정의된다:\n\\[b = \\frac{\\Delta y}{\\Delta x}\\]\n    - b &gt; 0: 직선이 오른쪽으로 올라간다.\n    - b &lt; 0: 직선이 오른쪽으로 내려간다.\n    - b = 0: 직선이 수평이다.\n절편 a는 직선이 y-축과 만나는 점의 y-좌표를 나타낸다. x = 0일 때, 직선 방정식에서 y = a가 된다.\n\n\n수평선 horizontal line\n기울기 b = 0인 경우, 직선은 수평선이 된다. 이러한 직선의 방정식은 \\(y = a\\)이다. 이 직선은 x-축과 평행하며, y-축 상에서 y = a를 지난다.\n\n\n수직선 vertical line\ny-축과 평행한 직선의 방정식으로 \\(x = c\\)이다. 이 직선은 x-축과 x = c에서 교차한다. 기울기가 정의되지 않으며, 수직선은 y-축과 항상 평행하다.\n\n\n\n\nchapter 3. 함수란?\n\n1. 함수 정의\n함수는 두 집합 사이의 특정 규칙에 따라 값을 대응시키는 관계를 나타낸다. 함수는 정의역과 치역으로 구성되며, 정의역의 각 원소에 대해 치역의 단 하나의 원소만 대응된다. 이를 통해 y가 x에 의해 결정된다고 표현하며, 수학적으로 다음과 같이 나타낸다. \\(y = f(x)\\)\n이는 ”y는 x의 함수이다”라고 읽는다.\n\n정의역 domain\n정의역은 함수에서 x가 가질 수 있는 값들의 집합을 말한다. 즉, 함수 f(x)가 유효하게 정의될 수 있는 모든 입력값의 집합이다.\n\n\n치역 range\n치역은 함수가 출력할 수 있는 값들의 집합이다. 정의역의 원소 x가 함수 f를 통해 출력되는 값 y = f(x)의 모임이 치역이다.\n\n\n대응 규칙\n함수는 정의역의 각 원소를 치역의 한 원소에 대응시키는 규칙을 가지고 있다. 각 정의역의 값 x는 치역에서 정확히 하나의 값 y에 대응해야 한다. (2)번은 동일 x-값에 대하여 2개 y-값이 대응되므로 함수가 아니고 다른 모든 것은 함수이다.\n\n\n\n\n\n\n\n2. 우함수와 기함수\n우함수 even function\n  함수 \\(f(x)\\)가 다음 조건을 만족하면 우함수라 한다:\n\\[f( - x) = f(x)\\text{모든}x \\in \\text{정의역(domain)}\\]\n우함수는 y-축을 기준으로 대칭적이다. 즉, 그래프의 왼쪽 부분을 y-축을 따라 접으면 오른쪽 부분과 정확히 일치한다.\n기함수 odd function\n  함수 \\(f(x)\\)가 다음 조건을 만족하면 기함수라 정의한다:\n\\[f( - x) = - f(x)\\text{모든}x \\in \\text{정의역(domain)}\\]\n기함수는 원점을 기준으로 대칭적이다. 즉, 그래프를 원점을 중심으로 180° 회전시키면 동일한 모양이 된다.\n\n\n3. 함수 종류\n\n\n(1) 함성함수 Composite Function\n합성함수는 두 함수 f(x)와 g(x)가 주어졌을 때, 함수 g(x)의 출력값이 함수 f(x)의 입력값으로 사용되는 새로운 함수이다. 이를 다음과 같이 나타낸다. \\((f \\circ g)(x) = f(g(x))\\)\n    - g(x): 먼저 적용되는 함수.\n    - f(x): g(x)의 출력값을 입력값으로 사용하는 함수.\n    - \\((f \\circ g)(x)\\): f(x)와 g(x)의 합성함수.\n합성함수 \\((f \\circ g)(x)\\)의 정의역은 g(x)와 f(x)가 동시에 유효하게 정의되는 입력값으로 구성된다. 즉, x는 g(x)의 정의역에 속하고, g(x)의 출력값은 f(x)의 정의역에 속해야 한다.\n\\((f \\circ g)(x)\\)는 다음 두 단계를 거친다:\n    - 먼저 x에 대해 g(x)를 계산하고 그런 다음, f(x)에 g(x)를 대입하여 f(g(x))를 계산한다.\n\\[f(x) = 2x + 1,g(x) = x^{2}\\]\n\\[f(g(x)) = f(x^{2}) = 2x^{2} + 1\\]\n\\[g(f(x)) = g(2x + 1) = (2x + 1)^{2}\\]\n\n\n(2) 절대값 함수\n숫자 x의 절대값(absolute value)은 x의 크기(거리를 나타냄)를 의미하며, 항상 0 이상의 값을 가진다. 절대값은 다음과 같이 정의된다.\n\\[|x| = \\{\\begin{matrix}\nx, & \\text{if}x \\geq 0 \\\\\n- x, & \\text{if}x &lt; 0\n\\end{matrix}\\]\n절대값은 숫자 x와 0 사이의 거리로 해석된다. 절대값의 결과는 항상 양수이거나 0이다.\n\n\n(3) 정수함수 integer function\n정수 함수는 숫자 x를 넘지 않는 최대 정수를 반환하는 함수이다. 이를 바닥함수 floor function라고도 하며, 다음과 같이 정의된다.\n\\[\\lfloor x\\rfloor = \\text{최대 정수}n\\text{such that}n \\leq x\\]\n    - \\(\\lfloor x\\rfloor\\): x를 넘지 않는 가장 큰 정수.\n    - \\(\\lfloor x\\rfloor\\)는 항상 \\(n \\leq x &lt; n + 1\\)을 만족한다.\n\n\n4. 함수의 사칙연산\n두 함수 f(x)와 g(x)가 주어졌을 때, 이들 함수에 대해 덧셈, 뺄셈, 곱셈, 나눗셈과 같은 사칙연산을 정의할 수 있다. 각 연산은 정의역에서 두 함수의 값에 기반하여 계산된다.\n\n\n함수의 덧셈/뺄셈\n\\[(f \\pm g)(x) = f(x) \\pm g(x)\\]\n정의역: f(x)와 g(x)가 동시에 정의된 구간.\n결과: f(x)의 값과 g(x)의 값을 더한(뺀) 결과.\n\n\n함수의 곱셈\n\\[(f \\cdot g)(x) = f(x) \\cdot g(x)\\]\n\n정의역: f(x)와 g(x)가 동시에 정의된 구간.\n결과: f(x)와 g(x)의 값을 곱한 결과.\n\n\n\n함수의 나눗셈\n\\[\\left( \\frac{f}{g} \\right)(x) = \\frac{f(x)}{g(x)},g(x) \\neq 0\\]\n\n정의역: f(x)와 g(x)가 동시에 정의되고, \\(g(x) \\neq 0\\)인 구간.\n결과: f(x)의 값을 g(x)의 값으로 나눈 결과. \n\n\n\n\nchapter 4. 함수의 응용 및 극한\n\n1. 함수의 통계 응용\n\n\n(1) 확률밀도함수 \\(f(x)\\)\n연속형확률변수의 분포를 나타내는 함수로, 특정 구간 내에서 값이 나타날 확률의 상대적인 가능성을 표현한다.\n    - 확률밀도함수 정의: \\(f(x) \\geq 0,\\int_{- \\infty}^{\\infty}f(x)dx = 1\\)\n    - 정규분포의 확률밀도함수: \\(f(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^{2}}}e^{- \\frac{(x - \\mu)^{2}}{2\\sigma^{2}}}\\)\n    - 데이터의 분포, 확률 계산 \\(P(a \\leq X \\leq b) = \\int_{a}^{b}f(x)dx\\)\n\n\n(2) 누적확률밀도함수\n확률변수가 특정 값 이하일 확률을 나타내는 함수이다.\n\n누적확률밀도함수 정의: \\(F(x) = P(X \\leq x) = \\int_{- \\infty}^{x}f(t)dt\\)\n정규분포 CDF: \\(F(x) = \\frac{1}{2}\\left\\lbrack 1 + \\text{erf}\\left( \\frac{x - \\mu}{\\sqrt{2}\\sigma} \\right) \\right\\rbrack\\)\n분위수 결정: \\(P(X \\leq x_{p}) = p\\) 만족하는 \\(X_{p}\\)를 찾음.\n\n\n\n(3) 회귀모형\n함수는 독립변수와 종속변수 간의 관계를 모델링하는 데 사용된다. 회귀모형은 함수 형태로 데이터의 추세를 설명한다.\n    - 선형회귀: \\(y = \\beta_{0} + \\beta_{1}x + \\epsilon\\)\n    - 비선형 회귀: \\(y = ae^{bx} + \\epsilon\\)\n    - 변수 간 관계 분석, 예측 모델 구축\n\n\n(4) 생존분석\n생존분석에서는 생존시간 분포를 분석하는 데 함수가 사용된다.\n\n생존survival 함수: \\(S(t) = P(T &gt; t) = 1 - F(t)\\)\n위험hazard 함수: \\(h(t) = \\frac{f(t)}{S(t)}\\)\n제품 수명 분석, 의료 데이터에서 생존 확률 평가\n\n\n\n(5) 시계열분석\n함수는 시간에 따른 데이터의 변화를 모델링하고 분석하는 데 사용된다.\n\n자기회귀 모델: \\(X_{t} = \\phi_{1}X_{t - 1} + \\phi_{2}X_{t - 2} + \\cdots + \\epsilon_{t}\\)\n주식 시장 예측, 온도 변화 모델링.\n\n\n\n(6) 함수와 몬테카를로 시뮬레이션\n함수는 확률 분포로부터 난수를 생성하여 복잡한 통계 문제를 해결하는 데 사용된다.\n\n\\(\\pi\\) 값 추정: \\(f(x) = \\sqrt{1 - x^{2}},\\text{for}x \\in \\lbrack 0,1\\rbrack\\)\n\n\n\n2. 함수의 극한\n\n\n(1) 극한 정의\n임의의 \\(\\varepsilon &gt; 0\\)가 주어졌을 때, 모든 \\(x\\)가 특정 값 \\(a\\)에 충분히 가까워질 때 \\((0 &lt; |x - a| &lt; \\delta),f(x)\\)가 특정 값 \\(L\\)에 가까워진다면, 함수 \\(f(x)\\)의 극한은 존재하며 그 극한값은 \\(L\\)이라고 정의한다. 이를 수학적으로 표현하면 \\(\\lim_{x \\rightarrow a}f(x) = L\\) 이다.\n\n\\(\\varepsilon\\): \\(f(x)\\)와 \\(L\\)사이의 허용 오차.\n\\(\\delta\\): \\(x\\)와 \\(a\\) 사이의 거리 제한.\n\n엄밀한 정의 (\\(\\varepsilon - \\delta\\)정의)\\(\\forall\\varepsilon &gt; 0,\\exists\\delta &gt; 0\\text{such that}0 &lt; |x - a| &lt; \\delta \\Longrightarrow |f(x) - L| &lt; \\varepsilon\\)이 의미는 \\(x\\)와 \\(a\\)에 충분히 가까워지면 \\((|x - a| &lt; \\delta)\\) 함수 \\(f(x)\\)의 값이 \\(L\\)에 충분히 가까워짐 \\((|f(x) - L| &lt; \\varepsilon)\\)을 보장한다.\n\n\n(2) 함수값과 극한값\n함수값 \\(f(a)\\)는 함수가 특정 점 \\(x = a\\)에서 실제로 가지는 값이다. 반면, 극한값 \\(\\lim_{x \\rightarrow a}f(x)\\)는 \\(x\\)가 \\(a\\)에 가까워질 때 \\(f(x)\\)가 수렴하는 값을 나타낸다. 함수값과 극한값은 다를 수 있으며, 함수가 \\(x = a\\)에서 정의되지 않아도 극한값은 존재할 수 있다.\n함수값 \\(f(a) = k\\)\n  함수가 \\(x = a\\)에서 정의되어 있다면 \\(f(a)\\)는 \\(k\\), 특정 값을 가진다.\n극한값\n  극한값은 좌극한(left-hand limit)과 우극한(right-hand limit)에 따라 달라질 수 있다:\n    - 좌극한 \\(L_{2}\\): \\(\\lim_{x \\rightarrow a^{-}}f(x) = L_{2}\\)\n    - 우극한 \\(L_{1}\\): \\(\\lim_{x \\rightarrow a^{+}}f(x) = L_{1}\\)\n    - 전체 극한은 좌극한과 우극한이 동일할 때 존재한다,\n\n\n(3) 연속함수 정의\n함수 \\(f(x)\\)가 \\(x = a\\)에서 연속하려면 다음 세 가지 조건을 모두 만족해야 한다:\n  1. \\(f(a)\\)가 정의되어 있어야 한다.\n  2. \\(\\lim_{x \\rightarrow a}f(x)\\)가 존재해야 한다.\n  3. 함수값과 극한값이 일치해야 한다. \\(\\lim_{x \\rightarrow a}f(x) = f(a)\\)\n\n\n(4) 극한 계산 규칙\n상수함수의 극한\n  \\(\\lim_{x \\rightarrow a}c = c\\), 상수 함수의 극한은 상수 자신이다.\n항등함수의 극한\n  \\[\\lim_{x \\rightarrow a}x = a\\]\n선형성\n  극한 연산은 선형성을 가진다:\n\\[\\lim_{x \\rightarrow a}\\lbrack f(x) \\pm g(x)\\rbrack = \\lim_{x \\rightarrow a}f(x) \\pm \\lim_{x \\rightarrow a}g(x)\\]\n곱셈\n  두 함수의 곱의 극한은 각 함수의 극한의 곱과 같다.\n\\[\\lim_{x \\rightarrow a}\\lbrack f(x) \\cdot g(x)\\rbrack = \\left( \\lim_{x \\rightarrow a}f(x) \\right) \\cdot \\left( \\lim_{x \\rightarrow a}g(x) \\right)\\]\n나눗셈\n  두 함수의 나눗셈의 극한은 각 함수의 극한의 나눗셈과 같다 (분모가 0이 아닌 경우)\n\\[\\lim_{x \\rightarrow a}\\frac{f(x)}{g(x)} = \\frac{\\lim_{x \\rightarrow a}f(x)}{\\lim_{x \\rightarrow a}g(x)},\\lim_{x \\rightarrow a}g(x) \\neq 0\\]\n거듭제곱\n  \\(\\lim_{x \\rightarrow a}\\lbrack f(x)\\rbrack^{n} = \\left( \\lim_{x \\rightarrow a}f(x) \\right)^{n}\\), 여기서 \\(n\\)은 정수이다.\n루트\n  \\[\\lim_{x \\rightarrow a}\\sqrt[n]{f(x)} = \\sqrt[n]{\\lim_{x \\rightarrow a}f(x)},\\text{if}\\lim_{x \\rightarrow a}f(x) \\geq 0\\]\n합성함수의 극한 (연쇄법칙)\n  만약 \\(g(x)\\)의 극한이 \\(a\\)로 접근할 때 \\(b\\)이고, \\(f(x)\\)가 \\(b\\)에서 연속이면\n\\(\\lim_{x \\rightarrow a}f(g(x)) = f\\left( \\lim_{x \\rightarrow a}g(x) \\right)\\) 이다.\nL’Hôpital’s Rule의 정의\n  함수 f(x)와 g(x)가 x \\to a에서 각각 0/0 형태 또는 \\infty/\\infty 형태를 가지는 경우, 두 함수의 극한은 다음과 같이 계산할 수 있다:\n\\[\\lim_{x \\rightarrow a}\\frac{f(x)}{g(x)} = \\lim_{x \\rightarrow a}\\frac{f'(x)}{g'(x)},\\text{if}\\lim_{x \\rightarrow a}\\frac{f'(x)}{g'(x)}\\text{exists.}\\]\n\n형태: \\(\\frac{0}{0}\\) 또는 \\(\\frac{\\infty}{\\infty}\\)와 같은 불정형 형태를 가져야 한다.\n미분 가능성: \\(f(x)\\)와 \\(g(x)\\)는 \\(x \\rightarrow a\\)에서 미분 가능해야 한다.\n분모의 도함수가 0이 아님: \\(g'(x) \\neq 0\\)인 구간에서 적용 가능\n\\(\\frac{0}{0}\\) 형태: \\(\\lim_{x \\rightarrow 0}\\frac{\\sin(x)}{x} = \\lim_{x \\rightarrow 0}\\frac{\\cos(x)}{1} = \\cos(0) = 1\\)\n\\(\\frac{\\infty}{\\infty}\\) 형태: \\(\\lim_{x \\rightarrow \\infty}\\frac{x}{e^{x}} = \\lim_{x \\rightarrow \\infty}\\frac{1}{e^{x}} = 0\\)\n\n무한대 있는 극한\n  \\(x\\)가 무한대 \\(\\infty\\)혹은 \\(- \\infty\\)로 접근할 때 함수 \\(f(x)\\)의 극한을 구하는 규칙이다.\n\\[lim_{x \\rightarrow \\pm \\infty}\\frac{1}{x} = 0\\]\n\\(lim_{x \\rightarrow \\pm \\infty}c = c\\), \\(c\\)는 상수\n함수가 분수의 형태를 가지면 분모의 가장 큰 \\(x\\)차수로 나누고 위의 규칙을 이용하라.\n특정 함수의 극한\n  - 지수 함수: \\(\\lim_{x \\rightarrow \\infty}e^{- x} = 0\\)\n  - 삼각 함수: \\(\\lim_{x \\rightarrow 0}\\frac{\\sin(x)}{x} = 1\\), \\(\\lim_{x \\rightarrow 0}\\frac{1 - \\cos(x)}{x^{2}} = \\frac{1}{2}\\)\n  - 로그 함수: \\(\\lim_{x \\rightarrow \\infty}\\ln(x) = \\infty\\)\n\n\n3. 수렴 convergence\n수렴의 정의\n  함수 \\(f(x)\\) 또는 수열 \\(\\{ a_{n}\\}\\)가 특정 값에 수렴한다는 것은 극한값이 존재하며, 일정 값에 점점 가까워진다는 것을 의미한다.\n수열의 수렴\n  수열 \\(\\{ a_{n}\\}\\)이 \\(L\\)로 수렴한다면, 임의의 \\(\\varepsilon &gt; 0\\)에 대해 \\(n \\geq N\\)일 때 다음 조건을 만족하는 \\(N\\)이 존재한다.\n\\(|a_{n} - L| &lt; \\varepsilon\\), 여기서 \\(L\\)은 수열의 극한값이다.\n함수의 수렴\n  함수 \\(f(x)\\)가 \\(L\\)로 수렴하면, \\(x \\rightarrow a\\)에서 \\(\\lim_{x \\rightarrow a}f(x) = L\\)\n수렴의 성질\n  수열이나 함수가 수렴하면 극한값은 유일하다.\n  수렴하는 함수나 수열은 경계값을 가지며, 점점 극한값에 가까워진다.\n극한과 수렴의 차이\n  극한은 특정 값에 접근하는 경향을 나타내며, 함수나 수열이 특정 점에서 어떻게 동작 하는지 설명한다.\n  수렴은 극한값이 존재하고 일정 값에 점점 가까워지는 성질을 나타낸다.\n\n\n4. 확률수렴과 분포수렴\n\n\n(1) 확률수렴 (Convergence in Probability)\n확률변수의 열 \\(\\{ X_{n}\\}\\)이 확률변수 \\(X\\)에 확률수렴한다는 것은, 임의의 \\(\\varepsilon &gt; 0\\)에 대해 다음 조건을 만족하는 \\(n \\rightarrow \\infty\\)가 존재함을 의미한다. \\(\\lim_{n \\rightarrow \\infty}P(|X_{n} - X| \\geq \\varepsilon) = 0\\)\n  - 표기: \\(X_{n}\\overset{P}{\\rightarrow}X\\)\n해석\n  확률적으로 \\(|X_{n} - X|\\)가 작아질 가능성이 1에 가까워짐을 나타낸다. 즉, \\(X_{n}\\)과 \\(X\\)가 점점 ”가까워진다”고 해석할 수 있다.\n성질\n  확률수렴의 유일성: 극한값 \\(X\\)는 유일하다.\n  확률수렴과 함수: \\(X_{n}\\overset{P}{\\rightarrow}X\\)이고 \\(g(x)\\)가 연속 함수라면 \\(g(X_{n})\\overset{P}{\\rightarrow}g(X)\\)\n통계학 응용\n  추정량의 일치성: 추정량 \\({\\widehat{\\theta}}_{n}\\)이 모수 \\(\\theta\\)에 확률수렴하면 \\({\\widehat{\\theta}}_{n}\\)은 일치추정량이다. 법칙의 수렴: 큰 수의 약법칙은 확률수렴으로 표현된다:\n\\[{\\overline{X}}_{n}\\overset{P}{\\rightarrow}\\mu\\]\n\n\n(2) 분포수렴 (Convergence in Distribution)\n확률변수의 수열 \\(\\{ X_{n}\\}\\)이 확률변수 \\(X\\)에 분포수렴한다는 것은, 모든 연속점 \\(x\\)에서 누적분포함수(FDF) \\(F_{X_{n}}(x)\\)가 \\(F(x)\\)로 수렴함을 의미한다. \\(\\lim_{n \\rightarrow \\infty}F_{X_{n}}(x) = F_{X}(x),\\forall x\\) \\(F_{X}(x)\\)에서 연속함수.\n  표기: \\(X_{n}\\overset{\\mathcal{D}}{\\rightarrow}X\\)\n해석\n  분포수렴은 \\(X_{n}\\)의 분포가 \\(X\\)의 분포로 점점 가까워지는 것을 의미한다. 개별적인 실현값이 아니라 분포 전체의 형태를 고려한다.\n성질\n  (1) 연속성: 분포수렴은 누적분포함수의 연속점에서 정의된다.\n  (2) 함수와 분포수렴: \\(X_{n}\\overset{\\mathcal{D}}{\\rightarrow}X\\)이고 \\(g(x)\\)가 연속 함수라면\n\\(g(X_{n})\\overset{\\mathcal{D}}{\\rightarrow}g(X)\\) 이다.\n응용\n  중심 극한 정리: 표본 평균이 정규분포로 수렴하는 현상은 분포수렴으로 나타낸다. \\(\\sqrt{n}({\\overline{X}}_{n} - \\mu)\\overset{\\mathcal{D}}{\\rightarrow}N(0,\\sigma^{2})\\)\n\n\n(3) 확률수렴과 분포수렴의 관계\n확률수렴 → 분포수렴\n\\[X_{n}\\overset{P}{\\rightarrow}X \\Longrightarrow X_{n}\\overset{\\mathcal{D}}{\\rightarrow}X\\]\n분포수렴 ≠ 확률수렴\n  분포수렴이 확률수렴을 보장하지 않는다. 예를 들어, \\(X_{n} \\sim U( - n,n)\\)은 \\(X = 0\\)에 분포수렴하나 확률수렴하지 않는다."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "세상의 모든 통계 이야기",
    "section": "",
    "text": "🎓 Welcome to Prof. Kwon’s 통계이야기\n\n\n\n\n\n1995년부터 한남대학교에서 제공해온 통계학 강의노트를 데이터 사이언스 중심으로 새롭게 구성했습니다. since 1999.03(첫 웹이지 구축) / 2023.01 (1차 수정)\n\n| put a ding in the universe |"
  },
  {
    "objectID": "notes/math/vector.html",
    "href": "notes/math/vector.html",
    "title": "수학의 기초 1. 함수",
    "section": "",
    "text": "chapter 1. 선형대수 개념\n\n1. 선형대수 정의\n선형대수(Linear Algebra)는 벡터 공간과 벡터 사이의 관계를 탐구하며, 벡터와 행렬을 이용한 수학적 표현과 계산을 다루는 수학의 한 분야이다. 주요 구성 요소에는 벡터, 행렬, 스칼라가 있으며, 주요 개념으로는 선형 변환, 고유값과 고유벡터, 내적(inner product), 외적(cross product), 행렬 분해 등이 있다.\n통계학에서 선형대수는 데이터를 벡터와 행렬로 표현함으로써 복잡한 수치 계산을 단순화하고 효율적으로 수행할 수 있게 해준다. 특히 고차원 데이터의 계산과 변환을 가능하게 하여, 데이터 구조에 대한 이해와 차원 축소에 핵심적인 역할을 한다. 또한 선형대수는 머신러닝과 통계 모델링의 기초를 이루며, 회귀 분석, 주성분 분석(PCA), 군집 분석 등 다양한 기법에서 필수적으로 활용된다.\n\n\n2. 선형대수와 선형변환\n선형대수는 선형적인 관계를 다루는 수학 분야로, 이론 내에서 이루어지는 모든 연산과 변환은 선형성을 만족해야 한다. 이에 따라 일반적인 함수는 이론의 중심 개념으로 다루어지지 않지만, 함수의 특수한 형태인 선형 변환(linear transformation)은 주요 개념으로 간주된다. 선형 변환은 벡터 공간의 구조를 보존하며, 행렬을 통해 구체적으로 표현될 수 있기 때문에 선형대수에서 핵심적인 역할을 수행한다.\n\n\n함수 \\(y = f(x)\\)\n\n함수는 두 집합 사이의 관계로, 각 입력값(정의역, domain)에 대해 정확히 하나의 출력값(공역, range)을 대응시키는 규칙이다.\n함수는 일반적으로 \\(f:D \\rightarrow R\\)와 같이 표기되며, \\(D\\)는 정의역, \\(R\\)는 공역입니다.\n특정함수에 대하여 함수값이 0인 \\(f(x) = 0\\)를 방정식이라 하고 이를 만족하는 \\(x\\)를 방정식의 해(root, solution)라고 한다.\n\n\n\n\n\n\n\n\n선형함수\n선형함수는 입력 변수와 출력 변수 사이의 관계를 직선으로 나타내는 함수로, 일반적으로 다음과 같은 형태로 표현된다: \\(f(x) = a + bx\\), \\(a:\\) 절편, \\(b:\\) 기울기\n\n가법성 additivity: \\(f(x + y) = f(x) + f(y)\\)\n동차성 homogeniety: \\(f(cx) = cf(x)\\), \\(c\\)는 상수\n\n\n\n선형변환\n선형 변환(linear transformation)은 벡터 공간에서 정의된 함수 중 하나로, 한 벡터를 동일하거나 다른 벡터 공간의 또 다른 벡터로 변환하는 함수의 특수한 형태이다. 이 변환은 선형성(linearity)이라 불리는 다음의 두 가지 성질을 만족해야 한다. \\(\\underset{¯}{u},\\underset{¯}{v}\\) 동일 차원의 벡터에 대하여 함수 \\(T\\)가 다음 조건을 만족하면 선형변환이다.\n\n덧셈에 대한 선형성: \\(T(\\underset{¯}{u} + \\underset{¯}{v}) = T(\\underset{¯}{u}) + T(\\underset{¯}{v})\\)\n스칼라 곱에 대한 선형성: \\(T(c\\underset{¯}{u}) = cT(\\underset{¯}{u})\\)\n\n\n\n\n\n\n\n\n\nchapter 2. 벡터 vector 기초\n\n1. 벡터정의\n벡터는 정렬된 유한한 수들의 목록으로, 일반적으로 정사각형 괄호 또는 곡선 괄호로 둘러싸인 수직 형태의 배열로 표현된다. 이러한 형태는 수평 배열인 행벡터(row vector)와 구별하여 열벡터(column vector)라고 부른다.\n\\(\\left( \\begin{array}{r}\n1 \\\\\n- 2 \\\\\n0\n\\end{array} \\right)\\), \\(\\left\\lbrack \\begin{array}{r}\n1 \\\\\n- 2 \\\\\n0\n\\end{array} \\right\\rbrack\\)벡터를 행으로 사용할 때는 쉼표로 구분되고 괄호로 둘러싸인 숫자로 쓴다. \\(\\left( \\begin{array}{r}\n1, - 2,0\n\\end{array} \\right)\\)\n배열의 값을 벡터의 원소 element 라 하고 원소의 개수를 벡터의 크기(차원 demension)라고 한다. 위 벡터는 크기가 3 이고 세 번째 원소는 0 이다. n 크기의 벡터는 n-벡터라고 불리고 1벡터는 숫자와 같은 것으로 간주한다. 즉, 우리는 1-벡터 [ 13 ]와 숫자 13을 구별하지 않으며 숫자는 스칼라 scalar 라 한다. 벡터의 각 원소는 스칼라이고 원소가 실수인 \\(a_{i} \\in R^{n}\\) 벡터를 실수 벡터라 한다.\n\n\n2. 벡터 기호\nn-벡터를 나타내기 위해 \\({\\underset{¯}{a}}_{n}\\)(구별이 가능한 경우 알파벳 \\(a\\)를 벡터로 표현) 기호를 사용한다. \\(a_{n}\\)벡터 의 i-번째 요소는 \\(a_{i}\\)로 표시되며, 여기서 첨자 i는 벡터의 크기인 1에서 \\(n\\)까지 정의되는 정수 인덱스이다.\n두 벡터 \\(a_{n},b_{n}\\)가 동일하다는 것은 (1)크기(차수)도 \\(n\\) 동일하고 (2) 각 대응 원소가 동일 \\(a_{i} = b_{i}\\)함을 의미한다.\n\n\n3. 특수한 벡터\n\n\n(1) 영벡터 zero vector\n모든 원소가 0인 벡터이며 \\(0_{n}\\)으로 표현된다. 일반적으로 모든 0 벡터는 0으로 표시되며, 숫자 0을 나타내는데 사용되는 것과 동일한 기호이다. 다른 크기의 제로 벡터를 나타내기 위해 모두 같은 기호 0을 사용하므로 기호 0은 문맥에 따라 다른 것을 의미할 수 있기 때문에 컴퓨터에서는 이를 과부하라 한다.\n\n\n(2) 단위벡터 unit vector\n(표준) 단위 벡터는 1인 하나의 원소를 제외한 모든 요소가 0과 같은 벡터이다. i-번째 단위 벡터(n 크기)는 i-번째 원소만 1을 가진 단위 벡터이며, \\(e_{i}\\)로 표현한다. 이렇게 되면 크기를 나타내는 첨자와 1인 원소 위치를 나타내는 첨자가 구별이 되지 않는 모호성을 갖는다.\n\n\n(3) 일벡터 ones vector\n모든 원소가 1인 n-벡터이며 \\(1_{n}\\)로 표현한다. 우리는 또한 벡터의 크기가 문맥에서 결정될 수 있다면 1로 쓴다.\n\n\n4. 벡터 개념\n\n\n(1) 위치 location\n2차원 공간, 즉 평면의 위치를 나타내는 데 사용될 수 있다. 3-벡터는 3차원(3-D) 공간에서 어떤 지점의 위치나 위치를 나타내는 데 사용된다. 벡터의 원소는 위치의 좌표를 제공한다.\n\n\n\n\n\n벡터는 주어진 시간에 평면이나 3차원 공간에서 움직이는 지점의 속도나 가속도를 나타내는 데 사용될 수 있다.\n\n\n\n\n\n\n\n(2) 희소성\n많은 원소가 0이면 희소하다고 한다. 그것의 희소성 패턴은 0이 아닌 항목의 인덱스 집합이다. \\(n\\)-벡터 \\(a_{n}\\)의 0이 아닌 항목의 수는 \\(nnz(a_{n})\\)로 표시한다다. 단위벡터는 0이 아닌 항목이 하나만 있기 있고 0 벡터는 0이 아닌 항목이 없기 때문에 희소한 벡터이다.\n\n\n(3) 이미지\n3차원 벡터는 빨간색, 녹색 및 파란색(R-G-B) 강도 값(0에서 1 사이)을 제공하는 항목을 통해 색상을 나타낸다. 벡터(0,0,0)는 검은색을 나타내고, 벡터(0, 1, 0)는 밝은 순수한 녹색을 나타내며, 벡터(1, 0.5, 0.5)는 분홍색을 나타낸다.\n\n\n\nchapter 3. 벡터 연산과 크기\n\n1. 벡터 연산\n\n\n(1) 벡터 합\n두 벡터를 합을 구한다는 것은 (1) 차수가 동일한 두 벡터의 (2) 동일 위치의 원소를 합하여 하나의 벡터를 계산한다는 것을 의미한다. 차도 동일하다.\n\\(\\left\\lbrack \\begin{array}{r}\n1 \\\\\n- 2 \\\\\n0\n\\end{array} \\right\\rbrack + \\left\\lbrack \\begin{array}{r}\n1 \\\\\n2 \\\\\n3\n\\end{array} \\right\\rbrack = \\left\\lbrack \\begin{array}{r}\n2 \\\\\n0 \\\\\n3\n\\end{array} \\right\\rbrack\\), \\(\\left\\lbrack \\begin{array}{r}\n1 \\\\\n- 2 \\\\\n0\n\\end{array} \\right\\rbrack - \\left\\lbrack \\begin{array}{r}\n1 \\\\\n2 \\\\\n3\n\\end{array} \\right\\rbrack = \\left\\lbrack \\begin{array}{r}\n0 \\\\\n- 4 \\\\\n- 3\n\\end{array} \\right\\rbrack\\)\n\n\n성질\n차수가 동일한 벡터 \\(a,b,c\\)에 대하여 다음이 성립한다.\n\n교환법칙 : \\(a + b = b + a\\)\n교환법칙 : \\((a + b) + c = a + (b + c)\\)\n영벡터를 더하거나 빼도 영향을 받지 않는다. \\(a \\pm 0 = a\\)\n벡터에서 자체 벡터를 빼면 영벡터가 된다. \\(a - a = 0\\)\n\n\n\n(2) 스칼라-벡터 곱\n벡터에 스칼라(즉, 숫자)를 곱하는 스칼라-벡터 곱셈은 벡터의 모든 요소에 스칼라를 곱하여 수행한다. 일반적으로 스칼라를 왼쪽, 벡터를 오른쪽에 적지만 순서를 바꾸어 사용해도 되고 계산 결과는 동일하다.\n\\(a = \\left\\lbrack \\begin{array}{r}\n1 \\\\\n- 2 \\\\\n0\n\\end{array} \\right\\rbrack\\)이면 \\(3a = a3 = \\left\\lbrack \\begin{array}{r}\n3 \\\\\n- 6 \\\\\n0\n\\end{array} \\right\\rbrack\\)\n\n\n성질\n벡터 \\(a\\), 스칼라 \\(c,k\\)에 대하여 다음이 성립한다.\n\n교환법칙 : \\(ka = ak\\)\n배분법칙 : \\((c + k)a = ca + ka\\)\n\n\n\n(3) 선형 결합 linear combination\n차수 \\(n\\)-벡터 \\(a_{1},a_{2},...,a_{m}\\), 스칼라 \\(k_{1},k_{2},...,k_{m}\\)에 대하여 다음 \\(n\\)-벡터를 벡터 \\(a_{1},a_{2},...,a_{m}\\)의 선형결합이라 하고 스칼라 \\(k_{1},k_{2},...,k_{m}\\)는 선형결합의 계수라 한다.\n\\[k_{1}a_{1} + k_{2}a_{2} + ... + k_{m}a_{m}\\]\n\n\\(k_{1} = k_{2} = ... = k_{m} = 1\\)이면, 선형결합은 벡터 합이다.\n\\(k_{1} = k_{2} = ... = k_{m} = \\frac{1}{m}\\)이면, 선형결합은 벡터 평균이다.\n\\(k_{1} + k_{2} + ... + k_{m} = 1\\)이면, 선형결합은 affine 결합이라 하고 모든 계수가 양수인 경우 선형결합을 가중평균이라 한다.\n\n\n\n(4) 내적 inner product\n두 벡터 간의 관계를 정의하고 벡터의 길이와 각도 등의 개념을 도입하는 중요한 연산이다. 차수(\\(m\\))가 동일한 두 벡터 (\\(u,v\\))의 내적 곱은 다음과 같이 정의하고 결과는 스칼라이다.\n\\[u^{T}v = \\lbrack u_{1},u_{2},...,u_{m}\\rbrack\\left\\lbrack \\begin{array}{r}\nv_{1} \\\\\nv_{2} \\\\\n... \\\\\nv_{m}\n\\end{array} \\right\\rbrack = u_{1}v_{1} + u_{2}v_{2} + ... + u_{m}v_{m} = \\overset{m}{\\sum_{i = 1}}u_{i}v_{i}\\]\n단, \\(u^{T}\\)는 \\(u\\)의 전치 transpose라 하고 열벡터를 행벡터로 변환한 것이다.\n【예제】 \\[\\lbrack 1,3,5\\rbrack^{T}\\left\\lbrack \\begin{array}{r}\n  0 \\\\\n   - 1 \\\\\n  1\n  \\end{array} \\right\\rbrack = (1)(0) + (3)( - 1) + (5)(1) = 2\\]\n\n\n내적 성질\n\nunit 벡터 : \\(e_{i}v = v_{i}\\)\n벡터 합 : \\(1_{m}^{T}v = \\overset{m}{\\sum_{i = 1}}v_{i}\\)\n벡터 평균 : \\(avg(v) = (1/n)1_{m}^{T}v = (1/n)\\overset{m}{\\sum_{i = 1}}v_{i}\\)\n벡터 제곱합 : \\(v^{T}v = v_{1}^{2} + v_{2}^{2} + ... + v_{m}^{2} = \\overset{m}{\\sum_{i = 1}}v_{i}^{2}\\)\n\n\n\nCauchy–Schwarz inequality\n차수 동일한 두 벡터의 내적 inner product에 대하여 다음이 성립한다.\n\\[\\parallel a^{T}b \\parallel \\leq \\parallel a \\parallel \\parallel b \\parallel\\]\n\\[|\\overset{n}{\\sum_{i}}a_{i}b_{i}| \\leq (\\sum a_{i}^{2})^{\\frac{1}{2}}(\\sum b_{i}^{2})^{\\frac{1}{2}}\\]\n\n\n(5) 외적 cross product\n주로 3차원 공간에서 두 벡터로부터 새로운 벡터를 생성하는 연산입니다. 이 연산의 결과는 두 벡터에 모두 수직인 벡터이며, 크기는 두 벡터가 이루는 평행사변형의 면적에 해당합니다.\n\n\n외적 정의\n벡터 \\(\\underset{¯}{a} = (a_{1},a_{2},a_{3})\\)와 벡터 \\(\\underset{¯}{b} = (b_{1},b_{2},b_{3})\\)의 외적 \\(\\underset{¯}{a} \\times \\underset{¯}{b}\\)는 다음과 같이 계산한다.\n\n\\(x\\) 성분: \\(a_{2}b_{3} - a_{3}b_{2}\\)\n\\(y\\) 성분: \\(a_{3}b_{1} - a_{1}b_{3}\\)\n\\(z\\) 성분: \\(a_{1}b_{2} - a_{2}b_{1}\\)\n\n\n\n\n\n\n【예】 벡터 \\(\\underset{¯}{a} = (2,3,4)\\)와 벡터 \\(\\underset{¯}{b} = (5,6,7)\\)의 외적은 \\(\\underset{¯}{c} = \\underset{¯}{a} \\times \\underset{¯}{b} = ( - 3,6, - 3)\\) 이다.\n외적은 벡터 \\(\\underset{¯}{a},\\underset{¯}{b}\\)와 수직(\\({\\underset{¯}{c}}^{T}\\underset{¯}{a} = 0\\), \\({\\underset{¯}{c}}^{T}\\underset{¯}{b} = 0\\))이며 외적의 크기(놈 norm)는 두 벡터가 이루는 평행사면형 면적이다.\n\n\n2. 선형함수\n\n\n선형함수 정의\n\\(f:R^{n} \\rightarrow R\\)는 크기 n-벡터를 실수(스칼라)로 매핑하는 함수이다. 함수 \\(f(x)\\)의 \\(x_{1},x_{2},...,x_{n}\\)은 함수 \\(f\\)의 인수 argument라 하고 결과 값 스칼라는 함수 값이다. \\(f(x) = f(x_{1},x_{2},...,x_{n})\\)\n【예제】\n\\[f:R^{4} \\rightarrow R$ : $f(x) = x_{1} - x_{2} + x_{4}^{2}\\]\n차수 n-벡터 \\(a,x\\)에 대하여 내적 함수 \\(f(x) = a^{T}x = scalar\\)는 선형함수일 때 다음이 성립한다. 단, \\(\\alpha,\\beta\\)는 스칼라, \\((x,y)\\)는 n-벡터이다. \\(f(\\alpha x + \\beta y) = \\alpha f(x) + \\beta f(y)\\)\n\n\n선형함수 조건\n다음 조건을 만족하는 \\(f:R^{n} \\rightarrow R\\) 는 선형함수이다. 단, \\(\\alpha\\)는 스칼라, \\((x,y)\\)는 n-벡터이다.\n\nHomogeniety : \\(f(\\alpha x) = \\alpha f(x)\\)\nAdditivity : \\(f(x + y) = f(x) + f(y)\\)\n\n\n\n(1) 절편 Affine 함수\n선형 함수에 상수 항을 추가한 형태의 함수이다. 이는 선형 변환과 평행 이동을 결합한 함수로, 다음과 같은 수식으로 표현된다.\nn-벡터, \\(x\\)에 대하여 다음 \\(f\\)는 절편 함수이다. 단, \\(a\\)는 n-벡터, \\(k\\)는 스칼라이다. \\(f(a^{T}x + k) = a^{T}f(x) + k\\)\n【예제】 \\(f(x) = 7 - 2x_{1} + 3x_{2} - x_{3}\\), \\(k = 7,a = \\left\\lbrack \\begin{array}{r}\n   - 2 \\\\\n  3 \\\\\n   - 1\n  \\end{array} \\right\\rbrack\\)\n\n\n(2) 선형함수의 내적 표현\n\\(e_{i}\\) 단위벡터, \\(x_{n}\\) 차수 n-벡터, \\(f\\) 선형함수라 하면, \\[\\begin{matrix}\nf(x) & = f(x_{1}e_{1} + x_{2}e_{2} + ... + x_{n}e_{n}) \\\\\n& = x_{1}f(e_{1}) + x_{2}f(e_{2}) + ... + x_{n}f(e_{n}) \\\\\n& = a^{T}x,wherea^{T} = \\lbrack f(e_{1}),f(e_{2}),...,f(e_{n})\\rbrack\n\\end{matrix}\\]\n\n\n(3) 사례 : sag 처짐 (단위: mm)\n하중벡터 \\(w = \\left( \\begin{array}{r}\nw_{1} \\\\\nw_{2} \\\\\nw_{3}\n\\end{array} \\right)\\)(단위:톤), 변형 compliance 민감도 벡터 \\(c = \\left( \\begin{array}{r}\nc_{1} \\\\\nc_{2} \\\\\nc_{3}\n\\end{array} \\right)\\)(단위:mm/톤)이라면 교량 처짐 sag은 \\(s = c^{T}w\\) (하중 가중합)이다.\n\n\n\n\n\n\n\n(4) 테일러 근사 Taylor proximation\n함수 \\(f:R^{n} \\rightarrow R\\)이 1차 미분이 가능하다고 하면 \\(n\\)-벡터 함수 \\(f(x)\\)의 근사값은 다음과 같이 구한다. 이를 1차 테일러 근사라 한다. 단, n-벡터 \\(z\\)는 n-벡터 \\(x\\)와 가까운 값이다.\n\\[\\widehat{f}(x) = f(z) + \\frac{\\partial f}{\\partial x_{1}}(z)(x_{1} - z_{1}) + ... + \\frac{\\partial f}{\\partial x_{n}}(z)(x_{n} - z_{n})\\]\n【예제】\n함수 \\(f:R^{2} \\rightarrow R\\)을 \\(f(x) = x_{1} + \\exp(x_{2} - x_{1})\\)라 하자. 이 함수는 선형함수는 아니다. 이를 선형함수로 근사하는 것을 테일러 근사라 한다. \\(z = (1,2)\\)라 하면,\n\\[\\triangledown f(z) = \\left\\lbrack \\begin{array}{r}\n1 - \\exp(z_{2} - z_{1}) \\\\  \n\\exp(z_{2} - z_{1})  \n\\end{array} \\right\\rbrack|_{z_{1} = 1,z_{2} = 2} = ( - 1.72,2.72)\\]\n그러므로 \\(z = (1,2)\\)에서 \\(f(x)\\)의 테일러 근사값은 다음과 같다:\n\\[\\widehat{f}(x) = 3.718 + \\left\\lbrack \\begin{array}{r}  - 1.72 \\\\  2.72 \\end{array} \\right\\rbrack^{T}(\\left\\lbrack \\begin{array}{r} x_{1} \\\\  x_{2} \\end{array} \\right\\rbrack - \\left\\lbrack \\begin{array}{r}  1 \\\\ 2  \\end{array} \\right\\rbrack)\\]\n\n\n(5) 회귀모형\n차원 2-예측(설명, 독립) 벡터 \\(x = \\left\\lbrack \\begin{array}{r}\nx_{1} \\\\\nx_{2}\n\\end{array} \\right\\rbrack\\), 회귀계수 벡터 \\(b = \\left\\lbrack \\begin{array}{r}\nb_{1} \\\\\nb_{2}\n\\end{array} \\right\\rbrack\\), 그리고 \\(a\\)을 절편 스칼라라 하면 회귀모형은 다음과 같다.\n\\(\\widehat{y} = \\left\\lbrack \\begin{array}{r}\n1 \\\\\nx\n\\end{array} \\right\\rbrack^{T}\\left\\lbrack \\begin{array}{r}\na \\\\\nb\n\\end{array} \\right\\rbrack = {\\overset{˜}{x}}^{T}\\overset{˜}{b}\\) OLS 추정치 : \\(\\widehat{\\overset{˜}{b}} = ({\\overset{˜}{x}}^{T}\\overset{˜}{x})^{- 1}{\\overset{˜}{x}}^{T}y\\)\n\n\n3. 벡터놈 norm\n\n\n(1) 정의\n벡터의 유클리디안 놈, \\(\\parallel x \\parallel\\)은 벡터의 크기에 대한 척도로 다음과 같이 구한다. 놈은 벡터의 원점에서의 거리이다.\n\\[\\parallel x \\parallel = \\sqrt{x_{1}^{2} + x_{2}^{2} + ... + x_{n}^{2}} = \\sqrt{x^{T}x}\\]\n【예제】\n\\[\\parallel \\left\\lbrack \\begin{array}{r}\n  0 \\\\\n   - 1 \\\\\n  1\n  \\end{array} \\right\\rbrack \\parallel = \\sqrt{2}$,\n  $\\parallel \\left\\lbrack \\begin{array}{r}\n   - 1 \\\\\n  2\n  \\end{array} \\right\\rbrack \\parallel = \\sqrt{5}\\]\n\n\n성질\n\n비음수 동차성: \\(\\parallel \\beta x \\parallel = |\\beta| \\parallel x \\parallel\\), where \\(\\beta\\)는 스칼라\n삼각 부등식: \\(\\parallel x + y \\parallel \\leq \\parallel x \\parallel + \\parallel y \\parallel\\)\n비음수: \\(\\parallel x \\parallel \\geq 0\\)\n\n\n\n(2) 놈의 종류\n\nL1 norm : \\(L_{1} = \\overset{n}{\\sum_{i}}|x_{i}|\\) 절대값의 합으로 맨하튼 Manhattan 놈이라고도 한다. 지도의 거리 측정에 사용된다.\nL2 norm : \\(L_{2} = (\\overset{n}{\\sum_{i}}x_{i}^{2})^{\\frac{1}{2}}\\) 제곱합의 제곱근으로 유클리디안 놈이라 한다. 통계학에서 가장 많이 사용된다. 회귀계수 추정치를 구하는 최소제곱추정치 구할 때 사용된다.\n\n\n\n\n\n\n#행렬 정의\nimport numpy as np\nA=np.array([[1,2,3], [4,5,7],[8,9,10]])\n#L1 norm Mahattan\nla.norm(A,axis=1,ord=1)\n【결과】 array([ 6., 16., 27.])\n#L2 norm Euclidean\nla.norm(A,axis=1,ord=2)\n【결과】 array([ 3.74165739, 9.48683298, 15.65247584])\n\n\n(3) 평균 제곱근 RMS root mean square value\n데이터 크기를 정량화하는데 사용되며 데이터의 평균적인 크기를 나타낸다. \\(rms(x) = \\frac{\\parallel x \\parallel}{\\sqrt{n}} = \\sqrt{\\frac{1}{n}\\sum x_{i}^{2}}\\)\n\n\n(4) 두 벡터의 합의 놈\n\\[\\parallel x + y \\parallel = \\sqrt{\\parallel x \\parallel^{2} + 2x^{T}y + \\parallel y \\parallel^{2}}\\]\n\n\n(5) Chebyshev inequality\n차수 n-벡터 \\(x\\), \\(x_{i}^{2} \\geq a^{2}\\)을 만족하는 원소 개수를 \\(k\\)라 하면, \\(\\parallel x \\parallel^{2} = x_{1}^{2} + ... + x_{2}^{2} \\geq ka^{2}\\)이다. \\(k \\leq n\\)이므로 \\(n \\leq \\frac{\\parallel x \\parallel}{a^{2}}\\)이다. 즉, 벡터의 어떠한 원소도 그 벡터의 놈보다 크지 않다.\n\\(\\frac{k}{n} \\leq (\\frac{rms(x)}{a})^{2}\\). 왼쪽 항은 벡터의 성분 중 절대값이 최소한 \\(a\\)이상인 성분의 비율을 나타낸다. 오른쪽 항은 \\(a\\)와 \\(rms(x)\\)의 비율의 제곱에 대한 역수이다. 예를 들어, 벡터의 성분 중 1/25 = 4% 이상은 RMS 값의 5배를 초과할 수 없다는 것을 의미한다.\n\n\n\nchapter 4. 벡터간 거리\n\n1. 유클리디안 거리\n\n\n(1) 정의\n차수가 동일한 두 벡터(\\(a,b\\))의 놈을 유클리디안 거리로 정의한다.\n\\[dist(a,b) = \\parallel a - b \\parallel = \\parallel b - a \\parallel\\]\n\\[||a - b|| = \\sqrt{(a_{1} - b_{1})^{2} + (a_{2} - b_{2})^{2} + ... + (a_{n} - b_{n})^{2}}\\]\n두 벡터의 Root Mean Square 편차 = \\(\\frac{\\parallel x - y \\parallel}{\\sqrt{n}}\\)\n【예제】\n\\[a = \\left\\lbrack \\begin{array}{r} 0 \\\\ - 1 \\\\ 1 \\end{array} \\right\\rbrack,b = \\left\\lbrack \\begin{array}{r} 1 \\\\ - 2 \\\\ 1  \\end{array} \\right\\rbrack,c = \\left\\lbrack \\begin{array}{r} 1 \\\\ 0 \\\\3 \\end{array} \\right\\rbrack\\] \\[dist(a,b) = \\sqrt{2},dist(b,c) = 2.8284\\]\n#행렬 정의\nimport numpy as np\na=np.array([[0],[-1],[1]])\nb=np.array([[1],[-2],[1]])\nc=np.array([[1],[0],[3]])\n#거리 계산\nnp.linalg.norm(a-b),np.linalg.norm(b-c)\n【결과】 (np.float64(1.4142135623730951), np.float64(2.8284271247461903))\n\n\n(2) 활용\n\nfeature distance: \\(\\parallel x - y \\parallel\\) 차수가 동일한 두 벡터의 거리를 개체의 유사성 척도로 사용한다.\nNearest neighbor: \\(\\parallel x - z_{i} \\parallel\\) 두 개체 간의 거리를 이용하여 유사한 개체를 군집으로 묶는다. k-means 알고리즘\nRMS prediction error: \\(rms(y - \\widehat{y})\\) 관측치와 예측치의 거리를 예측의 정확도 척도로 사용한다.\n\n#감성 분석\nimport numpy as np\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n# 데이터 준비\ntexts = [\"I love this product\", \"This is terrible\", \"Absolutely fantastic\", \"Not good at all\"]\nlabels = [1, 0, 1, 0]  # 1: 긍정, 0: 부정\n# TF-IDF 벡터화\nvectorizer = TfidfVectorizer()\nX = vectorizer.fit_transform(texts)\n# KNN 모델\nknn = KNeighborsClassifier(n_neighbors=1, metric='euclidean')\nknn.fit(X, labels)\n# 새로운 리뷰 분류\nnew_text = [\"I hate this product\"]\nnew_vector = vectorizer.transform(new_text)\nprediction = knn.predict(new_vector)\nprint(f\"Prediction: {'Positive' if prediction[0] == 1 else 'Negative'}\")\n【결과】 Prediction: Positive\n\n\n(3) 삼각 부등식\n차수가 동일한 n벡터 \\(a,b,c\\)에 대하여 다음이 발생한다.\n\\[\\parallel a - c \\parallel \\leq \\parallel a - b \\parallel + \\parallel b - c \\parallel\\]\n\n\n(4) triangle 부등식\n\\[\\parallel a + b \\parallel^{2} \\leq ( \\parallel a \\parallel + \\parallel b \\parallel )^{2}\\]\n\n\n(5) 맨해튼 거리\n\\[d(\\mathbf{a},\\mathbf{b}) = \\overset{n}{\\sum_{i = 1}}|a_{i} - b_{i}|\\]\n맨해튼 거리는 벡터 간의 축을 따라 이동한 거리의 합으로 이는 그리드 기반 공간에서 이동하는 경우에 적합하다. 맨해튼 거리라는 이름은 도로망이 격자 형태로 이루어진 맨해튼 도시 구조에서 유래되었다. 자동차나 사람이 이동할 때 대각선으로 이동하지 못하고 도로를 따라 움직이는 경우에 적합하다. 예: 두 위치 간 최단 이동 거리 계산.\n\n\n\n\n\n\n\n2. 유클리디안 거리와 통계\n\n\n(1) de-meanded 벡터\n【reall】 치수 n-벡터 \\(x_{n}\\), 평균은 \\(avg(x) = (1_{n}^{T}x)/n = (instat) = \\overline{x}\\)\n【정의】 \\(\\overset{˜}{x} = x - avg(x)1_{n}\\) : 벡터의 각 원소를 평균을 뺀 벡터\n【성질】 \\(avg(\\overset{˜}{x}) = 0\\)\n\n통계 분석: 데이터의 평균을 제거함으로써 분산이나 공분산과 같은 통계적 특성을 더 명확하게 분석할 수 있다.\n주성분 분석(PCA): 데이터의 분산을 분석하기 전에 데이터를 중심에 맞추기 위해 사용된다.\n회귀 분석: 회귀 분석에서 독립 변수와 종속 변수의 평균을 제거하여 상수항 없이 회귀 모델을 구축할 수 있다.\n\n\n\n(2) 표준편차 standard deviation\n\\[std(x) = \\sqrt{\\frac{(x_{1} - avg(x))^{2} + (x_{2} - avg(x))^{2} + ... + (x_{n} - avg(x))^{2})}{n}}\\]\n\\[std(x) = \\frac{\\parallel x - (1^{T}x/n)1 \\parallel}{\\sqrt{n}}\\]\n【응용】 투자에서 평균은 일정기간 평균 수익율, 표준편차는 위험 척도이다.\n\n\n표준편차 성질\n상수를 더해도 표준편차는 동일하다. \\(std(x + a1) = std(x)\\)\n스칼라(상수) 곱 : \\(std(kx) = |k|std(x)\\)\n\n\n평균, RMS, STD 관계\n\\(std(x)^{2} = rms(x)^{2} - avg(x)^{2}\\)\n(in stat) \\(std(x)^{2} = var(x)\\) 분산\n\n\n표준편차와 Chebychev 부등식\n만약 차원 \\(n\\)-벡터에서 \\(|x_{i} - avg(x)| \\geq a\\)을 만족하는 원소 개수를 \\(k\\)라 하면 \\(\\frac{k}{n} \\leq (\\frac{std(x)}{a})^{2}\\)이다. 벡터 \\(x\\) 평균으로부터 \\(k\\) 표준편차 이내에 있는 성분 비율은 최소 \\(1 - 1/k^{2}\\)이다.\n\\[P(|X - \\mu| &gt; k\\sigma) \\leq 1 - \\frac{1}{k^{2}}\\]\n예를 들어, 일정 기간 투자 평균 수익률은 8%이고, 리스크(표준편차)는 3%입니다. 체비셰프의 부등식에 따르면, 손실을 기록한 기간의 비율(즉, 0% 이하인 기간, 16% 이상인 기간)은 최대 (3/8)^2 = 14.1%이다.\n\n\n(3) 실증적 규칙\n\\[P(|X - \\mu| \\leq k\\sigma)\\]\n\n\\(k = 1\\), 데이터의 68.3%가 \\((\\mu - \\sigma,\\mu + \\sigma)\\) 내에 있음\n\\(k = 2\\), 데이터의 95.4%, \\(k = 3\\), 데이터의 99.9%\n\n\n\n\n\n\n\n\n특징\n실증적 규칙\n체비세프 규칙\n\n\n\n\n분포가정\n정규분포에만 적용 가능\n모든 분포에 적용 가능\n\n\n그래프 모양\n종형 곡선(정규분포)\n다양한 분포(정규분포, 비대칭, 멀티모달 등)\n\n\n데이터 범위\n평균과 표준편차로 대칭적인 확률 분포\n최소한의 비율을 보장하며 보수적(더 큰 범위를 포함)\n\n\n데이터 비율\n±1σ: 68%, ±2σ: 95%, ±3σ: 99.7%\n±2σ: ≥75%, ±3σ: ≥88.9%\n\n\n\n\n\n\n3. 거리와 개체 군집화\n\n\n(1) 개념\n\\(N\\)개의 차수 \\(n\\)-벡터 \\((x_{1},x_{2},...,x_{N})\\)에 대하여 각 벡터(개체) 쌍 사이의 거리로 측정하여 서로 가까운 클러스터 또는 클러스터로 묶는 작업을 다룬다. 클러스터링의 목표는 가능한 경우 벡터들을 \\(k\\)개의 클러스터 또는 클러스터로 묶거나 나누어, 각 클러스터 내의 벡터들이 서로 가깝도록 하는 것이다. 클러스터링은 벡터들이 객체의 특징을 나타낼 때 널리 사용된다. 다음은 \\(n = 2\\)(군집변수 2개), \\(k = 3\\)으로 클러스터링 한 사례이다.\n\n\n\n\n\n\n\n(2) 클러스터 할당\n\\(N\\)개 개체, \\(x_{i}\\)를 개체(\\(i = 1,2,...,N\\)), \\(c_{i}\\)는 \\(i\\)-개체가 할당된 클러스터이고 (\\(j = 1,2,...k\\)), \\(G_{j}\\)을 \\(j\\)-클러스터에 속한 개체의 집합이라 하자.\n\\[G_{j} = \\{ i|c_{i} = j\\}\\]\n클러스터을 대표하는 차원 \\(n\\)-벡터를 \\(z_{1},z_{2},...,z_{k}\\)라 하자. \\(i\\)-개체가 \\(j = c_{i}\\)에 있다면 \\(\\parallel x_{i} - z_{c_{i}} \\parallel\\)은 모든 클러스터 중 가장 가까워야 한다.\n\n\n(3) 클러스터 목적\n\\(J^{clust} = ( \\parallel x_{1} - z_{c_{1}} \\parallel + \\parallel x_{2} - z_{c_{2}} \\parallel + ... + \\parallel x_{N} - z_{c_{N}} \\parallel )/N\\) 함수를 최소화 하는 \\(z_{c_{1}},z_{c_{2}},...,z_{c_{N}}\\)을 구한다.\n\n\n(4) 최적 클러스링\n목적함수 \\(J^{clust}\\)을 최소화 하는 \\(z_{c_{1}},z_{c_{2}},...,z_{c_{N}}\\)을 찾는 것은 개체 수가 많고 차원 개수가 커지면 계산 회수가 기하 급수적으로 늘어나 불가능하다. 그러므로 최적 대신 차선 sub-optimal 방법으로 대표 벡터를 고정화 하는 k-평균 방법을 사용한다.\n\n\n\n\n\n\n\n4. k-means 알고리즘\n\n\n(1) 개념\n클러스터 할당과 클러스터 대표자를 선택하여 \\(J^{clust}\\)를 최소화하는 문제를 해결할 수 있을 것처럼 보이나 두 가지 선택은 순환적입니다. 즉, 각각의 선택이 다른 하나에 의존한다. 클러스터 대표자를 선택하고 클러스터 할당을 선택하는 것을 반복하는 것이 벡터 집합을 클러스터링하는 데 있어서 유명한 k-means 알고리즘이다. k-means 알고리즘은 1957년에 Stuart Lloyd와 독립적으로 Hugo Steinhaus에 의해 처음 제안되어 때때로 Lloyd 알고리즘이라고도 불린다. k-means라는 이름은 1960년대부터 사용되었다.\n\n\n(2) k-평균 알고리즘\n\\(N\\)개 개체를 \\(k\\)개 클러스터으로 분류한다고 가정하자. \\(z_{1},z_{2},...,z_{k}\\)을 각 클러스터의 대표 벡터라 하자. k-평균 알고리즘은 다음 작업을 반복 실행한다.\n(1) 대표 벡터를 결정하고 각 개체를 가장 가까운 대표 벡터의 클러스터으로 분류한다.\n(2) 클러스터에 할당된 개체의 중심점(평균 벡터)을 대표 벡터로 설정한다.\n(3) 수렴 조건 만족 때까지 위의 작업을 반복한다.\n\n\n(3) 이슈사항\n\n\n타이 브레이커\n  두 개 이상의 클러스터과 최소 거리인 개체는 클러스터 할당을 하지 않는다. 그러므로 이 개체는 다음 단계에서 대표 벡터 결정에는 활용되지 않는다.\n\n\n수렴 조건\n  개체의 클러스터 이동이 더 이상 발생하지 않으면 대표 벡터는 움직이지 않음을 의미하므로 클러스터링 결과는 동일해진다.\n\n\nk-평균 알고리즘은 직관적이다.\n  목표함수 \\(J^{clust}\\)을 최적화 하지 못하지만 반복을 통하여 줄여 나가게 된다.\n\n\n대표벡터 해석\n각 \\(N\\)개의 회사마다 총 자본화, 분기별 수익 및 위험, 거래량, 손익, 배당금 등과 같은 금융 및 사업 속성을 구성 요소로 하는 n-벡터을 이용하여 k-평균 클러스터링 결과 얻은 대표벡터를 이용하여 클러스터(군집)에 이름을 부여한다. 기업연수, 기업종류, 매출액 등 군집변수로 사용하지 않은 특성 벡터를 이용하여 개체 군의 이름을 부여하고 해석한다.\n\n\n클러스터 \\(k\\) 결정\n\\(k\\)의 결정은 다소 주관적이고 시행착오 방법을 사용한다. \\((k,J^{clust})\\)을 이용하여 Elbow Method 팔꿈치 기법을 사용한다. 군집 개수가 증가할수록 \\(J^{clust}\\)는 감소하게 되지만, 이 감소율이 꺾이는 지점을 찾아내는 방법이다.\n\n\n고정 대표 벡터 분할하기\n만약 \\(j\\) 클러스터을 대표하는 벡터 \\(z_{1},z_{2},...,z_{j}\\)르 고정하면 모든 개체 \\(x_{1},x_{2},...,x_{N}\\)을 최적 클러스터으로 분류 문제는 다음과 같다.\n\\[\\parallel x_{i} - z_{c_{i}} \\parallel = min_{j = 1,2,...,k} \\parallel x_{i} - z_{j} \\parallel\\]\n고정 대표 벡터를 활용하면 최적 클러스터링 문제는 다음과 같이 sub 최적 문제로 변환된다. 각 \\(N\\)개 개체에 최적 \\(j\\)-클러스터(거리가 가장 가까운 클러스터)을 결정하는 개별적 문제와 동일하다.\n\n\\[J^{clust} = min_{j = 1,2,...,k} \\parallel x_{1} - z_{j} \\parallel + ... + min_{j = 1,2,...,k} \\parallel x_{N} - z_{j} \\parallel )/N\\]\n\n고정 벡터를 group(or cluster) centroid라 한다.\n\n\n(4) 사례\n# 60000(train 훈련)/10000(test 테스트), 28x28\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.datasets import mnist\n# MNIST 데이터셋 로드 및 훈련데이터, 테스트데이터 분할 \n(x_train, y_train), (x_test, y_test) = mnist.load_data()\n# 데이터 형태 출력\nprint(f\"x_train shape: {x_train.shape}\")\nprint(f\"y_train shape: {y_train.shape}\")\nprint(f\"x_test shape: {x_test.shape}\")\nprint(f\"y_test shape: {y_test.shape}\")\n# 첫 10개 샘플 이미지와 레이블 시각화\nnum_samples = 10\nplt.figure(figsize=(10, 1))\nfor i in range(num_samples):\n    plt.subplot(1, num_samples, i+1)\n    plt.imshow(x_train[i], cmap='gray')\n    plt.title(y_train[i])\n    plt.axis('off')\nplt.show()\n\n\n\n\n\n# 훈련 데이터 클러스트링, 첫 20개 군집결과\n# 이미지 데이터를 2차원 배열로 변환\nx_train2 = x_train.reshape((x_train.shape[0], -1))\nx_test2 = x_test.reshape((x_test.shape[0], -1))\n# 데이터 정규화\nx_train2 = x_train2 / 255.0\nx_test2 = x_test2 / 255.0\n# k-means 모델 생성 및 학습\nkmeans = KMeans(n_clusters=10, random_state=42)\nkmeans.fit(x_train2)\n# 클러스터 할당 결과\ny_kmeans = kmeans.predict(x_train2)\n# 첫 20개 분류결과 이미지와 레이블 시각화\nnum_samples = 20\nplt.figure(figsize=(10, 1))\nfor i in range(num_samples):\n    plt.subplot(1, num_samples, i+1)\n    plt.imshow(x_train[i], cmap='gray')\n    plt.title(y_kmeans[i])\n    plt.axis('off')\nplt.show()\n 10개 클러스터명은 임의로 정해져 숫자와 매칭이 되지 않는다. 클러스터에 속한 이미지를 이용하여 결정한다. 클러스터9, 클러스터1에는 이미지 6/2이 두개이므로 숫자6,숫자2 클러스터으로 하면 된다. 클러스터3에는 2개 이지지 중 숫자5, 3, 8이 각각 1개이므로 나머지 클러스터3으로 분류된 이미지 번호 확인하여 숫자번호를 결정한다. 클러스터5에는 이미지9 2개, 이미지7, 이미지4 각각 1개이므로 클러스터5는 이미지9 군집으로 한다.\n# 클러스터 대표 이미지\n# 클러스터 3 평균벡터 출력\nplt.figure(figsize=(10, 1))\nplt.imshow((x_train[0]+x_train[7]+x_train[17])/3, cmap='gray')\nplt.title('cluster 3')\nplt.axis('off')\nplt.show()\n\n\n\n\n\n\n\n5. 벡터의 각도\n\n\n(1) 코사인 유사도\n벡터의 코사인 유사도(Cosine Similarity)는 두 벡터 간의 방향적 유사성을 측정하는 지표로, 벡터 간의 각도 \\(\\theta\\)의 코사인 값을 이용하여 계산된다.\n\\[\\text{Cosine Similarity} = cos(\\theta) = \\frac{\\mathbf{A} \\cdot \\mathbf{B}}{\\parallel \\mathbf{A} \\parallel \\parallel \\mathbf{B} \\parallel}\\]\n코사인 유사도와 유클리드 거리의 차이는 다음과 같다.\n\n코사인 유사도는 두 벡터의 방향에 집중하며, 벡터 크기의 차이를 무시한다.\n유클리드 거리는 두 벡터 사이의 실제 거리(크기 차이 포함)를 측정한다.\n예를 들어, 텍스트 데이터에서 코사인 유사도는 문서 간의 내용적 유사성을 비교하는 데 유리하며, 추천 시스템, 정보 검색, 클러스터링 등에서 널리 사용된다.\n\n【예제】\n\n\n\n\n\n\n\n(2) 코사인 유사도의 특징\n코사인 유사도 값의 범위는 [-1, 1]이고 다음의 특징을 갖는다.\n\\(cos(\\theta) = 1\\): 두 벡터가 완전히 같은 방향\n\\(cos(\\theta) = 0\\): 두 벡터가 직교(Orthogonal, 90도)\n\\(cos(\\theta) = - 1\\): 두 벡터가 완전히 반대 방향.\n코사인 유사도는 벡터의 크기가 아닌 방향만 고려되므로 벡터를 정규화하지 않고도 비교할 수 있다. 고차원 벡터에도 적용 가능하여 텍스트 데이터, 사용자 선호도 등 고차원 데이터에서 벡터 간 유사성 측정에 많이 사용된다.\n각도 종류\n\n각도가 \\(\\theta = 90^{o} = \\pi/2\\)이면 두 벡터는 직교 orthogonal 한다.\n각도가 \\(\\theta = 0^{o}\\)이면 두 벡터는 정렬 aligned 되어 있다.\n각도가 \\(\\theta = 180^{o} = \\pi\\)이면 두 벡터는 역정렬 anti-aligned 되어 있다.\n각도가 \\(\\theta &gt; 90^{o} = \\pi/2\\)이면 두 벡터의 각은 둔각 obtuse, \\(\\theta &lt; 90^{o} = \\pi/2\\)이면 두 벡터의 각은 예각 acute 이다.\n\n\n\n\n\n\n\n\n두 벡터 합의 놈과 각도\n\\[\\parallel x + y \\parallel^{2} = \\parallel x \\parallel^{2} + 2 \\parallel x \\parallel \\parallel y \\parallel \\cos(\\theta) + \\parallel y \\parallel^{2}\\]\n만약 \\(\\theta = 90^{o} = \\pi/2\\)이면 \\(\\parallel x + y \\parallel^{2} = \\parallel x \\parallel^{2} + \\parallel y \\parallel^{2}\\) (피타고라스 정리)\n\n\n6. 상관계수\n\n\n(1) 상관계수 정의\n만약 \\(\\overset{˜}{a} = a - avg(a)1,\\overset{˜}{b} = b - avg(b)1\\)이면, 상관계수(correlation coefficient) \\(\\rho\\)는 다음과 같이 정의된다.\n\\(\\rho = \\frac{{\\overset{˜}{a}}^{T}\\overset{˜}{b}}{\\parallel \\overset{˜}{a} \\parallel \\parallel \\overset{˜}{b} \\parallel}\\) ⇔ \\(\\rho = (\\frac{\\overset{˜}{a}}{std(a)})^{T}(\\frac{\\overset{˜}{b}}{std(b)})/n\\)\n\n\\(cov(a,b) = {\\overset{˜}{a}}^{T}\\overset{˜}{b}/n\\): 두 벡터의 공분산\n\\(var(a) = std(a)^{2}\\): 벡터의 분산\n상관계수와 공분산 관계: \\(cov(a,b) = \\rho std(a)std(b)\\)\n\\(\\rho = \\pm 1\\) (완전 상관) : 두 벡터가 (역)정렬되어 있음\n\\(\\rho = 0\\) (독립) : 두 벡터가 직교되어 있음. \\(cov(a,b) = 0\\)\n\n\n\n(2) 두 벡터 합의 분산\n\\[var(a + b) = var(a) + 2cov(a,b) + var(b)\\]\n\\[var(a + b) = var(a) + 2\\rho std(a)std(b) + var(b)\\]\n\n만약 \\(\\rho = 0\\)이면, \\(var(a + b) = var(a) + var(b)\\)\n만약 \\(\\rho = 1\\)이면, \\(var(a + b) = (std(a) + std(b))^{2}\\)\n만약 \\(\\rho = - 1\\)이면, \\(var(a + b) = (std(a) - std(b))^{2}\\)\n\n\n\n(3) 헤징 hedging 투자\n두 개 회사 주가 벡터 \\((a,b)\\)의 평균은\\(\\mu\\), 표준편차(위험) \\(\\sigma\\)이고 상관계수는 \\(\\rho\\)이다. 각각 50% 투자, \\(c = \\frac{(a + b)}{2}\\)의 평균 수익율과 표준편차은 다음과 같다.\n\n평균 : \\(avg(\\frac{a + b}{2}) = \\mu\\)\n표준편차 : \\(std(c) = \\sigma\\sqrt{(1 + \\rho)/2}\\)\n상관계수 \\(\\rho = 0\\)이면 (독립) 표준편차는 \\(\\frac{1}{\\sqrt{2}}\\)만큼 줄어든다.\n완벽한 상관관계가 있는 경우에만 표준편차는 동일하다.\n\n\n\n\nchapter 5. 선형독립\n\n1. 선형독립 정의\n\n\n(1) 선형 종속 linear dependence\n\\(k \\geq 2\\)개의 크기 n-벡터 \\(x_{1},x_{2},...,x_{k}\\)가 다음을 만족하면 선형종속이라 한다. 만약 \\(a_{1}x_{1} + a_{2}x_{2} + ... + a_{k}x_{k} = 0\\)을 만족하는 \\(a_{i}\\)가 적어도 하나는 0이 아니다.\n선형독립이면 적어도 하나의 \\(a_{i}\\)는 0이 아니므로 벡터 \\(x_{i}\\) 다음과 같이 다른 벡터의 선형함수로 표현될 수 있다.\n\\[x_{k} = \\frac{- a_{1}}{a_{i}}x_{1} + ... + \\frac{- a_{i - 1}}{a_{i}}x_{i - 1} + \\frac{- a_{i + 1}}{a_{i}}x_{i + 1} + ... + \\frac{- a_{k}}{a_{i}}x_{k}\\]\n【예제】\n\n\\(x_{1} = \\left\\lbrack \\begin{array}{r} &gt; 0 \\\\ &gt;  - 1 \\\\  &gt; 1 &gt; \\end{array} \\right\\rbrack,x_{2} = \\left\\lbrack \\begin{array}{r} &gt; 1 \\\\ &gt;  - 2 \\\\ &gt; 1  &gt; \\end{array} \\right\\rbrack,x_{3} = \\left\\lbrack \\begin{array}{r} &gt; 1 \\\\ &gt; 0 \\\\ &gt;  - 1  &gt; \\end{array} \\right\\rbrack\\) ⬄\\(- 2x_{1} + x_{2} - x_{3} = 0\\)\n\n\n\n(2) 선형 독립 linear independence\n만약 \\(a_{1}x_{1} + a_{2}x_{2} + ... + a_{k}x_{k} = 0\\)이 모든 \\(a_{k} = 0\\)일 때만 만족한다면, n-벡터 \\(x_{1},x_{2},...,x_{k}\\)을 선형독립이라 한다.\n【예제】\n\n\n\n\n\n\n\n\\[x_{1} = \\left\\lbrack \\begin{array}{r}\n1 \\\\\n0 \\\\\n0\n\\end{array} \\right\\rbrack,x_{2} = \\left\\lbrack \\begin{array}{r}\n0 \\\\\n- 1 \\\\\n1\n\\end{array} \\right\\rbrack,x_{3} = \\left\\lbrack \\begin{array}{r}\n- 1 \\\\\n1 \\\\\n1\n\\end{array} \\right\\rbrack\\]\n\n\n\n\n\n\n\n\n(3) 선형독립 벡터의 선형결합\n선형독립인 \\(x_{1},x_{2},...,x_{k}\\)의 선형결합의 모든 계수(\\(a_{k}\\))는 유일하다. 선형결합 \\(x = a_{1}x_{1} + a_{2}x_{2} + ... + a_{k}x_{k}\\)\n【증명】 다른 계수를 \\(b_{k}\\)라 하자. \\(x = b_{1}x_{1} + b_{2}x_{2} + ... + b_{k}x_{k}\\) \\(0 = (a_{1} - b_{1})x_{1} + (a_{2} - b_{2})x_{2} + ... + (a_{k} - b_{k})x_{k}\\)이다. \\(x_{1},x_{2},...,x_{k}\\)가 선형독립이므로 모든 \\((a_{i} - b_{i}) = 0\\) 만족한다.\n\n\n2. 기저\n\n\n(1) 기저 개념\n벡터 공간은 다양한 차원의 벡터로 이루어진 공간이며, 그 공간 안의 벡터들을 다른 벡터들의 선형 조합으로 표현할 수 있다. 이때, 특정 벡터 공간의 기저 basis 는 그 공간 안의 모든 벡터들을 생성할 수 있는 최소한의 독립적인 벡터들의 집합이다.\n예를 들어, 2차원 공간에서의 기저는 일반적으로 (1,0)과 (0,1)이다. 이 두 벡터는 선형 독립이며, 이들의 모든 선형 조합으로 2차원 평면 상의 어떤 점이든 표현할 수 있다. 따라서 (1,0)과 (0,1)은 2차원 공간의 기저입니다. 단, 벡터 공간의 기저는 유일하지 않다.\n\n\n\n\n\n크기 2인 벡터의 기저 벡터는 \\(k = 2\\)개이다. 위의 그림에서 \\(a_{3}\\)벡터는 \\((a_{1},a_{2})\\)(기저 벡터)의 선형결합으로 만들 수 있다.\n\n\n(2) 기저 정의\nn개의 선형독립인 크기 n-벡터를 기저 basis 라 한다. 즉, n-벡터 \\((x_{1},x_{2},...,x_{n})\\)가 기저이면, 모든 크기 n-벡터는 \\((x_{1},x_{2},...,x_{n})\\)의 선형 결합으로 표현할 수 있다.\n【증명】 (n+1)개 차원 n-벡터 \\((x_{1},x_{2},...,x_{n},y)\\)개가 있다고 가정하자. 단,\\((x_{1},x_{2},...,x_{n})\\) 선형독립이며 기저이다. 이들 벡터는 선형독립(차원개수 n보다 벡터 개수가 (n+1)로 크다)이므로 다음을 만족하는 모든 \\(a_{i}\\)가 0은 아니다. \\(a_{1}x_{1} + a_{2}x_{2} + ... + a_{n}x_{n} + a_{n + 1}y = 0\\)\n만약 \\(a_{n + 1} = 0\\)이면, \\(a_{1}x_{1} + a_{2}x_{2} + ... + a_{n}x_{n} = 0\\)을 만족하는 모든 \\(a_{i} = 0\\)이다. 왜냐하면 \\((x_{1},x_{2},...,x_{n})\\) 선형독립이기 때문이다.(모순)\n\n\n3. 직교정규\n\n\n(1) 정의\n만약 \\(\\parallel x_{i} \\parallel = 1\\)이고 \\(x_{i}^{T}x_{j} = 0fori \\neq j\\) (두 벡터 \\((x_{i},x_{j})\\)는 직교)이면, \\((x_{1},x_{2},...,x_{k})\\) 벡터 집합은 직교 정규 orthonormal 벡터라고 한다.\n직교정규성은 선형종속, 선형독립처럼 집합의 속성이지 개별 벡터의 속성은 아니다.\n\n\n(2) 예제\n\nn개의 단위벡터는 직교정규 벡터이다.\n직교정규벡터 \\(\\left\\lbrack \\begin{array}{r}\n   - 1 \\\\0 \\\\ 0\n  \\end{array} \\right\\rbrack,\\frac{1}{\\sqrt{2}}\\left\\lbrack \\begin{array}{r}\n  0 \\\\ 1 \\\\ 1\n  \\end{array} \\right\\rbrack,\\frac{1}{\\sqrt{2}}\\left\\lbrack \\begin{array}{r}\n  0 \\\\ - 1 \\\\ 1\n  \\end{array} \\right\\rbrack\\)\n직교정규 벡터는 선형독립이다.\n\n\n\n(3) 직교정규 성질\n\n벡터 \\(x\\)가 직교정규벡터 선형결합이면 \\(x = a_{1}x_{1} + a_{2}x_{2} + ... + a_{k}x_{k}\\) 내적을 이용하여 다음을 얻으므로 내적을 이용하여 계수를 얻을 수 있다.\n\\[x_{i}^{T}x = x_{i}^{T}(a_{1}x_{1} + a_{2}x_{2} + ... + a_{k}x_{k}) = a_{i}\\]\n벡터 \\((x_{1},x_{2},...,x_{k})\\)가 직교정규 (선형독립이고 기저임) 벡터이면 \\(x = (x_{1}^{T}x)x_{1} + (x_{2}^{T}x)x_{2} + ... + (x_{k}^{T}x)x_{k}\\)이 성립한다.\n\n벡터 (1, 2, 3)을 직교정규 벡터의 선형결합으로 표현하자.\n\\[\\left\\lbrack \\begin{array}{r}\n1 \\\\\n2 \\\\\n3\n\\end{array} \\right\\rbrack = 1\\left\\lbrack \\begin{array}{r}\n1 \\\\\n0 \\\\\n0\n\\end{array} \\right\\rbrack + 2\\left\\lbrack \\begin{array}{r}\n0 \\\\\n1 \\\\\n0\n\\end{array} \\right\\rbrack + 3\\left\\lbrack \\begin{array}{r}\n0 \\\\\n0 \\\\\n1\n\\end{array} \\right\\rbrack\\]\n\\(\\lbrack - 1 0 0\\rbrack\\left\\lbrack \\begin{array}{r}\n  1 \\\\\n  2 \\\\\n  3\n  \\end{array} \\right\\rbrack = - 1\\), \\(\\frac{1}{\\sqrt{2}}\\lbrack 011\\rbrack\\left\\lbrack \\begin{array}{r}\n  1 \\\\\n  2 \\\\\n  3\n  \\end{array} \\right\\rbrack = \\frac{5}{\\sqrt{2}}\\), \\(\\frac{1}{\\sqrt{2}}\\lbrack 0 - 11\\rbrack\\left\\lbrack \\begin{array}{r}\n  1 \\\\\n  2 \\\\\n  3\n  \\end{array} \\right\\rbrack = \\frac{1}{\\sqrt{2}}\\)\n\\[\\left\\lbrack \\begin{array}{r}\n1 \\\\\n2 \\\\\n3\n\\end{array} \\right\\rbrack = - 1\\left\\lbrack \\begin{array}{r}\n- 1 \\\\\n0 \\\\\n0\n\\end{array} \\right\\rbrack + \\frac{5}{2}\\left\\lbrack \\begin{array}{r}\n0 \\\\\n1 \\\\\n1\n\\end{array} \\right\\rbrack + \\frac{1}{2}\\left\\lbrack \\begin{array}{r}\n0 \\\\\n- 1 \\\\\n1\n\\end{array} \\right\\rbrack\\]\n\n\n4. Gram-Schmidt 알고리즘\n\n\n(1) 개념\nn-벡터 \\(x_{1},x_{2},...,x_{k}\\)가 선형 독립인지 여부를 결정할 수 있는 알고리즘으로 수학자 Jørgen Pedersen Gram과 Erhard Schmidt의 이름을 따서 명명되었다.\n만약 벡터들이 선형 독립이라면, Gram–Schmidt 알고리즘은 다음과 같은 속성을 가진 직교정규 벡터 \\(q_{1},q_{2},...,q_{k}\\) 을 생성한다.\n\n각 \\(i = 1,2,...,k\\)에서 \\(x_{i}\\)는 \\(q_{1},q_{2},...,q_{i}\\)의 선형결합이다.\n각 \\(i = 1,2,...,k\\)에서 \\(q_{i}\\)는 \\(x_{1},x_{2},...,x_{i}\\)의 선형결합이다.\n만약 \\(x_{1},x_{2},...,x_{i - 1}\\) 선형독립이나 \\(x_{1},x_{2},...,x_{i}\\)는 선형종속이면 멈춘다.\n\n\n\n(2) 알고리즘\n주어진 n-벡터 \\(x_{1},x_{2},...,x_{k}\\), \\(i = 1,2,...,k\\)일 때\n\n직교화 : \\({\\overset{˜}{q}}_{i} = x_{i} - (q_{1}^{T}x_{i})q_{1} - ... - (q_{i - 1}^{T}x_{i})q_{i - 1}\\)\n선형종속 검증 : 만약 \\({\\overset{˜}{q}}_{i} = 0\\)이면, 멈춘다.\n정규화 : \\(q_{i} = \\frac{{\\overset{˜}{q}}_{i}}{\\parallel q_{i} \\parallel}\\).\n\n이렇게 얻은 \\(q_{1},q_{2},...,q_{i}\\)는 직교정규 벡터이다. 알고리즘 적용 중 중간에 중단되면 기저젝터가 아니다.\n\n\n(3) Gram-Schmidt 알고리즘 예제\n\\(x_{1} = ( - 1,1, - 1,1),x_{2} = ( - 1,3, - 1,3),x_{3} = (1,3,5,7)\\) 에 대하여 Gram–Schmidt 알고리즘을 적용하자.\n\n\ni=1\n\\(\\parallel {\\overset{˜}{q}}_{1} \\parallel = 2\\)이므로 \\(q_{1} = \\frac{{\\overset{˜}{q}}_{1}}{\\parallel {\\overset{˜}{q}}_{1} \\parallel} = \\left\\lbrack \\begin{array}{r}\n- 1/2 \\\\\n1/2 \\\\\n- 1/2 \\\\\n1/2\n\\end{array} \\right\\rbrack\\)이다.\n\n\ni=2\n\\(q_{1}^{T}x_{2} = 4\\)이므로 \\({\\overset{˜}{q}}_{2} = x_{2} - (q_{1}^{T}x_{2})q_{1} = \\left\\lbrack \\begin{array}{r}\n1 \\\\\n1 \\\\\n1 \\\\\n1\n\\end{array} \\right\\rbrack\\)이고 \\(\\parallel {\\overset{˜}{q}}_{2} \\parallel = 2\\)이다. 그러므로 \\(q_{2} = \\frac{{\\overset{˜}{q}}_{2}}{\\parallel {\\overset{˜}{q}}_{2} \\parallel} = \\left\\lbrack \\begin{array}{r}\n1/2 \\\\\n1/2 \\\\\n1/2 \\\\\n1/2\n\\end{array} \\right\\rbrack\\).\n\n\ni=3\n\\(q_{1}^{T}x_{3} = 2,q_{2}^{T}x_{3} = 8\\)이므로 \\({\\overset{˜}{q}}_{3} = x_{3} - (q_{1}^{T}x_{3})q_{1} - (q_{2}^{T}x_{3})q_{2} = \\left\\lbrack \\begin{array}{r}\n- 2 \\\\\n- 2 \\\\\n2 \\\\\n2\n\\end{array} \\right\\rbrack\\)이고 \\(\\parallel {\\overset{˜}{q}}_{3} \\parallel = 4\\)이다. 그러므로 \\(q_{3} = \\frac{{\\overset{˜}{q}}_{3}}{\\parallel {\\overset{˜}{q}}_{3} \\parallel} = \\left\\lbrack \\begin{array}{r}\n- 1/2 \\\\\n- 1/2 \\\\\n1/2 \\\\\n1/2\n\\end{array} \\right\\rbrack\\).\n# Gram-Schmidt 알고리즘\nimport numpy as np\n\ndef gram_schmidt(A):\n    # Get the number of rows (n) and columns (k) in A\n    n, k = A.shape\n    # Initialize matrix Q with zeros, same shape as A\n    Q = np.zeros((n, k))\n    \n    for j in range(k):\n        # Start with the current column vector of A\n        v = A[:, j]\n        for i in range(j):\n            # Subtract the projection of v onto the ith orthonormal vector\n            v -= np.dot(Q[:, i], A[:, j]) * Q[:, i]\n        \n        # Normalize the vector\n        Q[:, j] = v / np.linalg.norm(v)\n    return Q\n# Example usage\nA = np.array([[-1,-1,1],\n              [1,3,3],\n              [-1,-1,5],\n              [1,3,7]], dtype=float)\n\ngram_schmidt(A)\n【결과】 array([[-0.5, 0.5, -0.5], [ 0.5, 0.5, -0.5], [-0.5, 0.5, 0.5], [ 0.5, 0.5, 0.5]])"
  },
  {
    "objectID": "notes/math/matrix.html",
    "href": "notes/math/matrix.html",
    "title": "수학의 기초 4. 행렬",
    "section": "",
    "text": "chapter 1. 행렬 기초\n\n1. 개념\n\n\n(1) 통계학과 행렬\n행렬은 통계학에서 데이터를 표현하고 분석하는 데 핵심적인 도구로 사용된다. 행렬은 대규모 데이터의 구조를 간단히 표현하고, 계산을 효율적으로 수행하여 통계학에서 중요한 역할을 한다.\n\n\n데이터 표현\n  데이터를 행렬로 저장하여 표 형식으로 표현한다. 다음은 관측값(행)과 변수(열)로 구성된 데이터 행렬이다.\n\\[X = \\begin{bmatrix}\nx_{11} & x_{12} & \\cdots & x_{1p} \\\\\nx_{21} & x_{22} & \\cdots & x_{2p} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nx_{n1} & x_{n2} & \\cdots & x_{np}\n\\end{bmatrix}\\]\n\n\n연산의 간결화\n  여러 변수와 관측값 간의 관계를 분석할 때 행렬식으로 간단히 표현하고 행렬 연산을 이용하여 추정값을 계산한다.\n\\(Y = X\\beta + \\epsilon\\), OLS 추정=\\(\\widehat{\\beta} = (X'X)^{- 1}X'Y\\)\n\n\n(2) 정의\n행과 열로 배열된 숫자, 기호 또는 표현식의 직사각형 배열을 행렬이라 한다. 행의 차수는 \\(m\\), 열의 차수는 \\(n\\)이다.\n\\(A_{m \\times n} = \\begin{bmatrix}\na_{11} & a_{12} & \\cdots & a_{1n} \\\\\na_{21} & a_{22} & \\cdots & a_{2n} \\\\\n\\cdots & \\cdots & \\cdots & \\cdots \\\\\na_{m1} & a_{m2} & \\cdots & a_{mn}\n\\end{bmatrix}\\) (간편식) \\(A = \\{ a_{ij}\\}\\)\n\n행렬의 각 셀을 원소 element라 한다.\n행의 차수 \\(m = 1\\)인 행렬을 열 column 벡터이다.\n열의 차수 \\(n = 1\\)인 행렬을 행 row 벡터이다.\n행의 차수, 열의 차수 모두 1인 행렬을 스칼라 scalar이다.\n행렬을 \\(n\\)-열벡터로 표현 : \\(A_{m \\times n} = \\begin{bmatrix}\na_{1} & a_{2} & \\cdots a_{n}\n\\end{bmatrix}\\)\n행렬을 \\(m\\)-헹벡터로 표현 : \\(A_{m \\times n} = \\left\\lbrack \\begin{array}{r}\na_{1} \\\\\na_{2} \\\\\n\\cdots \\\\\na_{m}\n\\end{array} \\right\\rbrack\\)\n\n\n\n(3) 동일 행렬이란\n\n\n행의 차수와 열의 차수가 같다. \\(A_{m \\times n} = B_{m \\times n}\\)\n대응하는 모든 원소 값은 동일하다. \\(\\{ a_{ij} = b_{ij}\\} foralli,j\\)\n\n\n\n2. 특수한 행렬\n\n\n영행렬 zero matrix\n  행렬의 모든 원소가 0인 행렬입니다. 기호 : \\(0_{m \\times n}or0\\) 숫자 0에 해당된다.\n\n\n정방행렬 square matrix\n  행렬의 행차수와 열차수가 동일한 행렬이다. 기호 : \\(A_{m \\times m} = A_{m}\\)\n\n\n대각행렬 diagonal matrix\n  대각원소를 제외한 모든 원소가 0인 정방행렬이다. 기호 : \\(A_{ij} = 0fori \\neq j\\), \\(diag(a_{11},a_{22},...,a_{mm})\\)\n\\[D = \\begin{pmatrix}\n- 1 & 0 \\\\\n0 & 7\n\\end{pmatrix}\\]\n\n\n대각합 trace\n  대각행렬의 대각원소의 합을 대각합이라 한다. \\(tr(D) = 6\\)\n\n\n단위행렬 identity matrix\n  정방행렬의 대각 원소가 모두 1이고 그외 원소는 0인 행렬로 숫자 1과 같은 역할을 한다. 기호 : \\(I_{ij} = \\{\\begin{array}{r}\n1i = j \\\\\n0i \\neq j\n\\end{array}\\) , \\(I_{m \\times m}orI_{m}\\)\n\\(A = \\begin{bmatrix}\n1 & 2 & 3 \\\\\n3 & 4 & 5\n\\end{bmatrix}\\)⇨ \\(A = \\begin{bmatrix}\n1 & 0 & 1 & 2 & 3 \\\\\n0 & 1 & 3 & 4 & 5 \\\\\n0 & 0 & 1 & 0 & 0 \\\\\n0 & 0 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 0 & 1\n\\end{bmatrix} = \\begin{bmatrix}\nI & A \\\\\n0 & I\n\\end{bmatrix}\\)\n\n\n삼각행렬 triangular matrix\n【상삼각행렬】 대각원소 아래 원소가 모두 0인 정방행렬이다. 기호 : \\(A_{ij} = 0fori &gt; j\\)\n【하삼각행렬】 대각원소 윗 원소가 모두 0인 정방행렬이다. 기호 : \\(A_{ij} = 0fori &lt; j\\)\n\n\n희소행렬 Sparse matrices\n행렬 원소의 대부분이 0인 행렬을 의미하며 \\(nnz(A)\\)은 행렬 \\(A_{m \\times n}\\)에서 0인 아닌 원소의 개수를 나타내며 \\(nnz(A)/(m \\times n)\\)을 행렬의 밀도라 정의한다.\n수학자 제임스 H. 윌킨슨(James H. Wilkinson)이 정의 : ”행렬이 충분히 많은 0 원소를 포함하고 있어 이를 활용하는 것이 유리한 경우, 그 행렬을 희소 행렬이라 한다.” 희소행렬은 컴퓨터에서 효율적으로 저장하고 조작할 수 있다.\n영행렬 &gt; 단위행렬 &gt; 대각행렬 &gt; 삼각행렬 : 대표적인 희소행렬\n\n\n3. 행렬 놈\n모든 원소의 제곱합의 양의 제곱근: \\(\\parallel A \\parallel = \\sqrt{\\overset{m}{\\sum_{i}}\\overset{n}{\\sum_{j}}a_{ij}}\\)\n행렬의 놈은 스칼라이며 행렬의 크기나 거리를 측정하며 행렬의 평균제곱근(Root Means Square)는 \\(RMS(A) = \\frac{\\parallel A \\parallel}{\\sqrt{mn}}\\)이다.\n\n\\(\\parallel A \\parallel \\geq 0\\) 행렬 놈은 0보다 크거나 같다.\n\\(\\parallel cA \\parallel = |c| \\parallel A \\parallel\\)\n\\(\\parallel A + B \\parallel \\leq \\parallel A \\parallel + \\parallel B \\parallel\\)\n\\(\\parallel A - B \\parallel\\) : 두 행렬의 유사성(거리)을 나타낸다.\n\\(\\parallel A \\parallel = \\parallel A^{T} \\parallel\\) : 원행렬 놈과 전치행렬 놈은 동일하다.\n\n\n\n4. 전치\n전치 transpose는 행과 열을 서로 바꾸는 연산: \\((A^{T})_{ij} = A_{ji}\\)\n\n\\((A^{T})^{T} = A\\) : 전치 행렬을 다시 전치하면 원래 행렬이 된다.\n\\((A + B)^{T} = A^{T} + B^{T}\\) : 행렬 합의 전치는 각 행렬의 전치 합과 같다.\n\\((cA)^{T} = cA^{T}\\) : 스칼라 곱의 전치는 스칼라 곱과 같다.\n\\((AB)^{T} = B^{T}A^{T}\\) : 행렬 곱의 전치는 각 행렬의 전치의 순서를 바꾼 곱과 같다.\n\n원행렬과 전치행렬과 동일한 행렬은 대칭행렬이다. \\(A = A^{T}\\)\n\n\n\nchapter 2. 행렬 연산\n\n1. 행렬 합 연산\n행렬의 합을 구하는 경우 두 행렬의 차수는 동일해야 하며(conformable for addition/substraction: 합 연산 적합) 각 행렬에서 대응하는 원소들의 합을 그 위치에 적으면 된다.\n\\[(A + B)_{m \\times n} = \\{ a_{ij} + b_{ij}\\}\\]\n\\[(A + B)_{m \\times n} = \\begin{bmatrix}\na_{11} + b_{11} & a_{12} + b_{12} & \\cdots & a_{1n} + b_{1n} \\\\\na_{21} + b_{21} & a_{22} + b_{22} & \\cdots & a_{2n} + b_{2n} \\\\\n\\cdots & \\cdots & \\cdots & \\cdots \\\\\na_{m1} + b_{m1} & a_{m2} + b_{m2} & \\cdots & a_{mn} + b_{mn}\n\\end{bmatrix}\\]\n\\[A = \\begin{bmatrix}\n1 & 3 & 5 \\\\\n7 & 3 & 1\n\\end{bmatrix}$, $B = \\begin{bmatrix}\n1 & 0 & 1 \\\\\n- 1 & 1 & 0\n\\end{bmatrix}$ ⇢ $A + B = \\begin{bmatrix}\n2 & 3 & 6 \\\\\n6 & 4 & 1\n\\end{bmatrix}\\]\n\n\n성질\n\n교환법칙 Commutativity : \\(A + B = B + A\\)\n결합법칙 Associativity : \\(A + (B + C) = (A + B) + C = A + B + C\\)\n영행렬과 합 : \\(A + 0 = 0 + A = A\\)\n합의 전치 : \\((A + B)^{T} = A^{T} + B^{T}\\)\n\n\n\n2. 스칼라-행렬 곱하기\n행렬 모든 원소에 스칼라 곱을 하여 결과는 원행렬과 동일한 차수의 행렬이다. (기호) \\(cA = \\{ ca_{ij}\\} = Ac\\) 다음의 성질을 갖는다.\n\n\\((cA)^{T} = cA^{T}\\)\n\\((c + d)A = cA + dA\\)\n\n\n\n3. 행렬x벡터 곱하기\n행렬 \\(A_{m \\times n}\\)와 행벡터 \\(x_{n}\\) 곱 연산은 다음과 같이 정의되며 결과는 행벡터 \\(y_{m \\times 1} = A_{m \\times n}x_{n \\times 1}\\)이며 차수는 \\(m\\)이다.\n\n\n\n\n\n\n연산 가능\n앞의 행렬(\\(A_{m \\times n}\\))의 열차수와 뒤의 행벡터(\\(x_{n}\\)) 행차수가 동일해야 한다.\n\n\n행 측면\n행렬 \\(A\\)의 \\(i\\)-번째 행벡터을 \\(a_{i}^{T}\\)라 하면 \\(y_{i} = a_{i}^{T}x\\)(내적)이다.\n\n\n열 측면\n\\(A\\)의 \\(k\\)-번째 열벡터을 \\(a_{k}\\)라 하면 \\(y = x_{1}a_{1} + x_{2}a_{2} + + ... + x_{n}a_{n}\\).\n\n\n\n\n\n\n\n행렬 \\(A\\)의 열벡터 선형독립이다\n만약 \\(x = 0\\)인 경우에만 \\(Ax = 0\\)이 성립하면, 열벡터는 선형독립이다.\n\n\n활용\n\n행렬 \\(A\\)가 영행렬이면 \\(Ax = 0\\)는 영벡터이다.\n행렬 \\(A\\)가 단위행렬이면 \\(Ax = x\\)이다.\n행렬 \\(A\\)의 \\(j\\)-번째 열벡터는 \\(Ae_{j} = a_{j}\\)이다.\n행렬 \\(A\\)의 \\(i\\)-번째 행벡터는 \\((A^{T}e_{i})^{T}\\)이다.\n\n\n\n예제\n(예측데이터 행렬) Feature matrix \\(X_{N \\times n}\\)는 \\(N\\)개의 객체에 대한 특성 \\(n\\)-벡터, 객체들에 대한 가중치 \\(w\\)-벡터(차수 \\(N\\))라 하자. \\(X^{T}w\\)는 객체들에 대한 가중 점수 벡터이다.\n(포트폴리오 자산 수익율) 포트폴리오 자산 수익율 행렬 \\(R_{T \\times n}\\)(\\(T\\) 기간 동안 \\(n\\)개의 자산의 수익률)이라 하고 \\(w\\)을 포트폴리오 \\(n\\)-벡터라 하면 \\(Rw\\)는 \\(T\\)기간 포트폴리오 수익률이다.\n(오디오 믹싱) \\(A\\)의 \\(k\\)개 열이 길이 \\(T\\)의 오디오 신호나 트랙을 나타내는 벡터들이고, \\(w\\)가 \\(k\\)-벡터인 경우를 가정하면 \\(Aw\\)는 오디오 신호들을 믹싱한 결과를 나타내는 \\(T\\)-벡터이다.\n(문서 점수화) 검색 엔진은 검색 쿼리를 기반으로 w를 선택하여 문서의 점수를 예측한다. \\(A\\)는 \\(N \\times n\\)크기의 문서-단어 행렬로, \\(N\\)개의 문서가 \\(n\\)개의 단어 사전을 사용하여 단어의 출현 빈도, \\(w\\)는 \\(n\\)-벡터로, 단어 사전 내 단어들에 대한 가중치로 \\(Aw\\)는 \\(N\\)-벡터로, 각 문서의 점수를 나타낸다.\n\n\n4. 행렬x행렬 곱하기\n\n\n(1) 정의\n행렬을 곱하기 위해서는 앞 행렬의 열 차수와 뒤 행렬의 행의 차수와 일치해야 곱이 가능하다. conformable for product 결과의 차수는 앞 행렬의 행 차수, 뒤 행렬의 열 차수를 갖는다.\n\\(A_{m \\times n}B_{n \\times p} = (AB)_{m \\times p}\\)\n\\(A = \\{ a_{ij}\\}\\), \\(B = \\{ b_{ij}\\}\\) ⇢ \\(AB = \\{\\overset{n}{\\sum_{k = 1}}a_{ik}b_{kj}\\}\\)\n\n\n\n\n\n\n\n(2) 곱의 성질\n\n결합 associate 법칙: \\((AB)C = A(BC)\\)\n배분 distribution 법칙: \\(A(B + C) = AB + AC\\)\n전치 : \\((AB)^{T} = B^{T}A^{T}\\)\n\\((A + B)(C + D) = AC + AD + BC + BD\\)\n\\(y^{T}(Ax) = (y^{T}A)x = (A^{T}y)^{T}x\\)\n\n\n\n(3) 행렬의 거듭제곱\n\\[A^{2} = AA$, $A^{3} = AAA$, $A^{4} = AAAA \\cdots \\]\n\n\ndirected graph\n인접 adjacency 행렬을 다음과 같이 정의하자.\n\\[A_{ij} = \\{\\begin{array}{r}\n\\text{1 there is a edge from vertex j to vertex i} \\\\\n\\text{0 otherwise}\n\\end{array}\\]\n\n\n\n\n\n\n\n멱등행렬 idempotent\n자신의 행렬 곱이 자신이 되는 행렬을 멱등행렬이라 한다. \\(M^{2} = M^{3} = ... = M\\) 자신의 곱이 연산 가능해야 하므로 멱등행렬이려면 정방행렬이어야 한다.\n\n\n5. QR 분해, Q는 직교행렬, R은 상삼각행렬\n\n\n(1) 직교행렬 orthonormal matrix\n열벡터 \\(A_{m \\times n}\\)의 n-벡터 \\(a_{1},a_{2},...,a_{m}\\)들이 orthonomal 하면, 즉 \\(A^{T}A = I\\)을 만족하는 행렬을 직교정규행렬이라 한다.만약 \\(A_{m \\times n}\\)는 직교정규행렬, \\(x,y\\)는 n-벡터라 하고 \\(f:R^{n} \\rightarrow R^{m}\\) 함수가 \\(z\\)를 \\(Az\\)로 매핑한다고 가정하자.\n\n\\(\\parallel Ax \\parallel = \\parallel x \\parallel\\) : 함수 \\(f\\)는 놈을 보존한다.\n\\((Ax)^{T}(Ay) = x^{T}y\\) : 함수 \\(f\\)는 두 벡터의 내적을 보존한다.\n\\(\\angle(Ax,Ay) = \\angle(x,y)\\) : 함수 \\(f\\)는 두 벡터의 각도을 보존한다.\n\n\n\n【recall】 Gram-Schmidt 알고리즘\n만약 벡터들이 선형 독립이라면, Gram–Schmidt 알고리즘은 다음과 같은 속성을 가진 직교정규 벡터 \\(q_{1},q_{2},...,q_{k}\\) 을 생성한다.\n\n\n(2) QR분해 \\(A = QR\\)\n행렬 \\(A_{n \\times k}\\)의 n-벡터 \\(a_{1},a_{2},...,a_{k}\\)가 선형 독립인 행렬이다. 여기에 Gram-Schmidt 알고리즘을 적용하여 얻은 직교정규 벡터 \\(q_{1},q_{2},...,q_{k}\\)으로 직교정규 행렬 \\(Q\\)을 생성하자. \\(Q^{T}Q = I\\)이다.\n\\(a_{i}\\)와 \\(q_{i}\\)의 관계식 : \\(a_{i} = (q_{1}^{T}a_{i})q_{1} + \\cdots + (q_{i - 1}^{T}a_{i})q_{i - 1} + \\parallel {\\overset{˜}{q}}_{i} \\parallel q_{i}\\)\n이를 다시 쓰면 \\(a_{i} = R_{1i} + \\cdots + R_{ii}q_{1}\\)이다. \\(R_{ij} = q_{i}^{T}a_{j}fori &lt; j\\), \\(R_{ij} = 0fori &gt; j\\), 그리고\\(R_{ii} = \\parallel {\\overset{˜}{q}}_{i} \\parallel\\)\n그러므로 \\(A_{n \\times k}\\) (열이 독립인 행렬)은 직교정규 행렬 \\(Q_{n \\times k}\\)과 \\(R_{k \\times k}\\) 상삼각행렬로 분해된다.\n\n\n(3) QR 분해 활용\n\n\n선형 시스템의 해 구하기, 최소자승 문제, 정규방정식 문제\n선형 방정식 \\(Ax = b\\)를 푸는 데 사용될 수 있다. \\(A = QR\\)로 분해하면 \\(QRx = b\\)가 되고 \\(R_{x} = Q^{T}b\\)이므로 \\(R\\)이 상삼각 행렬이므로 후진 대입을 사용하여 해, \\(x\\)를 효율적으로 구할 수 있다.\n\n\n고유값 계산\n\\(QR\\) 알고리즘을 이용하여 특정 행렬의 고유값을 계산할 수 있다. \\(QR\\) 분해를 사용한 고유값 계산 알고리즘은 변환 행렬을 상삼각 행렬로 변환하고, 이로부터 고유값을 추출한다.\n\n\n행렬의 특성 분석\n\\(QR\\) 분해는 행렬의 특성을 분석하는 데 도움을 준다. 예를 들어, 행렬의 계수(rank)를 결정하거나, 행렬이 정칙인지 (역행렬이 존재하는지) 파악하는데 사용될 수 있다.\nimport numpy as np\n# 행렬 A 정의\nA = np.array([[1, 1], [1, -1], [1, 1]])\n# QR 분해\nQ, R = np.linalg.qr(A)\n# 결과 출력\nprint(\"Q:\")\nprint(Q)\nprint(\"\\nR:\")\nprint(R)\n【결과】 Q: [[-0.57735027 0.40824829] [-0.57735027 -0.81649658] [-0.57735027 0.40824829]]\nR: [[-1.73205081 -0.57735027] [ 0. 1.63299316]]\n\n\n6. 역행렬\n\n\n(1) 왼쪽 오른쪽 역행렬\n만약 \\(XA = I\\) 만족하는 \\(X\\)가 존재하면 A는 left-invertible 이라 한다. 동일하게 \\(AX = I\\) 만족하는 \\(X\\)가 존재하면 A는 right-invertible 이라 한다.\n\n\nleft-invertible과 열 벡터는 선형독립\n만약 행렬 \\(A\\)가 left-inverse 행렬 \\(C\\) 갖는다면 행렬 \\(A\\)의 열벡터는 선형 독립이다.\n【증명】 \\(Ax = 0\\)을 만족하는 \\(x = 0\\)이므로 \\(A\\)의 열벡터는 선형 독립이다. \\(0 = CAx = Ix = x\\)\n\n\nleft-invertible 행렬(\\(C\\)) 갖는 \\(A\\) 선형방정식 \\(Ax = b\\) 해 구하기\n\\[C_{m \\times m}A_{m \\times n}x_{n} = C_{n \\times n}b_{n} \\rightarrow x_{n} = C_{n \\times n}b_{n}\\]\n\n\nright-invertible과 행 벡터는 선형독립\n만약 행렬 \\(A\\)가 right-inverse 행렬 \\(B\\) 갖는다면 행렬 \\(A\\)의 행벡터는 선형 독립이다.\n\n\nleft, right invertible 관계\n행렬 \\(A\\)의 right inverse \\(B\\)을 가지면 \\(B^{T}\\)는 \\(A^{T}\\)의 left inverse 행렬이다.\n【증명】 \\(AB = I \\rightarrow (AB)^{T} = I^{T} \\rightarrow B^{T}A^{T} = I\\)\n\n\nright-invertible 행렬(\\(B\\)) 갖는 \\(A\\) 선형방정식 \\(Ax = b\\) 해 구하기\n해는 \\(x = Bb\\)이다. 【증명】 \\(Ax = A(Bb) = (AB)b = b\\)\n\n\n(2) 역행렬 구하기\n행렬의 역수 개념이다. 3에 어떤 수를 곱하면 1이 될까? 답은 \\(\\frac{1}{3}\\)(역수)이다. 마찬가지로 행렬 \\(A\\)에 무엇을 곱하면 항등행렬 \\(I\\)가 될까? 이를 역행렬이라 한다. \\(AA^{- 1} = A^{- 1}A = I\\)\n\n\n행렬식 determinant\n행렬식은 정방행렬에서만 계산되며 결과는 스칼라이다. 기호는 \\(det(A)\\)혹은 \\(|A|\\)으로 표현한다. 다음은 행렬식 계산 방법이다.\n\\(A_{2 \\times 2} = \\begin{bmatrix}\na & b \\\\\nc & d\n\\end{bmatrix}\\) ⇢ \\(det(A) = ad - bc\\) \\(A = \\begin{bmatrix}\n1 & 3 \\\\\n2 & 4\n\\end{bmatrix}\\), \\(|A| = - 2\\)\n\n\n\n\n\n\n\n행렬식 성질\n\n\\(|A^{T}| = |A|\\)\n\\(|AB| = |BA|\\)\n\\(|AB| = |A||B|\\)\n한 열에 \\(k\\)배 한 후 다른 열에 더하여도 행렬식은 변하지 않는다.\n한 열이 다른 열의 선형결합으로 표현된다면 행렬식은 0이다.\n\n\n\n소행렬 minor\n\\(i\\)행, \\(j\\)열은 제외한 행렬을 소행렬(\\(M_{ij}\\))이라 하고 소행렬의 행렬식을 소행렬식(\\(|M_{ij}|\\))이라 한다. 일반적으로 소행렬은 소행렬식을 의미한다.\n\n\n\n\n\n\n\n여인수 cofactor\n\n\\(C_{ij} = ( - 1)^{i + j}|M_{ij}|\\)을 여인수라 한다. 여인수를 이용하여 다음과 같이 행렬식을 구할 수 있다.\n\\(|A_{n \\times n}| = \\overset{n}{\\sum_{i = 1}}a_{ij}( - 1)^{i + j}|M_{ij}|\\),\\(|A_{n \\times n}| = \\overset{n}{\\sum_{j = 1}}a_{ij}( - 1)^{i + j}|M_{ij}|\\)\n\n여인수 행렬 / 수반행렬 adjoint\n\\(C_{ij} = \\begin{bmatrix}\nC_{11} & C_{12} & C_{13} \\\\\nC_{21} & C_{22} & C_{23} \\\\\nC_{31} & C_{32} & C_{33}\n\\end{bmatrix}\\)⇢ \\(adj(A) = \\begin{bmatrix}\nC_{11} & C_{21} & C_{31} \\\\\nC_{12} & C_{22} & C_{32} \\\\\nC_{13} & C_{23} & C_{33}\n\\end{bmatrix}\\)\n\n\n역행렬 구하기\n정방행렬 \\(A\\)에 대하여 \\(AB = BA = I\\)을 만족하는 행렬 \\(B\\)를 \\(A\\)의 역행렬이라 하며 \\(A^{- 1}\\)로 표현한다.\n\\[A^{- 1} = \\frac{1}{|A|}adj(A)\\]\n\n\n역행렬 성질\n\n역행렬은 유일하고 \\((A^{- 1})^{- 1} = A\\)이 성립한다.\n\\((AB)^{- 1} = B^{- 1}A^{- 1}\\)\n\\((A^{T})^{- 1} = (A^{- 1})^{T}\\)\n\\(|A^{- 1}| = \\frac{1}{|A|}\\)\n\n\n\n계수 rank\n차수가 \\(n\\)인 정방행렬 \\(A_{n \\times n}\\)의 열벡터에 대하여 \\(k_{1}\\underset{¯}{a_{1}} + k_{2}\\underset{¯}{a_{2}} + ... + k_{n}\\underset{¯}{a_{n}} = \\underset{¯}{0}\\) 방정식이 모든 상수 \\(k_{j}\\)가 0일 때만 만족하는 경우 열벡터(\\(\\underset{¯}{a_{j}}\\))는 선형독립 linearly independent이라 한다. 만약 적어도 0이 아닌 상수가 하나라도 존재하면 종속이라 한다.\n정방행렬 \\(A_{n \\times n}\\)에 대하여 선형 독립인 행의 개수와 열의 개수 중 작은 것을 행렬의 계수라 한다. 행렬의 차수와 계수가 동일하면 이를 full-rank라 한다.\n\n\n행렬 \\(A_{n \\times n}\\)에 대하여 각 열은 동일하다.\n\n\n\n\n\n\n\n역행렬 \\(A^{- 1}\\)은 존재한다.\n역행렬 \\(A^{- 1}\\)은 존재하지 않는다.\n\n\n\n\n행렬식은 0이 아니다. \\(det(A) \\neq 0\\)\n행렬식은 0이다. \\(det(A) = 0\\)\n\n\nfull rank이다. \\(rank(A) = n\\)\nfull rank 아니다. \\(rank(A) &lt; n\\)\n\n\n행렬 A는 non-singular이다.\n행렬 A는 singular이다.\n\n\n\\(AX = \\underset{¯}{b}\\) 해가 존재한다.\n\\(AX = \\underset{¯}{b}\\) 해가 존재하지 않는다.\n\n\n\n\n\n\n\nchapter 3. 행렬 활용\n\n1. 연립방정식 해 구하기 \\(Ax = b\\)\n\n\n(1) \\(QR\\) 분해 이용\n\n행렬 \\(A\\)을 \\(QR\\)분해 한다. \\(A = QR\\)\n\\(Q^{T}b\\)을 구한다.\n후진 제거 방법으로 \\(Rx = Q^{T}b\\)을 구한다.\n\n\n\n(2) 역행렬 계산 \\(A^{- 1}\\)\n행렬 \\(A\\)의 역행렬 \\(A^{- 1}\\)을 이용하여 \\(\\widehat{x} = A^{- 1}b\\) 해를 구한다.\n\n\n2. 최소자승법\n\n\n(1) 최소자승 문제\n\\(A_{m \\times n}x_{n} = b_{m}\\)(단 \\(m &gt; n\\)) 선형방정식에서는 \\(m\\)개의 방정식이 \\(n\\)개 변수보다 많으므로 \\(b\\)가 행렬 \\(A\\)의 열의 선형결합일 때만 해를 갖는다. \\(b\\)을 어떻게 구할 것인가? 잔차 \\(r = Ax - b\\)최소화 하는 \\(x\\)을 찾는 것을 최소자승법이라 한다. \\(minmize \\parallel Ax - b \\parallel\\) \\(2x_{1} = 1, - x_{1} + x_{2} = 0,2x_{2} = - 1\\) : 방정식 3개, 미지수 2개\n\\(Ax = b\\): \\(\\begin{bmatrix}\n2 & 0 \\\\\n- 1 & 1 \\\\\n0 & 2\n\\end{bmatrix}\\left\\lbrack \\begin{array}{r}\nx_{1} \\\\\nx_{2}\n\\end{array} \\right\\rbrack = \\begin{bmatrix}\n1 & 0 & 1\n\\end{bmatrix}\\)\n\n\n(2) 최소자승 해 구하기\n\\(minmizef(x) = \\parallel Ax - b \\parallel^{2}\\) 해 \\(\\widehat{x}\\)는 \\(\\frac{\\partial f}{\\partial x_{i}}(\\widehat{x}) = 0,i = 1,2,...,n\\)을 만족하므로 \\(\\nabla f(x) = 2A^{T}(Ax - b)\\) 방정식에서 \\(\\nabla f(\\widehat{x}) = 0\\)이다. 그러므로 최소자승 해는 \\(\\widehat{x} = (A^{T}A)^{- 1}A^{T}b\\)이다.\n\n\n\n\n\n\n\n\\(A = QR\\) 분해 이용\n\\(Ax = b\\)의 최소자승 해는 \\(\\widehat{x} = R^{- 1}Q^{T}b\\)이다.\n\\[RMS = \\sqrt{\\parallel b - A\\widehat{x} \\parallel^{2}}\\]\n\n\n매출 광고\n행은 사회인구학적 특성 10개이고 열은 3개 광고 채널이고 \\(R_{ij}\\)는 \\(i\\)-사회인구학적특성의 \\(j\\)-광고채널의 1달러당 노출회수(단위: 1000)이다. 만약 각 사회인구학적 특성 집단별로 노출회수를 \\(10^{3}\\)으로 할 경우 광고비는 얼마?\n\n\n\n\n\n\\(R_{10 \\times 3}x_{3} = 10^{3}1_{3}\\)에 대한 최소자승해는 \\(\\widehat{x} = (62,100,1443)\\)으로 각 채널당 광고비이다. \\(RMS = 13.2\\%\\)이다.\n\n\n(3) 최소자승 데이터 적합\n\\(n\\)-벡터 \\(x\\)(feature 벡터, 독립변수), 스칼라 \\(y\\)는 다음 근사 함수 관계가 있다고 하자. \\(f:R^{n} \\rightarrow R,y \\approx f(x)\\)\n\n\n데이터\n\\[x^{(1)},x^{(2)},...,x^{(N)},y^{(1)},y^{(2)},...,y^{(N)}\\]\n\n\n모델 관측치 개수 \\(N\\), 예측변수 개수 \\(p\\)\nfeature 벡터와 스칼라 벡터 사이 함수 관계는 \\(f\\)(예측함수)은\\(y \\approx \\widehat{f}(x),where\\widehat{f}:R^{n} \\rightarrow R\\)\n\\(\\widehat{f}(x)\\)는 파라미터 \\(p\\)-벡터 \\(\\theta\\)의 선형 함수이다.\n\\(\\widehat{f}(x) = \\theta_{1}f_{1}(x) + \\theta_{2}f_{2}(x) + \\cdots + \\theta_{p}f_{p}(x)\\), where \\(f_{i}:R^{n} \\rightarrow R\\)\n\n\n예측값과 예측오차\n\\(y^{(i)} \\approx \\widehat{f}(x^{(i)})\\)이고 예측오차(잔차)는 \\(r^{(i)} = y^{(i)} - {\\widehat{y}}^{(i)}\\)이다.\n\n\n최소자승 모델 적합\n\\(i = 1,2,\\cdots,N,j = 1,2,\\cdots,p\\)\n\\(y^{d} = (y^{(1)},y^{(2)},...,y^{(N)})\\), \\({\\widehat{y}}^{d} = ({\\widehat{y}}^{(1)},{\\widehat{y}}^{(2)},...,{\\widehat{y}}^{(N)})\\)\n예측오차합 \\(\\parallel r^{d} = y^{d} - {\\widehat{y}}^{d} \\parallel^{2}\\)을 최소화 하는 모수 \\(\\theta\\)을 찾는다.\n\\[{\\widehat{y}}^{(i)} = A_{i1}\\theta_{1} + A_{i1}\\theta_{2} + \\cdots + A_{i1}\\theta_{p},whereA_{ij} = {\\widehat{f}}_{j}(x^{(i)})\\]\n\\({\\widehat{y}}^{d} = A\\theta\\)이므로 \\(\\parallel r^{d} \\parallel^{2} = \\parallel y^{d} - A\\theta \\parallel^{2}\\)이다.\n최소자승 추정 : \\(\\widehat{\\theta} = (A^{T}A)^{- 1}A^{T}y^{d}\\)\n\n\n상수항(절편) 있는 선형함수 최소자승 추정\n모든 \\(x\\)에 대하여 \\(f_{1}(x) = 1\\)을 갖는 상수함수를 고려하자. \\(\\widehat{f}(x) = \\theta_{1}\\)이고 \\(A_{(N \\times 1)} = 1_{N}\\)이다.\n\\[\\widehat{\\theta} = (A^{T}A)^{- 1}A^{T}y^{d} = N^{- 1}1^{T}y^{d} = avg(y^{d})\\]\n\n\n(4) 다항식 적합\n\n\n모형\n\\(\\widehat{f}(x) = \\theta_{1} + \\theta_{2}x + \\cdots + \\theta_{p}x^{p - 1}\\)\n\\[A = \\begin{bmatrix}\n1 & x^{(1)} & \\cdots & (x^{(1)})^{p - 1} \\\\\n1 & x^{(2)} & \\cdots & (x^{(2)})^{p - 1} \\\\\n\\cdots & & & \\\\\n1 & x^{(N)} & \\cdots & (x^{(N)})^{p - 1}\n\\end{bmatrix}\\]\n\n\nPiecewise-Linear Fit 분절선형 적합\n절단점 식별: 선의 기울기가 변하는 지점을 결정한다.\n선형 구간 적합: 절단점으로 분리된 각 데이터 구간에 선형 모델을 적합한다.\n구간 결합: 절단점에서 구간함수를 연결하여 연속적인 분절선형 함수를 형성한다.\n\n\n\n\n\n# Piecewise-Linear Fit\nimport numpy as np\n# 합성 데이터 생성\nnp.random.seed(0)\nx = np.linspace(0, 10, 100)\ny = np.piecewise(x, [x &lt; 4, (x &gt;= 4) & (x &lt; 7), x &gt;= 7],[lambda x: 2 * x + 1 + np.random.normal(size=len(x)),lambda x: -x + 5 + np.random.normal(size=len(x)),lambda x: 0.5 * x - 1 + np.random.normal(size=len(x))])\nimport matplotlib.pyplot as plt\nfrom scipy.optimize import curve_fit\n# 분절선형 함수 정의\ndef piecewise_linear(x, x0, x1, y0, y1, y2, k1, k2, k3):\n    conds = [x &lt; x0, (x &gt;= x0) & (x &lt; x1), x &gt;= x1]\n    funcs = [lambda x: k1 * x + y0, lambda x: k2 * x + y1, lambda x: k3 * x + y2]\n    return np.piecewise(x, conds, funcs)\n# 초기 파라미터 추정값\np0 = [4, 7, 1, 5, -1, 2, -1, 0.5]\n# 데이터를 분절선형 함수에 적합시킴\nparams, _ = curve_fit(piecewise_linear, x, y, p0=p0)\n# 데이터를 적합한 결과와 함께 플로팅\nx_fit = np.linspace(0, 10, 100)\ny_fit = piecewise_linear(x_fit, *params)\n\nplt.scatter(x, y, label='Data')\nplt.plot(x_fit, y_fit, color='red', label='Piecewise Linear Fit')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.legend()\nplt.show()\n\n\n3. 간선행렬\n간선 행렬 Incidence matrix은 그래프 이론에서 사용되는 개념으로, 정점과 vertices 간선 edges, nodes 사이의 관계를 나타내는 행렬입니다.\n간선 행렬 \\(G_{n \\times m}\\)은 정점이 \\(n\\)개, 간선이 \\(m\\)개이다.\n\\(A_{ij} = 1\\) : 정점 \\(i\\)와 간선 \\(j\\)와 연결되어 있고 정점 \\(i\\)는 끝 정점이 아니다.\n\\(A_{ij} = 1\\) : 정점 \\(i\\)와 간선 \\(j\\)와 연결되어 있고 정점 \\(i\\)는 끝 정점이다.\n\\(A_{ij} = 0\\) : 정점 \\(i\\)와 간선 \\(j\\)와 연결되어 않음\n\n\n\n\n\n\n\n4. 네트워크\n만약 \\(x\\)가 네트워크에서의 흐름을 나타내는 \\(m\\)-벡터라면, \\(x_{j}\\)는 간선 \\(j\\)를 통한 흐름으로 해석된다. 여기서 양의 값은 흐름이 간선 \\(j\\)의 방향으로 이동하고, 음의 값은 흐름이 간선 \\(j\\)의 반대 방향으로 이동함을 의미한다. 네트워크에서 간선이나 링크의 방향은 흐름의 방향을 지정하지 않고 그저 흐름 flow의 방향을 고려하는 것을 나타내는 것이다.\n네트워크에서의 흐름 보존은 흐름이 노드와 간선을 통해 어떻게 이동하는지를 설명하며, 각 노드로 들어오는 총 흐름이 노드에서 나가는 총 흐름과 같음을 보장한다.\n네트워크 구조를 나타내는 \\(G_{n \\times m}\\)를 사용하여\n\\(y = Gx\\)는 각 노드로 들어오는 순흐름을 나타내는 \\(n\\)-벡터이다.\n\\(y_{i}\\)는 \\(i\\)-노드로 들어오는 총 흐름에서 \\(i\\)-노드에서 나가는 총 흐름을 뺀 값이다 즉, \\(i\\)-노드에서의 흐름 잉여 surplus이다.\n요약하면, \\(y = Gx\\)는 네트워크 이론에서의 흐름 보존 원칙을 요약한 것으로, 각 요소 \\(y_{i}\\)는 노드 \\(i\\)에서의 순 흐름 균형을 나타내며 모든 들어오는 흐름과 나가는 흐름을 고려한다.\n만약 \\(Gx = 0\\)인 상태를 각 노드에서 총 들어오는 흐름과 총 나가는 흐름이 일치하기 때문에 흐름 보존이 일어난다고 말한다.\n\n\n\n\n\n위의 그래프에 의해 나타낸 네트워크에서 \\(x = (1, - 1,1,0,1)\\)이다. 소스는 source 노드에서 네트워크로 들어오거나 나가지만, 간선을 따라 흐르지는 않습니다. 위 그림에서 보여지는 것처럼 이러한 흐름들은 5-벡터 4소스로 나타낸다. \\(s_{i}\\)를 노드 \\(i\\)에서 외부에서 네트워크로 들어오는 흐름으로 생각할 수 있다. 즉, 어떤 간선을 통해서도 들어오지 않는 것이다. \\(s_{i} &gt; 0\\)일 때 외부흐름은 소스라고 부르며 \\(s_{i} &lt; 0\\)일 때 외부흐름은 싱크라고 부른다.\n소스 포함된 흐름 보전 : \\(Ax + s = 0\\)\n\n\n5. 선형함수 모델\n필드에서 발생하는 많은 함수나 변수 간의 관계는 선형 또는 아핀 함수로 근사될 수 있는데, 두 변수 집합 간의 선형 함수를 모형(model) 또는 근사(approximation) 값으로 정의한다.\n\n\n(1) 수요의 가격 탄력성(Price elasticity of demand)\n가격이 n개의 상품(서비스)에 의해 결정되는 n-벡터 p로 주어지고, 상품에 대한 수요가 n-벡터 d로 주어진다. n-벡터 \\(\\delta^{price}\\)를 가격변화 벡터라 하면 \\(\\delta^{price} = \\frac{(p_{i}^{new} - p_{i})}{p_{i}}\\)라 하자(\\(p^{new}\\)는 새로운 가격 n-벡터). n-벡터 \\(\\delta^{dem}\\)를 수요변화 벡터라 하면 \\(\\delta^{dem} = \\frac{(d_{i}^{new} - d_{i})}{d_{i}}\\)라 하자. \\(\\delta^{dem} = E^{d}\\delta^{price}\\), \\(E^{d}\\)는 (\\(n \\times n\\)) 수요 탄력성 행렬이다.\n\\(E_{11}^{d} = - 0.4\\), \\(E_{21}^{d} = 0.2\\) 가정해 보자. 이는 첫 번째 상품의 가격이 1% 증가할 때, 다른 가격은 동일한 상태에서 첫 번째 상품의 수요가 0.4% 감소하고, 두 번째 상품의 수요가 0.2% 증가할 것임을 의미한다. 두 번째 상품은 첫 번째 상품의 부분 대체품으로 작용하고 있다.\n\n\n(2) 탄성 변형 Elastic deformation\nf 를 구조물에 작용하는 특정 위치(및 방향)에 대한 힘(하중)을 나타내는 n-벡터라고 합시다. 구조물은 하중으로 인해 약간 변형될 것입니다. d는 하중으로 인해 구조물의 m개 지점에서 발생하는 변위(특정 방향으로)를 나타내는 m-벡터입니다. 변위와 하중 사이의 관계는 선형으로 잘 근사된다. d= Cf 여기서 C 는 m × n 컴플라이언스(compliance) 행렬이고 C 의 항목의 단위는 m/N입니다.\n\n\n(3) 테일러 근사\n함수 \\(f:R^{n} \\rightarrow R^{n}\\)이 1차 미분이 가능하다고 하면 테일러 근사는 \\(\\widehat{f}(x)_{i} = f_{i}(z) + \\triangledown f_{i}(z)^{T}(x - z)\\), 단 n-벡터 \\(z\\)는 n-벡터 \\(x\\)와 가까운 값이다.\n\\(\\widehat{f}(x) = f(z) + Df(z)(x - z)\\), 단.\\(Df(z)_{ij} = \\frac{\\partial f_{i}}{\\partial x_{i}}(z),i = 1,...,m,j = 1,...,n\\)\n\n\n(4) 회귀모형\n표본 크기 \\(N\\), 예측변수 벡터 \\(x^{(1)},x^{(2)},...,x^{(N)}\\)이다. \\(i\\)-개체의 예측치는 \\({\\widehat{y}}^{(i)} = (x^{(i)})^{T}\\beta + v,i = 1,2,...,N\\)이다. 그리고 \\(X\\)는 예측변수 행렬, \\(y\\)는 목표변수 벡터이다.\n\n잔차는 \\(r^{(i)} = y^{(i)} - {\\widehat{y}}^{(i)}\\).\n절편 없는 회귀모형 : \\({\\widehat{y}}^{d} = X^{T}\\beta + v1\\)\n절편 회귀모형 : \\({\\widehat{y}}^{d} = \\left\\lbrack \\begin{array}{r}\n1^{T} \\\\\nX\n\\end{array} \\right\\rbrack^{T}\\left\\lbrack \\begin{array}{r}\nv \\\\\n\\beta\n\\end{array} \\right\\rbrack\\)\n\n\n\n6. 선형 동적 시스템\n시간에 따라 변하는 상태 벡터의 선형 관계를 설명하는 모델로 시스템의 현재 상태가 다음 상태를 예측할 수 있는 간단한 수학적 구조이다. \\(x_{t}\\)가 현재 상태인 \\(x_{1},x_{2},\\cdots\\) n-벡터 시계열이라 하자. 예를 들면, \\((x_{5})_{3}\\) 3번째 포트폴리오의 5일째 주가가 된다.\n\n\n(1) 입력이 포함된 선형 동적 시스템\n\\[x_{t + 1} = A_{t}x_{t} + B_{t}u_{t},t = 1,2,...\\]\n\\(u_{t}\\) 는 시간 t 에서의 입력벡터이고 .B 는 입력행렬로, 입력 \\(u_{t}\\)(외생 변수라고도 함)가 상태 벡터 \\(x_{t}\\)에 미치는 영향을 설명한다.\n\n\n(2) \\(K\\)-Markov 모형\n\\[x_{t + 1} = A_{1}x_{t} + \\cdots + A_{K}x_{t - K + 1},t = K,K + 1,...\\]\n\n상태 State : 시스템이 존재할 수 있는 모든 가능한 상태들의 집합. 예를 들어, 날씨 예측 모델에서 상태는 ”맑음”, ”흐림”, ”비” 등이 될 수 있다. 시스템이 가질 수 있는 모든 상태들의 집합을 상태 공간 \\(S\\)라 한다.\n상태 전이 State Transition : 한 상태에서 다른 상태로의 전이. 상태 전이는 확률적으로 이루어지며 \\(P_{i}\\)는 초기상태 확률분포이다.\n전이 확률 Transition Probability : 현재 상태에서 다음 상태로 전이될 확률을 나타낸다. 이는 \\(P(x_{t + 1} = s_{j}|x_{t} = s_{i})\\)로 표현되며, 현재 상태 \\(i\\)에서 다음 시점에 상태 \\(j\\)로 전이될 확률이다.\n\n# Markov model\nimport numpy as np\n# 전이 행렬 정의\nP = np.array([[0.8, 0.2],[0.4, 0.6]])\n# 초기 상태 분포 정의\npi_0 = np.array([0.6, 0.4])\n# 상태 이름 정의\nstates = [\"Sunny\", \"Rainy\"]\n# 시뮬레이션을 위한 시간 단계 수\nnum_steps = 10\n# 초기 상태 선택\ncurrent_state = np.random.choice(states, p=pi_0)\nprint(f\"Day 0: {current_state}\")\n# 시뮬레이션 시작\nfor t in range(1, num_steps + 1):\n    if current_state == \"Sunny\":\n        next_state = np.random.choice(states, p=P[0])\n    else:\n        next_state = np.random.choice(states, p=P[1])\n    print(f\"Day {t}: {next_state}\")\n    current_state = next_state\n\n\n\n\n\n\n\n7. 인구 동태\n100-벡터 \\((x_{t})_{i}\\)는 \\(t\\) 시점의 \\((i - 1)\\)세 인구이다. 100- 벡터 \\(b\\)의 \\(b_{i}\\)는 \\((i - 1)\\)의 평균 출생율이다. 가임 연령을 고려하면 벡터 b의 원소는\\(b_{I} = 0fori &lt; 13ori &gt; 50\\)이다. 만약 사망, 이민 없다고 가정하면 내년 0세 인구는 \\((x_{t + 1})_{1} = b^{T}x_{t}\\)이다.\n나이 \\(i\\)세 \\((t + 1)\\) 시점의 인구수는 다음과 같다. \\(d_{i}\\)는 \\(i\\)세 사망자수이다.\\((x_{t + 1})_{i + 1} = (1 - d_{i})(x_{t})_{i},i = 1,2,\\cdots,99\\). 최종적으로 인구 동태 모형은 \\(x_{t + 1} = Ax_{t},t = 1,2,\\cdots\\)이다.\n\n\n전이행렬 \\(A\\)\n\\[A = \\begin{bmatrix}\nb_{1} & b_{2} & b_{3} & \\cdots & b_{98} & b_{99} & b_{100} & \\\\\n1 - d_{1} & 0 & 0 & \\cdots & 0 & 0 & 0 & \\\\\n0 & 1 - d_{2} & 0 & \\cdots & 0 & 0 & 0 & \\\\\n\\cdots & \\cdots & \\cdots & \\cdots & \\cdots & \\cdots & \\cdots & \\\\\n0 & 0 & 0 & \\cdots & 1 - d_{98} & 0 & 0 & \\\\\n0 & 0 & 0 & \\cdots & & 0 & 1 - d_{99} & 0\n\\end{bmatrix}\\]\n\n\n이민을 고려한 인구 동태 모형\n\\(x_{t + 1} = Ax_{t} + u_{t},t = 1,2,\\cdots\\), 벡터 \\((u_{t})_{i}\\)는 t-시점에 나이 \\((i - 1)\\)세의 순이민자수이다.\n\n\n간단한 인구동태 방정식\n\\[P_{t + 1} = P_{t} + (B_{t} - D_{t}) + M_{t}\\]\n\\(P_{t}\\) : \\(t\\) 시점의 인구수, \\(B_{t}\\) : \\(t\\) 시점의 출생자수, \\(D_{t}\\) : \\(t\\) 시점의 사망자수, \\(M_{t}\\) : \\(t\\) 시점의 순 이민자수\n# 인구동태모형\nimport numpy as np\nimport matplotlib.pyplot as plt\n# 초기 인구와 파라미터 설정 미국 23년 기준\ninitial_population = 330_000_000\nbirth_rate = 12.4 / 1000\ndeath_rate = 8.9 / 1000\nannual_net_migration = 1_000_000\nyears = 10\n# 인구 예측을 위한 배열 초기화\npopulation = np.zeros(years + 1)\npopulation[0] = initial_population\n# 연도별 인구 예측\nfor t in range(1, years + 1):\n    births = population[t - 1] * birth_rate\n    deaths = population[t - 1] * death_rate\n    population[t] = population[t - 1] + births - deaths + annual_net_migration\n# 결과 출력\nfor t in range(years + 1):\n    print(f\"Year {2023 + t}: {population[t]:,.0f}\")\n【결과】 Year 2024: 332,155,000 Year 2025: 334,317,542 Year 2026: 336,487,654 Year 2027: 338,665,361 Year 2028: 340,850,689 Year 2029: 343,043,667 Year 2030: 345,244,320 Year 2031: 347,452,675 Year 2032: 349,668,759Year 2033: 351,892,600\n\n\n8. 전염병 동태\n전염 역할 모델른 전염병의 전파와 확산을 연구하는 분야로, 이는 질병의 전염 방식과 전파 속도를 이해하고 예측하는 데 중점을 둔다.\n\n\n\\(SIRD\\) 모델 상태\n\\(x_{t} = (S,I,R,D),whereS + R + I + D = 1\\)\n\n감염 가능성 Susceptible (S): 현재는 비감염이지만 내일에는 질병에 감염될 수 있는 사람들\n감염 Infected (I): 현재 질병에 감염된 사람들.\n회복 Recovered (R): 질병을 회복하고 면역을 획득한 사람들.\n사망 Deceased (D): 질병으로 사망한 사람들.\n\n\n\n약학 모델 동력학\n\\(\\beta\\) : 감염 가능성에서 감염으로 전환될 감염율, \\(\\gamma\\) : 감염에서 회복으로 전화되는 회복율 \\(\\mu\\) : 감염에서 사망으로 전환되는 사망율이라면\n\\[\\begin{matrix}\n& \\frac{dS}{dt} = - \\beta SI,\\frac{dI}{dt} = - \\beta SI - \\gamma I\\mu I \\\\\n& \\frac{dR}{dt} = - \\gamma I,\\frac{dD}{dt} = \\mu I\n\\end{matrix}\\]\n\n\n사례연구\n만약 t기의 SIRD 벡터가 \\(x_{t} = (0.99,0.01,0,0)\\)라 하자. 그리고 감염 가능성 있는 인구 중 30%(\\(\\beta = 0.3\\))는 전염되고 전염자의 2%(\\(\\mu = 0.02\\))는 사망하고 회복율은 10%(\\(\\gamma = 0.1)\\)이라 하자. 그러므로 전염 상태로 남아 있는 전염자는 88%이다.\n\\(x_{t + 1} = Ax_{t}\\) 모형에서 \\(A = \\begin{bmatrix}\n0.99 & 0.1 & 0 & 0 \\\\\n0.01 & 0.88 & 0 & 0 \\\\\n0 & 0.1 & 1 & 0 \\\\\n0 & 0.02 & 0 & 1\n\\end{bmatrix}\\)\n# 전염병 동태모델 사례\nimport numpy as np\nfrom scipy.integrate import odeint\nimport matplotlib.pyplot as plt\n# 초기 조건\nS0 = 0.99   # 초기 감수성 인구 비율\nI0 = 0.01   # 초기 감염 인구 비율\nR0 = 0.0    # 초기 회복 인구 비율\nD0 = 0.0    # 초기 사망 인구 비율\ninitial_conditions = [S0, I0, R0, D0]\n# 파라미터\nbeta = 0.3   # 전염율\ngamma = 0.1  # 회복율\nmu = 0.02    # 사망율\n# SIRD 모델 미분 방정식\ndef sird_model(y, t, beta, gamma, mu):\n    S, I, R, D = y\n    dS_dt = -beta * S * I\n    dI_dt = beta * S * I - gamma * I - mu * I\n    dR_dt = gamma * I\n    dD_dt = mu * I\n    return [dS_dt, dI_dt, dR_dt, dD_dt]\n# 시간 벡터 (일 단위)\nt = np.linspace(0, 160, 160)\n# ODE 풀기\nsolution = odeint(sird_model, initial_conditions, t, args=(beta, gamma, mu))\nS, I, R, D = solution.T\n# 결과 그래프 출력\nplt.figure(figsize=(10, 6))\nplt.plot(t, S, label='Susceptible')\nplt.plot(t, I, label='Infected')\nplt.plot(t, R, label='Recovered')\nplt.plot(t, D, label='Deceased')\nplt.xlabel('Time (days)')\nplt.ylabel('Proportion of Population')\nplt.legend()\nplt.title('SIRD Model')\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nchapter 4. 고유치와 고유벡터\n\n1. 기초\n\n\n(1) 개념\n고유치는 행렬의 선형변환에서 중요한 특성을 나타내는 값이다. 특정 벡터(고유벡터)가 행렬 \\(A\\)에 의해 변환될 때, 방향은 변하지 않고 크기만 일정 비율로 변한다면, 이 비율을 고유치라고 한다.\n\n\n\n\n\n위 그래프는 행렬 \\(A = \\begin{bmatrix}\n3 & 1 \\\\\n0 & 2\n\\end{bmatrix}\\)의 고유치(\\(\\lambda = 3,2\\))와 고유벡터의 변환을 시각적으로 보여준다.\n\n빨간색 화살표: 첫 번째 고유벡터 \\(\\mathbf{v}_{1}\\)\n투명 빨간색 화살표: 첫 번째 고유벡터가 행렬 \\(A\\)에 의해 변환된 결과로, 고유치 \\(\\lambda_{1} = 3\\)에 의해 크기만 3배로 늘어난다.\n파란색 화살표: 두 번째 고유벡터 \\(\\mathbf{v}_{2}\\).\n투명 파란색 화살표: 두 번째 고유벡터가 행렬 A 에 의해 변환된 결과로, 고유치 \\(\\lambda_{2} = 2\\)에 의해 크기만 2배로 늘어난다.\n\n고유벡터의 방향은 행렬 변환 후에도 유지되며, 크기만 고유치 값에 따라 변한다. 이를 통해 고유치와 고유벡터의 개념을 시각적으로 이해할 수 있다.\n\n\n(2) 통계학 활용\n고유치 분석을 통해 얻을 수 있는 통계적 통찰은 다음과 같다.\n\n데이터의 분산 설명: 공분산 행렬의 고유치는 각 축의 분산 크기를 나타내며, 데이터가 어떤 축에서 더 많은 정보를 가지고 있는지 보여준다.\n중요한 변수 식별: PCA나 LDA에서 고유치를 사용해 데이터를 가장 잘 설명하는 주성분이나 판별 방향을 찾는다.\n데이터의 차원 축소: 가장 큰 고유치를 가진 축만 선택함으로써 데이터의 복잡성을 줄이고, 분석의 효율성을 높는다.\n시각화: MDS, PCA를 활용해 고차원 데이터를 저차원으로 투영하여 시각화할 수 있는다.\n\n\n\n주성분 분석(PCA, Principal Component Analysis)\nPCA는 데이터의 고차원 공간을 낮은 차원으로 축소하면서 데이터의 주요 정보를 보존하는 방법이다.\n\n데이터의 공분산 행렬에서 고유치를 계산하여 주성분의 중요도를 평가한다.\n가장 큰 고유치는 데이터의 분산을 가장 많이 설명하는 방향(주성분)을 나타낸다.\n예: 변수 100개로 구성된 데이터를 분석할 때, 고유치를 계산하여 주요한 2~3개의 주성분만 선택해 데이터 차원을 축소할 수 있다.\n\n\n\n선형 판별 분석(LDA, Linear Discriminant Analysis)\nLDA는 여러 클래스 간의 분산을 극대화하면서 각 클래스 내의 분산을 최소화하는 투영 방향을 찾는 방법이다.\n클래스 간 분산 행렬과 클래스 내 분산 행렬의 비율로 구성된 행렬의 고유치를 계산하여 최적의 분리 축을 결정한다.\n\n\n다차원 척도법(MDS, Multidimensional Scaling)\nMDS는 데이터 간의 거리 행렬을 기반으로 저차원 공간에 데이터를 시각화하는 방법이다.\n\n거리 행렬을 고유치 분해하여 데이터를 저차원 공간에 배치한다.\n가장 큰 고유치를 가진 방향이 데이터 구조의 주요 변화를 설명한다.\n\n\n\n공분산 행렬 및 상관 행렬 분석\n공분산 행렬이나 상관 행렬의 고유치는 데이터의 선형 독립성과 분산 구조를 분석하는 데 사용된다.\n\n고유치가 큰 방향은 데이터의 분산이 큰 축(정보가 많이 분포된 축)을 나타낸다.\n고유치가 0에 가까운 경우 변수들 간의 선형 종속성을 암시한다.\n\n\n\n행렬 분해 및 차원 축소\n고유치와 고유벡터는 행렬 분해 방법(예: 특이값 분해(SVD), 고유분해(Eigendecomposition))의 핵심이다.\n\n차원 축소, 데이터 압축, 노이즈 제거 등에 사용된다.\n예: 특이값 분해(SVD)는 추천 시스템이나 텍스트 분석(Latent Semantic Analysis, LSA)에서 널리 사용된다.\n\n\n\n시계열 데이터 분석 Autoregressive 모델(AR)\n시계열 모델에서 안정성을 분석할 때, 고유치를 통해 시스템의 특성을 평가한다. 예: 고유치가 1보다 크면 시스템이 불안정함을 나타낸다.\n\n\n2. 고유치, 고유벡터 구하기\n대칭행렬 \\(A_{n \\times n}\\)에 대하여 고유치 \\(\\lambda\\), 고유벡터 \\(\\underset{¯}{v}\\)는 다음 방정식이 성립한다. \\(A\\underset{¯}{v} = \\lambda\\underset{¯}{v}\\)\n\n\n(1) 고유치 eigenvalue 구하기\n\\(det(A - \\lambda I) = 0\\)을 만족하는 \\(\\lambda\\)를 고유치라 한다.\n고유치는 행렬 \\(A\\)의 차수만큼 존재한다. \\(\\lambda_{1},\\lambda_{2},...,\\lambda_{n}\\)\n\n\n(2) 고유벡터 eigenvector 구하기\n\\(A\\underset{¯}{v_{i}} = \\lambda_{i}\\underset{¯}{v_{i}}\\) 을 만족하는 벡터(\\(\\underset{¯}{v}\\))를 고유벡터라 한다.\n\\(det(A - \\lambda I) = 0\\)(singlular)가 성립하므로 고유벡터는 무수히 많이 존재한다.\n고유벡터 중 Norm(\\(\\underset{¯}{v}'\\underset{¯}{v} = 1\\))이 1인 고유 벡터를 주성분분석에서 사용한다.\n\n\n3. 고유치 활용\n\n\n(1) 고유치 분해 eigenvalue decomposition\n정방행렬 \\(A_{n \\times n}A\\)의 고유치(\\(\\lambda_{i}\\))를 대각원소로 하는 대각행렬 \\(\\Lambda\\), 고유벡터(\\(\\underset{¯}{v_{i}}\\))로 이루어진 직교 orthogonal 행렬 \\(Q\\)라 하면 행렬 \\(A\\)는 다음과 같이 고유치 분해 된다. \\(A = Q\\Lambda Q^{- 1}\\)\n\n\n(2) 주성분분석\n데이터 행렬 : \\(X_{n \\times p} = \\begin{bmatrix}\nx_{11} & x_{12} & \\cdots & x_{1p} \\\\\nx_{21} & x_{22} & \\cdots & x_{2p} \\\\\n\\cdots & \\cdots & \\cdots & \\cdots \\\\\nx_{n1} & x_{n2} & \\cdots & x_{np}\n\\end{bmatrix}\\) (변수 개수 \\(p\\))\n\n\\(\\underset{¯}{y} = P\\underset{¯}{x}\\) : 원 변수의 선형결합(선형계수 행렬은 고유벡터)으로 주성분변수를 만든다.\n\\(X'X\\) 고유치분해 : \\(X'X = (Q\\Lambda Q^{- 1})'(Q\\Lambda Q^{- 1}) = Q\\Lambda Q^{- 1}\\)\n\\(X\\)의 공분산행렬(측정 단위가 다른 경우 상관계수 행렬)로부터 고유치와 고유벡터(Norm=1인 정규고유벡터)를 구하여 서로 독립인 차원으로 변환한다.\n공분산행렬에 대한 고유치, 고유벡터 : \\(COV_{p \\times p}\\underset{¯}{v} = \\lambda\\underset{¯}{v}\\)\n공분산 행렬은 양의 정부호 행렬이므로 변수의 차수만큼의 고유치, 그에 대응하는 고유벡터가 존재한다.\n고유벡터는 원변수를 직교 축을 갖는 주성분 변수로 변환한다. 그러므로 차수는 줄어들지 않으나 모든 차원에서 관측값은 직교(독립)이다.\n주요 2~3개 차원만으로 \\(p\\)차원의 원변수 변동(정보)를 축약한다. 이를 주성분분석이라 한다.\n\n\n\n\n\n\n\n\n(3) 특이값 분해 Singular Value Decomposition\n\n\n\n\n\n\n직교행렬 \\(U\\)(\\(UU' = I\\)) : \\(AA'\\)의 고유벡터\n직교행렬 \\(V'\\)(\\(V'V = I\\)) : \\(A'A\\)의 고유벡터\n대각행렬 \\(\\Sigma\\)의 대각원소 : \\(AA'\\), \\(A'A\\)의 고유치분해 대각원소의 제곱근 값을 대각원소로 한다.\n\n\n\n(4) Cholesky factorization\n대칭행렬 \\(A\\)가 양의 정부호 행렬일 경우 사용되는 분해방법이다.\n\\(A = LL^{T}\\), \\(L\\) : 대각원소가 양이 하단 삼각행렬\n【활용】 최소제곱추정과 같은 최적해를 구할 때 사용하면 빠른 연산이 가능하다. \\(A\\underset{¯}{x} = \\underset{¯}{b}\\) (연립방정식) \\(\\underset{¯}{x} = A^{- 1}\\underset{¯}{b}\\) ➠ \\(LL^{T}\\underset{¯}{x} = \\underset{¯}{b}\\) 이것을 풀면 연산이 더 간편하다. \\(\\underset{¯}{x} = (LL^{T})^{- 1}\\underset{¯}{b} = (L^{- 1})'L^{- 1}\\underset{¯}{b}\\)\n#고유치, 고유벡터\nimport numpy as np\nA=np.array([[1,2,3], [4,5,7],[8,9,10]])\nimport numpy.linalg as la\nval,vec=la.eig(A)\nval,vec\n【결과】 (array([17.71571559, -1.44163052, -0.27408507]), array([[-0.21078452, -0.49872133, 0.47929184], [-0.52147269, -0.47685414, -0.81047488], [-0.82682291, 0.7238005 , 0.33676373]]))\n#고유벡터 분해\nimport numpy as np\nA=np.array([[1,2,3], \n  [4,5,7],\n  [8,9,10]])\nimport numpy.linalg as la\nval,vec=la.eig(A)\nS=np.diag(val); P=vec\nP@S@la.inv(P)\n【결과】 array([[ 1., 2., 3.], [ 4., 5., 7.], [ 8., 9., 10.]])\n#SVD decomposition\nu, s, vh = np.linalg.svd(A, full_matrices=True)\nu,s,vh\n【결과】 (array([[-0.19462586, -0.6193003 , -0.76064966], [-0.5071685 , -0.6002356 , 0.61846369], [-0.83958376, 0.50614657, -0.19726824]]), array([18.62202941, 1.46779937, 0.25609691]), array([[-0.48007495, -0.56284671, -0.67285334], [ 0.70100172, 0.21497525, -0.67998694], [ 0.52737523, -0.79811604, 0.29135228]]))\n#Cholesky decomposition\nimport numpy as np\nA=np.array([[25,15,-5], \n  [15,18,0],\n  [-5,0,11]])\nimport numpy.linalg as la\nnp.linalg.cholesky(A)\n【결과】 array([[ 5., 0., 0.], [ 3., 3., 0.], [-1., 1., 3.]])\n#확인 LL'\nnp.linalg.cholesky(A)@np.linalg.cholesky(A).T\n【결과】 array([[25., 15., -5.], [15., 18., 0.], [-5., 0., 11.]])\n\n\n\nchapter 5. 행렬미분\n\n1. 미분 공식\n\n\n(1) 벡터미분\n상수벡터 : \\({\\underset{¯}{a}}_{n} = \\left\\lbrack \\begin{array}{r}\na_{1} \\\\\na_{2} \\\\\n... \\\\\na_{n}\n\\end{array} \\right\\rbrack\\) 확률변수 벡터 : \\({\\underset{¯}{x}}_{n} = \\left\\lbrack \\begin{array}{r}\nx_{1} \\\\\nx_{2} \\\\\n... \\\\\nx_{n}\n\\end{array} \\right\\rbrack\\)\n확률변수 \\(x_{i} \\sim (iid)f(x)\\)는 확률표본이다.\n\\(\\frac{\\partial(\\underset{¯}{a}'\\underset{¯}{x})}{\\partial\\underset{¯}{x}} = \\underset{¯}{a}\\), \\(\\frac{\\partial(\\underset{¯}{x}'\\underset{¯}{a})}{\\partial\\underset{¯}{x}} = \\underset{¯}{a}\\)\n\n\n(2) 이차형식 미분\n\\(\\frac{\\partial(\\underset{¯}{x}'A\\underset{¯}{x})}{\\partial\\underset{¯}{x}} = (A + A')\\underset{¯}{x}\\) 만약 A가 대칭행렬이면) \\(2A\\underset{¯}{x}\\)\n\n\n2. 이차형식\n\n\n(1) 이차형식 정의\n정방행렬 : \\(A_{n \\times n} = \\begin{bmatrix}\na_{11} & a_{12} & \\cdots & a_{1n} \\\\\na_{21} & a_{22} & \\cdots & a_{2n} \\\\\n\\cdots & \\cdots & \\cdots & \\cdots \\\\\na_{n1} & a_{n2} & \\cdots & a_{nn}\n\\end{bmatrix}\\)\n이차형식 : \\(Q(x_{1},x_{2},...,x_{n}) = \\underset{¯}{x}'A\\underset{¯}{x}\\)\n\n2차형식의 경우 대칭행렬인 \\(A\\)는 적어도 한 개는 존재한다.\n\n\n\n(2) 이차형식 종류\n대칭행렬 \\(A\\), 이차형식 \\(Q(x_{1},x_{2},...,x_{n}) = \\underset{¯}{x}'A\\underset{¯}{x}\\)에 대하여\n모든 \\(x \\neq 0\\)에 대하여 \\(Q &gt; 0\\)이면 양의 정부호 positive definite\n모든 \\(x \\neq 0\\)에 대하여 \\(Q \\geq 0\\)이면 양의 반부호 positive semidefinite\n\n\n(3) 주축정리 The Principal Axes Theorem\n이차형식 \\(\\underset{¯}{x}'A\\underset{¯}{x}\\)을 교차항이 없는 이차형식 \\(\\underset{¯}{y}'D\\underset{¯}{y}\\)으로 변환하는 직교변환 \\(\\underset{¯}{x} = P\\underset{¯}{y}\\) 존재한다. \\(P\\)를 주축행렬이라 하고 대칭행렬 \\(A\\)의 고유벡터로 이루어져 있다.\n\n교차항이 없는 이차형식은 주축 변량에 대칭이다.\n\n\n\n\n\n\n\n\n(4) 이차형식과 고유치 관계\n\n이차형식 \\(Q = \\underset{¯}{x}'A\\underset{¯}{x}\\)이 양의 정부호이면 모든 고유치는 0보다 크다.\n양의 정부호 행렬의 역행렬도 양의 정부호 행렬이다.\n공분산 행렬은 양의 정부호 행렬이다.\n\n\n\n3. 이차형식 만들기\n\\[Q(x) = x_{1}^{2} + 2x_{2}^{2} - 7x_{3}^{2} - 4x_{1}x_{2} + 8x_{1}x_{3}\\]\n\n이차형식으로 만들면 다음과 같다. 제곱항은 그대로 대각원소로 하고 교차항은 1/2로 하여 각 셀에 배분한다.\n\n\\[Q(x) = \\begin{bmatrix}\nx_{1} & x_{2} & x_{3}\n\\end{bmatrix}\\begin{bmatrix}\n1 & - 2 & 4 \\\\\n- 2 & 2 & 0 \\\\\n4 & 0 & - 7\n\\end{bmatrix}\\left\\lbrack \\begin{array}{r}\nx_{1} \\\\\nx_{2} \\\\\nx_{3}\n\\end{array} \\right\\rbrack = \\underset{¯}{x}'A\\underset{¯}{x}\\]\n\n\\(\\underset{¯}{x} = P\\underset{¯}{y}\\), 주축행렬 \\(P\\)는 대칭행렬 \\(A\\)의 고유벡터이다.\n\\(A\\)의 교유치를 대각원소로 하는 행렬 \\(D = diag(\\lambda_{1},\\lambda_{2},\\lambda_{3})\\)를 이용하여 교차항이 없는 이차형식으로 변형한다.\n이렇게 되면 주축 변환된 이차형식의 변수 간에는 교차항이 없으므로 두 변수간에는 서로 독립이 된다.\n\\(Q(x) = \\underset{¯}{x}'A\\underset{¯}{x}\\) ⇢ \\(Q(y) = \\underset{¯}{y}'D\\underset{¯}{y}\\) (\\(\\underset{¯}{x} = P\\underset{¯}{y}\\))\n\n\n\n4. 선형 회귀모형\n\n\n(1) 데이터 구조\n목표변수 1개, \\(p\\)개 예측변수, 표본크기 n인 데이터를 가정하면 선형 회귀모형은 다음과 같다. \\(\\underset{¯}{y} = X\\underset{¯}{\\beta} + \\underset{¯}{e}\\)\n\\(\\left\\lbrack \\begin{array}{r}\ny_{1} \\\\\ny_{2} \\\\\n\\cdots \\\\\ny_{n}\n\\end{array} \\right\\rbrack\\)=\\(\\begin{bmatrix}\n1 & x_{11} & x_{12} & \\cdots & x_{1p} \\\\\n1 & x_{21} & x_{22} & \\cdots & x_{2p} \\\\\n\\cdots & \\cdots & \\cdots & \\cdots & \\\\\n1 & x_{n1} & x_{n2} & \\cdots & x_{np}\n\\end{bmatrix}\\left\\lbrack \\begin{array}{r}\na \\\\\nb_{1} \\\\\n\\cdots \\\\\nb_{p}\n\\end{array} \\right\\rbrack\\)+\\(\\left\\lbrack \\begin{array}{r}\ne_{1} \\\\\ne_{2} \\\\\n\\cdots \\\\\ne_{n}\n\\end{array} \\right\\rbrack\\)\n\n\n(2) 예측변수 데이터 행렬/벡터\n\\(X_{n \\times p} = \\begin{bmatrix}\nx_{11} & x_{12} & \\cdots & x_{1p} \\\\\nx_{21} & x_{22} & \\cdots & x_{2p} \\\\\n\\cdots & \\cdots & \\cdots & \\cdots \\\\\nx_{n1} & x_{n2} & \\cdots & x_{np}\n\\end{bmatrix}\\), \\(X_{n \\times p} = \\begin{bmatrix}\n{\\underset{¯}{x}}_{1} & {\\underset{¯}{x}}_{2} & \\cdots & {\\underset{¯}{x}}_{p} &\n\\end{bmatrix}\\)\n(데이터 벡터) \\({\\underset{¯}{x}}_{k} = \\left\\lbrack \\begin{array}{r}\nx_{1k} \\\\\nx_{2k} \\\\\n\\cdots \\\\\nx_{nk}\n\\end{array} \\right\\rbrack\\)\n\n\n(3) 확률변수 벡터, 평균벡터, 공분산행렬\n\\(\\underset{¯}{x} = \\left\\lbrack \\begin{array}{r}\nx_{1} \\\\\nx_{2} \\\\\n\\cdots \\\\\nx_{p}\n\\end{array} \\right\\rbrack\\), \\(x_{i}\\)는 확률변수이고 \\(E(x_{i}) = \\mu_{i},V(x_{i}) = \\sigma_{ii}\\),\n(두 변수의 공분산) \\(COV(x_{i},x_{j}) = \\sigma_{ij}\\)\n(평균벡터) \\(E(\\underset{¯}{x}) = \\underset{¯}{\\mu} = \\left\\lbrack \\begin{array}{r}\n\\mu_{1} \\\\\n\\mu_{2} \\\\\n\\cdots \\\\\n\\mu_{p}\n\\end{array} \\right\\rbrack\\)\n(공분산행렬) \\(COV(\\underset{¯}{x}) = \\Sigma = \\begin{bmatrix}\n\\sigma_{11} & \\sigma_{12} & \\cdots & \\sigma_{1p} \\\\\n\\sigma_{21} & \\sigma_{22} & \\cdots & \\sigma_{2p} \\\\\n\\cdots & \\cdots & \\cdots & \\cdots \\\\\n\\sigma_{p1} & \\sigma_{p2} & \\cdots & \\sigma_{pp}\n\\end{bmatrix}\\)\n상수벡터 : \\(\\underset{¯}{a} = \\left\\lbrack \\begin{array}{r}\na_{1},a_{2},\\cdots a_{p}\n\\end{array} \\right\\rbrack\\)\n\\(\\underset{¯}{a}'\\underset{¯}{x}\\)의 평균 : \\(E(\\underset{¯}{a}'\\underset{¯}{x}) = \\underset{¯}{a}'\\underset{¯}{\\mu}\\), 분산 \\(V(\\underset{¯}{a}'\\underset{¯}{x}) = \\underset{¯}{a}'\\underset{¯}{\\Sigma}\\underset{¯}{a}\\)\n\n\n(4) 선형 회귀모형\n\\(\\underset{¯}{y} = X\\underset{¯}{b} + \\underset{¯}{e}\\), \\(\\underset{¯}{e} \\sim N(\\underset{¯}{0},\\sigma^{2}I)\\)\n\n\n최소제곱법 추정\n\\[min_{a,b_{1},b_{2},...,b_{p}}\\sum e_{i}^{2} = min_{\\underset{¯}{b}}\\underset{¯}{e}'\\underset{¯}{e}\\]\n\\[Q(\\underset{¯}{b}) = \\underset{¯}{e}'\\underset{¯}{e} = (\\underset{¯}{y} - X\\underset{¯}{b})'(\\underset{¯}{y} - X\\underset{¯}{b}) = \\underset{¯}{y}'\\underset{¯}{y} + \\underset{¯}{b}'X'X\\underset{¯}{b} - 2\\underset{¯}{y}'X\\underset{¯}{b}\\]\n\\(\\frac{\\partial Q}{\\partial\\underset{¯}{b}} = 2X'X\\underset{¯}{b} - 2X'\\underset{¯}{y} = 0\\) ⇢ \\(\\widehat{\\underset{¯}{b}} = (X'X)^{- 1}X'\\underset{¯}{y}\\)\n\n\n적합치 fitted values 와 잔차 residuals\n적합치 : \\(\\widehat{\\underset{¯}{y}} = X\\widehat{\\underset{¯}{b}} = X(X'X)^{- 1}X'\\underset{¯}{y} = H\\underset{¯}{y}\\),\n\\(H = X(X'X)^{- 1}X'\\) hat 행렬이라 하고 대칭행렬이고 멱등행렬이다. \\(HH = H,H' = H\\)\n잔차 : \\(\\widehat{\\underset{¯}{e}} = \\underset{¯}{y} - \\widehat{\\underset{¯}{y}} = (I - H)\\underset{¯}{y}\\) \\(H\\)가 멱등행렬이면 \\((I - H)\\)도 멱등행렬이다.\n\n\n잔차의 분포\n\\(\\widehat{\\underset{¯}{e}} \\sim N(\\underset{¯}{0},\\sigma^{2}I)\\)\n오차의 가정 : \\(\\underset{¯}{e} \\sim N(\\underset{¯}{0},\\sigma^{2}I)\\) ⇢ \\(\\underset{¯}{y} \\sim N(X\\underset{¯}{b},\\sigma^{2}I)\\)\n그러므로 \\(E(\\widehat{\\underset{¯}{e}}) = (I - H)E(\\underset{¯}{y}) = (I - H)(X\\underset{¯}{b}) = (X\\underset{¯}{b} - HX\\underset{¯}{b}) = \\underset{¯}{0}V(\\widehat{\\underset{¯}{e}}) = V((I - H)\\underset{¯}{y}) = (I - H)\\sigma^{2}I(I - H)' = \\sigma^{2}I\\)\n\n\n목표변수 분해\n\\(\\underset{¯}{y} = H\\underset{¯}{y} + (I - H)\\underset{¯}{y}\\)=(설명하는 변동) + (설명하지 못하는 변동)\n\n\n\n\n\n높이를 최소화 하는 \\(\\underset{¯}{b}\\)를 구하는 것이 최소제곱추정법이다.\n\n\n추정치 분포\n\\(\\widehat{\\underset{¯}{b}} = (X'X)^{- 1}X'\\underset{¯}{y}\\)이고 \\(\\underset{¯}{y} \\sim N(X\\underset{¯}{b},\\sigma^{2}I)\\)이므로\n\\[E(\\widehat{\\underset{¯}{b}}) = (X'X)^{- 1}X'E(\\underset{¯}{y}) = (X'X)^{- 1}X'X\\underset{¯}{b} = \\underset{¯}{b}\\]\n\\[V(\\widehat{\\underset{¯}{b}}) = \\sigma^{2}(X'X)^{- 1}\\]\n\\(\\widehat{\\underset{¯}{b}} \\sim N(\\underset{¯}{b},\\sigma^{2}(X'X)^{- 1})\\), \\({\\widehat{\\sigma}}^{2} = SSE\\)\n\n\n변동 분해 ANOVA\n총변동 Total Sum of Squares : \\(SST = \\sum(y_{i} - \\overline{y})^{2}\\)\n\\(SST = \\sum y_{i}^{2} - \\frac{(\\sum y_{i})^{2}}{n} = \\underset{¯}{y}'\\underset{¯}{y} - (\\frac{1}{n})\\underset{¯}{y}'J_{n \\times n}\\underset{¯}{y}\\), \\(J\\)는 1행렬\n\\[SST = \\underset{¯}{y}'(I - (\\frac{1}{n})J)\\underset{¯}{y}\\]\n\n\n오차변동 Error Sum of Squares\n\\[SSE = \\sum(y_{i} - \\widehat{y_{i}})^{2}\\]\n\\[SSE = (\\underset{¯}{y} - X\\underset{¯}{b})'(\\underset{¯}{y} - X\\underset{¯}{b}) = \\underset{¯}{y}'\\underset{¯}{y} - \\underset{¯}{b}'X'\\underset{¯}{y} = \\underset{¯}{y}'(I - H)\\underset{¯}{y}\\]\n\n\n회귀변동 Regression Sum of Squares\n\\(SSR = \\sum(\\widehat{y_{i}} - \\overline{y})^{2}\\), \\(SSR = \\underset{¯}{y}'(H - (\\frac{1}{n})J)\\underset{¯}{y}\\)\n\\[SSR = SST - SSE = \\underset{¯}{b}X'\\underset{¯}{y} - (\\frac{1}{n})\\underset{¯}{y}'J\\underset{¯}{y}\\]\n\n\n결정계수\n\\(R^{2} = \\frac{SSR}{SST} = 1 - \\frac{SSE}{SST}\\) : 모형의 총변동 설명 비중\n\n\nSSE, SSR 분포 및 \\(\\sigma^{2}\\) 추정량\n\\(\\underset{¯}{x} \\sim N(\\underset{¯}{\\mu},\\Sigma)\\) 이면 이차형식 \\(\\underset{¯}{x}'A\\underset{¯}{x}\\)의 평균은\n\\(E(\\underset{¯}{x}'A\\underset{¯}{x}) = tr(A\\Sigma) + \\mu'A\\mu\\)이다.\n\\(\\underset{¯}{x} \\sim N(\\underset{¯}{\\mu},\\sigma^{2}I)\\) 이면 이차형식 \\(\\underset{¯}{x}'A\\underset{¯}{x}\\)(\\(A\\) 대칭행렬이고 멱등행렬이면)에 대하여 \\(\\frac{\\underset{¯}{x}'A\\underset{¯}{x}}{\\sigma^{2}} \\sim \\chi^{2}(df = rank(A))\\)이다.\n\\(SSE = \\underset{¯}{y}'(I - H)\\underset{¯}{y}\\), 이차형식이고 \\((I - H)\\)는 멱등행렬\n\\(rank(I - H) = n - p - 1\\)이므로 \\(\\frac{SSE}{\\sigma^{2}} \\sim \\chi^{2}(n - p - 1)\\)이다.\n\n\n오차 분산의 추정량 : \\(\\widehat{\\sigma^{2}} = MSE\\).\n\\(\\frac{SSR}{\\sigma^{2}} \\sim \\chi^{2}(p)\\), \\(F = \\frac{SSR/p}{SSE/(n - p - 1)} \\sim F(p,n - p - 1)\\)\n분산분석 표\n\n\n\n\n\n\n\n\n\n\n변동\n제곱변동\n자유도\n평균제곱\nF\n\n\n\n\n회귀\n\\[SSR\\]\n\\[p\\]\n\\[MSR = \\frac{SSR}{p}\\]\n\\[\\frac{MSR}{MSE}\\]\n\n\n오차\n\\[SSE\\]\n\\[n - p - 1\\]\n\\[MSE = \\frac{SSE}{n - p - 1}\\]\n\n\n총변동\n\\[SST\\]\n\\[n - 1\\]\n\\[{E(MSE) = \\sigma^{2}\n}{E(MSR) = \\sigma^{2} + b_{1}^{2}\\sum(x_{i} - \\overline{x})^{2}}\\]\n\n\n\n%% 강제 리렌더용 주석"
  },
  {
    "objectID": "consult.html",
    "href": "consult.html",
    "title": "통계상담",
    "section": "",
    "text": "📋 통계상담 안내\n데이터 분석, 통계 해석, 설문 설계 등 상담이 필요하신 분은 아래 폼을 제출해 주세요.\n👉 상담 신청하기\n\n온라인/비대면 상담 가능합니다."
  },
  {
    "objectID": "cardnews/news002.html",
    "href": "cardnews/news002.html",
    "title": "출산율 감소",
    "section": "",
    "text": "👶 출산율 감소, 바닥을 뚫다\n\n\n\n출산\n\n\n\n2023년 합계출산율: 0.72명\n전 세계 최저\n지방소멸 → 학교 폐교 → 일자리 축소의 악순환\n\n\n데이터가 보여주는 출산의 현실"
  },
  {
    "objectID": "cardnews/news004.html",
    "href": "cardnews/news004.html",
    "title": "전공별 취업률 격차",
    "section": "",
    "text": "title: “전공별 취업률 격차” format: html page-layout: full —"
  },
  {
    "objectID": "cardnews/news004.html#왜-이런-차이가-날까",
    "href": "cardnews/news004.html#왜-이런-차이가-날까",
    "title": "전공별 취업률 격차",
    "section": "💡 왜 이런 차이가 날까?",
    "text": "💡 왜 이런 차이가 날까?\n\n산업 수요와의 불균형\n졸업 후 진로 다양성, 인프라 차이\n지역 대학일수록 격차 더 큼"
  },
  {
    "objectID": "cardnews/news004.html#졸업생-1인의-목소리",
    "href": "cardnews/news004.html#졸업생-1인의-목소리",
    "title": "전공별 취업률 격차",
    "section": "💬 졸업생 1인의 목소리",
    "text": "💬 졸업생 1인의 목소리\n\n“취업률 통계는 높지만,\n실제로는 계약직, 인턴이 대부분이에요.”\n(사회계열 졸업생 인터뷰 중)"
  },
  {
    "objectID": "cardnews/news004.html#통계의-해석은-숫자-너머",
    "href": "cardnews/news004.html#통계의-해석은-숫자-너머",
    "title": "전공별 취업률 격차",
    "section": "📚 통계의 해석은 숫자 너머",
    "text": "📚 통계의 해석은 숫자 너머\n통계는 단순 수치보다 맥락과 경험을 함께 살필 때 의미를 갖습니다.\n\n전공 선택이 삶 전체에 어떤 영향을 주는가,\n그 통계로 함께 이야기해야 합니다."
  }
]