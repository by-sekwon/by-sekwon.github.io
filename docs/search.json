[
  {
    "objectID": "consult.html",
    "href": "consult.html",
    "title": "통계상담",
    "section": "",
    "text": "📋 통계상담 안내\n데이터 분석, 통계 해석, 설문 설계 등\n상담이 필요하신 분은 아래 폼을 제출해 주세요.\n👉 상담 신청하기\n\n온라인/비대면 상담 가능합니다."
  },
  {
    "objectID": "notes/survey/index.html",
    "href": "notes/survey/index.html",
    "title": "Survey 강의",
    "section": "",
    "text": "이 페이지는 Survey 주제의 강의 노트를 안내합니다.\n\n\n\n강의 파일 1\n강의 파일 2"
  },
  {
    "objectID": "notes/survey/index.html#강의-목록",
    "href": "notes/survey/index.html#강의-목록",
    "title": "Survey 강의",
    "section": "",
    "text": "강의 파일 1\n강의 파일 2"
  },
  {
    "objectID": "notes/inference/index.html",
    "href": "notes/inference/index.html",
    "title": "Inference 강의",
    "section": "",
    "text": "이 페이지는 Inference 주제의 강의 노트를 안내합니다.\n\n\n\n강의 파일 1\n강의 파일 2"
  },
  {
    "objectID": "notes/inference/index.html#강의-목록",
    "href": "notes/inference/index.html#강의-목록",
    "title": "Inference 강의",
    "section": "",
    "text": "강의 파일 1\n강의 파일 2"
  },
  {
    "objectID": "notes/ml/index.html",
    "href": "notes/ml/index.html",
    "title": "Ml 강의",
    "section": "",
    "text": "이 페이지는 Ml 주제의 강의 노트를 안내합니다.\n\n\n\n강의 파일 1\n강의 파일 2"
  },
  {
    "objectID": "notes/ml/index.html#강의-목록",
    "href": "notes/ml/index.html#강의-목록",
    "title": "Ml 강의",
    "section": "",
    "text": "강의 파일 1\n강의 파일 2"
  },
  {
    "objectID": "notes/math/derivate_integral.html",
    "href": "notes/math/derivate_integral.html",
    "title": "수학의 기초 2. 미분과 적분",
    "section": "",
    "text": "chapter 1. 미분\n세상에는 움직이지 않는 것이 없다. 우리가 함께 움직이고 있어 느끼지 못할 뿐, 지구는 자전 속도로 약 시속 1,660km, 공전 속도로 약 시속 10.75만 km의 속도로 계속 움직이고 있다. 코페르니쿠스가 제안한 지동설은 천체 관측을 통해 갈릴레오에 의해 뒷받침되었으며, 갈릴레오는 망원경 관찰을 통해 목성의 위성과 금성의 위상 변화를 발견함으로써 지동설을 지지했다. 또한, 행성의 운동 궤도가 완벽한 원이 아니라 타원이라는 사실을 밝혀내어 지동설을 더욱 확고히 한 케플러가 있었다. 이들과 같은 시대를 살았던 뉴턴은 “왜 달은 하늘에 떠 있는 반면, 사과는 땅으로 떨어질까?”라는 질문을 통해 만유인력의 법칙을 정립하고 중력과 중력 가속도를 발견했다.\n한편, 미분은 “변화”로 정의된다. 기존 상태와 변화된 상태는 다르며, 미분은 바로 그 상태 변화에 관심을 둔다. 거리의 변화는 속도, 속도의 변화는 가속도, 그리고 비용과 효용의 변화는 한계비용과 한계효용으로 나타난다. 미분은 함수의 한 점에서 접선의 기울기를 구하는 과정이며, 이를 미분 differentiation 이라 한다. 다항함수, 로그함수, 지수함수, 삼각함수 등 대부분의 함수는 미분가능하며, 특히 통계학에서 자주 사용하는 함수들은 모두 미분가능한 함수로 이루어져 있다.\n미분은 주어진 점에서 접선의 기울기를 계산할 수 있도록 하며 통계학에서는 최적화 문제, 회귀분석, 확률 밀도 함수의 특성 분석 등 다양한 분야에서 활용된다.\n\n1. 평균변화율 average rate of change\n구간 \\(a \\leq x \\leq b\\)에서 함수 \\(f\\)의 평균 변화량으로 \\(\\frac{rise}{run} = \\frac{\\Delta y}{\\Delta x} = \\frac{f(b) - f(b)}{b - a}\\)이다.\n\n\n\n\n\n\\(a \\leq x \\leq b\\) 구간에서 단위당 평균적으로 함수의 변화량을 측정한 것이다. 평균변화량은 고속도로 구간단속에 이용된다. 미분은 지점 과속 단속에 이용된다.\n\n\n\n\n\n측정 1 : 구간단속 시작, 종료 지점에서 과속여부 측정 (미분 응용)\n측정 2 : 예를 들어 구간 거리가 6km라 하자. 2분만에 구간을 통과했다면 평균속도는 \\(\\frac{6 - 0}{2 - 0} = 3km/min.\\)분당 3km를 달렸으므로 시간당 180km를 달렸으니 과속이 되는 것입니다. (평균변화량)\n\n\n2. 미분 정의\n함수 \\(f(x)\\)의 임의의 점 \\(x = a\\)에서의 미분값 \\(f'(a)\\)는 다음과 같이 정의된다. \\(f'(a) = \\lim_{h \\rightarrow 0}\\frac{f(a + h) - f(a)}{h}\\)\n\\(\\frac{f(a + h) - f(a)}{h}\\)는 Fermat’s Difference Quotient로 불리며, 점 \\(a\\)에서의 평균 변화율을 나타낸다.\n극한이 존재하면 \\(f'(a)\\)는 \\(x = a\\)에서의 접선의 기울기로 해석할 수 있다.\n미분 가능성\n  \\(f'(a)\\)가 존재하면, 점 \\(x = a\\)에서 함수 \\(f(x)\\)는 미분 가능하다고 한다. 함수 \\(f(x)\\)가 정의역 전체에서 미분 가능하, 함수 f(x)는 미분 가능 함수이다.\n미분의 기하학적 해석\n  미분값 \\(f'(a)\\)는 곡선 \\(y = f(x)\\)의 점 \\(x = a\\)에서의 접선의 기울기를 의미한다. \\(h\\)가 0으로 가까워질수록 평균 변화율은 접선의 기울기에 점점 가까워진다.\n미분 가능성과 연속성\n  함수 \\(f(x)\\)가 점 \\(x = a\\)에서 미분 가능하면 \\(f(x)\\)는 반드시 그 점에서 연속이다. 하지만, 연속이라고 해서 항상 미분 가능한 것은 아니다. 예를 들어, 절대값 함수 가능하지 않다.\n\n\n\n\n\n\n\n3. 미분 규칙\n상수 함수의 미분\n\\[\\frac{d}{dx}\\lbrack c\\rbrack = 0\\]\n거듭제곱 함수의 미분\n\\[\\frac{d}{dx}\\lbrack x^{n}\\rbrack = nx^{n - 1}, f(x) = x^{n}(n \\in \\mathbb{R})\\]\n【예제】 \\(f(x) = 2\\sqrt{x}\\) 을 미분하시오.\n\\[f'(x) = 2(\\frac{1}{2})x^{1/2 - 1} = x^{- 1/2} = \\frac{1}{\\sqrt{x}}\\]\n상수배의 미분\n\\[\\frac{d}{dx}\\lbrack c \\cdot f(x)\\rbrack = c \\cdot \\frac{d}{dx}\\lbrack f(x)\\rbrack\\]\n합/차의 미분\n\\[\\frac{d}{dx}\\lbrack f(x) \\pm g(x)\\rbrack = \\frac{d}{dx}\\lbrack f(x)\\rbrack \\pm \\frac{d}{dx}\\lbrack g(x)\\rbrack\\]\n곱의 미분\n\\[\\frac{d}{dx}\\lbrack f(x) \\cdot g(x)\\rbrack = f'(x) \\cdot g(x) + f(x) \\cdot g'(x)\\]\n나눗셈의 미분\n\\(\\frac{d}{dx}\\left\\lbrack \\frac{f(x)}{g(x)} \\right\\rbrack = \\frac{f'(x) \\cdot g(x) - f(x) \\cdot g'(x)}{\\lbrack g(x)\\rbrack^{2}}\\), \\(g(x) \\neq 0\\)\n체인룰 chain rule 연쇄규칙\n\\[\\frac{d}{dx}\\lbrack f(g(x))\\rbrack = f'(g(x)) \\cdot g'(x)\\]\n【예제】 \\(f(x) = 2\\sqrt{3x^{2} - 1}\\)을 미분하시오.\n\n\n\n\n\n  바깥부분 미분하고 안쪽 부분 그대로 적는다.\n  \\(2*(1/2){\\sqrt{(3x^{2} - 1)}}^{- 1/2}\\) 그리고 안쪽부분을 미분한다.\n\\[f'(x) = {\\sqrt{(3x^{2} - 1)}}^{- 1/2}6x = \\frac{6x}{\\sqrt{3x^{2} - 1}}\\]\n로그함수 미분\n\\[\\frac{d}{dx}\\lbrack\\log_{a}(x)\\rbrack = \\frac{1}{x\\ln(a)},x &gt; 0\\]\n\\[\\frac{d}{dx}\\lbrack\\ln(x)\\rbrack = \\frac{1}{x},x &gt; 0\\]\n【예제】 \\(f(x) = ln(x^{2} - 1)\\)을 미분하시오.\n  연쇄법칙 적용 : \\(f'(x) = \\frac{1}{x^{2} - 1}2x\\)\n지수함수 미분\n\\[\\frac{d}{dx}\\lbrack a^{x}\\rbrack = a^{x}\\ln(a)\\]\n\\[\\frac{d}{dx}\\lbrack e^{x}\\rbrack = e^{x}\\]\nimport sympy as sp\n\n# 변수와 함수를 정의\nx = sp.Symbol('x')\nf = 5*(x**2 - 2*x)**2\n\n# 함수 입력을 파싱하여 미분\nfunc = sp.sympify(f)\nderivative = sp.diff(func, x)\n\n\n4. 미분 응용\n\n\n(1) 최대, 최소\n\n\n1차 미분정리\n함수 f(x) 가 일정 구간 (a, b) 안의 모든 점에서 미분 가능하고, 구간 내 임의의 점 c 에서 1차 미분이 0이면, f(x) 함수는 c 점에서 지역 최대값이나 최소값을 갖는다. 이는 페르마의 정리에 Fermat’s 해당하며, 극대값 또는 극소값이 존재하는 필수 조건을 설명한다.\n함수 f(x) 가 c 에서 미분 가능하다면,\n극값이 c 에서 존재하면,\n반드시 \\(f'(c) = 0\\)이어야 한다.\n다만, \\(f'(c) = 0\\)이라고 해서 반드시 극값이 존재하는 것은 아니며, 이는 필요조건일 뿐 충분조건은 아니다. 극값의 존재를 확실히 판단하려면 2차 도함수 테스트나 첫 도함수의 부호 변화를 추가로 고려해야 한다.\n\n\n증가 함수와 감소 함수\n함수 f(x)가 구간 \\(I\\)에서 정의되어 있을 때,\n\\(x_{1} &lt; x_{2} \\Longrightarrow f(x_{1}) \\leq f(x_{2})\\)이면 구간 \\(I\\)에서 증가 함수이다.\n\\(x_{1} &lt; x_{2} \\Longrightarrow f(x_{1}) \\geq f(x_{2})\\) 이면 구간 \\(I\\)에서 감소 함수이다.\n\n\n1차 미분과 증가·감소 함수의 관계\n함수 f(x) 가 구간 \\(I\\)에서 미분 가능하다면,\n    - \\(f'(x) &gt; 0\\)이면, f(x) 는 구간 \\(I\\)에서 엄격히 증가한다\n    - \\(f'(x) &lt; 0\\)이면, f(x) 는 구간 \\(I\\)에서 엄격히 감소한다.\n\n\n오목성 concavity 정의\n함수 f(x) 의 기울기가 감소하는 경우 \\(f''(x) &lt; 0\\),\n    - 함수 f(x) 는 concave down (오목 아래)이다.\n    - 그래프가 아래로 휘어진 모양을 갖는다.\n함수 f(x) 의 기울기가 증가하는 경우 \\(f''(x) &gt; 0\\),\n    - 함수 f(x) 는 concave up (오목 위)이다.\n    - 그래프가 위로 휘어진 모양을 갖는다.\n\n\n\n\n\n\n\n변곡점 inflexion point 정의\n함수 \\(f(x)\\)의 오목성이 변하는 점이 있을 때, 이 점을 변곡점이라고 한다. 즉, \\(f(x)\\)가 \\(f’’(x) &gt; 0\\)에서 \\(f’’(x) &lt; 0\\)로 바뀌거나 \\(f’’(x) &lt; 0\\)에서 \\(f’’(x) &gt; 0\\)로 바뀌는 점이 변곡점이다.”\n1차 미분과 2차 미분을 이용한 최대, 최소 판단\n주어진 \\(f'(c) = 0\\)에서, \\(f''(c)\\)를 확인한다.\n  - \\(f''(c) &gt; 0\\) 이면 \\(x = c\\)에서 (지역) 최소값\n  - \\(f''(c) &lt; 0\\) 이면 \\(x = c\\)에서 (지역) 최대값\n  - \\(f''(c) = 0\\) 이고 \\(f''(x)\\) 부호가 바뀌면 \\(x = c\\)에서 변곡점\n\n\n(2) 통계학 응용\n단순 회귀모형 \\[y_{i} = \\beta_{0} + \\beta_{1}x_{i} + \\epsilon_{i},i = 1,2,\\ldots,n\\]\nOLS 추정치 \\[\\text{Minimize:}S(\\beta_{0},\\beta_{1}) = \\overset{n}{\\sum_{i = 1}}(y_{i} - \\beta_{0} - \\beta_{1}x_{i})^{2}\\]\n오차 제곱합   \\(S(\\beta_{0},\\beta_{1})\\)을 \\(\\beta_{0}\\)와 \\(\\beta_{1}\\)에 대해 편미분한 뒤 0으로 설정하여 최소값(OLS)을 찾는다.\n정규방정식\n\\[\\frac{\\partial S}{\\partial\\beta_{0}} = - 2\\overset{n}{\\sum_{i = 1}}(y_{i} - \\beta_{0} - \\beta_{1}x_{i}) = 0\\]\n\\[\\overset{n}{\\sum_{i = 1}}y_{i} = n\\beta_{0} + \\beta_{1}\\overset{n}{\\sum_{i = 1}}x_{i}\\]\n\\[\\frac{\\partial S}{\\partial\\beta_{1}} = - 2\\overset{n}{\\sum_{i = 1}}x_{i}(y_{i} - \\beta_{0} - \\beta_{1}x_{i}) = 0\\]\n\\[\\overset{n}{\\sum_{i = 1}}x_{i}y_{i} = \\beta_{0}\\overset{n}{\\sum_{i = 1}}x_{i} + \\beta_{1}\\overset{n}{\\sum_{i = 1}}x_{i}^{2}\\]\n두 식을 함께 사용하여 \\(\\beta_{0}\\)와 \\(\\beta_{1}\\)를 계산한다.\n\\[\\beta_{1} = \\frac{\\sum_{i = 1}^{n}(x_{i} - \\overline{x})(y_{i} - \\overline{y})}{\\sum_{i = 1}^{n}(x_{i} - \\overline{x})^{2}} = \\frac{\\text{Cov}(x,y)}{\\text{Var}(x)}\\]\n\\[\\beta_{0} = \\overline{y} - \\beta_{1}\\overline{x}\\]\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.optimize import curve_fit\n\n# 가상의 데이터 생성 n=20\nnp.random.seed(0)\nx_data = np.linspace(-5, 5, 20)\ny_data = 2 * x_data**3 - 3 * x_data**2 + 4 * x_data + 10 + np.random.normal(0, 10, 20)\n\n# 직선 적합 함수\ndef linear(x, a, b):\n    return a * x + b\n# 2차 함수 적합 함수\ndef quadratic(x, a, b, c):\n    return a * x**2 + b * x + c\n# 3차 함수 적합 함수\ndef cubic(x, a, b, c, d):\n    return a * x**3 + b * x**2 + c * x + d\n# 최소자승법을 이용한 직선, 2차, 3차 적합\nparams_linear, _ = curve_fit(linear, x_data, y_data)\nparams_quadratic, _ = curve_fit(quadratic, x_data, y_data)\nparams_cubic, _ = curve_fit(cubic, x_data, y_data)\n\n# 적합된 함수의 값을 계산\ny_fit_linear = linear(x_data, *params_linear)\ny_fit_quadratic = quadratic(x_data, *params_quadratic)\ny_fit_cubic = cubic(x_data, *params_cubic)\n\n# Residual Sum of Squares 계산\nrss_linear = np.sum((y_data - y_fit_linear) ** 2)\nrss_quadratic = np.sum((y_data - y_fit_quadratic) ** 2)\nrss_cubic = np.sum((y_data - y_fit_cubic) ** 2)\n\n# 최소자승법을 이용한 직선, 2차, 3차 적합\nparams_linear, _ = curve_fit(linear, x_data, y_data)\nparams_quadratic, _ = curve_fit(quadratic, x_data, y_data)\nparams_cubic, _ = curve_fit(cubic, x_data, y_data)\n\n# 적합된 함수의 값을 계산\ny_fit_linear = linear(x_data, *params_linear)\ny_fit_quadratic = quadratic(x_data, *params_quadratic)\ny_fit_cubic = cubic(x_data, *params_cubic)\n\n# Residual Sum of Squares 계산\nrss_linear = np.sum((y_data - y_fit_linear) ** 2)\nrss_quadratic = np.sum((y_data - y_fit_quadratic) ** 2)\nrss_cubic = np.sum((y_data - y_fit_cubic) ** 2)\n\n# 그래프 그리기\nplt.figure(figsize=(10, 6))\nplt.scatter(x_data, y_data, label='data', color='black')\nplt.plot(x_data, y_fit_linear, label='Linear fit (y = {:.2f}x + {:.2f}) : RSS={:.2f}'.format(params_linear[0], params_linear[1],rss_linear), color='blue')\nplt.plot(x_data, y_fit_quadratic, label='Quardratic fit (y = {:.2f}x^2 + {:.2f}x + {:.2f}) : RSS={:.2f}'.format(params_quadratic[0], params_quadratic[1], params_quadratic[2],rss_quadratic), color='green')\nplt.plot(x_data, y_fit_cubic, label='Cubic fit (y = {:.2f}x^3 + {:.2f}x^2 + {:.2f}x + {:.2f}) : RSS={:.2f}'.format(params_cubic[0], params_cubic[1], params_cubic[2], params_cubic[3],rss_cubic), color='red')\nplt.axhline(0, color='grey', lw=0.5, ls='--')\nplt.axvline(0, color='grey', lw=0.5, ls='--')\nplt.title('fit by OLS')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.legend()\nplt.grid()\nplt.show()\n\n\n\n\n\n\n\n(3) 한계효용체감의 법칙\n한계효용 marginal utility은 재화가 증가 혹은 감소함에 따라 주관적으로 매겨지는 경제적 효용(혹은 가치)의 관계에 대한 개념으로 합리적인 경제에서 인간 행동은 자신에게 가장 시급한 욕구를 충족하는 일을 가장 먼저 하거나 가치를 두는 특성이 있다. 따라서 어떤 사람이 재화나 용역을 이용하여 효용을 얻고자 할 때 주관적으로 판단되는 욕망 충족의 정도인 효용의 가치가 높은 것부터 낮은 것 쪽으로 추구한다. 재화나 용역의 한계효용은 그 재화나 용역을 사용하는 것을 증가하거나 감소함에 따라 변화한 가치의 양을 상정한 것인데 이런 변화에서 추가의 1단위 즉 경계인 단위에서의 재화나 용역의 효용을 한계효용이라고 한다.[위키피디아]\n\n\n\n\n\n총효용 total utility 은 주어진 기간 동안 소비된 특정 상품의 모든 단위에서 얻은 총만족입니다. 한계효용 marginal utility 마지막 소비량에서 상품 소비의 1단위 변화로 인해 발생하는 총 효용의 변화이다. 더 많은 단위의 상품을 구매하면 한계 효용은 감소하기 시작하지만 총 효용은 계속해서 감소 비율이 줄어든다. 한계효용이 0가 되는 포화점 satiety에 이르렀을 때 이 지점에서의 총효용은 최대가 된다. 이 지점에서 소비가 더 증가하면 한계 효용은 음수가 되고 총 효용은 감소하기 시작한다.\n\n\n(4) Cobb-Douglas 생산함수\n\\(Q = f(K,L) = AL^{\\alpha}K^{\\beta}\\), \\(Q\\)= 생산, \\(K\\)=자본, \\(L\\)=노동, \\(A,\\alpha,\\beta\\)는 모수이다. \\(K,L\\)에 대하여 각각 편미분 하면 다음과 같다.\n  - 양변에 로그를 취한다. \\(ln(Q) = lnA + \\alpha lnL + \\beta lnK\\)\n  - \\(\\frac{\\partial(lnQ)}{\\partial L} = \\alpha\\) : 한계 노동 생산량\n  - \\(\\frac{\\partial(lnQ)}{\\partial K} = \\beta\\) : 한계 자본 생산량\n\n\n\n\nchapter 2. 적분\n고대 수학자들은 직선으로 이루어진 도형의 면적을 비교적 쉽게 계산할 수 있었습니다. 사각형, 삼각형, 평행사변형, 사다리꼴과 같은 도형은 밑변과 높이를 활용한 간단한 공식을 통해 면적을 구할 수 있었기 때문입니다. 그러나 곡선이 포함된 도형의 면적을 계산하는 문제는 훨씬 더 복잡한 도전 과제였습니다.\n곡선이 포함된 도형의 면적을 구하기 위해 현대 수학에서는 적분이라는 개념이 도입되었습니다. 이는 고대 그리스의 수학자 아르키메데스가 처음으로 탐구한 주제 중 하나였습니다. 아르키메데스는 곡선 아래의 면적을 구하기 위해 곡선을 아주 작은 직사각형들로 나누고, 그 면적을 합산하여 근사값을 구하는 방식을 사용했습니다. 이 과정은 시간이 지나며 점점 더 체계적으로 발전하였고, 마침내 미적분학으로 이어졌습니다.\n아이작 뉴턴과 고트프리트 라이프니츠는 아르키메데스의 아이디어를 발전시켜 적분과 미분이라는 두 가지 핵심 개념을 정립하였고, 이를 통해 곡선 아래의 면적을 정확히 계산할 수 있는 도구를 완성했습니다. 오늘날 우리가 사용하는 적분법은 이들의 연구에 기반을 두고 있으며, 곡선의 면적뿐만 아니라 물리학, 공학, 경제학 등 다양한 분야에서 중요한 역할을 하고 있습니다.\n적분은 통계학에서 확률 계산, 기대값, 분산, 베이지안 추론 등 다양한 개념과 도구에 중요한 역할을 합니다. 확률을 곡선 아래 면적으로 해석하는데서부터 시작해, 통계적 추론의 기초를 형성하는 데 적분이 필수적입니다. 이러한 적분 개념은 통계학 이론뿐만 아니라 데이터 분석, 머신러닝, 신뢰구간 계산 등 실무적인 응용에서도 널리 사용됩니다.\n\n1. 부정 적분\n함수 F(x) 가 주어진 함수 f(x) 에 대해 정의역의 모든 점에서 \\(F'(x) = f(x)\\)를 만족한다면, F(x) 를 f(x) 의 역-미분 anti-derivative 또는 원시함수 primitive function 합니다. 이는 적분이 미분의 역연산임을 의미합니다.\n적분이 미분의 역연산이라는 사실을 처음 체계적으로 증명하고 이를 수학적으로 정립한 사람들은 아이작 뉴턴(Isaac Newton)과 고트프리트 라이프니츠(Gottfried Wilhelm Leibniz)입니다. 이들은 독립적으로 미적분학의 기본 개념을 발전시켰으며, 이 과정에서 적분과 미분의 관계를 설명한 미적분학의 기본정리를 도출했습니다.\n\n\n정적분과 미분의 관계\n특정 구간에서의 정적분은 미분을 통해 함수의 값을 복원할 수 있습니다. 예를 들어, 함수 f(x) 에 대해 다음과 같은 정적분이 있을 때,\n\\(F(x) = \\int_{a}^{x}f(t)dt\\). 이를 x 에 대해 미분하면 \\(\\frac{d}{dx}F(x) = f(x)\\)\n즉, 적분을 통해 구한 누적 변화량을 다시 미분하면, 원래의 함수로 돌아갑니다.\n적분과 미분은 서로 반대되는 과정처럼 보이지만, 실제로는 상호보완적입니다. 적분은 함수의 누적적인 변화(예: 곡선 아래의 면적)를 측정하며, 미분은 순간적인 변화(예: 기울기)를 측정합니다.\n\n\n2. 정적분\n\n\n(1) 정적분 개념\n정적분(면적)은 부정적분(역-미분 함수)과는 다른 접근 방식에서 출발합니다. 그러나 이 두 개념은 17세기에 뉴턴(Newton)과 라이프니츠(Leibniz)에 의해 서로 밀접하게 연결되었고, 이를 통합하여 적분(integral)이라고 명명하였습니다.\n우선, 정적분의 개념을 살펴보겠습니다. 구간 [a, b]에서 함수 f(x) 아래의 면적을 어떻게 구할 수 있을까요? 이를 위해 구간 [a, b]를 여러 작은 구간으로 나눈 다음, 각 구간에서 직사각형의 면적을 계산하여 합산하는 방법을 생각할 수 있습니다. 이러한 직사각형의 면적 합은 점점 더 작은 구간으로 나눌수록 실제 면적에 근사하게 됩니다.\n\n\n\n\n\n함수와 x-축 사이에 형성된 이 면적은 정적분이라 하며, 이는 구간 [a, b]에서 함수 f(x)와 x-축 사이의 공간에 해당합니다. 직사각형을 이용해 근사한 면적은 실제 면적보다 클 수도 있고 작을 수도 있습니다. 하지만 구간을 점점 더 세분화하면, 이 근사값은 실제 정적분 값에 수렴하게 됩니다.\n정적분은 함수의 곡선 아래의 면적을 계산하는 방법으로 출발했지만, 부정적분(역-미분 함수)과의 연결을 통해 더욱 강력한 수학적 도구로 발전하였습니다.\n\n\n(2) 정적분과 부정적분의 관계\n함수 f(x) 가 구간 [a, b]에서 연속일 때:\n\n부정적분(역-미분 함수): 함수 F(x) 가 f(x) 의 부정적분이라면 \\(F'(x) = f(x)\\)\n정적분(구간의 면적): 함수 f(x) 의 정적분은 구간 [a, b]에서 f(x) 와 x -축 사이의 면적을 나타냅니다. \\(\\int_{a}^{b}f(x)dx\\)\n뉴턴-라이프니츠 정리: 부정적분과 정적분은 다음과 같이 연결됩니다. \\(\\int_{a}^{b}f(x)dx = F(b) - F(a)\\)\n여기서 F(x) 는 f(x) 의 부정적분입니다.\n\n이 정리는 정적분(구간에서의 면적 계산)이 부정적분(역-미분 함수)을 사용하여 계산될 수 있음을 보여줍니다.\n\n\n(3) 정적분 규칙\n\n\n특정 점에서의 확률\n\\[\\int_{a}^{a}f(x)dx = 0\\]\n이는 구간의 길이가 0 일 때, 정적분의 결과가 항상 0 임을 나타냅니다(통계적으로: 연속 확률변수에서 특정 점에서의 확률은 0 이다).\n\n\n구간 순서 반대\n\\[\\int_{a}^{b}f(x)dx = - \\int_{b}^{a}f(x)dx\\]\n구간의 순서를 바꾸면 정적분의 부호가 반대가 됩니다.\n\n\n상수 배율\n\\[\\int_{a}^{b}c \\cdot f(x)dx = c\\int_{a}^{b}f(x)dx(\\text{c is constant})\\]\n적분 내부에 상수가 곱해져 있을 경우, 상수를 적분 기호 밖으로 꺼낼 수 있습니다.\n\n\n합과 차\n\\[\\int_{a}^{b}\\left( f(x) \\pm g(x) \\right)dx = \\int_{a}^{b}f(x)dx \\pm \\int_{a}^{b}g(x)dx\\]\n적분은 덧셈과 뺄셈 연산에 대해 분배법칙을 따릅니다.\n\n\nDomination Rule\n만약 \\(f(x) \\geq 0\\)가 구간 [a, b]에서 항상 성립하면\n\\(\\int_{a}^{b}f(x)dx \\geq 0\\) 이다. 통계적으로 확률변수의 분포 함수는 항상 0 이상 이므로, 확률값은 항상 0 이상이다.\n\n\n부등식 관계\n만약 \\(f(x) \\leq g(x)\\)가 구간 [a, b]에서 항상 성립하면\n\\(\\int_{a}^{b}f(x)dx \\leq \\int_{a}^{b}g(x)dx\\) 이다.\n\n\n구간 쪼개기\n\\[\\int_{a}^{c}f(x)dx + \\int_{c}^{b}f(x)dx = \\int_{a}^{b}f(x)dx\\]\n적분 구간을 나누어 계산할 수 있습니다.\n\n\n확률밀도함수 전체 구간\n\\(\\int_{- \\infty}^{\\infty}f(x)dx = 1\\). 확률밀도함수(PDF)는 전체 구간에서의 적분, 확률의 총합이 1 임을 나타냅니다.\n\n\n지수함수 적분\n\\[\\int a^{x}dx = \\frac{a^{x}}{\\ln a} + C(a &gt; 0,a \\neq 1)\\]\n\\[\\int e^{x}dx = e^{x} + C\\]\n\n\n로그함수 적분\n\\[\\int\\log_{a}(x)dx = \\frac{1}{\\ln(a)}\\left( x\\ln(x) - x \\right) + C\\]\n\\[\\int\\ln(x)dx = x\\ln(x) - x + C\\]\n\n\n특수한 적분\n\\[\\int\\frac{1}{x} = ln|x| + C\\]\n\n\n치환적분\n함수 g(x) 가 x 에 대한 미분가능한 함수이고, f(u) 가 u = g(x) 에 대한 함수라고 가정하겠습니다.\n\\[\\int f(g(x)) \\cdot g'(x)dx = \\int f(u)du\\]\n\\[u = g(x) , du = g'(x)dx\\]\n【사례】 \\(\\int x \\cdot e^{x^{2}}dx = \\frac{1}{2}e^{x^{2}} + C\\)\n\\(u = x^{2}\\)로 치환하면, \\(du = 2xdx\\). 따라서 \\(xdx = \\frac{1}{2}du\\)\n\\(\\int x \\cdot e^{x^{2}}dx = \\int e^{u} \\cdot \\frac{1}{2}du = \\frac{1}{2}\\int e^{u}du\\)=\\(\\frac{1}{2}\\int e^{u}du = \\frac{1}{2}e^{u} + C\\)\n\\(u = x^{2}\\) 이므로 \\(\\int x \\cdot e^{x^{2}}dx = \\frac{1}{2}e^{x^{2}} + C\\) 이다.\n\n\n부분적분\n함수 u(x) 와 v(x) 가 미분 가능할 때, 다음 공식이 성립합니다:\n\\[\\int udv = uv - \\int vdu\\]\n\n\\(u\\): 미분할 함수 (\\(u \\rightarrow du\\))\n\\(dv\\): 적분할 함수 (\\(dv \\rightarrow v\\))\n\n【사례】 \\(\\int xe^{x}dx\\)\n1) 함수 선택: \\(u = x,dv = e^{x}dx\\)\n2) 미분 및 적분: \\(u \\rightarrow du = dx\\),\\(dv \\rightarrow v = e^{x}\\)\n3) 부분적분 공식 적용: \\(\\int xe^{x}dx = uv - \\int vdu\\)\n\\(= xe^{x} - \\int e^{x}dx\\)\\(= xe^{x} - e^{x} + C\\).\n【사례】 \\(\\int_{0}^{1}x^{2} + \\sqrt{x}dx\\) 구하시오.\n\\(f(x) = x^{2} + \\sqrt{x}\\)이므로 \\(F(x) = \\frac{1}{3}x^{3} + \\frac{2}{3}x^{\\frac{3}{2}}\\)\n\\(F(1) = 1\\), \\(F(0) = 0\\)이므로 1이다.\n\\[\\int_{0}^{1}x^{2} + \\sqrt{x}dx = \\frac{1}{3}x^{3} + \\frac{2}{3}x^{\\frac{3}{2}}\\rbrack_{0}^{1} = 1 - 0 = 1\\]\n#부정적분\nfrom sympy import *\nx=Symbol('x')\nintegrate(x**2+x**(0.5), x)\n\\[ x^3/3 + 0.66667x^{1.5}\n\\]\n#정적분\nfrom scipy.integrate import quad\ndef integrand(x):\n   return x**2+x**(0.5)\nquad(integrand,0, 1)\n【결과】 첫번째 값은 적분값이고 두 번째는 적분 값을 얼마나 근사하게 계산하였는지 값이다. 완벽한 값이면 0이어야 하나 출력된 값은 0.0(14개)11…이다. root는 실제 근이다. (1.0, 1.1102230246251565e-15)\n【사례】 표준 정규확률분포함수\\(\\int_{0}^{\\infty}\\frac{1}{\\sqrt{2\\pi}}e^{- \\frac{x^{2}}{2}}dx\\) 구하시오.\n#부정적분\nfrom sympy import *\nimport numpy as np\nx=Symbol('x')\nintegrate(1/(2*np.pi)**0.5*exp(-x**2/2), x)\n\\[\n0.199471140200716 \\sqrt{2} \\sqrt{\\pi} \\, \\mathrm{erf}\\left( \\frac{\\sqrt{2}x}{2} \\right)\n\\]\nimport numpy as np\n#정적분\nfrom scipy.integrate import quad\ndef integrand(x):\n   return 1/(2*np.pi)**0.5*exp(-x**2/2)\nquad(integrand,0,np.inf)\n【결과】 (0.49999999999999983, 5.08909572547112e-09)\n\n\n(4) 표적분 tabular integral\n표 적분은 부분적분을 용이하게 한다. 미분 부분 \\(f(x)\\)는 미분하면서 차수가 용이해야 하고, 적분함수 \\(g(x)\\)는 용이하게 적분할 수 있어야 한다.\n(방법1) 미분 부분이 0이 될 때까지 미분과 적분을 반복 시행한다.\n\\[\\int_{a}^{b}udv = (1)*(a) - (2)*(b) + (3)(c)...\\rbrack_{a}^{b}\\]\n(방법2)한 번만 미분하고 \\(\\int_{a}^{b}udv = (1)*(a) - \\int_{a}^{b}(2)*(b)dx\\)\n\n\n\n\n\n【예제】 \\(\\int_{0}^{\\infty}xe^{- x}dx\\) 표 적분하시오.\n\n\n\n\n\n(방법1) \\(x( - e^{- x}) - e^{- x}\\rbrack_{0}^{\\infty} = 1\\)\n(방법2) \\(x( - e^{- x})\\rbrack_{0}^{\\infty} - \\int_{0}^{\\infty} - e^{- x} = 1\\)으로 계산한다.\n【예제】 \\(\\int_{1}^{2}ln(x)dx\\) 표 적분하시오.\n\n\n\n\n\n\\[\\int_{1}^{2}ln(x)dx = ln(x)x\\rbrack_{1}^{2} - \\int_{0}^{1}1dx = 2ln(2) - ln(1) - x\\rbrack_{0}^{1} = 0.386\\]\n\n\n3. 적분 응용\n연속형 확률분포의 확률밀도함수\n\n\n\n\n\n연속형 확률변수 X 의 확률밀도함수 f(x) 는 특정 구간에서 확률을 계산하는 데 사용됩니다. 이때 확률은 적분을 통해 구합니다:\n\\[P(a \\leq X \\leq b) = \\int_{a}^{b}f(x)dx\\]\nf(x) 는 음수가 아니며, 전체 구간에서의 적분값은 항상 1이 됩니다:\n\\[\\int_{- \\infty}^{\\infty}f(x)dx = 1\\]\n【예제】 정규분포 N(0, 1) 에서 \\(P( - 1 \\leq Z \\leq 1) = \\int_{- 1}^{1}\\phi(z)dz\\) 이다. 여기서 \\(\\phi(z) = \\frac{1}{\\sqrt{2\\pi}}e^{- z^{2}/2}\\)는 표준정규분포의 확률밀도함수입니다.\n누적확률분포함수 cumulative probability density fuction\n\n\n\n\n\n기대값\n  연속형 확률변수 X 의 확률밀도함수 f(x)라 하면 기대값은 \\(E(X) = \\int xf(x)dx\\) 이다.\n적분과 백분위값\n  백분위값 percentile은 확률분포에서 특정 비율의 누적 확률을 기준으로 하는 값입니다. P번째 백분위값은 확률변수 X의 값 \\(X_{P}\\)로, 확률변수가 \\(X_{P}\\)이하일 확률이 \\(\\frac{P}{100}\\)이 되는 값입니다.\n\\[F(x_{P}) = \\int_{- \\infty}^{x_{P}}f(x)dx = \\frac{P}{100}\\]\n  - \\(f(x)\\): 확률밀도함수(PDF)\n  - \\(F(x)\\): 누적분포함수(CDF)\n  - \\(x_{P}\\): \\(P\\)번째 백분위값"
  },
  {
    "objectID": "notes/math/function.html",
    "href": "notes/math/function.html",
    "title": "수학의 기초 1. 함수",
    "section": "",
    "text": "chapter 1. 기초\n\n1. 함수와 통계학\n함수는 통계학에서 데이터를 설명하고 모델링하는 수단이며, 이론적 개념을 수학적으로 표현하는 핵심 도구이다. 데이터 간의 관계를 나타내고, 확률분포, 추정, 검정 등 다양한 통계 기법에서 필수적인 역할을 수행한다.\n통계함수\n  통계함수는 독립변수(\\(x\\))와 종속변수(\\(y\\)) 데이터 간 관계를 설명한다. \\(y = f(x) + e\\)로 표현되며 \\(e\\)는 오차항이다.\n확률밀도함수\n  확률밀도함수 \\(p(x)\\)는 확률변수의 확률이 함수값이다.\n기대값\n  확률변수의 평균적인 값이다. \\(E(X) = \\sum xp(x)\\)\n\n\n2. 함수와 시리즈\n시리즈는 복잡한 함수를 단순한 다항식으로 근사하거나, 함수의 특성을 분석하는 데 사용된다. 시리즈는 유한하거나 무한한 항들로 이루어진 수열의 합으로 정의된다.\n유한 시리즈: \\(S_{n} = {\\sum_{i = 1}^{n}}a_{k}\\)\n무한 시리즈: \\(S_{\\infty} = {\\sum_{i = 1}^{\\infty}}a_{k}\\)\n이항시리즈 binomial series\n\\[(a + b)^{n} = a^{n} + \\binom{n}{1}a^{n - 1}b + ... + \\binom{n}{n - 1}ab^{n - 1} + b^{n}\\]\n특수한 경우\n\\[\\frac{1}{(1 + x)^{2}} = - 1 + 2x - 3x^{2} + 4x^{3} - ...\\]\n\\[\\frac{1}{1 + x} = 1 - x + x^{2} - x^{3} + x^{4} - ...\\]\n지수시리즈 exponential series\n\\[e^{x} = 1 + x + \\frac{x^{2}}{2!} + \\frac{x^{3}}{3!} + ...\\]\n\\[e^{x} = lim_{n \\rightarrow}^{\\infty}(1 + \\frac{x}{n})^{n}\\]\n\\[ln(1 + x) = x - \\frac{{}^{2}}{2} + \\frac{x^{3}}{3} - \\frac{x^{4}}{4} + ..., - 1 &lt; x &lt; 1\\]\n산술시리즈 arithmetic series\n\\[S_{n} = a + (a + d) + (a + 2d) + \\cdots + \\lbrack a + (n - 1)d\\rbrack\\]\n      - \\(a\\): 첫 번째 항, \\(d\\): 공차(항 사이의 일정한 차이), \\(n\\): 항의 개수\n\\[S_{n} = \\frac{n}{2}\\lbrack 2a + (n - 1)d\\rbrack\\]\n기하시리즈 geometric series\n\\[S_{n} = a + ar + ar^{2} + \\cdots + ar^{n - 1}\\]\n     - \\(a\\): 첫 번째 항, \\(r\\): 공비(항 사이의 일정한 차이)\n\\[S_{n} = \\frac{a(1 - r^{n})}{1 - r},r \\neq 1\\]\n무한 기하시리즈: \\(S_{n} = \\frac{a}{1 - r}, - 1 &lt; r &lt; 1\\)\n\n\n3. 통계학 주요상수\n지수 exponent \\(e\\)\n  자연로그 함수의 밑으로 정의되며, 무한 급수로 표현된다.\n  \\(e \\approx 2.71828182845904\\ldots\\)(무리수)\n  통계학의 주요 확률분포함수(정규분포, 포아송분포)의 항이다.\n자연상수 \\(ln2\\)\n\\[\\ln 2 \\approx 0.69314718056\\ldots \\text{(무리수)}\\]\n  정보 이론: 1비트의 정보. 이진수 체계와 로그 연산.\n황금비 \\(\\phi \\approx 1.61803398874989\\ldots\\)\n  \\(a/b = (a + b)/a\\)를 만족하는 비율 \\(\\phi = \\frac{1 + \\sqrt{5}}{2}\\)\n오일러상수\n  조화급수와 자연로그의 차이로 정의된다.\n\\[\\gamma = \\lim_{n \\rightarrow \\infty}\\left( \\overset{n}{\\sum_{k = 1}}\\frac{1}{k} - \\ln n \\right) \\approx 0.577215664901532\\ldots\\]\n\n\n기호\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n소문자\nα\nβ\nγ\nδ\nε\nζ\nη\nθ\n\n\n대문자\nΑ\nΒ\nΓ\nΔ\nΕ\nΖ\nΗ\nΘ\n\n\n발음\nalpha\nbeta\ngamma\ndelta\nepsilon\nzeta\neta\ntheta\n\n\n소문자\nι\nκ\nχ\nλ\nμ\nν\nξ\nο\n\n\n대문자\nΙ\nΚ\nΧ\nΛ\nΜ\nΝ\nΞ\nΟ\n\n\n발음\niota\nkappa\nchi\nlambda\nmu\nnu\nxi(ksi)\nomicron\n\n\n소문자\nπ\nρ\nσ\nτ\nυ\nϕ\nψ\nω\n\n\n대문자\nΠ\nΡ\nΣ\nΤ\nΥ\nΦ\nΨ\nΩ\n\n\n발음\npi\nrho\nsigma\ntau\nupsilon\nphi\npsi\nomega\n\n\n\n\n\n\n\nchapter 2. 좌표와 직선방정식\n\n1. 이차원평면과 데카르트 좌표\n\n\n\n\n\n이차원 평면에서 모든 점은 숫자 좌표로 coordinate 표현할 수 있으며, 점들의 집합으로 이루어진 선이나 곡선은 좌표방정식으로 나타낼 수 있다. 이를 위해 이차원 평면에는 두 개의 직선이 설정된다.. 수평선인 \\(x\\)-축 axis과 수직선인 \\(y\\)-축이다. 이 두 직선은 원점에서 직각으로 교차하며, 원점은 두 축의 기준점이 된다.\n원점을 기준으로 \\(x\\)-축에서 \\(a\\)만큼, \\(y\\)-축에서 \\(b\\)만큼 떨어진 점의 좌표는 \\((a,b)\\)로 표기된다. 이러한 표기 방식은 데카르트 좌표라고 한다. 여기서 \\(a\\)와 \\(b\\)는 각각 \\(x\\)-좌표와 \\(y\\)-좌표를 나타내며, 이 값들은 모두 실수 값으로 구성된다.\n데카르트 Cartesian 좌표계는 이차원 평면에서 점의 위치를 명확하고 직관적으로 나타내는 데 사용되며, 수학적 분석 및 응용의 기초가 된다. 이를 활용하면 점, 선, 곡선, 그리고 다양한 기하학적 형태를 방정식으로 표현하고, 이를 통해 여러 문제를 해결할 수 있다.\n\n\n2. 직선과 증가\n\n\n직선\n두 점을 가장 짧은 거리로 연결하는 선을 직선이라고 한다. 직선은 두 점 사이의 최단 경로로 정의되며, 그 위에는 무수히 많은 점이 존재한다. 좌표평면에서 직선은 중요한 기하학적 구조로, 점과 점 사이의 관계를 나타내는 기본 도구이다.\n\n\n증가량 (Increment)\n\n\n\n\n\n좌표평면에서 두 점 \\((x_{1},y_{1})\\)과 \\((x_{2},y_{2})\\)의 이동을 고려할 때, x-좌표와 y-좌표의 변화량을 각각 증가량이라고 한다.\nx-좌표의 증가량: \\(\\Delta x = x_{2} - x_{1}\\)\ny-좌표의 증가량: \\(\\Delta y = y_{2} - y_{1}\\)\n증가량의 부호와 크기는 두 점의 좌표 차이에 의해 결정되며, x-좌표나 y-좌표의 변화 방향을 나타낸다.\n\n\n기울기 slope\n증가량은 두 점을 지나는 직선의 기울기를 계산하는 데 활용된다. 기울기 m은 두 점 사이의 x-좌표의 증가량에 대한 y-좌표의 증가량의 비율로 정의되며, 다음과 같은 식으로 표현된다:\n\\[m = \\frac{\\Delta y}{\\Delta x} = \\frac{y_{2} - y_{1}}{x_{2} - x_{1}},\\Delta x \\neq 0\\]\n    - \\(m &gt; 0\\): 직선이 오른쪽으로 올라간다.\n    - \\(m &lt; 0\\): 직선이 오른쪽으로 내려간다.\n    - \\(m = 0\\): 직선이 수평이다.\n    - \\(m\\)이 정의되지 않음 (\\(\\Delta x = 0\\)): 직선이 수직이다.\n\n\n수평 parallel과 수직 perpendicular\n두 직선 \\(L_{1}\\)과 \\(L_{2}\\)의 기울기가 동일하면, 즉 \\(m_{1} = m_{2}\\)이면 두 직선은 서로 평행 하다고 한다. 이 경우, 두 직선은 교차하지 않으며, 동일한 방향으로 뻗어 있다.\n두 직선 \\(L_{1}\\)과 \\(L_{3}\\)의 기울기의 곱이 -1이면, 즉 \\(m_{1} \\cdot m_{2} = - 1\\)이면 두 직선은 서로 수직하다고 한다. 이는 두 직선이 교차할 때 \\(90^{\\circ}\\)의 각을 이루는 경우이다.\n\n\n3. 직선 방정식 linear equation\n직선 방정식은 직선 위의 모든 점의 좌표를 만족하며, 직선 이외의 점의 좌표에서는 만족하지 않는 방정식이다. 좌표평면에서 직선은 절편 intercept과 기울기 slope를 이용해 다음과 같은 일반적인 형태로 표현된다. \\(y = bx + a\\)\n\n\\(b\\): 직선의 기울기, \\(a\\): y-축과 교차하는 절편\n\n\n\n직선 구성요소\n기울기 \\(b\\)는 직선이 얼마나 가파르게 증가하거나 감소하는지를 나타내며, x-좌표의 변화량에 대한 y-좌표의 변화량의 비율로 정의된다:\n\\[b = \\frac{\\Delta y}{\\Delta x}\\]\n    - b &gt; 0: 직선이 오른쪽으로 올라간다.\n    - b &lt; 0: 직선이 오른쪽으로 내려간다.\n    - b = 0: 직선이 수평이다.\n절편 a는 직선이 y-축과 만나는 점의 y-좌표를 나타낸다. x = 0일 때, 직선 방정식에서 y = a가 된다.\n\n\n수평선 horizontal line\n기울기 b = 0인 경우, 직선은 수평선이 된다. 이러한 직선의 방정식은 \\(y = a\\)이다. 이 직선은 x-축과 평행하며, y-축 상에서 y = a를 지난다.\n\n\n수직선 vertical line\ny-축과 평행한 직선의 방정식으로 \\(x = c\\)이다. 이 직선은 x-축과 x = c에서 교차한다. 기울기가 정의되지 않으며, 수직선은 y-축과 항상 평행하다.\n\n\n\n\nchapter 3. 함수란?\n\n1. 함수 정의\n함수는 두 집합 사이의 특정 규칙에 따라 값을 대응시키는 관계를 나타낸다. 함수는 정의역과 치역으로 구성되며, 정의역의 각 원소에 대해 치역의 단 하나의 원소만 대응된다. 이를 통해 y가 x에 의해 결정된다고 표현하며, 수학적으로 다음과 같이 나타낸다. \\(y = f(x)\\)\n이는 ”y는 x의 함수이다”라고 읽는다.\n\n정의역 domain\n정의역은 함수에서 x가 가질 수 있는 값들의 집합을 말한다. 즉, 함수 f(x)가 유효하게 정의될 수 있는 모든 입력값의 집합이다.\n\n\n치역 range\n치역은 함수가 출력할 수 있는 값들의 집합이다. 정의역의 원소 x가 함수 f를 통해 출력되는 값 y = f(x)의 모임이 치역이다.\n\n\n대응 규칙\n함수는 정의역의 각 원소를 치역의 한 원소에 대응시키는 규칙을 가지고 있다. 각 정의역의 값 x는 치역에서 정확히 하나의 값 y에 대응해야 한다. (2)번은 동일 x-값에 대하여 2개 y-값이 대응되므로 함수가 아니고 다른 모든 것은 함수이다.\n\n\n\n\n\n\n\n2. 우함수와 기함수\n우함수 even function\n  함수 \\(f(x)\\)가 다음 조건을 만족하면 우함수라 한다:\n\\[f( - x) = f(x)\\text{모든}x \\in \\text{정의역(domain)}\\]\n우함수는 y-축을 기준으로 대칭적이다. 즉, 그래프의 왼쪽 부분을 y-축을 따라 접으면 오른쪽 부분과 정확히 일치한다.\n기함수 odd function\n  함수 \\(f(x)\\)가 다음 조건을 만족하면 기함수라 정의한다:\n\\[f( - x) = - f(x)\\text{모든}x \\in \\text{정의역(domain)}\\]\n기함수는 원점을 기준으로 대칭적이다. 즉, 그래프를 원점을 중심으로 180° 회전시키면 동일한 모양이 된다.\n\n\n3. 함수 종류\n\n\n(1) 함성함수 Composite Function\n합성함수는 두 함수 f(x)와 g(x)가 주어졌을 때, 함수 g(x)의 출력값이 함수 f(x)의 입력값으로 사용되는 새로운 함수이다. 이를 다음과 같이 나타낸다. \\((f \\circ g)(x) = f(g(x))\\)\n    - g(x): 먼저 적용되는 함수.\n    - f(x): g(x)의 출력값을 입력값으로 사용하는 함수.\n    - \\((f \\circ g)(x)\\): f(x)와 g(x)의 합성함수.\n합성함수 \\((f \\circ g)(x)\\)의 정의역은 g(x)와 f(x)가 동시에 유효하게 정의되는 입력값으로 구성된다. 즉, x는 g(x)의 정의역에 속하고, g(x)의 출력값은 f(x)의 정의역에 속해야 한다.\n\\((f \\circ g)(x)\\)는 다음 두 단계를 거친다:\n    - 먼저 x에 대해 g(x)를 계산하고 그런 다음, f(x)에 g(x)를 대입하여 f(g(x))를 계산한다.\n\\[f(x) = 2x + 1,g(x) = x^{2}\\]\n\\[f(g(x)) = f(x^{2}) = 2x^{2} + 1\\]\n\\[g(f(x)) = g(2x + 1) = (2x + 1)^{2}\\]\n\n\n(2) 절대값 함수\n숫자 x의 절대값(absolute value)은 x의 크기(거리를 나타냄)를 의미하며, 항상 0 이상의 값을 가진다. 절대값은 다음과 같이 정의된다.\n\\[|x| = \\{\\begin{matrix}\nx, & \\text{if}x \\geq 0 \\\\\n- x, & \\text{if}x &lt; 0\n\\end{matrix}\\]\n절대값은 숫자 x와 0 사이의 거리로 해석된다. 절대값의 결과는 항상 양수이거나 0이다.\n\n\n(3) 정수함수 integer function\n정수 함수는 숫자 x를 넘지 않는 최대 정수를 반환하는 함수이다. 이를 바닥함수 floor function라고도 하며, 다음과 같이 정의된다.\n\\[\\lfloor x\\rfloor = \\text{최대 정수}n\\text{such that}n \\leq x\\]\n    - \\(\\lfloor x\\rfloor\\): x를 넘지 않는 가장 큰 정수.\n    - \\(\\lfloor x\\rfloor\\)는 항상 \\(n \\leq x &lt; n + 1\\)을 만족한다.\n\n\n4. 함수의 사칙연산\n두 함수 f(x)와 g(x)가 주어졌을 때, 이들 함수에 대해 덧셈, 뺄셈, 곱셈, 나눗셈과 같은 사칙연산을 정의할 수 있다. 각 연산은 정의역에서 두 함수의 값에 기반하여 계산된다.\n\n\n함수의 덧셈/뺄셈\n\\[(f \\pm g)(x) = f(x) \\pm g(x)\\]\n정의역: f(x)와 g(x)가 동시에 정의된 구간.\n결과: f(x)의 값과 g(x)의 값을 더한(뺀) 결과.\n\n\n함수의 곱셈\n\\[(f \\cdot g)(x) = f(x) \\cdot g(x)\\]\n\n정의역: f(x)와 g(x)가 동시에 정의된 구간.\n결과: f(x)와 g(x)의 값을 곱한 결과.\n\n\n\n함수의 나눗셈\n\\[\\left( \\frac{f}{g} \\right)(x) = \\frac{f(x)}{g(x)},g(x) \\neq 0\\]\n\n정의역: f(x)와 g(x)가 동시에 정의되고, \\(g(x) \\neq 0\\)인 구간.\n결과: f(x)의 값을 g(x)의 값으로 나눈 결과. \n\n\n\n\nchapter 4. 함수의 응용 및 극한\n\n1. 함수의 통계 응용\n\n\n(1) 확률밀도함수 \\(f(x)\\)\n연속형확률변수의 분포를 나타내는 함수로, 특정 구간 내에서 값이 나타날 확률의 상대적인 가능성을 표현한다.\n    - 확률밀도함수 정의: \\(f(x) \\geq 0,\\int_{- \\infty}^{\\infty}f(x)dx = 1\\)\n    - 정규분포의 확률밀도함수: \\(f(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^{2}}}e^{- \\frac{(x - \\mu)^{2}}{2\\sigma^{2}}}\\)\n    - 데이터의 분포, 확률 계산 \\(P(a \\leq X \\leq b) = \\int_{a}^{b}f(x)dx\\)\n\n\n(2) 누적확률밀도함수\n확률변수가 특정 값 이하일 확률을 나타내는 함수이다.\n\n누적확률밀도함수 정의: \\(F(x) = P(X \\leq x) = \\int_{- \\infty}^{x}f(t)dt\\)\n정규분포 CDF: \\(F(x) = \\frac{1}{2}\\left\\lbrack 1 + \\text{erf}\\left( \\frac{x - \\mu}{\\sqrt{2}\\sigma} \\right) \\right\\rbrack\\)\n분위수 결정: \\(P(X \\leq x_{p}) = p\\) 만족하는 \\(X_{p}\\)를 찾음.\n\n\n\n(3) 회귀모형\n함수는 독립변수와 종속변수 간의 관계를 모델링하는 데 사용된다. 회귀모형은 함수 형태로 데이터의 추세를 설명한다.\n    - 선형회귀: \\(y = \\beta_{0} + \\beta_{1}x + \\epsilon\\)\n    - 비선형 회귀: \\(y = ae^{bx} + \\epsilon\\)\n    - 변수 간 관계 분석, 예측 모델 구축\n\n\n(4) 생존분석\n생존분석에서는 생존시간 분포를 분석하는 데 함수가 사용된다.\n\n생존survival 함수: \\(S(t) = P(T &gt; t) = 1 - F(t)\\)\n위험hazard 함수: \\(h(t) = \\frac{f(t)}{S(t)}\\)\n제품 수명 분석, 의료 데이터에서 생존 확률 평가\n\n\n\n(5) 시계열분석\n함수는 시간에 따른 데이터의 변화를 모델링하고 분석하는 데 사용된다.\n\n자기회귀 모델: \\(X_{t} = \\phi_{1}X_{t - 1} + \\phi_{2}X_{t - 2} + \\cdots + \\epsilon_{t}\\)\n주식 시장 예측, 온도 변화 모델링.\n\n\n\n(6) 함수와 몬테카를로 시뮬레이션\n함수는 확률 분포로부터 난수를 생성하여 복잡한 통계 문제를 해결하는 데 사용된다.\n\n\\(\\pi\\) 값 추정: \\(f(x) = \\sqrt{1 - x^{2}},\\text{for}x \\in \\lbrack 0,1\\rbrack\\)\n\n\n\n2. 함수의 극한\n\n\n(1) 극한 정의\n임의의 \\(\\varepsilon &gt; 0\\)가 주어졌을 때, 모든 \\(x\\)가 특정 값 \\(a\\)에 충분히 가까워질 때 \\((0 &lt; |x - a| &lt; \\delta),f(x)\\)가 특정 값 \\(L\\)에 가까워진다면, 함수 \\(f(x)\\)의 극한은 존재하며 그 극한값은 \\(L\\)이라고 정의한다. 이를 수학적으로 표현하면 \\(\\lim_{x \\rightarrow a}f(x) = L\\) 이다.\n\n\\(\\varepsilon\\): \\(f(x)\\)와 \\(L\\)사이의 허용 오차.\n\\(\\delta\\): \\(x\\)와 \\(a\\) 사이의 거리 제한.\n\n엄밀한 정의 (\\(\\varepsilon - \\delta\\)정의)\\(\\forall\\varepsilon &gt; 0,\\exists\\delta &gt; 0\\text{such that}0 &lt; |x - a| &lt; \\delta \\Longrightarrow |f(x) - L| &lt; \\varepsilon\\)이 의미는 \\(x\\)와 \\(a\\)에 충분히 가까워지면 \\((|x - a| &lt; \\delta)\\) 함수 \\(f(x)\\)의 값이 \\(L\\)에 충분히 가까워짐 \\((|f(x) - L| &lt; \\varepsilon)\\)을 보장한다.\n\n\n(2) 함수값과 극한값\n함수값 \\(f(a)\\)는 함수가 특정 점 \\(x = a\\)에서 실제로 가지는 값이다. 반면, 극한값 \\(\\lim_{x \\rightarrow a}f(x)\\)는 \\(x\\)가 \\(a\\)에 가까워질 때 \\(f(x)\\)가 수렴하는 값을 나타낸다. 함수값과 극한값은 다를 수 있으며, 함수가 \\(x = a\\)에서 정의되지 않아도 극한값은 존재할 수 있다.\n함수값 \\(f(a) = k\\)\n  함수가 \\(x = a\\)에서 정의되어 있다면 \\(f(a)\\)는 \\(k\\), 특정 값을 가진다.\n극한값\n  극한값은 좌극한(left-hand limit)과 우극한(right-hand limit)에 따라 달라질 수 있다:\n    - 좌극한 \\(L_{2}\\): \\(\\lim_{x \\rightarrow a^{-}}f(x) = L_{2}\\)\n    - 우극한 \\(L_{1}\\): \\(\\lim_{x \\rightarrow a^{+}}f(x) = L_{1}\\)\n    - 전체 극한은 좌극한과 우극한이 동일할 때 존재한다,\n\n\n(3) 연속함수 정의\n함수 \\(f(x)\\)가 \\(x = a\\)에서 연속하려면 다음 세 가지 조건을 모두 만족해야 한다:\n  1. \\(f(a)\\)가 정의되어 있어야 한다.\n  2. \\(\\lim_{x \\rightarrow a}f(x)\\)가 존재해야 한다.\n  3. 함수값과 극한값이 일치해야 한다. \\(\\lim_{x \\rightarrow a}f(x) = f(a)\\)\n\n\n(4) 극한 계산 규칙\n상수함수의 극한\n  \\(\\lim_{x \\rightarrow a}c = c\\), 상수 함수의 극한은 상수 자신이다.\n항등함수의 극한\n  \\[\\lim_{x \\rightarrow a}x = a\\]\n선형성\n  극한 연산은 선형성을 가진다:\n\\[\\lim_{x \\rightarrow a}\\lbrack f(x) \\pm g(x)\\rbrack = \\lim_{x \\rightarrow a}f(x) \\pm \\lim_{x \\rightarrow a}g(x)\\]\n곱셈\n  두 함수의 곱의 극한은 각 함수의 극한의 곱과 같다.\n\\[\\lim_{x \\rightarrow a}\\lbrack f(x) \\cdot g(x)\\rbrack = \\left( \\lim_{x \\rightarrow a}f(x) \\right) \\cdot \\left( \\lim_{x \\rightarrow a}g(x) \\right)\\]\n나눗셈\n  두 함수의 나눗셈의 극한은 각 함수의 극한의 나눗셈과 같다 (분모가 0이 아닌 경우)\n\\[\\lim_{x \\rightarrow a}\\frac{f(x)}{g(x)} = \\frac{\\lim_{x \\rightarrow a}f(x)}{\\lim_{x \\rightarrow a}g(x)},\\lim_{x \\rightarrow a}g(x) \\neq 0\\]\n거듭제곱\n  \\(\\lim_{x \\rightarrow a}\\lbrack f(x)\\rbrack^{n} = \\left( \\lim_{x \\rightarrow a}f(x) \\right)^{n}\\), 여기서 \\(n\\)은 정수이다.\n루트\n  \\[\\lim_{x \\rightarrow a}\\sqrt[n]{f(x)} = \\sqrt[n]{\\lim_{x \\rightarrow a}f(x)},\\text{if}\\lim_{x \\rightarrow a}f(x) \\geq 0\\]\n합성함수의 극한 (연쇄법칙)\n  만약 \\(g(x)\\)의 극한이 \\(a\\)로 접근할 때 \\(b\\)이고, \\(f(x)\\)가 \\(b\\)에서 연속이면\n\\(\\lim_{x \\rightarrow a}f(g(x)) = f\\left( \\lim_{x \\rightarrow a}g(x) \\right)\\) 이다.\nL’Hôpital’s Rule의 정의\n  함수 f(x)와 g(x)가 x \\to a에서 각각 0/0 형태 또는 \\infty/\\infty 형태를 가지는 경우, 두 함수의 극한은 다음과 같이 계산할 수 있다:\n\\[\\lim_{x \\rightarrow a}\\frac{f(x)}{g(x)} = \\lim_{x \\rightarrow a}\\frac{f'(x)}{g'(x)},\\text{if}\\lim_{x \\rightarrow a}\\frac{f'(x)}{g'(x)}\\text{exists.}\\]\n\n형태: \\(\\frac{0}{0}\\) 또는 \\(\\frac{\\infty}{\\infty}\\)와 같은 불정형 형태를 가져야 한다.\n미분 가능성: \\(f(x)\\)와 \\(g(x)\\)는 \\(x \\rightarrow a\\)에서 미분 가능해야 한다.\n분모의 도함수가 0이 아님: \\(g'(x) \\neq 0\\)인 구간에서 적용 가능\n\\(\\frac{0}{0}\\) 형태: \\(\\lim_{x \\rightarrow 0}\\frac{\\sin(x)}{x} = \\lim_{x \\rightarrow 0}\\frac{\\cos(x)}{1} = \\cos(0) = 1\\)\n\\(\\frac{\\infty}{\\infty}\\) 형태: \\(\\lim_{x \\rightarrow \\infty}\\frac{x}{e^{x}} = \\lim_{x \\rightarrow \\infty}\\frac{1}{e^{x}} = 0\\)\n\n무한대 있는 극한\n  \\(x\\)가 무한대 \\(\\infty\\)혹은 \\(- \\infty\\)로 접근할 때 함수 \\(f(x)\\)의 극한을 구하는 규칙이다.\n\\[lim_{x \\rightarrow \\pm \\infty}\\frac{1}{x} = 0\\]\n\\(lim_{x \\rightarrow \\pm \\infty}c = c\\), \\(c\\)는 상수\n함수가 분수의 형태를 가지면 분모의 가장 큰 \\(x\\)차수로 나누고 위의 규칙을 이용하라.\n특정 함수의 극한\n  - 지수 함수: \\(\\lim_{x \\rightarrow \\infty}e^{- x} = 0\\)\n  - 삼각 함수: \\(\\lim_{x \\rightarrow 0}\\frac{\\sin(x)}{x} = 1\\), \\(\\lim_{x \\rightarrow 0}\\frac{1 - \\cos(x)}{x^{2}} = \\frac{1}{2}\\)\n  - 로그 함수: \\(\\lim_{x \\rightarrow \\infty}\\ln(x) = \\infty\\)\n\n\n3. 수렴 convergence\n수렴의 정의\n  함수 \\(f(x)\\) 또는 수열 \\(\\{ a_{n}\\}\\)가 특정 값에 수렴한다는 것은 극한값이 존재하며, 일정 값에 점점 가까워진다는 것을 의미한다.\n수열의 수렴\n  수열 \\(\\{ a_{n}\\}\\)이 \\(L\\)로 수렴한다면, 임의의 \\(\\varepsilon &gt; 0\\)에 대해 \\(n \\geq N\\)일 때 다음 조건을 만족하는 \\(N\\)이 존재한다.\n\\(|a_{n} - L| &lt; \\varepsilon\\), 여기서 \\(L\\)은 수열의 극한값이다.\n함수의 수렴\n  함수 \\(f(x)\\)가 \\(L\\)로 수렴하면, \\(x \\rightarrow a\\)에서 \\(\\lim_{x \\rightarrow a}f(x) = L\\)\n수렴의 성질\n  수열이나 함수가 수렴하면 극한값은 유일하다.\n  수렴하는 함수나 수열은 경계값을 가지며, 점점 극한값에 가까워진다.\n극한과 수렴의 차이\n  극한은 특정 값에 접근하는 경향을 나타내며, 함수나 수열이 특정 점에서 어떻게 동작 하는지 설명한다.\n  수렴은 극한값이 존재하고 일정 값에 점점 가까워지는 성질을 나타낸다.\n\n\n4. 확률수렴과 분포수렴\n\n\n(1) 확률수렴 (Convergence in Probability)\n확률변수의 열 \\(\\{ X_{n}\\}\\)이 확률변수 \\(X\\)에 확률수렴한다는 것은, 임의의 \\(\\varepsilon &gt; 0\\)에 대해 다음 조건을 만족하는 \\(n \\rightarrow \\infty\\)가 존재함을 의미한다. \\(\\lim_{n \\rightarrow \\infty}P(|X_{n} - X| \\geq \\varepsilon) = 0\\)\n  - 표기: \\(X_{n}\\overset{P}{\\rightarrow}X\\)\n해석\n  확률적으로 \\(|X_{n} - X|\\)가 작아질 가능성이 1에 가까워짐을 나타낸다. 즉, \\(X_{n}\\)과 \\(X\\)가 점점 ”가까워진다”고 해석할 수 있다.\n성질\n  확률수렴의 유일성: 극한값 \\(X\\)는 유일하다.\n  확률수렴과 함수: \\(X_{n}\\overset{P}{\\rightarrow}X\\)이고 \\(g(x)\\)가 연속 함수라면 \\(g(X_{n})\\overset{P}{\\rightarrow}g(X)\\)\n통계학 응용\n  추정량의 일치성: 추정량 \\({\\widehat{\\theta}}_{n}\\)이 모수 \\(\\theta\\)에 확률수렴하면 \\({\\widehat{\\theta}}_{n}\\)은 일치추정량이다. 법칙의 수렴: 큰 수의 약법칙은 확률수렴으로 표현된다:\n\\[{\\overline{X}}_{n}\\overset{P}{\\rightarrow}\\mu\\]\n\n\n(2) 분포수렴 (Convergence in Distribution)\n확률변수의 수열 \\(\\{ X_{n}\\}\\)이 확률변수 \\(X\\)에 분포수렴한다는 것은, 모든 연속점 \\(x\\)에서 누적분포함수(FDF) \\(F_{X_{n}}(x)\\)가 \\(F(x)\\)로 수렴함을 의미한다. \\(\\lim_{n \\rightarrow \\infty}F_{X_{n}}(x) = F_{X}(x),\\forall x\\) \\(F_{X}(x)\\)에서 연속함수.\n  표기: \\(X_{n}\\overset{\\mathcal{D}}{\\rightarrow}X\\)\n해석\n  분포수렴은 \\(X_{n}\\)의 분포가 \\(X\\)의 분포로 점점 가까워지는 것을 의미한다. 개별적인 실현값이 아니라 분포 전체의 형태를 고려한다.\n성질\n  (1) 연속성: 분포수렴은 누적분포함수의 연속점에서 정의된다.\n  (2) 함수와 분포수렴: \\(X_{n}\\overset{\\mathcal{D}}{\\rightarrow}X\\)이고 \\(g(x)\\)가 연속 함수라면\n\\(g(X_{n})\\overset{\\mathcal{D}}{\\rightarrow}g(X)\\) 이다.\n응용\n  중심 극한 정리: 표본 평균이 정규분포로 수렴하는 현상은 분포수렴으로 나타낸다. \\(\\sqrt{n}({\\overline{X}}_{n} - \\mu)\\overset{\\mathcal{D}}{\\rightarrow}N(0,\\sigma^{2})\\)\n\n\n(3) 확률수렴과 분포수렴의 관계\n확률수렴 → 분포수렴\n\\[X_{n}\\overset{P}{\\rightarrow}X \\Longrightarrow X_{n}\\overset{\\mathcal{D}}{\\rightarrow}X\\]\n분포수렴 ≠ 확률수렴\n  분포수렴이 확률수렴을 보장하지 않는다. 예를 들어, \\(X_{n} \\sim U( - n,n)\\)은 \\(X = 0\\)에 분포수렴하나 확률수렴하지 않는다."
  },
  {
    "objectID": "cards/news01.html",
    "href": "cards/news01.html",
    "title": "뉴스 속 통계 이야기",
    "section": "",
    "text": "최근 발표된 통계를 통해 우리 사회의 고령화 속도를 살펴봅니다."
  },
  {
    "objectID": "cards/news01.html#뉴스-통계청-발표로-본-인구-구조-변화",
    "href": "cards/news01.html#뉴스-통계청-발표로-본-인구-구조-변화",
    "title": "뉴스 속 통계 이야기",
    "section": "",
    "text": "최근 발표된 통계를 통해 우리 사회의 고령화 속도를 살펴봅니다."
  },
  {
    "objectID": "python/analysis.html",
    "href": "python/analysis.html",
    "title": "세상의 모든 통계 이야기",
    "section": "",
    "text": "x = [1, 2, 3]\ny = [3, 6, 9]\nfor xi, yi in zip(x, y):\n    print(xi, yi)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "세상의 모든 통계 이야기",
    "section": "",
    "text": "🎓 Welcome to Prof. Kwon’s 통계이야기\n\n\n\n\n\n한남대학교에서 1999년부터 제공하던 강의노트를 이곳으로 옮겨 왔으며\n데이터 사이언스 중심으로 강의노트를 개편하고 있습니다. since 1999.03 / 2024.01\n\n| put a ding in the universe |\n\n\n\n권세혁 교수 통계 웹사이트에 오신 걸 환영합니다!\n\n📘 강의노트: 수학기초\n🐍 파이썬 분석\n📰 카드뉴스 콘텐츠"
  },
  {
    "objectID": "notes/math/vector.html",
    "href": "notes/math/vector.html",
    "title": "수학의 기초 3. 벡터",
    "section": "",
    "text": "chapter 1. 선형대수 개념\n\n1. 선형대수 정의\n선형대수(Linear Algebra)는 벡터 공간과 벡터 간의 관계를 탐구하며, 벡터와 행렬을 활용한 수학적 표현과 계산을 다루는 수학 분야이다. 주요 구성 요소로는 벡터, 행렬, 스칼라가 있으며, 핵심 개념으로는 선형 변환, 고유값과 고유벡터, 내적(inner product), 외적(cross product), 그리고 행렬 분해 등이 포함된다.\n통계학에서 선형대수는 데이터를 벡터와 행렬로 표현하여 복잡한 수치 계산을 단순화하고 효율적으로 수행할 수 있게 해 준다. 특히, 고차원 데이터의 계산 및 변환을 가능하게 하며, 이는 데이터 분석의 구조 이해와 차원 축소에 핵심적인 역할을 한다. 또한, 선형대수는 머신러닝과 통계 모델링의 기초를 이루며, 회귀 분석, 주성분 분석(PCA), 군집 분석 등 다양한 기법에서 필수적으로 활용된다.\n\n\n2. 선형대수와 선형변환\n선형대수는 선형적인 관계를 다루는 학문으로, 모든 연산과 변환이 선형성을 만족해야 한다. 따라서 일반적인 함수는 직접적으로 주요 개념으로 다루어지지 않지만, 선형 변환이라는 개념이 함수의 특수한 형태로 다뤄진다. 선형변환은 벡터 공간의 구조를 유지하며, 행렬로 표현될 수 있어 선형대수의 핵심 요소 중 하나로 자리 잡고 있다.\n\n\n함수 \\(y=f(x)\\)\n두 집합 사이의 관계로, 각 입력값(정의역 domain)에 대해 정확히 하나의 출력값(공역 range)을 대응시키는 규칙이다. 함수는 f : X → Y로 표기되며, X는 정의역, Y는 공역입니다. 함수값이 0인 f(x) = 0를 방정식이라 하고 이를 만족하는 x를 방정식의 해(root, solution)라고 한다.\n\n\n\n\n\n\n\n선형함수\n선형함수는 입력 변수와 출력 변수 사이의 관계를 직선으로 나타내는 함수입니다. 일반적으로 다음과 같은 형태로 표현되며 다음 두 성질을 갖는다. f(x) = a + bx, a: 절편, b: 기울기\n  가법성 additivity: f(x + y) = f(x) + f(y)\n  동차성 homogeniety: f(cx) = cf(x), c는 상수\n\n\n선형변환\n선형변환은 벡터 공간에서 벡터를 다른 벡터로 변환하는 함수의 특수한 형태이다. 이 변환은 선형성(linearity)이라는 두 가지 성질을 만족해야 한다. \\(\\underset{¯}{u},\\underset{¯}{v}\\) 동일 차원의 벡터에 대하여 함수 T가 다음 조건을 만족하면 선형변환이다.\n  덧셈에 대한 선형성: \\(T(\\underset{¯}{u} +\n\\underset{¯}{v}) = T(\\underset{¯}{u}) + T(\\underset{¯}{v})\\)\n  스칼라 곱에 대한 선형성: \\(T(c\\underset{¯}{u}) =\ncT(\\underset{¯}{u})\\) , $ = (\n\n\n\n\n\n\n\n\nchapter 2. 벡터\n\n1. 벡터정의\n벡터는 정렬된 유한한 숫자 목록으로 일반적으로 정사각형 또는 곡선 괄호로 둘러싸인 수직 형태의 배열로 작성된다. 수평 배열의 행벡터와 구별하기 위하여 열벡터라고 하기도 한다.\n\\(\\left( \\begin{array}{r}\n1 \\\\\n- 2 \\\\\n0\n\\end{array} \\right)\\), \\(\\left\\lbrack\n\\begin{array}{r}\n1 \\\\\n- 2 \\\\\n0\n\\end{array} \\right\\rbrack\\)\n벡터를 행으로 사용할 때는 쉼표로 구분되고 괄호로 둘러싸인 숫자로 쓴다. \\(\\left(\n\\begin{array}{r}\n1, - 2,0\n\\end{array} \\right)\\)\n배열의 값을 벡터의 원소 element 라 하고 원소의 개수를 벡터의 크기(차원 demension)라고 한다. 위 벡터는 크기가 3 이고 세 번째 원소는 0 이다. n 크기의 벡터는 n-벡터라고 불리고 1벡터는 숫자와 같은 것으로 간주한다. 즉, 우리는 1-벡터 [ 13 ]와 숫자 13을 구별하지 않으며 숫자는 스칼라 scalar 라 한다. 벡터의 각 원소는 스칼라이고 원소가 실수인 ai ∈ Rn 벡터를 실수 벡터라 한다.\n\n\n2. 벡터 기호\nn-벡터를 나타내기 위해 \\({\\underset{¯}{a}}_{n}\\)(구별이 가능한 경우 알파벳 a를 벡터로 표현) 기호를 사용한다. an벡터 의 i-번째 요소는 ai로 표시되며, 여기서 첨자 i는 벡터의 크기인 1에서 n까지 정의되는 정수 인덱스이다. 두 벡터 an, bn가 동일하다는 것은 (1)크기(차수)도 n 동일하고 (2) 각 대응 원소가 동일 ai = bi함을 의미한다.\n\n\n3, 특수한 벡터\n\n\n(1) 영벡터 zero vector\n모든 원소가 0인 벡터이며 0n으로 표현된다. 일반적으로 모든 0 벡터는 0으로 표시되며, 숫자 0을 나타내는데 사용되는 것과 동일한 기호이다. 다른 크기의 제로 벡터를 나타내기 위해 모두 같은 기호 0을 사용하므로 기호 0은 문맥에 따라 다른 것을 의미할 수 있기 때문에 컴퓨터에서는 이를 과부하라 한다.\n\n\n(2) 단위벡터 unit vector\n(표준) 단위 벡터는 1인 하나의 원소를 제외한 모든 요소가 0과 같은 벡터이다. i-번째 단위 벡터(n 크기)는 i-번째 원소만 1을 가진 단위 벡터이며, ei로 표현한다. 이렇게 되면 크기를 나타내는 첨자와 1인 원소 위치를 나타내는 첨자가 구별이 되지 않는 모호성을 갖는다.\n\n\n(3) 일벡터 ones vector\n모든 원소가 1인 n-벡터이며 1n로 표현한다. 우리는 또한 벡터의 크기가 문맥에서 결정될 수 있다면 1을 쓴다.\n\n\n4. 벡터 개념\n\n\n(1) 위치 location\n2차원 공간, 즉 평면의 위치를 나타내는 데 사용될 수 있다. 3-벡터는 3차원(3-D) 공간에서 어떤 지점의 위치나 위치를 나타내는 데 사용된다. 벡터의 원소는 위치의 좌표를 제공한다. 벡터는 주어진 시간에 평면이나 3차원 공간에서 움직이는 지점의 속도나 가속도를 나타내는 데 사용될 수 있다.\n\n\n\n\n\n\n\n(2) 희소성\n많은 원소가 0이면 희소하다고 한다. 그것의 희소성 패턴은 0이 아닌 항목의 인덱스 집합이다. n-벡터 an의 0이 아닌 항목의 수는 nnz(an)로 표시한다다. 단위벡터는 0이 아닌 항목이 하나만 있기 있고 0 벡터는 0이 아닌 항목이 없기 때문에 희소한 벡터이다.\n\n\n(3) 이미지\n3차원 벡터는 빨간색, 녹색 및 파란색(R-G-B) 강도 값(0에서 1 사이)을 제공하는 항목을 통해 색상을 나타낸다. 벡터(0,0,0)는 검은색을 나타내고, 벡터(0, 1, 0)는 밝은 순수한 녹색을 나타내며, 벡터(1, 0.5, 0.5)는 분홍색을 나타낸다.\n\n\n\nchapter 3. 벡터 연산과 크기 \n\n1. 벡터 연산\n\n\n(1) 벡터 합\n두 벡터를 합을 구한다는 것은 (1) 차수가 동일한 두 벡터의 (2) 동일 위치의 원소를 합하여 하나의 벡터를 계산한다는 것을 의미한다. 차도 동일하다.\n\\(\\left\\lbrack \\begin{array}{r}\n1 \\\\\n- 2 \\\\\n0\n\\end{array} \\right\\rbrack + \\left\\lbrack \\begin{array}{r}\n1 \\\\\n2 \\\\\n3\n\\end{array} \\right\\rbrack = \\left\\lbrack \\begin{array}{r}\n2 \\\\\n0 \\\\\n3\n\\end{array} \\right\\rbrack\\), \\(\\left\\lbrack \\begin{array}{r}\n1 \\\\\n- 2 \\\\\n0\n\\end{array} \\right\\rbrack - \\left\\lbrack \\begin{array}{r}\n1 \\\\\n2 \\\\\n3\n\\end{array} \\right\\rbrack = \\left\\lbrack \\begin{array}{r}\n0 \\\\\n- 4 \\\\\n- 3\n\\end{array} \\right\\rbrack\\)\n\n\n성질\n\n차수가 동일한 벡터 a, b, c에 대하여 다음이 성립한다.\n교환법칙 : a + b = b + a\n교환법칙 : (a + b) + c = a + (b + c) 영벡터를 더하거나 빼도 영향을 받지 않는다. a ± 0 = a 벡터에서 자체 벡터를 빼면 영벡터가 된다. a − a = 0\n\n\n\n(2) 스칼라-벡터 곱\n벡터에 스칼라(즉, 숫자)를 곱하는 스칼라-벡터 곱셈은 벡터의 모든 요소에 스칼라를 곱하여 수행한다. 일반적으로 스칼라를 왼쪽, 벡터를 오른쪽에 적지만 순서를 바꾸어 사용해도 되고 계산 결과는 동일하다.\n\\(a = \\left\\lbrack \\begin{array}{r}\n1 \\\\\n- 2 \\\\\n0\n\\end{array} \\right\\rbrack\\)이면 \\(3a = a3\n= \\left\\lbrack \\begin{array}{r}\n3 \\\\\n- 6 \\\\\n0\n\\end{array} \\right\\rbrack\\)\n\n\n성질\n벡터 a, 스칼라 c, k에 대하여 다음이 성립한다. 교환법칙 : ka = ak 배분법칙 : (c + k)a = ca + ka\n\n\n(3) 선형 결합 linear combination\n차수 n-벡터 a1, a2, …, am, 스칼라 k1, k2, …, km에 대하여 다음 n-벡터를 벡터 a1, a2, …, am의 선형결합이라 하고 스칼라 k1, k2, …, km는 선형결합의 계수라 한다.\n\nk1a1 + k2a2 + … + kmam\nk1 = k2 = … = km = 1이면, 선형결합은 벡터 합이다. \\(k_{1} = k_{2} = ... = k_{m} =\n\\frac{1}{m}\\)이면, 선형결합은 벡터 평균이다.\nk1 + k2 + … + km = 1이면, 선형결합은 affine 결합이라 하고 모든 계수가 양수인 경우 선형결합을 가중평균이라 한다.\n\n\n\n(4) 내적 inner product\n두 벡터 간의 관계를 정의하고 벡터의 길이와 각도 등의 개념을 도입하는 중요한 연산이다. 차수(m)가 동일한 두 벡터 (u, v)의 내적 곱은 다음과 같이 정의하고 결과는 스칼라이다. \\[u^{T}v = \\lbrack\nu_{1},u_{2},...,u_{m}\\rbrack\\left\\lbrack \\begin{array}{r}\nv_{1} \\\\\nv_{2} \\\\\n... \\\\\nv_{m}\n\\end{array} \\right\\rbrack = u_{1}v_{1} + u_{2}v_{2} + ... + u_{m}v_{m} =\n\\overset{m}{\\sum_{i = 1}}u_{i}v_{i}\\] 단, uT는 u의 전치 transpose라 하고 열벡터를 행벡터로 변환한 것이다. 【예제】\n\n\n\n\n\n\n\n\\[\\lbrack 1,3,5\\rbrack^{T}\\left\\lbrack\n\\begin{array}{r}\n0 \\\\\n- 1 \\\\\n1\n\\end{array} \\right\\rbrack = (1)(0) + (3)( - 1) + (5)(1) =\n2\\]\n\n\n\n\n\n\n내적 성질\n\nunit 벡터 : eiv = vi\n벡터 합 : \\(1_{m}^{T}v = \\overset{m}{\\sum_{i\n= 1}}v_{i}\\)\n벡터 평균 : \\(avg(v) = (1/n)1_{m}^{T}v =\n(1/n)\\overset{m}{\\sum_{i = 1}}v_{i}\\)\n벡터 제곱합 : \\(v^{T}v = v_{1}^{2} +\nv_{2}^{2} + ... + v_{m}^{2} = \\overset{m}{\\sum_{i =\n1}}v_{i}^{2}\\)\n\n\n\nCauchy–Schwarz inequality\n차수 동일한 두 벡터의 내적 inner product에 대하여 다음이 성립한다.  ∥ aTb ∥  ≤  ∥ a ∥  ∥ b∥ \\[|\\overset{n}{\\sum_{i}}a_{i}b_{i}| \\leq\n(\\sum a_{i}^{2})^{\\frac{1}{2}}(\\sum\nb_{i}^{2})^{\\frac{1}{2}}\\]\n\n\n(5) 외적 cross product\n주로 3차원 공간에서 두 벡터로부터 새로운 벡터를 생성하는 연산입니다. 이 연산의 결과는 두 벡터에 모두 수직인 벡터이며, 크기는 두 벡터가 이루는 평행사변형의 면적에 해당합니다.\n\n\n외적 정의\n벡터 \\(\\underset{¯}{a} =\n(a_{1},a_{2},a_{3})\\)와 벡터 \\(\\underset{¯}{b} = (b_{1},b_{2},b_{3})\\)의 외적 \\(\\underset{¯}{a}\n\\times \\underset{¯}{b}\\)는 다음과 같이 계산한다.\n\nx 성분: a2b3 − a3b2\ny 성분: a3b1 − a1b3\nz 성분: a1b2 − a2b1\n\n\n\n\n\n\n【예제】 벡터 \\(\\underset{¯}{a} =\n(2,3,4)\\)와 벡터 \\(\\underset{¯}{b} =\n(5,6,7)\\)의 외적은 \\(\\underset{¯}{c} =\n\\underset{¯}{a} \\times \\underset{¯}{b} = ( - 3,6, - 3)\\) 이다. 외적은 벡터 \\(\\underset{¯}{a},\\underset{¯}{b}\\)와 수직(\\({\\underset{¯}{c}}^{T}\\underset{¯}{a} =\n0\\), \\({\\underset{¯}{c}}^{T}\\underset{¯}{b} =\n0\\))이며 외적의 크기(놈 norm)는 두 벡터가 이루는 평행사면형 면적이다.\n\n\n2. 선형함수\n\n\n선형함수 정의\nf : Rn → R는 크기 n-벡터를 실수(스칼라)로 매핑하는 함수이다. 함수 f(x)의 x1, x2, …, xn은 함수 f의 인수 argument라 하고 결과 값 스칼라는 함수 값이다. f(x) = f(x1, x2, …, xn)\n【예제】 \\[\nf: \\mathbb{R}^4 \\to \\mathbb{R}, \\quad f(x) = x_1 - x_2 + x_4^2\n\\]\n차수 n-벡터 a, x에 대하여 내적 함수 f(x) = aTx = scalar는 선형함수일 때 다음이 성립한다. 단, α, β는 스칼라, (x, y)는 n-벡터이다. f(αx + βy) = αf(x) + βf(y)\n\n\n선형함수 조건\n다음 조건을 만족하는 f : Rn → R 는 선형함수이다. 단, α는 스칼라, (x, y)는 n-벡터이다.\n\nHomogeniety : f(αx) = αf(x)\nAdditivity : f(x + y) = f(x) + f(y)\n\n\n\n(1) 절편 Affine 함수\n선형 함수에 상수 항을 추가한 형태의 함수이다. 이는 선형 변환과 평행 이동을 결합한 함수로, 다음과 같은 수식으로 표현된다. n-벡터, x에 대하여 다음 f는 절편 함수이다. 단, a는 n-벡터, k는 스칼라이다. f(aTx + k) = aTf(x) + k\n【예제】\n\n\n\n\n\n\n\nf(x) = 7 − 2x1 + 3x2 − x3, \\(k = 7,a = \\left\\lbrack \\begin{array}{r}\n- 2 \\\\\n3 \\\\\n- 1\n\\end{array} \\right\\rbrack\\)\n\n\n\n\n\n\n(2) 선형함수의 내적 표현\nei 단위벡터, xn 차수 n-벡터, f 선형함수라 하면, \\[\\begin{matrix}\nf(x) = f(x_{1}e_{1} + x_{2}e_{2} + ... + x_{n}e_{n}) \\\\\n= x_{1}f(e_{1}) + x_{2}f(e_{2}) + ... + x_{n}f(e_{n}) \\\\\n= a^{T}x,wherea^{T} = \\lbrack\nf(e_{1}),f(e_{2}),...,f(e_{n})\\rbrack\n\\end{matrix}\\]\n\n\n(3) 사례 : sag 처짐 (단위: mm)\n하중벡터 \\(w = \\left( \\begin{array}{r}\nw_{1} \\\\\nw_{2} \\\\\nw_{3}\n\\end{array} \\right)\\)(단위:톤), 변형 compliance 민감도 벡터 \\(c = \\left( \\begin{array}{r}\nc_{1} \\\\\nc_{2} \\\\\nc_{3}\n\\end{array} \\right)\\)(단위:mm/톤)이라면 교량 처짐 sag은 s = cTw (하중 가중합)이다.\n\n\n\n\n\n\n\n(4) 테일러 근사 Taylor 함수\nf : Rn → R이 1차 미분이 가능하다고 하면 n-벡터 함수 f(x)의 근사값은 다음과 같이 구한다. 이를 1차 테일러 근사라 한다. 단, n-벡터 z는 n-벡터 x와 가까운 값이다. \\[\\widehat{f}(x) = f(z) + \\frac{\\partial\nf}{\\partial x_{1}}(z)(x_{1} - z_{1}) + ... + \\frac{\\partial f}{\\partial\nx_{n}}(z)(x_{n} - z_{n})\\]\n【예제】\n\n\n\n\n\n\n\n함수 f : R2 → R을 f(x) = x1 + exp(x2 − x1)라 하자. 이 함수는 선형함수는 아니다. 이를 선형함수로 근사하는 것을 테일러 근사라 한다. z = (1, 2)라 하면, \\[\\triangledown f(z) = \\left\\lbrack\n\\begin{array}{r}\n1 - exp(z_{2} - z_{1}) \\\\\nexp(z_{2} - z_{1})\n\\end{array} \\right\\rbrack|_{z_{1} = 1,z_{2} = 2} = ( -\n1.72,2.72)\\] 그러므로 z = (1, 2)에서 f(x)의 테일러 근사값은 \\(\\widehat{f}(x) = 3.718 + \\left\\lbrack\n\\begin{array}{r}\n- 1.72 \\\\\n2.72\n\\end{array} \\right\\rbrack^{T}(\\left\\lbrack \\begin{array}{r}\nx_{1} \\\\\nx_{2}\n\\end{array} \\right\\rbrack - \\left\\lbrack \\begin{array}{r}\n1 \\\\\n2\n\\end{array} \\right\\rbrack)\\)이다.\n\n\n\n\n\n\n(5) 회귀모형\n차원 2-예측(설명, 독립) 벡터 \\(x =\n\\left\\lbrack \\begin{array}{r}\nx_{1} \\\\\nx_{2}\n\\end{array} \\right\\rbrack\\), 회귀계수 벡터 \\(b = \\left\\lbrack \\begin{array}{r}\nb_{1} \\\\\nb_{2}\n\\end{array} \\right\\rbrack\\), 그리고 a을 절편 스칼라라 하면 회귀모형은 다음과 같다. \\(\\widehat{y} = \\left\\lbrack\n\\begin{array}{r}\n1 \\\\\nx\n\\end{array} \\right\\rbrack^{T}\\left\\lbrack \\begin{array}{r}\na \\\\\nb\n\\end{array} \\right\\rbrack = {\\overset{˜}{x}}^{T}\\overset{˜}{b}\\) OLS 추정치 : \\(\\widehat{\\overset{˜}{b}} =\n({\\overset{˜}{x}}^{T}\\overset{˜}{x})^{-\n1}{\\overset{˜}{x}}^{T}y\\)\n\n\n3. 벡터놈 norm\n\n\n(1) 정의\n벡터의 유클리디안 놈,  ∥ x∥은 벡터의 크기에 대한 척도로 다음과 같이 구한다. 놈은 벡터의 원점에서의 거리이다. \\[\\parallel x \\parallel = \\sqrt{x_{1}^{2}\n+ x_{2}^{2} + ... + x_{n}^{2}} = \\sqrt{x^{T}x}\\]\n【예제】\n\n\n\n\n\n\n\n\\(\\parallel \\left\\lbrack \\begin{array}{r}\n0 \\\\\n- 1 \\\\\n1\n\\end{array} \\right\\rbrack \\parallel = \\sqrt{2}\\), \\(\\parallel \\left\\lbrack \\begin{array}{r}\n- 1 \\\\\n2\n\\end{array} \\right\\rbrack \\parallel = \\sqrt{5}\\)\n\n\n\n\n\n\n성질\n\n비음수 동차성:  ∥ βx ∥  = |β| ∥ x∥, where β는 스칼라\n삼각 부등식:  ∥ x + y ∥  ≤  ∥ x ∥ + ∥ y∥\n비음수:  ∥ x ∥  ≥ 0\n\n\n\n(2) 놈의 종류\n\nL1 norm : \\(L_{1} =\n\\overset{n}{\\sum_{i}}|x_{i}|\\) 절대값의 합으로 맨하튼 Manhattan 놈이라고도 한다. 지도의 거리 측정에 사용된다.\nL2 norm : \\(L_{2} =\n(\\overset{n}{\\sum_{i}}x_{i}^{2})^{\\frac{1}{2}}\\) 제곱합의 제곱근으로 유클리디안 놈이라 한다. 통계학에서 가장 많이 사용된다. 회귀계수 추정치를 구하는 최소제곱추정치 구할 때 사용된다.\n\n\n\n\n\n\n#행렬 정의\nimport numpy as np\nA=np.array([[1,2,3], [4,5,7],[8,9,10]])\n#L1 norm Mahattan\nla.norm(A,axis=1,ord=1)\n【결과】 array([ 6., 16., 27.])\n#L2 norm Euclidean\nla.norm(A,axis=1,ord=2)\n【결과】 array([ 3.74165739, 9.48683298, 15.65247584])\n\n\n(3) 평균 제곱근 RMS root mean square value\n데이터 크기를 정량화하는데 사용되며 데이터의 평균적인 크기를 나타낸다. \\(rms(x) = \\frac{\\parallel x\n\\parallel}{\\sqrt{n}} = \\sqrt{\\frac{1}{n}\\sum x_{i}^{2}}\\)\n\n\n(4) 두 벡터의 합의 놈\n\\[\\parallel x + y \\parallel =\n\\sqrt{\\parallel x \\parallel^{2} + 2x^{T}y + \\parallel y\n\\parallel^{2}}\\]\n\n\n(5) Chebyshev inequality\n차수 n-벡터 x, xi2 ≥ a2을 만족하는 원소 개수를 k라 하면,  ∥ x∥2 = x12 + … + x22 ≥ ka2이다. k ≤ n이므로 \\(n \\leq \\frac{\\parallel x\n\\parallel}{a^{2}}\\)이다. 즉, 벡터의 어떠한 원소도 그 벡터의 놈보다크지 않다.\n\\(\\frac{k}{n} \\leq\n(\\frac{rms(x)}{a})^{2}\\). 왼쪽 항은 벡터의 성분 중 절대값이 최소한 a이상인 성분의 비율을 나타낸다. 오른쪽 항은 a와 rms(x)의 비율의 제곱에 대한 역수이다. 예를 들어, 벡터의 성분 중 1/25 = 4% 이상은 RMS 값의 5배를 초과할 수 없다는 것을 의미한다.\n\n\n\nchapter 4. 벡터간 거리 \n\n1. 유클리디안 거리\n\n\n(1) 정의\n차수가 동일한 두 벡터(a, b)의 놈을 유클리디안 거리로 정의한다. dist(a, b) =  ∥ a − b ∥  =  ∥ b − a∥ \\[||a - b|| = \\sqrt{(a_{1} - b_{1})^{2} +\n(a_{2} - b_{2})^{2} + ... + (a_{n} - b_{n})^{2}}\\] 두 벡터의 Root Mean Square 편차 = \\(\\frac{\\parallel x - y\n\\parallel}{\\sqrt{n}}\\)\n【예제】\n#행렬 정의\nimport numpy as np\na=np.array([[0],[-1],[1]])\nb=np.array([[1],[-2],[1]])\nc=np.array([[1],[0],[3]])\n#거리 계산\nnp.linalg.norm(a-b),np.linalg.norm(b-c)【결과】 (np.float64(1.4142135623730951), np.float64(2.8284271247461903))\n(2) 활용\n\nfeature distance:  ∥ x − y∥ 차수가 동일한 두 벡터의 거리를 개체의 유사성 척도로 사용한다.\nNearest neighbor:  ∥ x − zi∥ 두 개체 간의 거리를 이용하여 유사한 개체를 군집으로 묶는다. k-means 알고리즘\nRMS prediction error: rms(y − ŷ) 관측치와 예측치의 거리를 예측의 정확도 척도로 사용한다.\n\n\n\n# 감성분석\nimport numpy as np\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n# 데이터 준비\ntexts = [\"I love this product\", \"This is terrible\", \"Absolutely fantastic\", \"Not good at all\"]\nlabels = [1, 0, 1, 0]  # 1: 긍정, 0: 부정\n# TF-IDF 벡터화\nvectorizer = TfidfVectorizer()\nX = vectorizer.fit_transform(texts)\n# KNN 모델\nknn = KNeighborsClassifier(n_neighbors=1, metric='euclidean')\nknn.fit(X, labels)\n# 새로운 리뷰 분류\nnew_text = [\"I hate this product\"]\nnew_vector = vectorizer.transform(new_text)\nprediction = knn.predict(new_vector)\nprint(f\"Prediction: {'Positive' if prediction[0] == 1 else 'Negative'}\")\n【결과】 Prediction: Positive\n\n(3) 삼각 부등식\n차수가 동일한 n벡터 a, b, c에 대하여 다음이 발생한다.\n\n(4) triangle 부등식\n ∥ a + b∥2 ≤ ( ∥ a ∥ + ∥ b ∥ )2\n\n(5) 맨해튼 거리\n\\[d(\\mathbf{a},\\mathbf{b}) =\n\\overset{n}{\\sum_{i = 1}}|a_{i} - b_{i}|\\] 맨해튼 거리는 벡터 간의 축을 따라 이동한 거리의 합으로 이는 그리드 기반 공간에서 이동하는 경우에 적합하다. 맨해튼 거리라는 이름은 도로망이 격자 형태로 이루어진 맨해튼 도시 구조에서 유래되었다. 자동차나 사람이 이동할 때 대각선으로 이동하지 못하고 도로를 따라 움직이는 경우에 적합하다. 예: 두 위치 간 최단 이동 거리 계산.\n\n\n\n\n\n\n2. 유클리디안 거리와 통계\n\n(1) de-meanded 벡터\n【reall】 치수 n-벡터 xn, 평균은 \\(avg(x) = (1_{n}^{T}x)/n = (instat) =\n\\overline{x}\\)\n【정의】 \\(\\overset{˜}{x} = x -\navg(x)1_{n}\\) : 벡터의 각 원소를 평균을 뺀 벡터\n【성질】 \\(avg(\\overset{˜}{x}) = 0\\)\n\n통계 분석: 데이터의 평균을 제거함으로써 분산이나 공분산과 같은 통계적 특성을 더 명확하게 분석할 수 있다.\n주성분 분석(PCA): 데이터의 분산을 분석하기 전에 데이터를 중심에 맞추기 위해 사용된다.\n회귀 분석: 회귀 분석에서 독립 변수와 종속 변수의 평균을 제거하여 상수항 없이 회귀 모델을 구축할 수 있다.\n\n\n(2) 표준편차 standard deviation\n\\[std(x) = \\sqrt{\\frac{(x_{1} -\navg(x))^{2} + (x_{2} - avg(x))^{2} + ... + (x_{n} -\navg(x))^{2})}{n}}\\] \\[std(x) = \\frac{\\parallel x - (1^{T}x/n)1\n\\parallel}{\\sqrt{n}}\\] 【응용】 투자에서 평균은 일정기간 평균 수익율, 표준편차는 위험 척도이다.\n\n(3) 표준편차 성질\n\n상수를 더해도 표준편차는 동일하다. std(x + a1) = std(x)\n스칼라(상수) 곱 : std(kx) = |k|std(x)\n\n\n(4) 평균, RMS, STD 관계\nstd(x)2 = rms(x)2 − avg(x)2 (in stat) std(x)2 = var(x) 분산\n\n(5) 표준편차와 Chebychev 부등식\n만약 차원 n-벡터에서 |xi − avg(x)| ≥ a을 만족하는 원소 개수를 k라 하면 \\(\\frac{k}{n} \\leq\n(\\frac{std(x)}{a})^{2}\\)이다. 벡터 x 평균으로부터 k 표준편차 이내에 있는 성분 비율은 최소 1 − 1/k2이다.\n\\[\nP(|X - \\mu| &gt; k\\sigma) \\leq 1 -\\frac{1}{k^{2}}\n\\] 예를 들어, 일정 기간 투자 평균 수익률은 8%이고, 리스크(표준편차)는 3%입니다. 체비셰프의 부등식에 따르면, 손실을 기록한 기간의 비율(즉, 0% 이하인 기간, 16% 이상인 기간)은 최대 (3/8)^2 = 14.1%이다.\n\n(6) 실증적 규칙\nP(|X − μ| ≤ kσ)\n\n\nk = 1, 데이터의 68.3%가 (μ − σ, μ + σ) 내에 있음\n\n\nk = 2, 데이터의 95.4%, k = 3, 데이터의 99.9%\n\n\n\n\n\n\n\n\n3. 거리와 개체 군집화\n\n(1) 개념\nN개의 차수 n-벡터 (x1, x2, …, xN)에 대하여 각 벡터(개체) 쌍 사이의 거리로 측정하여 서로 가까운 클러스터 또는 클러스터로 묶는 작업을 다룬다. 클러스터링의 목표는 가능한 경우 벡터들을 k개의 클러스터 또는 클러스터로 묶거나 나누어, 각 클러스터 내의 벡터들이 서로 가깝도록 하는 것이다. 클러스터링은 벡터들이 객체의 특징을 나타낼 때 널리 사용된다. 다음은 n = 2(군집변수 2개), k = 3으로 클러스터링 한 사례이다.\n\n\n\n\n\n\n(2) 클러스터 할당\nN개 개체, xi를 개체(i = 1, 2, …, N), ci는 i-개체가 할당된 클러스터이고 (j = 1, 2, …k), Gj을 j-클러스터에 속한 개체의 집합이라 하자. Gj = {i|ci = j} 클러스터을 대표하는 차원 n-벡터를 z1, z2, …, zk라 하자. i-개체가 j = ci에 있다면  ∥ xi − zci∥은 모든 클러스터 중 가장 가까워야 한다.\n\n(3) 클러스터 목적\nJclust = ( ∥ x1 − zc1 ∥ + ∥ x2 − zc2 ∥ +…+ ∥ xN − zcN ∥ )/N 함수를 최소화 하는 zc1, zc2, …, zcN을 구한다.\n\n(4) 최적 클러스링\n목적함수 Jclust을 최소화 하는 zc1, zc2, …, zcN을 찾는 것은 개체 수가 많고 차원 개수가 커지면 계산 회수가 기하 급수적으로 늘어나 불가능하다. 그러므로 최적 대신 차선 sub-optimal 방법으로 대표 벡터를 고정화 하는 k-평균 방법을 사용한다.\n\n\n\n\n\n\n4. k-means 알고리즘\n\n(1) 개념\n클러스터 할당과 클러스터 대표자를 선택하여 Jclust를 최소화하는 문제를 해결할 수 있을 것처럼 보이나 두 가지 선택은 순환적입니다. 즉, 각각의 선택이 다른 하나에 의존한다. 클러스터 대표자를 선택하고 클러스터 할당을 선택하는 것을 반복하는 것이 벡터 집합을 클러스터링하는 데 있어서 유명한 k-means 알고리즘이다. k-means 알고리즘은 1957년에 Stuart Lloyd와 독립적으로 Hugo Steinhaus에 의해 처음 제안되어 때때로 Lloyd 알고리즘이라고도 불린다. ’k-means’라는 이름은 1960년대부터 사용되었다.\n\n(2) k-평균 알고리즘\nN개 개체를 k개 클러스터으로 분류한다고 가정하자. z1, z2, …, zk을 각 클러스터의 대표 벡터라 하자. k-평균 알고리즘은 다음 작업을 반복 실행한다.\n  1. 대표 벡터를 결정하고 각 개체를 가장 가까운 대표 벡터의 클러스터으로 분류한다.\n  2. 클러스터에 할당된 개체의 중심점(평균 벡터)을 대표 벡터로 설정한다.\n  3. 수렴 조건 만족 때까지 위의 작업을 반복한다.\n\n(3)이슈사항\n\n타이 브레이커\n두 개 이상의 클러스터과 최소 거리인 개체는 클러스터 할당을 하지 않는다. 그러므로 이 개체는 다음 단계에서 대표 벡터 결정에는 활용되지 않는다.\n\n수렴 조건\n개체의 클러스터 이동이 더 이상 발생하지 않으면 대표 벡터는 움직이지 않음을 의미하므로 클러스터링 결과는 동일해진다.\n\nk-평균 알고리즘은 직관적이다.\n목표함수 Jclust을 최적화 하지 못하지만 반복을 통하여 줄여 나가게 된다.\n\n대표벡터 해석\n  각 N개의 회사마다 총 자본화, 분기별 수익 및 위험, 거래량, 손익, 배당금 등과 같은 금융 및 사업 속성을 구성 요소로 하는 n-벡터을 이용하여 k-평균 클러스터링 결과 얻은 대표벡터를 이용하여 클러스터(군집)에 이름을 부여한다. 기업연수, 기업종류, 매출액 등 군집변수로 사용하지 않은 특성 벡터를 이용하여 개체군의 이름을 부여하고 해석한다.\n\n클러스터 k 결정\nk 의 결정은 다소 주관적이고 시행착오 방법을 사용한다. (k, Jclust)을 이용하여 Elbow Method 팔꿈치 기법을 사용한다. 군집 개수가 증가할수록 Jclust는 감소하게 되지만, 이 감소율이 꺾이는 지점을 찾아내는 방법이다.\n\n고정 대표 벡터 분할하기\n만약 j 클러스터을 대표하는 벡터 z1, z2, …, zj르 고정하면 모든 개체 x1, x2, …, xN을 최적 클러스터으로 분류 문제는 다음과 같다.  ∥ xi − zci ∥  = minj = 1, 2, …, k ∥ xi − zj∥ 고정 대표 벡터를 활용하면 최적 클러스터링 문제는 다음과 같이 sub 최적 문제로 변환된다. 각 N개 개체에 최적 j-클러스터(거리가 가장 가까운 클러스터)을 결정하는 개별적 문제와 동일하다.\n\nJclust = minj = 1, 2, …, k ∥ x1 − zj ∥ +… + minj = 1, 2, …, k ∥ xN − zj ∥ )/N\n\n고정 벡터를 group(or cluster) centroid라 한다.\n\n(4) 사례\n# 60000(train 훈련)/10000(test 테스트), 28x28\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.datasets import mnist\n# MNIST 데이터셋 로드 및 훈련데이터, 테스트데이터 분할 \n(x_train, y_train), (x_test, y_test) = mnist.load_data()\n# 데이터 형태 출력\nprint(f\"x_train shape: {x_train.shape}\")\nprint(f\"y_train shape: {y_train.shape}\")\nprint(f\"x_test shape: {x_test.shape}\")\nprint(f\"y_test shape: {y_test.shape}\")\n# 첫 10개 샘플 이미지와 레이블 시각화\nnum_samples = 10\nplt.figure(figsize=(10, 1))\nfor i in range(num_samples):\n    plt.subplot(1, num_samples, i+1)\n    plt.imshow(x_train[i], cmap='gray')\n    plt.title(y_train[i])\n    plt.axis('off')\nplt.show()\n\n\n\n\n\n# 훈련 데이터 클러스트링, 첫 20개 군집결과\n# 이미지 데이터를 2차원 배열로 변환\nx_train2 = x_train.reshape((x_train.shape[0], -1))\nx_test2 = x_test.reshape((x_test.shape[0], -1))\n# 데이터 정규화\nx_train2 = x_train2 / 255.0\nx_test2 = x_test2 / 255.0\n# k-means 모델 생성 및 학습\nkmeans = KMeans(n_clusters=10, random_state=42)\nkmeans.fit(x_train2)\n# 클러스터 할당 결과\ny_kmeans = kmeans.predict(x_train2)\n# 첫 20개 분류결과 이미지와 레이블 시각화\nnum_samples = 20\nplt.figure(figsize=(10, 1))\nfor i in range(num_samples):\n    plt.subplot(1, num_samples, i+1)\n    plt.imshow(x_train[i], cmap='gray')\n    plt.title(y_kmeans[i])\n    plt.axis('off')\nplt.show()\n\n\n\n\n\n#클러스터 3 평균벡터 출력 클러스터 대표 이미지\nplt.figure(figsize=(10, 1))\nplt.imshow((x_train[0]+x_train[7]+x_train[17])/3, cmap='gray')\nplt.title('cluster 3')\nplt.axis('off')\nplt.show()\n\n\n\n\n\n\n5. 벡터의 각도\n\n(1) 코사인 유사도\n벡터의 코사인 유사도(Cosine Similarity)는 두 벡터 간의 방향적 유사성을 측정하는 지표로, 벡터 간의 각도 θ의 코사인 값을 이용하여 계산된다. \\[\\text{Cosine Similarity} = \\cos(\\theta)\n= \\frac{\\mathbf{A} \\cdot \\mathbf{B}}{\\parallel \\mathbf{A} \\parallel\n\\parallel \\mathbf{B} \\parallel}\\] 코사인 유사도와 유클리드 거리의 차이는 다음과 같다.\n\n\n코사인 유사도는 두 벡터의 방향에 집중하며, 벡터 크기의 차이를 무시한다.\n\n\n유클리드 거리는 두 벡터 사이의 실제 거리(크기 차이 포함)를 측정한다.\n\n\n예를 들어, 텍스트 데이터에서 코사인 유사도는 문서 간의 내용적 유사성을 비교하는 데 유리하며, 추천 시스템, 정보 검색, 클러스터링 등에서 널리 사용된다.\n\n\n【예제】\n\n\n\n\n\n\n\n\\[a = \\left\\lbrack \\begin{array}{r}\n0 \\\\\n- 1 \\\\\n1\n\\end{array} \\right\\rbrack,b = \\left\\lbrack \\begin{array}{r}\n1 \\\\\n- 2 \\\\\n1\n\\end{array} \\right\\rbrack,c = \\left\\lbrack \\begin{array}{r}\n1 \\\\\n0 \\\\\n3\n\\end{array} \\right\\rbrack\\] \\[dist(a,b) = \\sqrt{2},dist(b,c) =\n2.8284\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n벡터 A = [1, 2, 3], B = [4, 5, 6]이 주어졌을 때\n\n벡터 내적: A ⋅ B = 1 ⋅ 4 + 2 ⋅ 5 + 3 ⋅ 6 = 32\n벡터 크기: \\[\\parallel \\mathbf{A} \\parallel =\n\\sqrt{1^{2} + 2^{2} + 3^{2}} = \\sqrt{14}, \\parallel \\mathbf{B} \\parallel\n= \\sqrt{4^{2} + 5^{2} + 6^{2}} = \\sqrt{77}\\]\n코사인 유사도: \\[\\cos(\\theta) = \\frac{\\mathbf{A} \\cdot \\mathbf{B}}{\\parallel \\mathbf{A} \\parallel \\parallel \\mathbf{B} \\parallel} = \\frac{32}{\\sqrt{14} \\cdot \\sqrt{77}} \\approx\n0.975\\]\n\n\n\n\n\n\n\n(2) 코사인 유사도의 특징\n코사인 유사도 값의 범위는 [-1, 1]이고 다음의 특징을 갖는다.\n\ncos (θ) = 1: 두 벡터가 완전히 같은 방향\ncos (θ) = 0: 두 벡터가 직교(Orthogonal, 90도)\ncos (θ) = −1: 두 벡터가 완전히 반대 방향.\n\n코사인 유사도는 벡터의 크기가 아닌 방향만 고려되므로 벡터를 정규화하지 않고도 비교할 수 있다. 고차원 벡터에도 적용 가능하여 텍스트 데이터, 사용자 선호도 등 고차원 데이터에서 벡터 간 유사성 측정에 많이 사용된다. 각도 종류\n\n\n각도가 θ = 90o = π/2이면 두 벡터는 직교 orthogonal 한다.\n\n\n각도가 θ = 0o이면 두 벡터는 정렬 aligned 되어 있다.\n\n\n각도가 θ = 180o = π이면 두 벡터는 역정렬 anti-aligned 되어 있다.\n\n\n각도가 θ &gt; 90o = π/2이면 두 벡터의 각은 둔각 obtuse, θ &lt; 90o = π/2이면 두 벡터의 각은 예각 acute 이다.\n\n\n\n\n\n\n\n\n\n두 벡터 합의 놈과 각도\n ∥ x + y∥2 =  ∥ x∥2 + 2 ∥ x ∥  ∥ y ∥ cos(θ)+ ∥ y∥2 만약 θ = 90o = π/2이면  ∥ x + y∥2 =  ∥ x∥2+ ∥ y∥2 (피타고라스 정리)\n\n\n6. 상관계수\n\n\n(1) 상관계수 정의\n만약 \\(\\overset{˜}{a} = a -\navg(a)1,\\overset{˜}{b} = b - avg(b)1\\)이면, 상관계수(correlation coefficient) ρ는 다음과 같이 정의된다. \\(\\rho =\n\\frac{{\\overset{˜}{a}}^{T}\\overset{˜}{b}}{\\parallel \\overset{˜}{a}\n\\parallel \\parallel \\overset{˜}{b} \\parallel}\\) ⇔ \\(\\rho =\n(\\frac{\\overset{˜}{a}}{std(a)})^{T}(\\frac{\\overset{˜}{b}}{std(b)})/n\\)\n\n\\(cov(a,b) =\n{\\overset{˜}{a}}^{T}\\overset{˜}{b}/n\\): 두 벡터의 공분산\nvar(a) = std(a)2: 벡터의 분산 상관계수와 공분산 관계: cov(a, b) = ρstd(a)std(b)\nρ = ±1 (완전 상관) : 두 벡터가 (역)정렬되어 있음\nρ = 0 (독립) : 두 벡터가 직교되어 있음. cov(a, b) = 0\n\n\n\n(2) 두 벡터 합의 분산\nvar(a + b) = var(a) + 2cov(a, b) + var(b)\nvar(a + b) = var(a) + 2ρstd(a)std(b) + var(b)\n\n\n만약 ρ = 0이면, var(a + b) = var(a) + var(b)\n\n\n만약 ρ = 1이면, var(a + b) = (std(a) + std(b))2\n\n\n만약 ρ = −1이면, var(a + b) = (std(a) − std(b))2\n\n\n\n\n(3) 헤징 hedging 투자\n두 개 회사 주가 벡터 (a, b)의 평균은μ, 표준편차(위험) σ이고 상관계수는 ρ이다. 각각 50% 투자, \\(c = \\frac{(a + b)}{2}\\)의 평균 수익율과 표준편차은 다음과 같다.\n\n\n평균 : \\(avg(\\frac{a + b}{2}) =\n\\mu\\)\n\n\n표준편차 : \\(std(c) = \\sigma\\sqrt{(1 +\n\\rho)/2}\\)\n\n\n상관계수 ρ = 0이면 (독립) 표준편차는 \\(\\frac{1}{\\sqrt{2}}\\)만큼 줄어든다.\n\n\n완벽한 상관관계가 있는 경우에만 표준편차는 동일하다.\n\n\n\n\n\nchapter 5. 선형독립 \n\n1. 선형독립 정의\n\n\n(1) 선형 종속 linear dependence\nk ≥ 2개의 크기 n-벡터 x1, x2, …, xk가 다음을 만족하면 선형종속이라 한다. 만약 a1x1 + a2x2 + … + akxk = 0을 만족하는 ai가 적어도 하나는 0이 아니다. 선형독립이면 적어도 하나의 ai는 0이 아니므로 벡터 xi 다음과 같이 다른 벡터의 선형함수로 표현될 수 있다. \\[x_{k} = \\frac{- a_{1}}{a_{i}}x_{1} + ...\n+ \\frac{- a_{i - 1}}{a_{i}}x_{i - 1} + \\frac{- a_{i + 1}}{a_{i}}x_{i +\n1} + ... + \\frac{- a_{k}}{a_{i}}x_{k}\\] 【예제】\n\n\n\n\n\n\n\n\n\\(x_{1} = \\left\\lbrack \\begin{array}{r}\n0 \\\\\n- 1 \\\\\n1\n\\end{array} \\right\\rbrack,x_{2} = \\left\\lbrack \\begin{array}{r}\n1 \\\\\n- 2 \\\\\n1\n\\end{array} \\right\\rbrack,x_{3} = \\left\\lbrack \\begin{array}{r}\n1 \\\\\n0 \\\\\n- 1\n\\end{array} \\right\\rbrack\\) ⬄−2x1 + x2 − x3 = 0\n\n\n\n\n\n\n\n(2) 선형 독립 linear independence\n만약 a1x1 + a2x2 + … + akxk = 0이 모든 ak = 0일 때만 만족한다면, n-벡터 x1, x2, …, xk을 선형독립이라 한다. 【예제】\n\n\n\n\n\n\n\n\n\\[x_{1} = \\left\\lbrack \\begin{array}{r}\n1 \\\\\n0 \\\\\n0\n\\end{array} \\right\\rbrack,x_{2} = \\left\\lbrack \\begin{array}{r}\n0 \\\\\n- 1 \\\\\n1\n\\end{array} \\right\\rbrack,x_{3} = \\left\\lbrack \\begin{array}{r}\n- 1 \\\\\n1 \\\\\n1\n\\end{array} \\right\\rbrack\\]\n\n\n\n\n\n\n\n(3) 선형독립 벡터의 선형결합\n선형독립인 x1, x2, …, xk의 선형결합의 모든 계수(ak)는 유일하다. 선형결합 x = a1x1 + a2x2 + … + akxk 증명 다른 계수를 bk라 하자. x = b1x1 + b2x2 + … + bkxk 0 = (a1 − b1)x1 + (a2 − b2)x2 + … + (ak − bk)xk이다. x1, x2, …, xk가 선형독립이므로 모든 (ai − bi) = 0 만족한다.\n\n\n2. 기저\n\n\n(1) 기저 개념\n벡터 공간은 다양한 차원의 벡터로 이루어진 공간이며, 그 공간 안의 벡터들을 다른 벡터들의 선형 조합으로 표현할 수 있다. 이때, 특정 벡터 공간의 기저 basis 는 그 공간 안의 모든 벡터들을 생성할 수 있는 최소한의 독립적인 벡터들의 집합이다. 예를 들어, 2차원 공간에서의 기저는 일반적으로 (1,0)과 (0,1)이다. 이 두 벡터는 선형 독립이며, 이들의 모든 선형 조합으로 2차원 평면 상의 어떤 점이든 표현할 수 있다. 따라서 (1,0)과 (0,1)은 2차원 공간의 기저입니다. 단, 벡터 공간의 기저는 유일하지 않다. 크기 2인 벡터의 기저 벡터는 k = 2개이다. 위의 그림에서 a3벡터는 (a1, a2)(기저벡터)의 선형 결합으로 만들 수 있다.\n\n\n\n\n\n\n\n(2) 기저 정의\nn개의 선형독립인 크기 n-벡터를 기저 basis 라 한다. 즉, n-벡터 (x1, x2, …, xn)가 기저이면, 모든 크기 n-벡터는 (x1, x2, …, xn)의 선형 결합으로 표현할 수 있다.\n【증명】 (n+1)개 차원 n-벡터 (x1, x2, …, xn, y)개가 있다고 가정하자. 단,(x1, x2, …, xn) 선형독립이며 기저이다. 이들 벡터는 선형독립(차원개수 n보다 벡터 개수가 (n+1)로 크다)이므로 다음을 만족하는 모든 ai가 0은 아니다. a1x1 + a2x2 + … + anxn + an + 1y = 0\n만약 an + 1 = 0이면, a1x1 + a2x2 + … + anxn = 0을 만족하는 모든 ai = 0이다. 왜냐하면 (x1, x2, …, xn) 선형독립이기 때문이다.(모순)\n\n\n3. 직교정규\n\n\n(1) 정의\n만약  ∥ xi ∥  = 1이고 xiTxj = 0fori ≠ j (두 벡터 (xi, xj)는 직교)이면, (x1, x2, …, xk) 벡터 집합은 직교 정규 orthonormal 벡터라고 한다. 직교정규성은 선형종속, 선형독립처럼 집합의 속성이지 개별 벡터의 속성은 아니다.\n\n(2) 예제\nn개의 단위벡터는 직교정규 벡터이다. 직교정규벡터 \\(\\left\\lbrack \\begin{array}{r}\n- 1 \\\\\n0 \\\\\n0\n\\end{array} \\right\\rbrack,\\frac{1}{\\sqrt{2}}\\left\\lbrack\n\\begin{array}{r}\n0 \\\\\n1 \\\\\n1\n\\end{array} \\right\\rbrack,\\frac{1}{\\sqrt{2}}\\left\\lbrack\n\\begin{array}{r}\n0 \\\\\n- 1 \\\\\n1\n\\end{array} \\right\\rbrack\\) 직교정규 벡터는 선형독립이다.\n\n\n\n\n(3)직교정규 성질\n\n\n벡터 x가 직교정규벡터 선형결합이면 x = a1x1 + a2x2 + … + akxk 내적을 이용하여 다음을 얻으므로 내적을 이용하여 계수를 얻을 수 있다. xiTx = xiT(a1x1 + a2x2 + … + akxk) = ai\n\n\n\n\n벡터 (x1, x2, …, xk)가 직교정규 (선형독립이고 기저임) 벡터이면 x = (x1Tx)x1 + (x2Tx)x2 + … + (xkTx)xk이 성립한다.\n\n\n벡터 (1, 2, 3)을 직교정규 벡터의 선형결합으로 표현하자. \\[\\left\\lbrack \\begin{array}{r}\n1 \\\\\n2 \\\\\n3\n\\end{array} \\right\\rbrack = 1\\left\\lbrack \\begin{array}{r}\n1 \\\\\n0 \\\\\n0\n\\end{array} \\right\\rbrack + 2\\left\\lbrack \\begin{array}{r}\n0 \\\\\n1 \\\\\n0\n\\end{array} \\right\\rbrack + 3\\left\\lbrack \\begin{array}{r}\n0 \\\\\n0 \\\\\n1\n\\end{array} \\right\\rbrack\\]\n\n\n\\(\\lbrack - 100\\rbrack\\left\\lbrack\n\\begin{array}{r}\n1 \\\\\n2 \\\\\n3\n\\end{array} \\right\\rbrack = - 1\\), \\(\\frac{1}{\\sqrt{2}}\\lbrack 011\\rbrack\\left\\lbrack\n\\begin{array}{r}\n1 \\\\\n2 \\\\\n3\n\\end{array} \\right\\rbrack = \\frac{5}{\\sqrt{2}}\\), \\(\\frac{1}{\\sqrt{2}}\\lbrack 0 - 11\\rbrack\\left\\lbrack\n\\begin{array}{r}\n1 \\\\\n2 \\\\\n3\n\\end{array} \\right\\rbrack = \\frac{1}{\\sqrt{2}}\\)\n\n\n\\[\\left\\lbrack \\begin{array}{r}\n1 \\\\\n2 \\\\\n3\n\\end{array} \\right\\rbrack = - 1\\left\\lbrack \\begin{array}{r}\n- 1 \\\\\n0 \\\\\n0\n\\end{array} \\right\\rbrack + \\frac{5}{2}\\left\\lbrack \\begin{array}{r}\n0 \\\\\n1 \\\\\n1\n\\end{array} \\right\\rbrack + \\frac{1}{2}\\left\\lbrack \\begin{array}{r}\n0 \\\\\n- 1 \\\\\n1\n\\end{array} \\right\\rbrack\\]\n\n\n4. Gram-Schmidt 알고리즘\n\n\n(1) 개념\nn-벡터 x1, x2, …, xk가 선형 독립인지 여부를 결정할 수 있는 알고리즘으로 수학자 Jørgen Pedersen Gram과 Erhard Schmidt의 이름을 따서 명명되었다. 만약 벡터들이 선형 독립이라면, Gram–Schmidt 알고리즘은 다음과 같은 속성을 가진 직교정규 벡터 q1, q2, …, qk 을 생성한다.\n\n\n각 i = 1, 2, …, k에서 xi는 q1, q2, …, qi의 선형결합이다.\n\n\n\n\n각 i = 1, 2, …, k에서 qi는 x1, x2, …, xi의 선형결합이다.\n\n\n만약 x1, x2, …, xi − 1 선형독립이나 x1, x2, …, xi는 선형종속이면 멈춘다.\n\n\n\n\n알고리즘\n\n\n주어진 n-벡터 x1, x2, …, xk, i = 1, 2, …, k일 때\n\n\n직교화 : \\({\\overset{˜}{q}}_{i} = x_{i}\n- (q_{1}^{T}x_{i})q_{1} - ... - (q_{i - 1}^{T}x_{i})q_{i -\n1}\\)\n\n\n선형종속 검증 : 만약 \\({\\overset{˜}{q}}_{i} = 0\\)이면, 멈춘다.\n\n\n정규화 : \\(q_{i} =\n\\frac{{\\overset{˜}{q}}_{i}}{\\parallel q_{i} \\parallel}\\).\n\n\n이렇게 얻은 q1, q2, …, qi는 직교정규 벡터이다. 알고리즘 적용 중 중간에 중단되면 기저젝터가 아니다.\n\n\n(2) Gram-Schmidt 알고리즘 예제\nx1 = (−1, 1, −1, 1), x2 = (−1, 3, −1, 3), x3 = (1, 3, 5, 7) 에 대하여 Gram–Schmidt 알고리즘을 적용하자.\n\ni=1\n\\(\\parallel {\\overset{˜}{q}}_{1} \\parallel =\n2\\)이므로 \\(q_{1} =\n\\frac{{\\overset{˜}{q}}_{1}}{\\parallel {\\overset{˜}{q}}_{1} \\parallel} =\n\\left\\lbrack \\begin{array}{r}\n- 1/2 \\\\\n1/2 \\\\\n- 1/2 \\\\\n1/2\n\\end{array} \\right\\rbrack\\)이다.\n\n\ni=2\nq1Tx2 = 4이므로 \\({\\overset{˜}{q}}_{2} = x_{2} -\n(q_{1}^{T}x_{2})q_{1} = \\left\\lbrack \\begin{array}{r}\n1 \\\\\n1 \\\\\n1 \\\\\n1\n\\end{array} \\right\\rbrack\\)이고 \\(\\parallel {\\overset{˜}{q}}_{2} \\parallel =\n2\\)이다. 그러므로 \\(q_{2} =\n\\frac{{\\overset{˜}{q}}_{2}}{\\parallel {\\overset{˜}{q}}_{2} \\parallel} =\n\\left\\lbrack \\begin{array}{r}\n1/2 \\\\\n1/2 \\\\\n1/2 \\\\\n1/2\n\\end{array} \\right\\rbrack\\).\n\n\ni=3\nq1Tx3 = 2, q2Tx3 = 8이므로 \\({\\overset{˜}{q}}_{3} = x_{3} -\n(q_{1}^{T}x_{3})q_{1} - (q_{2}^{T}x_{3})q_{2} = \\left\\lbrack\n\\begin{array}{r}\n- 2 \\\\\n- 2 \\\\\n2 \\\\\n2\n\\end{array} \\right\\rbrack\\)이고 \\(\\parallel {\\overset{˜}{q}}_{3} \\parallel =\n4\\)이다. 그러므로 \\(q_{3} =\n\\frac{{\\overset{˜}{q}}_{3}}{\\parallel {\\overset{˜}{q}}_{3} \\parallel} =\n\\left\\lbrack \\begin{array}{r}\n- 1/2 \\\\\n- 1/2 \\\\\n1/2 \\\\\n1/2\n\\end{array} \\right\\rbrack\\).\n# Gram-Schmidt 알고리즘\nimport numpy as np\n\ndef gram_schmidt(A):\n    # Get the number of rows (n) and columns (k) in A\n    n, k = A.shape\n    # Initialize matrix Q with zeros, same shape as A\n    Q = np.zeros((n, k))\n    \n    for j in range(k):\n        # Start with the current column vector of A\n        v = A[:, j]\n        for i in range(j):\n            # Subtract the projection of v onto the ith orthonormal vector\n            v -= np.dot(Q[:, i], A[:, j]) * Q[:, i]\n        \n        # Normalize the vector\n        Q[:, j] = v / np.linalg.norm(v)\n    return Q\n# Example usage\nA = np.array([[-1,-1,1],\n              [1,3,3],\n              [-1,-1,5],\n              [1,3,7]], dtype=float)\n\ngram_schmidt(A)\n【결과】 array([[-0.5, 0.5, -0.5], [ 0.5, 0.5, -0.5], [-0.5, 0.5, 0.5], [ 0.5, 0.5, 0.5]])"
  },
  {
    "objectID": "notes/math/index.html",
    "href": "notes/math/index.html",
    "title": "세상의 모든 통계 이야기",
    "section": "",
    "text": "📘 Math 강의\n통계학을 이해하는 필요한 기초수학 주제의 강의 노트를 안내합니다.\n\n함수와 통계\n미분과 적분"
  },
  {
    "objectID": "notes/advance_stat/index.html",
    "href": "notes/advance_stat/index.html",
    "title": "Advance Stat 강의",
    "section": "",
    "text": "이 페이지는 Advance Stat 주제의 강의 노트를 안내합니다.\n\n\n\n강의 파일 1\n강의 파일 2"
  },
  {
    "objectID": "notes/advance_stat/index.html#강의-목록",
    "href": "notes/advance_stat/index.html#강의-목록",
    "title": "Advance Stat 강의",
    "section": "",
    "text": "강의 파일 1\n강의 파일 2"
  },
  {
    "objectID": "notes/dl/index.html",
    "href": "notes/dl/index.html",
    "title": "Dl 강의",
    "section": "",
    "text": "이 페이지는 Dl 주제의 강의 노트를 안내합니다.\n\n\n\n강의 파일 1\n강의 파일 2"
  },
  {
    "objectID": "notes/dl/index.html#강의-목록",
    "href": "notes/dl/index.html#강의-목록",
    "title": "Dl 강의",
    "section": "",
    "text": "강의 파일 1\n강의 파일 2"
  },
  {
    "objectID": "notes/intro_stat/index.html",
    "href": "notes/intro_stat/index.html",
    "title": "Intro Stat 강의",
    "section": "",
    "text": "이 페이지는 Intro Stat 주제의 강의 노트를 안내합니다.\n\n\n\n강의 파일 1\n강의 파일 2"
  },
  {
    "objectID": "notes/intro_stat/index.html#강의-목록",
    "href": "notes/intro_stat/index.html#강의-목록",
    "title": "Intro Stat 강의",
    "section": "",
    "text": "강의 파일 1\n강의 파일 2"
  }
]