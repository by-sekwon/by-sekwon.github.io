[
  {
    "objectID": "cardnews/index.html",
    "href": "cardnews/index.html",
    "title": "📰 카드뉴스 모음",
    "section": "",
    "text": "통계를 주제로 한 다양한 카드뉴스를 주제별로 모았습니다.\n클릭하면 자세한 내용을 보실 수 있어요 😊\n\n\n\n\n\n #### 고령화 속도 65세 이상 비율이 역대 최고!\n자세히 보기 ▶\n\n\n #### 출산율 감소 합계출산율 0.72명의 의미는?\n자세히 보기 ▶\n\n\n\n\n\n\n\n\n #### 청년 고용률 청년 고용의 구조적 문제\n자세히 보기 ▶\n\n\n #### 전공별 격차 전공 따라 취업률이 이렇게나!\n자세히 보기 ▶"
  },
  {
    "objectID": "cardnews/index.html#인구와-사회",
    "href": "cardnews/index.html#인구와-사회",
    "title": "📰 카드뉴스 모음",
    "section": "",
    "text": "#### 고령화 속도 65세 이상 비율이 역대 최고!\n자세히 보기 ▶\n\n\n #### 출산율 감소 합계출산율 0.72명의 의미는?\n자세히 보기 ▶"
  },
  {
    "objectID": "cardnews/index.html#청년과-고용",
    "href": "cardnews/index.html#청년과-고용",
    "title": "📰 카드뉴스 모음",
    "section": "",
    "text": "#### 청년 고용률 청년 고용의 구조적 문제\n자세히 보기 ▶\n\n\n #### 전공별 격차 전공 따라 취업률이 이렇게나!\n자세히 보기 ▶"
  },
  {
    "objectID": "cardnews/news001.html",
    "href": "cardnews/news001.html",
    "title": "고령화 속도",
    "section": "",
    "text": "👵 고령화 속도, 세계 최고\n\n\n\n고령화\n\n\n\n2024년 한국 65세 이상 인구: 18.4%\n고령사회(14%) 기준 이미 초과\n초고령사회(20%) 진입도 눈앞\n\n\n📌 정책은 얼마나 따라가고 있을까?"
  },
  {
    "objectID": "cardnews/news003.html",
    "href": "cardnews/news003.html",
    "title": "청년 고용률",
    "section": "",
    "text": "청년 고용률\n\n\n2023년 기준, 15~29세 청년 고용률은 45.4%\n코로나 이후 회복 중이지만 여전히 회복세는 느림.\n\n\n\n\n청년층 일자리 중 단기 계약직, 인턴 비중이 높음\n\n정규직 진입률은 10년 전보다 낮아진 상태\n\n\n🔍 “취업했다”는 통계 뒤에 숨은 고용의 질 문제를 들여다봐야 합니다.\n\n\n\n\n\n\n일자리 찾기 → 계약직 → 재취업 준비 → 불안정 반복\n이직까지 평균 1.5년 소요\n\n\n\n\n\n\n\nNote\n\n\n\n📌 통계청 자료 기준: 2023 경제활동인구조사 청년 부문\n\n\n\n\n\n\n\n청년 고용률 수치만 보는 게 아니라\n고용의 질, 지속성, 업종 다양성을 함께 살펴야 합니다."
  },
  {
    "objectID": "cardnews/news003.html#정규직-비중은-여전히-낮아",
    "href": "cardnews/news003.html#정규직-비중은-여전히-낮아",
    "title": "청년 고용률",
    "section": "",
    "text": "청년층 일자리 중 단기 계약직, 인턴 비중이 높음\n\n정규직 진입률은 10년 전보다 낮아진 상태\n\n\n🔍 “취업했다”는 통계 뒤에 숨은 고용의 질 문제를 들여다봐야 합니다."
  },
  {
    "objectID": "cardnews/news003.html#반복되는-패턴",
    "href": "cardnews/news003.html#반복되는-패턴",
    "title": "청년 고용률",
    "section": "",
    "text": "일자리 찾기 → 계약직 → 재취업 준비 → 불안정 반복\n이직까지 평균 1.5년 소요\n\n\n\n\n\n\n\nNote\n\n\n\n📌 통계청 자료 기준: 2023 경제활동인구조사 청년 부문"
  },
  {
    "objectID": "cardnews/news003.html#정책이-나아가야-할-방향은",
    "href": "cardnews/news003.html#정책이-나아가야-할-방향은",
    "title": "청년 고용률",
    "section": "",
    "text": "청년 고용률 수치만 보는 게 아니라\n고용의 질, 지속성, 업종 다양성을 함께 살펴야 합니다."
  },
  {
    "objectID": "notes/intro_stat/normality.html",
    "href": "notes/intro_stat/normality.html",
    "title": "기초통계 6. 정규변환",
    "section": "",
    "text": "chapter 1. 치우침\n분포의 치우침(skewness)은 통계학에서 데이터 분포의 비대칭성을 나타내는 개념으로, 관측값들이 평균을 중심으로 얼마나 균형 있게 분포되어 있는지를 설명하는 데 사용된다. 분포가 완전히 대칭적이라면 치우침은 0이 되지만, 현실의 많은 데이터는 다양한 정도의 비대칭성을 보인다. 치우침 값이 양수이면 분포가 오른쪽(큰 값 쪽)으로 긴 꼬리를 가지며, 음수이면 왼쪽(작은 값 쪽)으로 긴 꼬리를 가진다.\n이러한 치우침 개념은 통계학적으로 매우 중요한 의미를 가진다. 첫째, 치우침은 평균과 중앙값의 위치 차이를 설명하며, 대표값 선택이나 데이터 해석의 기준을 제공한다. 예를 들어, 우측으로 치우친 분포에서는 평균이 극단값의 영향을 받아 실제 중심 경향을 왜곡할 수 있기 때문에, 중앙값이 더 적절한 대표값이 될 수 있다.\n둘째, 치우침은 정규성 가정의 검토에 핵심적인 역할을 한다. 많은 통계 기법(예: 회귀분석, 분산분석, t-검정 등)은 데이터가 정규분포를 따른다는 전제를 가지고 있으며, 치우침은 이러한 가정의 타당성을 점검하는 지표로 사용된다. 치우침이 클 경우에는 변수 변환(예: 로그, 제곱근)을 고려하거나, 비모수적인 접근법으로 전환할 필요가 있다.\n셋째, 치우침은 이상치나 극단값의 존재 가능성을 시사한다. 한쪽 방향으로 치우친 분포는 그 방향에 극단적인 값이 존재할 가능성이 높으며, 이는 해석이나 모델의 안정성에 영향을 미친다.\n요약하면, 치우침은 단순히 분포의 모양을 묘사하는 데 그치지 않고, 데이터의 중심 경향 이해, 적절한 통계 분석 기법의 선택, 이상치 탐지 등 다양한 통계적 판단에 영향을 주는 핵심 개념이다. 따라서 통계학에서는 치우침을 정량적으로 측정하고 시각적으로 확인하는 것이 중요한 분석 절차로 간주된다.\n\n1. 치우침 개념\n\n(1) 확률분포함수\n확률분포함수는 데이터(확률변수)에 대한 모든 정보를 가지고 있다. 확률변수 관심 구간 \\((a,b)\\)에 데이터가 발생할 가능성(확률)을 알 수 있다. 최대값, 최소값, 데이터의 기대값(중앙 위치), 흩어짐 정도 등을 알 수 있다.\n\n\n\n\n\n서로 상이한 집단이 혼재되어 있는지를 보여주는 최빈값(봉우리)의 개수도 나타내고 봉우리 중심으로 데이터의 흩어진 형태도 보여준다. 봉우리를 중심으로 데이터가 벨모양인지 아니면 꼬리가 서로 다른 형태인지 보여준다.\n\n\n(2) 치우침 종류\n종모양의 좌우 대칭인 분포와 달리 한 쪽 꼬리가 긴 형태를 갖는 분포는 치우침 skewed 이 있다고 한다. 오른쪽 꼬리가 긴 형태를 우로 right 치우침 혹은 양의 positive 치우침이라 하고 왼쪽 꼬리가 긴 형태를 좌로 left 치우침 혹은 음의 negative 치우침이라 한다.\n\n\n\n\n\n좌로 치우침\n좌로 치우침(left-skewed distribution)은 분포의 왼쪽 꼬리가 길게 늘어진 형태를 말한다. 이러한 분포에서는 상대적으로 작은 값들이 많거나 극단적으로 작은 값이 일부 존재하기 때문에, 평균이 중앙값보다 더 작게 계산되는 경향이 있다.\n이는 평균이 값의 크기(산술적 중심)를 기준으로 계산되는 반면, 중앙값은 값의 순서(정렬된 위치상의 중심)를 기준으로 결정되기 때문이다. 따라서 좌측 꼬리에 위치한 작은 관측값들은 평균을 끌어내리지만, 중앙값에는 상대적으로 덜 영향을 미친다.\n예를 들어, 대부분의 학생이 높은 점수를 받은 시험에서 소수의 낙제자가 존재할 경우, 평균 점수는 낮아질 수 있지만 중앙값은 여전히 높은 수준에 머무르게 된다. 이와 같은 상황이 바로 좌로 치우친 분포의 전형적인 사례다.\n우로 치우침\n우로 치우침(right-skewed distribution)은 분포의 오른쪽 꼬리가 길게 늘어진 형태로, 일부 큰 값들이 평균을 끌어올리는 특징을 가진다.\n이러한 분포에서는 극단적으로 큰 관측값들이 전체 평균을 높이는 방향으로 작용하기 때문에, 평균은 중앙값보다 큰 값으로 계산되며, 분포의 오른쪽에 위치하게 된다. 반면 중앙값은 값의 순서를 기준으로 정해지므로, 이러한 극단값의 영향을 덜 받아 상대적으로 안정적인 위치에 머문다.\n즉, 오른쪽 꼬리에 있는 소수의 큰 값들이 전체의 크기 중심(평균)을 오른쪽으로 이동시키며, 이로 인해 평균이 중앙값보다 오른쪽에 놓이게 된다. 대표적인 예로는 소득 분포가 있으며, 대부분의 사람들은 중간 이하의 소득을 가지지만 소수의 고소득자가 평균을 크게 끌어올려 평균 &gt; 중앙값이 되는 현상이 나타난다.\n\n\n\n2. 치우침 개념\n치우침 문제와 통계적 추론의 관계\n치우침은 단일 변수의 분포 형태를 이해하는 데 중요한 개념으로, 통계 분석 초기 단계에서 데이터를 시각화하고 분포의 대칭성 여부를 점검하는 데 사용된다. 특히 확률표본을 전제로 할 때, 표본의 확률분포 함수는 모집단 확률변수의 분포와 동일하다고 간주할 수 있으므로, 히스토그램 등을 통해 데이터의 분포 형태를 살펴보는 것이 바람직하다.\n그러나 통계학의 모수 추론에서는 실제로 모집단의 확률분포 형태보다 추정량의 샘플링 분포가 더 중요한 역할을 한다. 평균이나 비율과 같은 추정량은 중심극한정리에 의해, 모집단이 비정규분포일지라도 표본 크기가 충분히 크다면 그 샘플링 분포는 정규분포에 근사하게 된다. 일반적으로 표본 크기가 20~30 이상일 경우 이러한 근사 정규성이 성립한다고 본다. 따라서 대표본을 확보한 경우에는 모집단의 분포가 치우쳐 있더라도, 평균이나 비율에 관한 통계적 검정에는 큰 문제가 되지 않는다.\n반면 표본의 크기가 작거나, 평균이 아닌 분산과 같은 다른 모수에 대해 추론하고자 할 때는 모집단이 정규분포를 따른다는 전제가 필요해진다. 예를 들어, 소표본에서의 평균 추론에 사용되는 t-검정은 모집단의 정규성을 가정하고 있으며, 표본분산의 샘플링 분포가 카이제곱분포를 따른다는 사실 또한 모집단이 정규분포일 때에만 성립한다. 이러한 경우에는 데이터가 치우쳐 있을 경우 통계적 추론의 타당성이 저해될 수 있다.\n요약하자면, 데이터 분포의 치우침은 단순히 분포 형태의 시각적 특징을 넘어, 통계적 추론의 방법 선택과 정당성에 영향을 미치는 요소이다. 대표본을 확보하면 중심극한정리에 따라 정규성 가정을 크게 걱정하지 않아도 되지만, 소표본이거나 정규성을 전제로 하는 모수 추론을 수행할 경우에는 반드시 데이터의 치우침과 분포 형태를 점검하고, 필요에 따라 적절한 자료 변환이나 분석 방법을 선택해야 한다.\n통계 모형 (변수 관계)\n통계 모형은 변수들 간의 관계를 수량적으로 설명하고 예측하는 도구이며, 이때 변수들은 크게 두 가지로 구분된다. 결과에 해당하는 변수는 목표변수라 하며, 이는 확률변수로 간주되어 확률분포함수를 가진다. 반면, 목표변수에 영향을 주는 변수들은 예측변수라고 하며, 이는 일반적으로 수집된 값으로 주어진 결정변수로 취급되어 확률분포함수를 갖지 않는다.\n통계 모형, 특히 회귀분석과 같은 예측 모형에서는 오차항의 분포가 정규분포를 따른다는 가정이 매우 중요하다. 오차항이 정규분포를 따를 때, 회귀계수나 평균 등의 추정량에 대한 통계량의 샘플링 분포가 정규분포 또는 t, F 분포와 같은 이론분포를 따르게 되며, 이를 통해 모수에 대한 추론, 즉 가설검정이나 신뢰구간 추정이 가능해진다.\n이러한 이론적 전제에 따라, 통계 모형에서는 목표변수만 확률변수로 간주되지만, 오차항이 정규분포를 따르면 결과적으로 목표변수도 정규분포에 근사하게 된다. 따라서 목표변수가 정규분포에서 크게 벗어나 있는 경우, 특히 치우침이 심한 경우에는 모형의 적합성이 떨어질 수 있고, 추론 결과의 신뢰성도 낮아질 수 있다.\n이러한 이유로 통계 모형에서는 예측변수와 목표변수 모두가 정규분포에 근사할수록 분석의 안정성과 해석 가능성이 높아진다. 특히 목표변수의 분포에 치우침이 있다고 판단되면, 사전에 로그변환이나 제곱근 변환 등 적절한 자료 변환을 수행한 후 모형에 적용하는 것이 일반적이다. 예를 들어, 소득, 가격, 수능점수, 교통사고 발생 건수, 하루 코로나 확진자 수 등은 분포가 비대칭적이거나 장꼬리를 가지는 경우가 많아, 변환을 통해 정규성에 근접하게 만든 후 분석에 사용하는 것이 바람직하다.\n결국 통계 모형의 안정성과 타당성을 확보하기 위해서는 변수의 분포, 특히 목표변수의 치우침 여부를 사전에 점검하고 필요한 경우 적절한 변환을 적용하는 것이 필수적인 절차라고 할 수 있다.\n\n\n2. 치우침 진단 통계량\n크기 왜도 pearson moment skewness\n표본왜도 : \\(skew = \\frac{\\sum(x_{i} - \\overline{x})^{3}/n}{(\\sum(x_{i} - \\overline{x})^{2}/n)^{\\frac{3}{2}}}\\)\n좌우대칭인 정규분포, \\(t\\)-분포 등의 왜도는 0이고 우로 치우친 분포 평균이 1인 지수분포는 2이다.\n순서 왜도\n정규분포=0, 우로 치우침 +, 좌로 치우침을 나타내지만 각 통계량의 분포를 모르므로 정규분포 가설을 검정할 수 없다.\npearson first skewness (mode skewness)\n\\[skew = \\frac{mean - mode}{std}\\]\nPearson's second skewness coefficient (median skewness)\n\\[skew = \\frac{3(mean - median)}{std}\\]\n사분위 기반 왜도\n\\[skew = \\frac{(Q_{3} + Q_{1} - 2Q_{2})}{IQR}\\]\nGroeneveld & Meeden’s coefficient\n\\[skew = \\frac{mean - median}{E(|X - median|)}\\]\n\n\n\nchapter 2. 정규성 검정\n\n1. 정규성 검정 필요성\n정규성 검정(normality test)은 주어진 데이터가 정규분포를 따른다고 볼 수 있는지를 통계적으로 판단하는 절차이다. 이는 많은 통계 기법들이 데이터 또는 오차항이 정규분포를 따른다는 전제 하에 수행되기 때문에, 분석의 전제 조건을 확인하는 중요한 단계로 간주된다.\n정규성 검정의 필요성은 통계 분석의 타당성을 확보하고, 분석 결과에 대한 신뢰도와 해석 가능성을 높이기 위한 데 있다. 많은 모수적 통계 방법들은 데이터가 정규분포를 따른다는 가정을 전제로 하며, 이 가정이 충족되지 않을 경우 분석 결과는 왜곡되거나 잘못된 결론을 초래할 수 있다.\n정규성 검정은 단순한 분포 진단을 넘어서, 분석 방법의 선택, 모형의 타당성 평가, 해석의 신뢰도 확보라는 측면에서 통계학적으로 필수적인 과정이다.\n모수적 방법의 전제 확인: 평균에 대한 t-검정, 회귀분석, 분산분석(ANOVA) 등 대부분의 고전적 통계 기법은 오차항 또는 데이터 자체가 정규분포를 따른다고 가정한다. 이 가정이 충족되어야 통계량의 분포가 이론적으로 성립하며, p값이나 신뢰구간이 의미를 가진다.\n소표본 분석에서의 중요성: 중심극한정리에 따라 대규모 표본에서는 정규성의 중요성이 다소 줄어들 수 있지만, 표본의 크기가 작을수록 정규성 가정이 더 민감하게 작용한다. 이 경우 정규성을 만족하지 않으면 검정력이 저하되거나 제1종 오류율이 왜곡될 수 있다.\n모형 진단과 적합성 평가: 회귀모형 등에서는 종속변수보다 오차항의 정규성이 중요한데, 이 정규성이 깨지면 회귀계수의 t-검정, 모형의 유의성 검정(F-검정) 등의 신뢰성이 떨어진다. 따라서 잔차 분석과 정규성 검정은 모형 적합성 진단의 기본 절차이다.\n자료 변환 또는 분석 전략 결정: 정규성 가정을 만족하지 않을 경우, 로그 변환, 제곱근 변환, Box–Cox 변환 등의 자료 변환을 고려해야 하며, 경우에 따라 비모수적 검정이나 로버스트 분석 기법을 사용해야 한다. 정규성 검정은 이러한 분석 전략 변경의 필요성을 판단하는 기초가 된다.\n정규성 검정은 크게 두 가지 방법으로 수행된다. 하나는 시각적 방법이고, 다른 하나는 정량적 방법이다. 시각적 방법으로는 히스토그램, Q–Q 그래프, P–P 그래프 등이 있으며, 이는 데이터가 정규분포와 얼마나 유사한지 직관적으로 확인할 수 있게 해준다. 정량적 방법으로는 샤피로–윌크 검정, 콜모고로프–스미르노프 검정, 앤더슨–달링 검정, 자크–베라 검정 등이 사용되며, 정규성을 통계적으로 검정할 수 있다.\n정규성 검정의 결과는 단순히 가정 충족 여부를 확인하는 데 그치지 않는다. 정규성 위배가 확인될 경우, 연구자는 변수를 변환하거나, 비모수적 방법으로 분석 전략을 변경하거나, 로버스트 통계기법을 적용해야 한다. 따라서 정규성 검정은 통계 분석의 신뢰성을 확보하기 위한 필수적인 절차로서 기능한다.\n\n\n2. 정규성 검정방법\n통계적 가설\n정규성 검정은 데이터(확률변수)의 분포가 정규분포를 따르는지 검정하는 분포 적합성 goodness of fits 검정이다.\n\n귀무가설 : 데이터 모집단 분포는 정규분포이다.\n대립가설 : 정규분포를 따르지는 않는다. 그러므로 어떤 분포인지는 모른다.\n\n#데이터 생성하기\nimport numpy as np\ndata=np.random.exponential(0.5,100)\ndata.mean()\n0.455767\n평균이 0.5인 지수분포를 따르는 난수 데이터 100개를 표본추출하여 변수명 X로 한 data를 만들었다. 평균이 0.5인 모집단에서 생성했지만 실제 평균은 0.456이었다. 랜덤 생성이므로 실행할 때마다 결과는 다르다.\n#그래프 요약, 히스토그램, 상자수염그림\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsns.set(style=\"ticks\")\nf, (ax_box, ax_hist) = plt.subplots(2,sharex=True)\nsns.boxplot(X, ax=ax_box)\nsns.distplot(X, ax=ax_hist)\nplt.show()\n지수분포로 우로 치우친 형태를 가지고 있다. 극단치 ourliers 여러 개가 존재한다.\n\n\n\n\n\nShapiro Wilk W-통계량\n\\(W = \\frac{\\left( \\sum_{i = 1}^{n}a_{i}x_{(i)} \\right)^{2}}{\\sum_{i = 1}^{n}(x_{i} - \\overline{x})^{2}}\\), 여기서 \\(x_{(i)}\\)는 정렬된 표본 데이터 (오름차순 순위), \\(\\overline{x}\\)은 표본 평균, \\(a_{i}\\)는 기대값과 분산공분산 행렬을 기반으로 계산된 상수 (분산행렬을 통해 계산됨)이다.\n다음은 정규성 검정 통계량 변환 (Z-값으로 표준화)이다. \\(W_{n}\\)는 Shapiro–Wilk 검정 통계량이고 \\(\\mu,\\sigma,\\gamma\\)은 표본크기 n에 따라 정해지는 보정 계수이다.\n\\[Z_{n} = \\{\\begin{matrix}\n\\frac{- \\log(\\gamma - \\log(1 - W_{n})) - \\mu}{\\sigma}, & \\text{if}4 \\leq n \\leq 11 \\\\\n\\frac{\\log(1 - W_{n}) - \\mu}{\\sigma}, & \\text{if}12 \\leq n \\leq 2000\n\\end{matrix}\\]\n#사피로윌크 정규성검정\nimport scipy.stats as stats\nchisq,p_value=stats.shapiro(data)\nprint(\"검정통계량=\",chisq,\"유의확률\",p_value)\n검정통계량= 0.84497141 유의확률7.765954990190949e-09\n유의확률이 &lt;0.001이므로 귀무가설(정규분포를 따른다)이 기각되어 데이터는 정규분포를 따르지 않는다.\nKolmogorov D-통계량\n\\(D = \\max_{x}\\left| F_{n}(x) - \\Phi(x) \\right|\\), 여기서 \\(\\Phi(x)\\)는 이론적 분포함수, \\(F_{n}(x)\\)는 데이터 분포함수이다. 유의확률이 매우 작아 귀무가설이 기각되어 정규분포를 따르지 않는다. 모수 추정 결과를 매개변수로 넣을 수 있다.\nimport scipy.stats as stats\nstats.kstest(data,'norm',args=(data.mean(),data.std()))\nKstestResult(statistic=np.float64(0.1484722011700535), pvalue=np.float64(0.02177174264952343) 유의확률이 0.05보다 작으므로 귀무가설이 기각되어 정규분포를 따르지 않는다.\n적합성 검정 가능한 분포\ndist{’norm’, ’expon’, ’logistic’, ’gumbel’, ’gumbel_l’, ’gumbel_r’, ’extreme1’} 가능한 함수\n\n귀무가설 : 데이터는 지수분포를 따른다.\n대립가설 : 데이터는 지수분포를 따르지 않는다.\n\n유의확률이 0.629로 귀무가설을 기각하지 못하므로 지수분포를 따른다.\nimport scipy.stats as stats\nstats.kstest(data,'expon',args=(0,data.mean()))\nKstestResult(statistic=np.float64(0.047886640415504195), pvalue=np.float64(0.9675172438463417), 유의확률이 0.96이므로 귀무가설이 채택되어 데이터는 지수분포를 따른다.\nAnderson-Darling AD 통계량\n\\(A^{2} = n\\int_{- \\infty}^{\\infty}\\frac{\\lbrack F_{n}(x) - \\Phi(x)\\rbrack^{2}}{\\Phi(x)(1 - \\Phi(x))}d\\Phi(x)\\), 여기서 \\(\\Phi(x)\\)는 이론적 분포함수, \\(F_{n}(x)\\)는 데이터 분포함수이다\n통계량 값이 6.69로 가장 큰 기각역 값 1.053 (여기에 해당하는 유의수준은 0.01)보다 크므로 귀무가설이 기각되어 정규분포를 따르지 않는다.\nimport scipy.stats as stats\nstats.anderson(data,dist='norm')\nAndersonResult(statistic=np.float64(4.621782528955748), critical_values=array([0.555, 0.632, 0.759, 0.885, 1.053]), significance_level=array([15. , 10. , 5. , 2.5, 1. ]), fit_result= params: FitParams(loc=np.float64(0.46777570761475146), scale=np.float64(0.4466578954689332))\n통계량 4.621782528955748이 유의수준 1% 기각역 1.053보다 매우 크므로 귀무가설이 기각되어 정규분포를 따르지 않는다.\n대부분의 분포에 대한 적합성 검정은 가능하다. scipy.stats 분포\n\n귀무가설 : 데이터는 지수분포를 따른다.\n대립가설 : 데이터는 지수분포를 따르지 않는다.\n\nimport scipy.stats as stats\nstats.anderson(data,dist='expon')\nAndersonResult(statistic=np.float64(0.2290116329263725), critical_values=array([0.917, 1.072, 1.333, 1.596, 1.945]), significance_level=array([15. , 10. , 5. , 2.5, 1. ])\n유의수준 15% 기각값보다 작으므로 귀무가설을 기각하지 못하므로 지수분포를 따른다.\n\n\n3. 시각적 방법\nProbability Plot은 정규성 검정 또는 두 분포의 유사성을 시각적으로 평가하기 위한 방법으로, 이론적 분포함수와 데이터의 분포함수가 얼마나 유사한지를 그래프를 통해 보여준다. 이러한 시각적 방법은 데이터의 분포 특성, 특히 비대칭성이나 꼬리 분포 등을 직관적으로 파악하는 데 유용하다.\n\n두 데이터의 실증적 empirical 분포함수는 동일한가?\n이론적 분포함수와 데이터의 분포함수는 동일한가? 보여주는 시각적 그래프\n\nProbability-Probability plot\n두 분포의 누적분포함수(CDF) 값을 비교하는 그래프이다. 일반적으로 X축에는 이론 분포(예: 정규분포)의 누적확률 값을, Y축에는 표본 데이터의 누적확률 값을 대응시켜 그린다. 만약 두 분포가 유사하다면, 모든 점이 대각선 직선 위에 놓이게 된다. 이 방법은 분포 전체의 전반적인 적합도를 확인하는 데 적합하다.\n두 데이터의 누적분포함수를 2차원 그래프에 표현함, X-축에는 정규분포의 누적분포함수 \\(\\phi(z)\\), Y-축에는 데이터 누적분포함수 \\(F(x)\\)\nQuantile-Quantile plot\n두 분포의 분위(quantile) 값을 비교하는 그래프이다. X축에는 이론 분포의 p-백분위 값, Y축에는 표본 데이터의 p-백분위 값을 대응시킨다. 두 분포가 유사하면 데이터 점들은 대각선 직선을 따라 정렬되며, 직선에서 벗어나는 양상은 치우침, 첨도, 이상치 등을 시사할 수 있다. 특히 Q–Q Plot은 분포의 꼬리 부분 적합도를 확인하는 데 유용하다.\n두 데이터의 누적분포함수를 2차원 그래프에 표현함, X-축에는 임의의 한 데이터의 백분위 값 \\(x_{i} = F^{- 1}\\left( \\frac{i - 0.5}{n} \\right)\\) 혹은 이론 정규분포의 백분위 값 \\(x_{i} = \\Phi^{- 1}\\left( \\frac{i - 0.5}{n} \\right)\\), Y-축은 데이터의 순서통계량 \\(y_{(i)}\\)이다.\n\n\n\n\n\n\n\n\n\n\n\n\n\nchapter 3. 정규변환\n\n1. 정규변환 개념\n통계학에서 많은 추론 방법은 정규분포를 전제로 한다. 예를 들어, t-검정, 분산분석, 선형회귀, 분산분석, 신뢰구간 추정 등의 고전적 통계기법들은 대부분 변수 또는 오차항이 정규분포를 따른다는 가정 하에 수행된다. 그러나 실제 자료는 이상치, 왜도, 첨도 등의 특성으로 인해 이러한 정규성 가정을 만족하지 않는 경우가 많다. 이러한 상황에서 데이터를 통계 분석이 가능한 형태로 전처리하기 위한 기법 중 하나가 바로 정규변환이다.\n정규변환이란, 정규분포를 따르지 않는 자료를 가능한 한 정규분포에 가깝게 변환하는 방법을 의미한다. 이는 분석 대상 변수에 수학적 변환(예: 로그, 제곱근, Box–Cox 등)을 적용하여 분포의 비대칭성이나 이상치를 완화하고, 정규성에 가까운 형태로 만드는 과정이다. 즉, 변수 \\(X\\)를 어떤 함수 \\(g( \\cdot )\\)를 통해 \\(Y = g(X)\\)로 변환함으로써 \\(Y \\sim N(\\mu,\\sigma^{2})\\)에 근접하도록 만드는 것이 목표이다.\n\n\n2. 정규변환 방법\n\n간단한 정규변환 방식\n\n우측 치우침 (Positive Skewness)\n데이터가 오른쪽 꼬리를 길게 갖고 있음 → 많은 소수 값 + 일부 극단적으로 큰 값이 존재한다.\n\\[\\sqrt{X} \\rightarrow \\ln(X) \\rightarrow \\frac{1}{X}\\]\n\\(\\sqrt{X}\\): 왜도 완화 (온건한 변환)\n\\(\\ln(X)\\): 지수적 증가 완화, 양수 변수에 적합\n\\(\\frac{1}{X}\\): 극단적 치우침 완화 (하지만 해석 어려워짐)\n좌측 치우침 (Negative Skewness)\n데이터가 왼쪽 꼬리를 길게 갖고 있음 → 많은 큰 값 + 일부 작은 값이 존재한다.\n\\[\\sqrt{\\max(X + 1) - X} \\rightarrow \\ln(\\max(X + 1) - X) \\rightarrow \\frac{1}{\\max(X + 1) - X}\\]\n여기서 \\(\\max(X + 1) - X\\)는 데이터를 좌우 반전하여 우측 치우침처럼 만들어 변환한 뒤, 분석 전에 다시 해석 가능한 형태로 돌리기 위한 기법이다.\n\\(\\max(X + 1) - X\\): 데이터 반전 (거꾸로 뒤집음)\n그 뒤에 우측 치우침 처리 방식(\\(\\sqrt{},\\ln(),1/X\\)) 적용\n반전 데이터에서 변환을 적용하면 좌측 치우침 완화 가능\n\nModified Tukey Ladder of Power 변환\n\n실제 데이터를 분석할 때, 변수의 분포는 종종 정규성을 위배하는 형태를 가진다. 이러한 왜도나 첨도를 조정하여 정규분포에 근접하게 만들기 위한 기법 중 하나가 Tukey 사다리 변환이다. 이 방법은 John Tukey에 의해 제안되었으며, 변수에 다양한 지수(\\(\\lambda\\))를 적용함으로써 분포의 비대칭성을 완화하고 분석의 적합성을 높이는 데 목적이 있다.\n\\[Y = \\{\\begin{matrix}\nX^{\\lambda}, & \\text{if}\\lambda &gt; 0 \\\\\n\\ln(X), & \\text{if}\\lambda = 0 \\\\\n- X^{\\lambda}, & \\text{if}\\lambda &lt; 0\n\\end{matrix}\\]\n부호 반전이 포함된 이유는, λ가 음수일 경우 변환 결과가 기존 변수와 반대 방향의 변화를 가지게 되기 때문이다. 예를 들어, \\(\\lambda = - 1\\)이면 \\(X^{- 1} = 1/X\\)은 X가 클수록 작아지므로, 부호를 반전하여 관계를 원래 방향과 맞추는 것이다.\n\n\n\n\n\n\n\n\nλ 값\n변환 형태\n적용 예시\n\n\n2\n\\[X^{2}\\]\n좌측 꼬리 완화\n\n\n0.5\n\\[\\sqrt{X}\\]\n온건한 정규화\n\n\n0\n\\[\\ln(X)\\]\n지수적 분포의 선형화\n\n\n-0.5\n\\[- 1/\\sqrt{X}\\]\n극단적 우측 왜도 완화\n\n\n-1\n\\[- 1/X\\]\n역수형 자료의 정규화\n\n\n\n이 변환은 분석 대상 변수의 왜도 방향과 정도에 따라 지수 형태의 수학적 변환을 적용함으로써, 데이터를 정규분포에 가깝게 만드는 실용적인 방법이다. 이 변환은 지수(λ)의 부호에 따라 양의 치우침 또는 음의 치우침을 완화하는 데 사용되며, 특히 회귀분석, 분산분석 등 정규성 가정을 요구하는 통계기법에서 전처리 단계로 유용하게 활용된다. 부호 반전(-)은 해석상의 방향성을 유지하기 위한 실용적 조정이며, 정규변환의 목적에 부합한다.\n\nBox–Cox 변환 (Box–Cox Transformation)\n\nBox–Cox 변환은 1964년 George Box와 David Cox가 제안한 기법으로, 정규성 확보와 분산 안정화를 동시에 추구할 수 있는 유연한 변환 방법이다. 특정 수학적 함수 하나에 의존하는 것이 아니라, 지수 매개변수 \\(\\lambda\\)를 조절함으로써 다양한 변환을 포함할 수 있도록 설계되어 있다. 이는 통계 분석에서 종속변수의 정규성을 확보하거나, 선형 모형에서 잔차의 분산을 일정하게 만들고자 할 때 매우 유용하게 사용된다. \\(Y = \\{\\begin{matrix}\n\\frac{X^{\\lambda} - 1}{\\lambda}, & \\text{if}\\lambda \\neq 0 \\\\\n\\ln(X), & \\text{if}\\lambda = 0\n\\end{matrix}\\)\n이는 Tukey의 Ladder of Power 변환과 형태는 유사하지만, \\(\\lambda\\)의 값을 최대우도추정(MLE) 으로 찾는다는 점에서 실용적으로 더욱 발전된 형태이다. 그리고 \\(X\\)값은 반드시 양수이어야 하므로 음수가 있는 데이터는 최소값이 0을 초과하도록 변환 한 후 Box–Cox 변환을 적용해야 한다.\n\nYeo–Johnson 변환\n\nYeo–Johnson 변환은 2000년에 In-Kwon Yeo와 Richard A. Johnson이 제안한 정규성 확보를 위한 변환 기법으로, Box–Cox 변환의 확장판이다. Box–Cox 변환은 입력값이 양수일 때만 정의되지만, Yeo–Johnson 변환은 음수와 0까지 포함한 데이터를 변환할 수 있다는 장점이 있다. 따라서, 데이터에 음수나 0이 포함되어 있거나, Box–Cox로 정규성이 확보되지 않는 경우 더 유연하게 적용 가능한 대안이다.\n\\[Y(\\lambda) = \\{\\begin{matrix}\n\\frac{(X + 1)^{\\lambda} - 1}{\\lambda}, & \\text{if}X \\geq 0,\\lambda \\neq 0 \\\\\n\\log(X + 1), & \\text{if}X \\geq 0,\\lambda = 0 \\\\\n- \\frac{( - X + 1)^{2 - \\lambda} - 1}{2 - \\lambda}, & \\text{if}X &lt; 0,\\lambda \\neq 2 \\\\\n\\bullet \\log( - X + 1), & \\text{if}X &lt; 0,\\lambda = 2\n\\end{matrix}\\]\nfrom scipy.stats import boxcox, anderson\nimport seaborn as sns\n\n# 1. 데이터 불러오기 및 전처리\ntitanic = sns.load_dataset(\"titanic\")\ndata = titanic['age'].dropna()\n\n# 2. 양수만 필터링 (Box–Cox는 양수 자료만 허용)\ndata = data[data &gt; 0]\n\n# 3. Box–Cox 변환 및 최적의 λ 추정\ntransformed_data, best_lambda = boxcox(data)\n\nprint(f\"최적의 λ = {best_lambda:.4f}\")\n\n# 4.Lilliefors 검정 (정규성 검정)\nstat, p_value = lilliefors(transformed_data)\nprint(f\"Lilliefors 통계량 D = {stat:.4f}, p-value = {p_value:.4f}\")\nBOX-COX 최적의 λ = 0.7628  Lilliefors 통계량 D = 0.0647, p-value = 0.0010\nfrom sklearn.preprocessing import PowerTransformer\n\n# 2. Yeo–Johnson 변환기 정의 및 적용\npt = PowerTransformer(method='yeo-johnson', standardize=False)\ntransformed = pt.fit_transform(data).flatten()\n\n# 3. 최적 λ 확인\nbest_lambda = pt.lambdas_[0]\nprint(f\"Yeo–Johnson 최적 λ = {best_lambda:.4f}\")\n\n# 4. 정규성 검정 (Anderson–Darling)\nstat, p_value = lilliefors(transformed)\nprint(f\"Lilliefors 통계량 D = {stat:.4f}, p-value = {p_value:.4f}\")\nYeo–Johnson 최적 λ = 0.7583  Lilliefors 통계량 D = 0.0634, p-value = 0.0010\nBox–Cox 및 Yeo–Johnson 변환은 모두 정규성을 확보하기 위한 강력한 기법이나 다음과 같은 경우에는 통계적으로 유의미한 정규성 개선이 이루어지지 않을 수 있다.\n\n본질적으로 정규분포가 아닌 데이터 구조\n다봉성(multi-modality)이나 이질성(heterogeneity)의 존재\n극단적인 이상치 또는 높은 왜도\n표본 수가 너무 커서 민감한 정규성 검정이 기각을 유발 (n이 클수록 아주 작은 일탈도 기각됨)\n\n\n\n3. BOX-COX 정규변환 사례\n#데이터 생성하기\nimport numpy as np\nX=np.random.exponential(0.5,100)\nX.mean()\n평균이 0.5인 지수분포를 따르는 난수 데이터 100개를 표본추출하여 변수명 X로 한 data를 만들었다. 평균이 0.5인 모집단에서 생성했지만 실제 평균은 0.456이었다. 랜덤 생성이므로 실행할 때마다 결과는 다르다.\n#그래프 요약, 히스토그램, 상자수염그림\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsns.set(style=\"ticks\")\nf, (ax_box, ax_hist) = plt.subplots(2,sharex=True)\nsns.boxplot(X, ax=ax_box)\nsns.distplot(X, ax=ax_hist)\nplt.show()\n지수분포로 우로 치우친 형태를 가지고 있다. 극단치 ourliers 여러 개가 존재한다.\n\n\n\n\n\n#BOX-cox 정규변환\nimport scipy.stats as stats\nXt,lmd = stats.boxcox(X)\nlmd\n0.2959412663878202 최적 값은 0.296이다.\n#변환 데이터 정규성 검정\nimport scipy.stats as stats\nstats.shapiro(Xt)\n유의확률이 0.69이므로 귀무가설이 채택되어 변환한 정규성을 만족한다. (0.9903486967086792, 0.6927880644798279)"
  },
  {
    "objectID": "notes/intro_stat/time_series.html",
    "href": "notes/intro_stat/time_series.html",
    "title": "기초통계 9. 시계열분석",
    "section": "",
    "text": "chapter 1. 시계열 자료\n\n1. 시계열데이터 정의\n시계열(time series) 자료란 관측치가 시간의 흐름에 따라 일정한 순서로 기록된 데이터를 말한다. 반면, 특정 시점에서 여러 개체를 동시에 조사한 자료는 횡단(cross-sectional) 자료라고 한다.\n(표현) \\(\\{ Y_{t};t = 1,2,...,T\\}\\)\n횡단 자료에서는 개체 구분을 위해 보통 아래첨자 i를 사용하고, 시계열 자료에서는 시간을 나타내기 위해 아래첨자 t를 사용한다. 여기서 t는 연, 월, 일, 분, 초 등 다양한 시간 단위를 가질 수 있다. 대표적인 시계열 자료의 예로는 KOSPI 주가, 삼성전자 월별 매출액, 소매물가지수, 실업률, 환율 등이 있다.\n\n\n2. 시계열데이터 분석 목적\n시계열 데이터 분석의 가장 중요한 목적은 미래 값을 예측하는 것이다. 본 강의 역시 이 예측에 중점을 둔다. 그러나 시계열 분석의 목적은 단순한 예측에만 머물지 않는다. 먼저, 시계열 데이터를 가설적 확률모형으로 표현함으로써 데이터의 생성 메커니즘을 이해하고, 추세·계절성·불규칙 요인 등으로 분해하여 간결하게 요약할 수 있다. 경제통계에서 계절 성분을 식별하고 제거하는 계절조정(seasonal adjustment) 과정은 장기 추세 분석에서 필수적이다. 또한, 잡음을 제거하거나 신호를 추출하는 등 데이터의 특성을 파악하고 조정하는 과정도 중요한 분석 목적이다.\n이와 함께, 시계열 분석은 미래 예측에 널리 활용된다. 상품 매출, 실업률, 환율, 인구 변화 등 다양한 현상의 미래 값을 추정하기 위해 추세 분석, 평활법, 시계열 분해, ARMA 모형 등이 사용된다. 더 나아가, 시스템의 동작 원리를 이해하고 제어하는 데에도 시계열 분석이 이용된다. 예를 들어, 한 시계열을 다른 시계열로부터 예측하거나(전달함수 분석), 광고비 데이터를 활용해 판매량을 예측하는 경우가 있다. 저수지 운영과 같이 유입량 변동을 모형화하여 특정 기간 내 고갈될 확률을 추정하는 시뮬레이션 분석도 시계열 분석의 한 예다.\n마지막으로, 시계열 데이터는 가설 검정에도 사용된다. 예를 들어, 장기간의 기온 데이터를 분석하여 지구 온난화 추세를 검증하거나, 경제 지표의 장기 변화 패턴을 검정하는 방식이다. 이처럼 시계열 분석은 과거의 패턴과 구조를 규명하고, 이를 기반으로 미래를 예측하며, 나아가 의사결정과 정책 수립을 지원하는 핵심 도구라 할 수 있다.\n\n\n3. 시계열분석 역사\n시계열 분석의 기원은 17세기로 거슬러 올라간다. 당시 태양 흑점 관측 자료와 밀 가격 지수 변동을 설명하기 위해 사인과 코사인 함수가 사용되었다. 1926년 Yule은 자기회귀이동평균(ARMA) 개념을 제시하였고, 1937년 Walker가 ARMA 모형을 구체적으로 제안하였다. 이 시기에는 주기적 변동을 제거하기 위해 이동평균법(moving average)이 널리 사용되었다.\nARMA 모형의 추정 기법은 1960년 Durbin에 의해, 그리고 1970년 Box와 Jenkins에 의해 체계적으로 정립되었다. Box와 Jenkins는 『Time Series Analysis』라는 고전적 저서를 출간하여 시계열 분석을 현대 통계학의 중요한 한 분야로 자리매김하게 하였다. 한편, 1957년 Holt는 지수 평활법을, 1960년 Winter는 계절성을 고려한 지수 평활법을 제안하였다.\n계절 변동 제거를 위한 공식적 방법도 발전하였다. 1967년 미국 인구조사국은 경기 지수 등 경제 시계열에 대해 X-11 계절변동 분해법을 제안하였다. 그러나 X-11은 이동평균 개념을 사용하므로 시계열의 양 끝 관측치를 활용하지 못하는 한계가 있었다. 이를 보완하기 위해 1975년 캐나다 통계청은 X11-ARMA 방법을 개발하였으며, 현재 우리나라에서도 이 방법이 사용되고 있다.\nBox–Jenkins 모형에서 파생된 시계열 분석의 또 다른 흐름은 비선형 모형의 발전이다. Engle이 제안한 ARCH(Autoregressive Conditional Heteroscedasticity) 모형과, 이를 일반화한 GARCH(Generalized ARCH) 모형은 시계열의 분산이 시간에 따라 변하는 경우를 다룰 수 있도록 하였다. 이러한 모형들은 금융 시계열처럼 변동성이 시간에 따라 크게 달라지는 자료를 분석하는 데 특히 유용하다.\n\n\n4. 시간도표\n시계열 분석의 출발점은 데이터를 시간 순서에 따라 시각화하는 것이다. 시간도표(Time plot)은 관측된 데이터 포인트를 시간 순서대로 배열하여 나타낸 그래프로, 시계열 데이터 분석의 첫 단계에서 매우 유용하다. 이를 통해 데이터의 전반적인 경향, 계절성, 변동성) 등을 직관적으로 파악할 수 있다.\n시간도표에서는 보통 x축에 시간을, y축에 해당 시점에서 관측된 값을 표시한다. 시간 축은 연도, 월, 일, 시각 등 데이터 수집 주기에 따라 다양하게 설정될 수 있으며, 값 축에는 주가, 온도, 매출액 등 분석 대상 변수의 값이 표시된다. 데이터 포인트는 점이나 선으로 표현되며, 연속적으로 연결함으로써 시간에 따른 변화를 한눈에 볼 수 있다.\n이 그래프를 통해 시계열 데이터의 여러 특성을 시각적으로 식별할 수 있다. 장기적인 상승 또는 하락 경향을 나타내는 추세, 일정 주기로 반복되는 패턴을 의미하는 계절성, 계절성보다 더 긴 주기의 변동을 보이는 순환, 데이터의 변동 폭과 불안정성을 나타내는 변동성, 그리고 전반적인 패턴에서 크게 벗어난 이상치(outlier) 등을 확인할 수 있다.\n예제 데이터\nseaborn 내장 데이터 : 1949년 1월~1960년 12월 호주 월별 항공 승객(백만명)\n#예제 데이터 가져오기\nimport pandas as pd\nimport seaborn as sns \nfrom datetime import datetime\ndf=sns.load_dataset(\"flights\")\ndf['date']=df.apply(lambda x: datetime.strptime(f\"{x['year']}-{x['month']}\", '%Y-%b').date(), axis=1)\n\n#시간 도표\nimport matplotlib.pyplot as plt\ndf.plot(x='date',y='passengers',title='Airline passengers over time')\nplt.show()\n\n\n\n\n\n\n\n5. 시계열데이터 성분\n시계열 데이터 \\(\\{ Y_{t};t = 1,2,...,T\\}\\)는 크게 네 가지 성분으로 이루어진다. 첫째, 장기간에 걸쳐 지속적인 상승이나 하락과 같은 변화를 보이는 경향(Trend) 성분이 있다. 둘째, 1년 이내와 같이 일정한 주기로 반복되는 변동 패턴을 나타내는 계절성(Seasonality) 성분이 존재한다. 셋째, 계절성보다 긴 주기의 변동을 의미하는 순환(Cycle) 성분이 있으며, 주로 경기 변동이나 경제 순환과 같은 장기 요인을 반영한다. 마지막으로, 특정한 주기나 규칙이 없이 불규칙하게 나타나는 불규칙(Irregular) 성분이 있다.\n이 가운데 경향, 계절성, 순환은 비교적 규칙적인 패턴을 가지는 성분이며, 불규칙 성분은 예측이 불가능한 잡음 형태로 나타난다. 계절성과 주기 변동은 차분(differencing) 과정을 통해 제거할 수 있으며, 특히 순환 성분은 시계열 패턴을 대표하는 중요한 변동 요인으로 간주된다.\n\n\n\n\n\n경향(Trend) \\(T_{t}\\)\n경향은 시계열 데이터에서 장기간에 걸쳐 나타나는 지속적인 변화 패턴을 의미한다. 데이터가 시간이 지남에 따라 꾸준히 증가하거나 감소하는지, 혹은 상대적으로 일정한 수준을 유지하는지를 파악할 수 있다. 경향 패턴은 일반적으로 장기적 요인—예를 들어 인구 증가, 기술 발전, 경제 성장 또는 장기 경기침체—에 의해 발생한다.\n형태에 따라 경향은 직선 경향(linear trend)과 이차 경향(quadratic trend) 등으로 구분된다. 직선 경향은 일정한 속도로 변화하는 패턴을, 이차 경향은 변화 속도 자체가 시간이 지남에 따라 증가하거나 감소하는 패턴을 나타낸다.\n순환(cycle) \\(C_{t}\\)\n순환은 시계열 데이터에서 일정한 주기와 진폭을 가지고 반복되는 변동 패턴을 의미한다. 이는 계절성보다 더 긴 기간에 걸쳐 나타나며, 경제 경기 변동이나 기후 변화와 같이 장기 요인에 의해 발생하는 경우가 많다. 순환 패턴은 사인 곡선(sine curve)과 같이 일정한 간격으로 고점과 저점이 반복되는 형태로 나타나며, 이러한 주기적 변동은 장기 예측이나 시스템 동작 분석에서 중요한 단서를 제공한다.\n계절성(seasonality) \\(S_{t}\\)\n계절성은 시계열 데이터에서 일정한 주기를 가지고 반복되는 변동 패턴을 말한다. 순환과 달리 주기의 길이가 고정되어 있다는 특징이 있으며, 주별, 월별, 분기별, 연도별 등 일정 간격마다 유사한 형태의 변동이 반복된다. 예를 들어, 여름철 음료 매출 증가, 연말 소비 지출 증가, 특정 계절의 농산물 가격 변동 등이 계절성 패턴에 해당한다. 계절성은 주기성이 뚜렷하기 때문에 차분(differencing)이나 계절 조정기법을 통해 비교적 쉽게 제거하거나 분석에 반영할 수 있다.\n불규칙성(irregular) \\(I_{t}\\)\n불규칙성은 시계열 데이터에서 경향, 순환, 계절성과 같은 규칙적인 패턴이 전혀 존재하지 않는 변동 성분을 의미한다. 이는 예측할 수 없는 우연한 요인이나 측정상의 오차 등에 의해 발생하며, 통계 모형에서 오차항에 해당한다. 불규칙 성분은 일반적으로 평균이 0이고 분산이 일정하며, 서로 독립적인 백색잡음(white noise)으로 가정된다. 이러한 성분은 데이터 분석에서 제거하거나 모형화하기 어렵기 때문에, 주로 다른 패턴 성분을 추출한 후 남은 잔차로 해석된다.\n\n\n3. 시계열데이터 분석방법\n\n\n\n\n\n회귀분석(계량경제) 방법, BOX-JENKINS 방법, 지수 평활법, 시계열 분해 방법이 있다. 회귀분석 방법과 BOX-JENKINS 방법(ARMA)은 수학적 이론 모형에 의존하고 시간에 따라 변동이 많은(빠른) 시계열 자료에 적용된다. 지수 평활법이나 시계열 분해 방법은 다소 직관적인 방법이며 시간에 따른 변동이 느린 데이터를 분석하는데 사용된다.\n과거의 데이터 패턴을 활용하여 미래 값을 예측, 설명변수가 있는 시계열 모형은 econometric 계량경제모형\nfrequency domain : Fourier 분석에 기초, spectrum density function\ntime domain : 자기상관함수 이용, 관측값들의 시간적 변화 탐색\n평활법 : 과거 값의 평균으로 미래 값을 예측하는 방법\n\n이동 평균법(moving average): 최근 데이터의 평균을 (혹은 중앙치) 예측치로 사용하는 방법이다. 각 과거치에는 동일한 가중치가 주어지면 과거 패턴 인식이 주목적이다.\n지수 평활법(exponential smoothing): 현재 가까운 시점에 가장 많은 가중치 주고 멀어질수록 낮은 가중치를 주는 방법이다. 경향이나 계절성 존재여부에 따라 단순지수, 이중지수, 삼중지수, 계절지수 평활법 등이 있다.\n\nARMA 모형\n\\(Y_{t} = \\mu + \\alpha_{1}Y_{t - 1} + ... + \\alpha_{p}Y_{t - p} + \\beta_{1}e_{t - 1} + ... + \\beta_{q}e_{t - q} + e_{t - 1}\\)\n시계열 데이터 \\(\\{ Y_{t};t = 1,2,...,T\\}\\)에 대한 모형화를 통하여 미래 값을 예측하는 방법이다. 설명변수가 종속변수 자신의 과거 값인 AR(Auto Regressive) 모형 , 설명변수가 오차항의 과거 값인 MA(Moving Average) 모형, 그리고 AR과 MA 모형의 결합인 ARMA 모형이 있다.\n계량경제 Econometrics Model\n\\(Y_{t} = \\mu + \\alpha_{1}X_{1t} + \\alpha_{2}X_{2t} + ... + \\alpha_{p}X_{pt} + e_{t - 1}\\)\n시계열 데이터 \\(\\{ Y_{t};t = 1,2,...,T\\}\\) 종속변수로 하고 p개의 (X1, X2, ..., Xp)를 설명하는 회귀분석 모형, 유의한 (영향을 미치는) 설명변수를 찾을 수 있으나 예측에 어려움이 있다. 왜냐하면 설명변수의 예측치도 있어야 하기 때문이다. 그러므로 설명변수의 경우에는 \\(t\\)시점 대신 이전 시점\\(t - 1,t - 2,...\\)을 사용한다.\n성분 관련 분석\n\n추세분석 : 추세(일차식, 이차식, 로그형태 등) 성분을 파악한다.\n변동 분해 : 3개 주요 성분 (주기, 계절성, 경향)을 분해하는 방법\nX-11 방법 : 정부통계에서 가장 많이 쓰이는 계절 조정 방법\n\n\n\n6. 최적 모형 선택 통계량\n\n예측오차: \\(e_{t} = Y_{t} - {\\widehat{Y}}_{t}\\)\nMean Absolute Error (MAE) 평균절대오차: \\(MAE = \\frac{1}{T}\\overset{T}{\\sum_{i = 1}}|e_{t}|\\)\nMean Absolute Percentage Error (MAPE) 평균절대오차비율: \\(MAPE = \\frac{1}{T}\\overset{T}{\\sum_{i = 1}}|\\frac{e_{t}}{Y_{t}}| \\times 100(\\%)\\)\nMean Squared Error (MSE) 평균오차자승합: \\(MSE = \\frac{1}{T}\\overset{T}{\\sum_{i = 1}}e_{t}^{2}\\)\nRoot-Mean Squared Error (MSE) 제곱근MSE: \\(RMSE = \\sqrt{\\frac{1}{T}\\overset{T}{\\sum_{i = 1}}e_{t}^{2}}\\)\n\n\n\n\nchapter 1. 시계열 모형\n\n1. 시계열 모형 개념\n정의\n시계열 분석에서 중요한 출발점은 관측 자료를 설명할 수 있는 적절한 확률모형 또는 모형 계열을 선택하는 것이다. 미래 관측값은 본질적으로 불확실하므로, 각 시점의 관측값 \\(\\{ x_{t}\\}\\)는 확률변수 \\(\\{ X_{t}\\}\\)의 실현값이라고 가정한다.\n시계열 모형이란, 관측된 자료 \\(\\{ x_{t}\\}\\)가 실현값이라고 가정되는 확률변수열 \\(\\{ X_{t}\\}\\)의 결합분포를 구체적으로 명세한 것이다. 경우에 따라서는 모든 결합분포를 명시하는 대신 평균과 공분산만을 제시하는 경우도 있다.\n이론적으로 완전한 확률 시계열 모형은 \\(\\{ X_{1},X_{2},\\ldots\\}\\)의 모든 결합분포를 명시한다. 예를 들어,\n\\[P(X_{1} \\leq x_{1},\\ldots,X_{n} \\leq x_{n}), - \\infty &lt; x_{1},\\ldots,x_{n} &lt; \\infty,n = 1,2,\\ldots\\]\n와 같이 모든 시점 n에 대한 확률을 규정해야 한다. 그러나 실제 분석에서 이렇게 완전한 명세는 매우 드물다. 대부분의 시계열은 너무 많은 모수를 포함하기 때문에, 분석 가능성과 효율성을 위해 1차 및 2차 모멘트(평균과 공분산)와 같이 보다 제한적인 정보만을 사용하는 경우가 많다.\n이러한 평균과 공분산 같은 2차 특성만으로 시계열을 묘사하는 방식을 2차 특성 접근이라 한다. 특히 결합분포가 다변량 정규분포인 경우, 이러한 2차 특성만으로도 결합분포 전체를 완전히 결정할 수 있다. 비록 2차 특성 접근법은 정보의 일부를 포기하는 결과를 낳지만, 최소평균제곱오차 예측이론과 결합하면 실용적이고 효율적인 시계열 분석이 가능하다.\n실제 분석에서는 하나의 시계열 자료만 관측 가능한 경우가 많다. 예를 들어, 1900~1996년 동안 국내 연간 강수량 자료는 오직 한 가지 실현값일 뿐이며, 이는 가능한 많은 경우 중 하나일 뿐이다. 이러한 상황에서 우리는 가용한 자료를 설명하고 예측할 수 있는 간단한 시계열 모형을 설정하고, 더 나아가 다양한 응용 목적에 맞는 모형군을 확장해 나가게 된다.\n시계열 모형화의 일반적 접근 방법\n시계열 분석을 시작할 때는 먼저 데이터를 시각화하여 주요 특징을 파악한다. 이를 통해 추세나 계절성이 있는지, 뚜렷한 구조 변화가 있는지, 혹은 이상치가 존재하는지를 확인한다.\n다음 단계는 추세와 계절 성분을 제거하여 정상성(stationarity)을 가진 잔차를 만드는 것이다. 데이터의 변동 폭이 시계열 수준과 비례하는 경우, 로그 변환(log transformation)과 같이 분산 안정화를 위한 사전 변환을 적용할 수 있다. 예를 들어, 모든 값이 양수이도록 상수를 더한 뒤 로그를 취하면 변동성이 일정한 데이터로 변환할 수 있다. 추세와 계절성 제거 방법으로는 모형을 적합하여 해당 성분을 추정 후 제거하거나, 차분(differencing)을 이용해 \\(Y_{t} = X_{t} - X_{t - d}\\)와 같이 원 시계열을 일정 시차 d만큼 차감하는 방식이 있다. 목표는 정상성을 만족하는 잔차를 만드는 것이다.\n정상화된 잔차에 대해서는 표본 자기상관함수 등 여러 통계량을 활용하여 적절한 모형을 선택하고 적합한다. 이후에는 잔차를 예측하고, 앞서 적용했던 변환을 역변환하여 원래 시계열의 예측값을 얻는다.\n또 다른 접근 방법으로는 시계열을 주파수 영역에서 해석하는 푸리에 변환을 이용하여, 서로 다른 주파수를 가진 사인파와 코사인파 성분으로 표현하는 방식이 있다. 이 방법은 신호 처리나 구조물 공학과 같은 공학적 응용에서 특히 중요하며, 구조물의 공진 주파수가 외부 하중의 주파수와 일치하지 않도록 설계하는 데 활용될 수 있다.\n\n\n2. 시계열 기본 모형\n\n(1) 평균 0인 모형\n시계열 모형 중 가장 단순한 형태는 추세나 계절 성분이 전혀 없고, 관측값들이 서로 독립이며 동일한 분포를 따르는 경우이다. 특히 평균이 0인 확률변수열 \\(\\{ X_{1},X_{2},\\ldots\\}\\)로 구성된 경우로 iid 잡음 모형이라고도 한다. 각 확률변수는 동일한 누적분포함수 \\(F( \\cdot )\\)를 가지며, 결합분포는 다음과 같이 곱으로 분리된다.\n\\[P(X_{1} \\leq x_{1},\\ldots,X_{n} \\leq x_{n}) = P(X_{1} \\leq x_{1})\\cdots P(X_{n} \\leq x_{n}) = F(x_{1})\\cdots F(x_{n})\\]\n이는 시계열의 모든 시점이 서로 독립임을 의미한다. 따라서 어떤 시점의 값이 주어져도 다른 시점의 값을 예측하는 데 전혀 도움이 되지 않는다. 수식으로 표현하면 다음과 같다.\n\\[P(X_{n + h} \\leq x \\mid X_{1} = x_{1},\\ldots,X_{n} = x_{n}) = P(X_{n + h} \\leq x)\\]\n예측의 관점에서, 관측값 \\((X_{1},\\ldots,X_{n})\\)을 바탕으로 미래값 \\(X_{n + h}\\)를 최소평균제곱오차(MSE) 기준으로 예측하는 함수 f를 찾을 경우, 이 모형에서는 항상 0이 최적 예측값이 된다. 즉, iid 잡음 모형은 예측에는 아무런 실질적 정보를 제공하지 않는다.\n그럼에도 불구하고 iid 잡음 모형은 시계열 분석에서 중요한 의미를 가진다. 이는 보다 복잡한 시계열 모형을 구축할 때 기초 구성요소로 사용될 수 있으며, 실제 시계열이 iid 잡음으로부터 얼마나 벗어나는지를 측정하는 기준이 되기 때문이다.\nBinary Process\niid 잡음 모형의 한 예로, 각 시점 \\(t = 1,2,\\ldots\\)에서 확률 \\(p\\)로 \\(X_{t} = 1\\), 확률 \\((1 - p)\\)로 \\(X_{t} = 0\\)이 나오는 확률변수열 \\(\\{ X_{t}\\}\\)를 생각할 수 있다. 동전을 던져 앞면이 나오면 +1, 뒷면이 나오면 0을 부여하는 실험이 여기에 해당한다.\nRandom Walk\n랜덤 워크는 iid 잡음을 누적합하여 생성되는 시계열이다. 초기값을 \\(S_{0} = 0\\)으로 두고, \\(S_{t} = X_{1} + X_{2} + \\cdots + X_{t},t = 1,2,\\ldots\\)로 정의하면, \\(\\{ X_{t}\\}\\)가 평균 0인 iid 잡음일 때 \\(\\{ S_{t}\\}\\)는 평균이 0인 랜덤 워크가 된다.\n\n\n(2) 추세와 계절성을 포함한 모형\n많은 시계열 자료에는 뚜렷한 추세가 존재한다. 예를 들어, 호주 월별 탑승객수 자료에는 장기적인 증가 경향이 분명히 나타난다. 이러한 경우, 평균이 0인 모형은 적합하지 않으며, 추세를 포함한 모형이 필요하다.\n\\(X_{t} = m_{t} + Y_{t}\\), 여기서 \\(m_{t}\\)는 시간이 지남에 따라 서서히 변하는 함수로 추세 성분이라 하며, \\(Y_{t}\\)는 평균이 0인 확률 과정이다. 추세 성분를 추정하는 대표적인 방법 중 하나가 최소제곱법이다. 최소제곱법에서는 \\(m_{t}\\)를 매개변수화된 함수로 가정하고, 관측자료 \\(\\{ x_{1},\\ldots,x_{n}\\}\\)에 대해 \\(\\overset{n}{\\sum_{t = 1}}(x_{t} - m_{t})^{2}\\)을 최소화하도록 모수 값을 추정한다. 예를 들어, 2차 다항식 추세를 가정하면 \\(m_{t} = a_{0} + a_{1}t + a_{2}t^{2}\\)와 같이 표현할 수 있으며, \\(a_{0},a_{1},a_{2}\\)는 최소제곱법을 통해 추정된다.\n이러한 추세 모형은 장기 변화 패턴을 파악하는 데 유용하며, 필요할 경우 계절성이나 순환 성분을 추가로 포함시켜 더 복잡한 시계열 모형을 구성할 수 있다.\nHarmonic Regression(조화회귀)\n많은 시계열 데이터는 날씨와 같이 계절적으로 변하는 요인의 영향을 받는다. 이러한 효과는 주기가 고정되어 알려진 주기적 성분으로 모형화할 수 있다. 예를 들어, ’교통사고 사망자 수’ 자료는 매년 7월에 최고점, 2월에 최저점을 보이며, 주기 12의 계절 요인을 강하게 시사한다.\n계절 요인은 추세가 없고 잡음은 허용하는 단순한 모형으로 다음과 같이 표현할 수 있다. \\(X_{t} = s_{t} + Y_{t}\\), 여기서 \\(s_{t}\\)는 주기 \\(d\\)를 가지는 주기 함수이며(\\(s_{t - d} = s_{t}\\)), 편리한 선택으로 사인과 코사인 파형의 합인 조화(harmonics)를 사용할 수 있다. 즉,\n\\(s_{t} = a_{0} + \\overset{k}{\\sum_{j = 1}}\\left\\lbrack a_{j}\\cos(\\lambda_{j}t) + b_{j}\\sin(\\lambda_{j}t) \\right\\rbrack\\)로 표현되며, \\(a_{0},a_{1},\\ldots,a_{k},b_{1},\\ldots,b_{k}\\)는 미지의 모수이고, \\(\\lambda_{1},\\ldots,\\lambda_{k}\\)는 고정된 주파수로 \\(2\\pi/d\\)의 정수배에 해당한다.\n이 모형은 푸리에 급수와 동일한 구조를 가지며, \\(k\\)는 사용되는 조화의 개수를 의미한다. 예를 들어, 주기 \\(d = 365\\)의 일별 데이터에서 기본 파형 하나만 적합하려면 \\(k = 1,f_{1} = 1\\)을 선택한다. 반면, 주기 365를 1, 2, 3, 4로 나눈 파형의 조합을 사용하려면 \\(k = 4\\)로 설정하고 각 \\(f_{j} = j\\)로 지정한다. 조화회귀는 계절성이 뚜렷하고 주기가 일정한 시계열에서 매우 효과적이며, 특히 추세 없이 주기 성분과 잡음만 있는 경우 계절 변동을 설명하는 데 유용하다.\n정상성 모형 stationary model\n시계열 \\(\\{ X_{t},t = 0, \\pm 1,\\ldots\\}\\)가 정상적(stationary)이라고 말하려면, 시계열의 통계적 특성이 시간에 따라 변하지 않아야 한다. 보다 구체적으로, 평균과 분산이 일정하고, 공분산이 두 시점의 절대적 시간 위치가 아니라 시차(lag)에만 의존해야 한다.\n【정의】 \\(E(X_{t}^{2}) &lt; \\infty\\)인 시계열 \\(\\{ X_{t}\\}\\)에 대해, 평균 함수는 \\(\\mu_{X}(t) = E(X_{t})\\), 공분산 함수는 \\(\\gamma_{X}(r,s) = Cov(X_{r},X_{s}) = E\\lbrack(X_{r} - \\mu_{X}(r))(X_{s} - \\mu_{X}(s))\\rbrack\\)\n로 정의된다.\n【정의】 \\(E(X_{t}^{2}) &lt; \\infty\\)인 시계열 \\(\\{ X_{t}\\}\\)가 약한 정상성을 가지려면\n1. 평균 \\(\\mu_{X}(t)\\)가 시간 t에 무관해야 하며,\n2. 공분산 \\(\\gamma_{X}(t + h,t)\\)가 t에 무관하고 시차 h에만 의존해야 한다.\n엄밀한 정상성(strict stationarity)은 모든 h와 \\(n &gt; 0\\)에 대해, \\((X_{1},\\ldots,X_{n}),(X_{1 + h},\\ldots,X_{n + h})\\)가 동일한 결합분포를 가지는 경우를 말한다.\n약한 정상성 조건 (2)에 따라, 정상 시계열의 공분산 함수는 시차 h에 대한 함수로 정의된다. \\(\\gamma_{X}(h): = \\gamma_{X}(h,0) = \\gamma_{X}(t + h,t)\\)\n【자기상관함수 정의】 정상 시계열 \\(\\{ X_{t}\\}\\)의 \\(lagh\\)에서의 자기공분산함수(Autocovariance Function, ACF)는 \\(\\gamma_{X}(h) = Cov(X_{t + h},X_{t})\\)로 정의된다. 이때 \\(lagh\\)에서의 자기상관함수는 \\(\\rho_{X}(h) = \\frac{\\gamma_{X}(h)}{\\gamma_{X}(0)} = Cor(X_{t + h},X_{t})\\)로 정의되며, 이는 시계열이 시간 간격 \\(h\\)만큼 떨어진 두 시점에서 얼마나 선형적으로 관련되어 있는지를 나타낸다.\n【예제 iid 잡음】 \\(\\{ X_{t}\\}\\)가 평균 0, 분산 \\(\\sigma^{2}\\)를 가지는 독립 동일분포(iid) 확률변수열이라고 하자. (1) \\(E(X_{t}) = 0\\) (2) \\(\\gamma_{X}(t + h,t) = \\{\\begin{matrix}\n\\sigma^{2}, & \\text{if}h = 0 \\\\\n0, & \\text{if}h \\neq 0\n\\end{matrix}\\) 모두 t에 의존하지 않으므로 정상성 모델이다. 같은 이유로 백색잡음도 정상성 모델이다.\n【예제 랜덤 워크】 \\(\\{ S_{t}\\}\\)가 iid 잡음 \\(\\{ X_{t}\\}\\)를 누적합하여 만든 랜덤워크라 하면 공분산 \\(\\gamma_{S}(t + h,t) = Cov(S_{t + h},S_{t}) = t\\sigma^{2}\\)는 시차 t에 의존하므로 비정상성 모델이다.\n\n\n\n\nchapter 3. 평활법\n\n1. 이동평균법\n계열 데이터는 주기성이나 불규칙성을 포함하는 경우가 많으므로, 이러한 단기 변동을 완화하고 전반적인 추세를 파악하기 위해 과거 관측값을 평균하는 방법이 사용된다. 이와 같이 과거의 일정 개수 관측값을 평균하여 예측값을 구하는 방법을 이동평균법(Moving Average)이라 한다.\n이동평균법은 예측 시점 이전의 일정 기간(예: 최근 3개월, 5일 등)에 해당하는 과거 자료의 평균을 사용하여 다음 시점의 값을 추정한다. 모든 과거치에 동일한 가중치를 부여한다는 점에서, 과거치에 서로 다른 가중치를 부여하는 지수평활법(Exponential Smoothing)과 구별된다.\n이 방법은 단기 변동을 제거하고 장기 경향을 파악하는 데 효과적이며, 특히 계절 변동이나 잡음을 제거한 추세 분석과 단기 예측에 자주 활용된다.\n이동평균(Moving Average, MA) 계산과 예측\n이동평균법은 과거 m개의 관측값의 평균을 이용하여 다음 시점의 값을 예측하는 방법이다. 시점 t에서의 단순 이동평균(Simple Moving Average)은 다음과 같이 계산된다.\n\\[{\\widehat{X}}_{t + 1} = \\frac{X_{t} + X_{t - 1} + \\cdots + X_{t - m + 1}}{m}\\]\n이동평균에는 일반 이동평균과 중심 이동평균(Centered Moving Average, CMA)가 있다.\n일반 이동평균은 예측 시점 직전의 m개 자료를 단순 평균하여 사용한다.\n중심 이동평균은 m개의 자료 가운데 중앙 시점을 대표값으로 사용하며, 특히 계절성을 제거하는 과정에서 유용하다.\n예를 들어, m = 3인 경우,\n\n일반 이동평균: \\(MA_{t} = \\frac{X_{t} + X_{t - 1} + X_{t - 2}}{3}\\)\n중심 이동평균: 중앙시점이 t이면 \\(CMA_{t} = \\frac{X_{t - 1} + X_{t} + X_{t + 1}}{3}\\)이 된다.\n\n중심 이동평균은 시계열을 부드럽게 하면서도 시점의 중심에 해당하는 값을 대표값으로 하므로, 특히 계절 조정 과정에서 계절성을 제거하는 데 자주 사용된다.\n이동평균 주기 m의 결정\n이동평균법에서 주기 m은 데이터의 특성과 분석 목적에 따라 설정한다. 일반적으로 m은 데이터 주기의 배수를 활용하여 결정하며, 이는 변동의 주요 주기를 부드럽게 제거하고 장기 경향을 파악하기 위함이다. 예를 들어,\n\n주가 데이터(일별): m = 5 (1주), m = 20 (1개월), m = 60 (분기), m = 120 (반년)\n월별 데이터: m = 12 (1년), m = 24 (2년), m = 36 (3년)\n\n이동평균법의 특징\n이동평균법은 시계열 데이터에서 계절성과 불규칙 변동을 제거함으로써 전반적인 추세를 직관적으로 파악할 수 있는 장점을 가진다. 주기 m의 길이에 따라 분석 초점이 달라지는데, m이 길면 장기 패턴을, m이 짧으면 단기 패턴을 진단하는 데 유용하다. 특히 주기가 길어질수록 시계열에서 주기적 변동이 사라지고 직선에 가까운 장기 추세선이 나타난다.\n이 방법은 각 시점에서 자신을 포함한 m개의 관측치를 동일한 가중치로 평균하여, 시계열 자료 \\(\\{ Y_{t}\\}\\)의 전반적인 패턴을 인식한다. 이러한 단순한 계산 방식 덕분에 이해와 구현이 용이하며, 시각적으로도 직관적인 정보를 제공한다.\n이동평균법의 한계와 문제점\n이동평균법은 계산이 단순하고 직관적인 장점이 있지만, 몇 가지 한계가 존재한다. 첫째, 과거 관측치에 동일한 가중치를 부여한다는 점이다. 이는 시간적으로 가까운 자료와 먼 과거 자료를 동일하게 취급하므로, 최근 정보의 중요성이 반영되지 못하고 경우에 따라 추세 판단이 왜곡될 가능성이 있다.\n둘째, 이동평균법은 구조적으로 차기 1기만 예측이 가능하다. 따라서 장기 예측보다는 과거 자료를 부드럽게 하여 전반적인 추세를 파악하거나 단기 패턴을 분석하는 데 적합하다. 이러한 이유로, 이동평균법은 독립적인 예측 도구라기보다 시계열 모형화 이전의 탐색적 분석이나 데이터 전처리 단계에서 주로 활용된다.\n#예제 데이터 가져오기\nimport pandas as pd\nimport seaborn as sns \nfrom datetime import datetime\ndf=sns.load_dataset(\"flights\")\ndf['date']=df.apply(lambda x: datetime.strptime(f\"{x['year']}-{x['month']}\", '%Y-%b').date(), axis=1)\n\n#이동평균 주기=12, 일년\ndf['rolling_avg'] = df['passengers'].rolling(window=12).mean()\nimport matplotlib.pyplot as plt\nplt.plot(df['date'],df['passengers'], 'b')  \nplt.plot(df['date'],df['rolling_avg'], 'r') \nplt.show()\n\n\n\n\n\n\n\n2. 지수 평활법 exponential smoothing\n이동평균법은 모든 관측치에 동일한 가중치를 부여하므로, 최근 자료와 오래된 자료가 동일하게 반영된다는 한계가 있다. 이로 인해 최신 정보의 중요성이 충분히 반영되지 못하고, 과거의 추세 패턴이 과도하게 포함되는 경향이 있다.\n이러한 단점을 보완한 방법이 지수평활법이다. 지수평활법은 최근 관측치에 더 높은 가중치를 부여하고, 시점이 멀어질수록 가중치를 지수적으로 감소시키는 방식으로 과거 자료를 반영한다. 가중치는 일반적으로 평활 상수(smoothing constant) \\(\\alpha(0 &lt; \\alpha \\leq 1)\\)로 조절하며, \\(\\alpha\\)값이 클수록 최근 자료의 비중이 커지고, 작을수록 장기 패턴을 더 많이 반영하게 된다.\n이 방법은 단순한 계산 구조에도 불구하고 예측 능력이 뛰어나며, 단기 미래 예측에 널리 활용된다. 특히 시계열 데이터에서 경향이 크지 않은 경우, 지수평활법은 과거 패턴을 부드럽게 반영하면서도 변화에 신속하게 대응할 수 있다.\n1. 단순 지수평활법(Simple Exponential Smoothing): 추세나 계절성이 없는 시계열에 사용되는 가장 기본적인 형태이다. 미래 예측값은 이전 예측값과 새로운 관측값의 가중 평균으로 계산되며, 평활 상수 \\(\\alpha\\)로 최근 자료의 반영 비율을 조절한다.\n2. 이중 지수평활법(Double Exponential Smoothing, Holt’s Method): 시계열에 추세가 존재하는 경우 사용한다. 레벨(level)과 추세(trend)를 각각 추정하여, 단순 지수평활법에 추세 항을 추가한 형태이다. 이를 통해 변화하는 추세를 반영한 예측이 가능하다.\n3. 삼중 지수평활법(Triple Exponential Smoothing, Holt–Winters Method): 시계열에 추세와 계절성이 모두 존재하는 경우 사용한다. 레벨, 추세, 계절성(seasonal component)을 동시에 추정하며, 계절성이 고정폭(additive)인지 비례형(multiplicative)인지에 따라 두 가지 형태가 있다.\n4. 적응형 지수평활법(Adaptive Exponential Smoothing): 데이터의 변화 속도에 따라 평활 상수 \\(\\alpha\\)를 자동으로 조정하는 방법이다. 급격한 변동 시에는 \\(\\alpha\\)를 크게, 안정적인 시기에는 작게 설정하여 예측 정확도를 높인다.\n\n(1) 단순지수평활법 Simple ES\n단순 지수평활법은 주기(순환)만 존재하고 추세나 계절성이 없는 시계열 자료에 적합한 예측 방법이다. 이 방법은 과거의 모든 관측값을 활용하되, 최근 관측치에 더 큰 가중치를 부여하고 시점이 멀어질수록 가중치를 지수적으로 감소시키는 특징을 가진다. 이를 통해 최근 데이터의 변화가 예측에 빠르게 반영되면서도 과거 자료가 완전히 무시되지 않는다.\n모형 구조\n단순 지수평활법의 평활값(예측값)은 다음과 같이 정의된다.\n\\(\\widehat{Y}t = \\alpha Y_{t} + (1 - \\alpha)\\widehat{Y}t - 1\\). 여기서 \\(Y_{t}\\)는 시점 t에서의 실제 관측값, \\({\\widehat{Y}}_{t}\\)는 시점 t에서의 평활값, 그리고 \\(\\alpha\\)는 평활 상수(smoothing constant)이다(\\(0 &lt; \\alpha \\leq 1\\)).\n이 식은 최근 관측값 \\(Y_{t}\\)와 직전 평활값 \\({\\widehat{Y}}_{t - 1}\\)의 가중 평균이며, 과거 관측값에 대한 가중치는 \\((1 - \\alpha)^{k}\\)로 지수적으로 감소한다.\n가중치 해석\n가중치 \\(\\alpha\\)가 클수록 최근 자료의 비중이 커지고, 과거 자료의 영향은 급격히 줄어든다. 반대로 \\(\\alpha\\)가 작으면 과거 자료의 영향이 오래 지속되며, 변화에 대한 반응 속도는 느려진다. Brown은 경험적으로 \\(\\alpha\\)를 0.05~0.3 범위에서 선택할 것을 권장하였다.\n다음 그림은 레벨이 t=40에서 점프하도록 만든 예시 데이터로 α=0.7은 변화에 빠르게 반응하고 α=0.2는 느리게 따라간다.\n\n\n\n\n\n주기와 평활 상수의 관계\nMontgomery & Johnson(1976)은 데이터 주기 \\(P\\)와 평활 상수 \\(\\alpha\\)의 관계를 다음과 같이 제안하였다. \\(\\alpha \\approx 1 - {0.8}^{(1/P)}\\). 예를 들어, 주기가 12개월인 월별 데이터의 경우 \\(\\alpha \\approx 0.18\\) 정도가 적절하다.\n초기값 설정\n초기 평활값 \\({\\widehat{Y}}_{1}\\)은 예측의 안정성을 위해 중요하다. 일반적으로 첫 번째 관측값 \\({\\widehat{Y}}_{1}\\) 또는 초기 몇 개 자료의 평균을 사용하며, 경우에 따라 최소제곱법을 통해 추정하기도 한다.\n예측 범위\n단순 지수평활법은 구조적으로 1기 예측(one-step-ahead)에 최적화되어 있다. 장기 예측의 경우, 2기 이후의 예측치는 모두 동일하게 \\({\\widehat{Y}}_{t + 1}\\)로 수렴한다. 따라서 장기 예측에는 적합하지 않고, 단기 예측이나 단기 추세 분석에 주로 활용된다.\n# 예제 데이터 가져오기\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom datetime import datetime\nfrom statsmodels.tsa.api import SimpleExpSmoothing\n\n# flights 데이터 로드\ndf = sns.load_dataset(\"flights\")\ndf['date'] = df.apply(lambda x: datetime.strptime(f\"{x['year']}-{x['month']}\", '%Y-%b'), axis=1)\ndf.set_index('date', inplace=True)\n\n# 단순 지수평활 모델 생성 및 적합\nmodel = SimpleExpSmoothing(df['passengers'], initialization_method=\"heuristic\")\nfit_model = model.fit(optimized=True)  # alpha 자동 추정\n\n# 결과 출력\nprint(\"Optimal alpha:\", fit_model.params['smoothing_level'])\nprint(\"SSE:\", fit_model.sse)\n\n# 12개월 예측\nforecast_steps = 12\nforecast = fit_model.forecast(steps=forecast_steps)\n\n# 시각화\nplt.figure(figsize=(10,5))\nplt.plot(df.index, df['passengers'], label='Observed', color='black')\nplt.plot(fit_model.fittedvalues.index, fit_model.fittedvalues, label='Fitted (SES)', color='blue')\nplt.plot(pd.date_range(df.index[-1] + pd.DateOffset(months=1), periods=forecast_steps, freq='M'),\n         forecast, label='Forecast', color='red', linestyle='--')\nplt.title(\"Simple Exponential Smoothing - Observed vs Fitted & Forecast\")\nplt.xlabel(\"Date\")\nplt.ylabel(\"Passengers\")\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\nOptimal alpha: 0.9999999850988388\nSSE: 162545.8192551743\n\n\n\n\n\n\\(\\alpha\\)가 1에 가까운 이유\n데이터에 강한 추세나 계절성이 있는 경우\n\nSES는 추세나 계절성을 모형화하지 못한다.\n그러다 보니 추세나 계절성 변화에 맞추려고 \\(\\alpha\\)를 크게 잡아서 최근 데이터만 따라가는 방식으로 SSE를 최소화하려고 한다.\n\n데이터 변동이 커서 과거 평균이 예측력을 떨어뜨리는 경우\n\n예측 오차를 줄이려면 과거치보다 최근값을 더 신뢰하는 게 유리하다고 판단한다.\n최적화 알고리즘이 \\(\\alpha\\)를 1 근처로 끌어올린다.\n\n\n\n(2) Holt’s Method 이중지수평활법\n추세가 있는 시계열 데이터를 다룰 때, 단순 지수평활법을 확장한 방법이다. 수준(Level)과 기울기(Trend)를 동시에 추정하여 예측한다.\n관측값 \\(Y_{1},Y_{2},\\ldots,Y_{n}\\)이 주어졌을 때, 수준 \\({\\widehat{a}}_{n}\\), 추세 \\(\\widehat{b}n\\)을 추정하고, 미래 \\(h\\)시점 예측치는 다음과 같이 구한다.\n\\[\\widehat{Yn + h} = {\\widehat{a}}_{n} + h{\\widehat{b}}_{n},h = 1,2,\\ldots\\]\n재귀식\n수준 갱신: \\(\\widehat{a}n + 1 = \\alpha Yn + 1 + (1 - \\alpha)({\\widehat{a}}_{n} + {\\widehat{b}}_{n})\\)\n추세 갱신: \\(\\widehat{b}n + 1 = \\beta(\\widehat{a}n + 1 - {\\widehat{a}}_{n}) + (1 - \\beta){\\widehat{b}}_{n}\\), 여기서 \\(\\alpha,\\beta \\in (0,1)\\)는 평활 상수입니다.\n예측값: \\({\\widehat{Y}}_{n + 1} = {\\widehat{a}}_{n} + {\\widehat{b}}_{n}\\)\n초기값 설정\n가장 단순 설정법 (Holt, 1957): \\({\\widehat{a}}_{2} = Y_{2}\\), \\({\\widehat{b}}_{2} = Y_{2} - Y_{1}\\)\n평균 기반 초기치: \\(\\widehat{a}1 = \\frac{1}{m}\\sum{t = 1}^{m}Y_{t}\\), \\({\\widehat{b}}_{1} = \\frac{Y_{m} - Y_{1}}{m - 1}\\)\n지수평활 기반 사전 추정: R forecast 패키지, Python statsmodels의 ExponentialSmoothing\n특징\n\\(\\alpha\\)는 수준의 변화에 대한 반응 속도, \\beta는 추세 변화에 대한 반응 속도를 조절.\n\\(\\alpha\\)와 \\(\\beta\\) 모두 0~1 사이에서 설정하며, 보통 MSE 최소화를 기준으로 추정\n추세가 있는 시계열에서 단기 예측에 강점.\n\n\n\n\n\n\\(\\alpha = 0.2,\\beta = 0.1\\) → 추세 반영이 느리고, 예측선이 부드러우며 변동에 둔감하다.\n\\(\\alpha = 0.8,\\beta = 0.1\\) → 최근 데이터에 민감하게 반응하여 빠르게 변화를 따라가지만, 변동이 클 수 있다.\n\\(\\alpha = 0.5,\\beta = 0.5\\) → 수준과 추세 모두를 비교적 빠르게 반영하여 적절한 타협형 곡선을 제공한다.\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom datetime import datetime\nfrom statsmodels.tsa.api import Holt\n\n# flights 데이터\ndf = sns.load_dataset(\"flights\")\ndf[\"date\"] = df.apply(lambda x: datetime.strptime(f\"{x['year']}-{x['month']}\", \"%Y-%b\"), axis=1)\ndf = df.set_index(\"date\").asfreq(\"MS\")   # 월초 빈도 지정\n\n# Holt: 수준+추세, 초기치도 추정\nholt = Holt(df[\"passengers\"], initialization_method=\"estimated\")\nfit  = holt.fit(optimized=True)\n\n# 어떤 키들이 있는지 확인\nprint(\"params keys:\", list(fit.params.keys()))\n\nalpha = fit.params[\"smoothing_level\"]\n# 버전 호환: trend 키 이름이 다를 수 있어 get 체인으로 안전하게 꺼내기\nbeta  = fit.params.get(\"smoothing_trend\", fit.params.get(\"smoothing_slope\"))\n\nprint(\"Optimal alpha:\", alpha)\nprint(\"Optimal beta :\", beta)\nprint(\"SSE:\", fit.sse)\n\n# 12개월 예측\nfcst = fit.forecast(12)\n\n# 시각화\nplt.figure(figsize=(10,5))\nplt.plot(df.index, df[\"passengers\"], label=\"Observed\", color=\"black\")\nplt.plot(fit.fittedvalues.index, fit.fittedvalues, label=\"Fitted (Holt)\", color=\"blue\")\nplt.plot(pd.date_range(df.index[-1] + pd.DateOffset(months=1), periods=12, freq=\"MS\"),\n         fcst, label=\"Forecast (12M)\", color=\"red\", linestyle=\"--\")\nplt.title(\"Holt’s Linear Trend – Observed vs Fitted & 12M Forecast\")\nplt.xlabel(\"Date\"); plt.ylabel(\"Passengers\"); plt.grid(True); plt.legend(); plt.tight_layout()\nplt.show()\nOptimal alpha: 0.9999999850988388\nOptimal beta : 0.0\nSSE: 161787.91754445358\n\n\n\n\n\n요약하면 β=0는 ”추세 업데이트를 하지 않는 게 SSE가 가장 낮았다”는 뜻입니다. 왜 그럴까요?\n1. 계절성을 못 잡은 Holt: flights 데이터는 추세 + 강한 계절성. Holt(수준+추세)만 쓰면 계절 파형을 설명 못 해서, ’추세까지 억지로 추정’하는 것보다 최근값을 거의 그대로 따라가는(α≈1) 편이 SSE가 더 낫다. 그 결과 추세항은 쓸모가 없으니 β→0으로 붙는다.\n2. 식 구조상 중복성: α가 1에 가까우면 \\(a_{t} \\approx Y_{t}\\). 이때 \\(a_{t} - a_{t - 1}\\)가 이미 최근 변화(추세 비슷한 것)를 반영해서, 별도의 추세 평활을 해도 이득이 거의 없습니다. 최적화는 자연히 경계값 β=0으로 밀린다.\n3. 초기/경계 최적화 효과: initialization_method=“estimated\"로 초기추세가 작게 잡히거나 노이즈·계절파형 때문에 추세항이 오히려 오차를 키우면, 알고리즘은 β를 0으로 둬서 SES와 거의 동일한 모형으로 수렴합니다.\nDamped trend 방법\nHolt의 선형 방법에 의한 예측값은 \\(hb_{t}\\)를 지니고 있어 미래로 갈수록 지속적인 추세 (증가 또는 감소)를 모함하게 된다. 이로 인하여 특히 더 긴 예측 기간에 대해 과도하게 예측되는 경향이 있음을 나타낸다. 하여, Gardner & McKenzie( 1985 ) 는 향후 언젠가 추세를 평평한 선으로 \"감쇠\"시키는 매개 변수를 도입했다. 감쇠된 추세를 포함하는 방법은 매우 성공적인 것으로 입증되었다.\n\\({\\widehat{y}}_{t + h} = a_{t} + h \\cdot b_{t}\\) (Holt 방법)\n감쇄 적용: \\({\\widehat{y}}_{t + h} = a_{t} + \\left( \\frac{1 - \\phi^{h}}{1 - \\phi} \\right)b_{t}\\), 여기서 \\(\\phi\\)는 0~1 사이의 감쇠계수이다.\nfrom statsmodels.tsa.api import Holt\nimport seaborn as sns\nfrom datetime import datetime\nimport matplotlib.pyplot as plt\n\n# 1. 데이터 준비\ndf = sns.load_dataset(\"flights\")\ndf['date'] = df.apply(lambda x: datetime.strptime(f\"{x['year']}-{x['month']}\", \"%Y-%b\"), axis=1)\ny = df.set_index(\"date\")[\"passengers\"].asfreq(\"MS\")\n\n# 2. Holt 모형 + 감쇠 추세\nholt_damped = Holt(y, damped_trend=True, initialization_method=\"estimated\").fit(optimized=True)\n\n# 3. SSE와 파라미터 출력\nprint(\"SSE :\", holt_damped.sse)\nprint(\"alpha:\", holt_damped.params['smoothing_level'])\nprint(\"beta :\", holt_damped.params['smoothing_trend'])\nprint(\"phi  :\", holt_damped.params['damping_trend'])  # 감쇠계수\n\n# 4. 예측\nsteps = 12\nfcst = holt_damped.forecast(steps)\n\n# 5. 시각화\nplt.figure(figsize=(10,5))\nplt.plot(y, label=\"Observed\", color=\"black\")\nplt.plot(holt_damped.fittedvalues, label=\"Fitted (Holt+Damped)\", color=\"blue\")\nplt.plot(fcst, label=\"Forecast (12M)\", color=\"red\", linestyle=\"--\")\nplt.legend()\nplt.grid(True)\nplt.show()\nSSE : 161801.78439102264\nalpha: 0.9999999850988388\nbeta : 0.0\nphi : 0.995\n왜 감쇄모형을 적용했는데 SSE는 증가했나?\n1. 지금 모형은 사실상 SES에 가깝습니다\n\n추정 결과: α ≈ 1, β = 0, φ = 0.995.\nβ=0이면 추세 업데이트를 안 합니다. φ(감쇠계수)가 있어도 추세 항이 움직이지 않으니 감쇠 효과가 거의 작동하지 않음 → Holt+damped가 SES와 거의 동일한 행동을 하다.\n이런 상황에선 파라미터 하나(φ)만 늘어난 셈이라, 훈련 SSE가 좋아질 이유가 없습니다(오히려 최적화의 미세한 차이로 약간 커질 수 있음).\n\n2. flights 데이터의 본질: 강한 추세 + 계절성\n\nHolt(추세만) 또는 Holt+damped(추세+감쇠)로는 계절 패턴을 잡지 못한다.\n그 결과 SSE를 줄이려면 최근값을 강하게 따라가는 게 최선 → \\(\\alpha\\)는 1에, \\(\\beta\\)는는 0으로 가게 된다.\n그러므로 Holt와 Holt+damped의 SSE 차이는 미미하거나, 가끔 damped 쪽이 더 나빠 보이게 된다.\n\n\n\n(3) Holt–Winters Method 삼중지수평활법\n이전 설명한 지수평활법은 계절성분이 없는 경우 사용된다. 그러므로 계절성이 있는 시계열 데이터에는 적합하지 않다. 강우량, 월별 수출량, 여행 승객 수 등은 계절성을 가지고 있다.\nHolt–Winters’ Additive Method\n계절효과의 크기가 시계열 전체 수준(Level)**에 관계없이 거의 일정할 때 사용한다. 즉, 계절성이 절대적인 차이로 반복된다.\n\\(\\begin{matrix}\n\\widehat{y}t + h & = a_{t} + b_{t}h + st + h - m \\\\\na_{t} & = \\alpha(y_{t} - s_{t - m}) + (1 - \\alpha)(a_{t - 1} + b_{t - 1}) \\\\\nb_{t} & = \\beta(a_{t} - a_{t - 1}) + (1 - \\beta)b_{t - 1} \\\\\ns_{t} & = \\gamma(y_{t} - a_{t}) + (1 - \\gamma)s_{t - m}\n\\end{matrix}\\), 여기서 \\(a_{t}\\)는 수준, \\(b_{t}\\)는 추세, \\(s_{t}\\)는 계절성, 그리고 \\(m\\)은 주기이다.\nHolt–Winters’ Multiplicative Method\n계절효과의 크기가 수준에 비례하여 변할 때 사용하게 되는데 계절성이 비율적 차이로 반복된다. 예를 들면, 매년 12월에는 항상 약 20% 증가, 여름에는 15% 감소하는 시계열 데이터에 적용된다.\\(\\begin{matrix}\n\\widehat{y}t + h & = (a_{t} + b_{t}h) \\cdot st + h - m \\\\\na_{t} & = \\alpha\\frac{y_{t}}{s_{t - m}} + (1 - \\alpha)(a_{t - 1} + b_{t - 1}) \\\\\nb_{t} & = \\beta(a_{t} - a_{t - 1}) + (1 - \\beta)b_{t - 1} \\\\\ns_{t} & = \\gamma\\frac{y_{t}}{a_{t}} + (1 - \\gamma)s_{t - m}\n\\end{matrix}\\), 여기서 \\(\\gamma\\)는 계절성분 갱신 시 적용되는 평활 상수로 절 패턴의 변화 속도를 얼마나 빠르게 반영할지를 결정한다.\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom datetime import datetime\nfrom statsmodels.tsa.holtwinters import ExponentialSmoothing\n\n# 1) 데이터 로드 & 시계열 인덱스 설정\ndf = sns.load_dataset(\"flights\")\ndf[\"date\"] = df.apply(lambda x: datetime.strptime(f\"{x['year']}-{x['month']}\", \"%Y-%b\"), axis=1)\ny = df.set_index(\"date\")[\"passengers\"].asfreq(\"MS\")  # 월초 빈도\n\n# 2) Holt–Winters Additive (trend='add', seasonal='add')\nhw_add = ExponentialSmoothing(\n    y, trend=\"add\", seasonal=\"add\", seasonal_periods=12,\n    initialization_method=\"estimated\"\n).fit(optimized=True)\n\n# 3) Holt–Winters Multiplicative (trend='add', seasonal='mul')\nhw_mul = ExponentialSmoothing(\n    y, trend=\"add\", seasonal=\"mul\", seasonal_periods=12,\n    initialization_method=\"estimated\"\n).fit(optimized=True)\n\n# 4) 12개월 예측\nsteps = 12\nfcst_add = hw_add.forecast(steps)\nfcst_mul = hw_mul.forecast(steps)\n\n# 5) 파라미터 & SSE 출력\nprint(\"[Additive]\")\nprint(\" alpha:\", hw_add.params.get(\"smoothing_level\"))\nprint(\" beta :\", hw_add.params.get(\"smoothing_trend\"))\nprint(\" gamma:\", hw_add.params.get(\"smoothing_seasonal\"))\nprint(\" SSE  :\", hw_add.sse, \"\\n\")\n\nprint(\"[Multiplicative]\")\nprint(\" alpha:\", hw_mul.params.get(\"smoothing_level\"))\nprint(\" beta :\", hw_mul.params.get(\"smoothing_trend\"))\nprint(\" gamma:\", hw_mul.params.get(\"smoothing_seasonal\"))\nprint(\" SSE  :\", hw_mul.sse)\n\n# 6) 시각화\nplt.figure(figsize=(11,5))\nplt.plot(y.index, y, label=\"Observed\", color=\"black\")\nplt.plot(hw_add.fittedvalues.index, hw_add.fittedvalues, label=\"Fitted (Additive)\")\nplt.plot(hw_mul.fittedvalues.index, hw_mul.fittedvalues, label=\"Fitted (Multiplicative)\")\nplt.plot(pd.date_range(y.index[-1] + pd.DateOffset(months=1), periods=steps, freq=\"MS\"),\n         fcst_add, label=\"Forecast Add\", linestyle=\"--\")\nplt.plot(pd.date_range(y.index[-1] + pd.DateOffset(months=1), periods=steps, freq=\"MS\"),\n         fcst_mul, label=\"Forecast Mul\", linestyle=\"--\")\nplt.title(\"Holt–Winters: Additive vs Multiplicative\")\nplt.xlabel(\"Date\"); plt.ylabel(\"Passengers\"); plt.grid(True); plt.legend(); plt.tight_layout()\nplt.show()\n[Additive]\nalpha: 0.2525303255513238\nbeta : 0.0\ngamma: 0.7474696744486762\nSSE : 21564.432209838982\n[Multiplicative]\nalpha: 0.31858664757791894\nbeta : 0.0\ngamma: 0.6013533719848393\nSSE : 15952.880434994611\n\n\n\n\n\n\n\n\n\nchapter 4. ARMA 모형\nARIMA(Auto-Regressive Integrated Moving Average) 모형은 George Box와 Gwilym Jenkins가 제안한 시계열 모형으로, 자기회귀(AR), 차분(Integrated), 이동평균(MA) 요소를 결합하여 시계열 데이터를 설명하고 예측한다.\n시계열 자료 \\(\\{ Y_{t}\\}\\)의 과거 값 \\(\\{ Y_{t - 1},Y_{t - 2},\\ldots\\}\\)이 설명변수가 되는 경우를 자기회귀 모형(AR)이라 한다. 이전 관측치들의 가중치가 동일하면 이동평균법, 가중치가 최근 관측치에서 크고 과거로 갈수록 지수적으로 감소하면 지수평활법이라 한다.\n과거 관측치로 설명되지 않는 부분, 즉 잔차(오차항) \\(\\{\\varepsilon_{t - 1},\\varepsilon_{t - 2},\\ldots\\}\\)를 설명변수로 사용하는 모형이 이동평균 모형(MA)이다. 여기에 비정상 시계열을 정상 시계열로 변환하기 위한 차분(differencing) 과정을 나타내는 Integrated를 결합한 것이 ARIMA 모형이다.\n\n1. ARMA( p, q) Processes\n【정의】 \\(\\{ X_{t}\\}\\)가 정상성을 가지며, 모든 t에 대해 다음을 만족하면 \\(ARMA(p,q)\\)과정이라고 한다.\n\\[X_{t} - \\phi_{1}X_{t - 1} - \\cdots - \\phi_{p}X_{t - p} = Z_{t} + \\theta_{1}Z_{t - 1} + \\cdots + \\theta_{q}Z_{t - q}\\]\n여기서 \\(\\{ Z_{t}\\} \\sim WN(0,\\sigma^{2})\\)이고, 다항식 \\((1 - \\phi_{1}z - \\cdots - \\phi_{p}z^{p}),(1 + \\theta_{1}z + \\cdots + \\theta_{q}z^{q})\\)는 공통 인자를 갖지 않는다.\n【 간편식】 \\(\\phi(B)X_{t} = \\theta(B)Z_{t}\\), 여기서 \\(\\phi( \\cdot )\\)와 \\(\\theta( \\cdot )\\)는 각각 p차와 q차의 다항식이고, B는 후진 이동 연산자(backward shift operator)이다.\n\\[\\phi(z) = 1 - \\phi_{1}z - \\cdots - \\phi_{p}z^{p}\\]\n\\[\\theta(z) = 1 + \\theta_{1}z + \\cdots + \\theta_{q}z^{q}\\]\n\\[B^{j}X_{t} = X_{t - j},B^{j}Z_{t} = Z_{t - j},j = 0, \\pm 1,\\ldots\\]\n시계열 \\(\\{ X_{t}\\}\\)는 \\(\\phi(z) \\equiv 1\\)이면 p차 자기회귀 과정(\\(AR(p)\\))이라 하고, \\(\\theta(z) \\equiv 1\\)이면 q차 이동평균 과정(\\(MA(q)\\))이라 한다.\n\n\n2. 정상성 stationarity과 인과성\n\n(1) 정상성\nARMA 모형에서 정상성이 중요한 이유는, 이 모형이 과거 값과 오차항의 일정한 관계를 바탕으로 미래를 예측하는 구조이기 때문이다. 정상성은 평균과 분산이 시간에 따라 변하지 않고, 자기상관 구조가 시점과 무관하게 일정하게 유지된다는 특성을 의미한다. 이러한 특성이 보장되어야 과거 패턴이 미래에도 그대로 적용될 수 있고, 모형의 계수도 시점에 따라 변하지 않아 안정적으로 해석할 수 있다.\n첫째, 모수의 안정성 측면에서 정상성은 매우 중요하다. ARMA 모형은 과거 관측치와 오차항의 선형 결합으로 표현되는데, 데이터가 비정상적이면 평균과 분산이 시간에 따라 변하여 계수 추정치도 시점에 따라 달라질 수 있다. 이는 모형을 불안정하게 만들고, 동일한 계수를 유지해야 한다는 전제를 무너뜨린다. 반면 정상성 조건이 충족되면 계수는 시점에 관계없이 일정하게 유지될 수 있다.\n둘째, 예측 가능성을 위해서도 정상성이 필요하다. 정상성 시계열은 시간에 관계없이 일정한 패턴을 유지하므로, 과거의 패턴을 기반으로 미래를 예측할 수 있다. 그러나 비정상 시계열은 평균, 분산, 자기상관 구조가 변하기 때문에 과거 관계식이 미래에 그대로 적용되지 않는다. 예를 들어, 단순한 추세형 주가 시계열은 비정상적이어서 과거 패턴이 반복되지 않는 경우가 많다.\n셋째, 자기상관(ACF)와 부분자기상관(PACF) 분석이 가능하려면 정상성이 필요하다. ARMA 모형의 차수(p, q)를 결정하고 모수를 추정하는 과정은 ACF와 PACF 패턴을 해석하는 데 기반한다. 데이터가 비정상이면 ACF/PACF가 일정하게 수렴하지 않고, 이론적으로 기대되는 절단(cut-off) 또는 지수적 감소 형태가 나타나지 않는다. 따라서 정상성이 없으면 AR과 MA 차수를 논리적으로 결정하기 어렵다.\n넷째, 수리적 해석 가능성에서도 정상성은 필수적이다. 정상성 조건이 충족되면 ARMA 방정식의 해를 무한 MA 표현(ψ-가중치)으로 전개할 수 있으며, 이를 통해 모형의 안정성과 예측 오차 구조를 분석할 수 있다. 그러나 비정상 과정에서는 이러한 전개가 발산하여 수리적 해석이 불가능해진다.\n이론적 정상성 조건 (모형 계수 기반)\nARMA 모형이 주어졌을 때, AR 부분의 특성방정식(characteristic equation)의 모든 근이 단위원 밖에 있으면 정상성이다.\nAR(p) 모형: \\(Y_{t} = \\phi_{1}Y_{t - 1} + \\phi_{2}Y_{t - 2} + \\cdots + \\phi_{p}Y_{t - p} + e_{t}\\)\n특성방정식: \\(1 - \\phi_{1}z - \\phi_{2}z^{2} - \\cdots - \\phi_{p}z^{p} = 0\\), 모든 해 z가 \\(|z| &gt; 1\\)이면 정상성이다. 계수 추정 후 정상성 검정이 가능하다.\n데이터 기반 정상성 검정 (Unit Root Test)\n(1) ADF 검정 (Augmented Dickey–Fuller test)\n\n귀무가설 H_0: 단위근 존재 → 비정상(Non-stationary)\n대립가설 H_1: 정상성\n\n(2) KPSS 검정 (Kwiatkowski–Phillips–Schmidt–Shin test)\n\n귀무가설 H_0: 정상성\n대립가설 H_1: 비정상\n\nADF와 반대 논리 → 두 검정을 함께 쓰면 더 확실.\n(3) Phillips–Perron (PP) 검정\nADF와 유사하지만 자기상관과 이분산성을 보정한 버전이다.\n비정상성 해결방안\n첫째, 차분(differencing)\n\n일반 차분: \\(Y_{t} - Y_{t - 1}\\) 형태로 한 시점 전 값을 빼서 추세를 제거한다.\n계절 차분: \\(Y_{t} - Y_{t - s}\\) 형태로 s주기 전 값을 빼서 계절성을 제거한다. 예를 들어 월별 데이터에서 s=12면 연간 계절 패턴을 없앨 수 있다.\n\n차분 횟수 d는 최소한으로 하는 것이 좋으며, 과도한 차분은 오히려 잡음을 증가시킨다.\n둘째, 변환(transformation)\n데이터의 분산이 시간에 따라 변하는 경우, 로그 변환이나 제곱근 변환을 적용해 분산을 안정화시킬 수 있다.\n\n로그 변환: 급격히 증가하는 값의 분산을 줄이는 데 효과적\nBox–Cox 변환: 로그 변환을 포함하는 더 일반적인 형태로, 최적 변환 파라미터를 추정해 적용 가능\n\n셋째, 추세 제거(detrending)\n추세가 뚜렷한 경우, 회귀분석으로 추세 성분을 추정한 뒤 이를 제거해 잔차(residual)만 분석한다. 예를 들어, 시간 변수를 독립변수로 하는 선형 회귀를 통해 추세를 제거할 수 있다.\n넷째, 계절성 조정(seasonal adjustment)\n계절성이 있는 경우, 계절성 분해(예: STL decomposition, X-13ARIMA-SEATS)를 통해 계절 성분을 제거한다. 이렇게 하면 남은 비계절 부분이 정상성을 가질 가능성이 높아진다.\n다섯째, 통계적 단위근 검정 후 보정\nADF(augmented Dickey–Fuller), KPSS, Phillips–Perron 등의 단위근 검정을 통해 비정상성을 확인한 뒤, 적절한 차분이나 변환을 적용한다.\n\n\n(2) 인과성과 가역성\n【정의】 시계열 \\(\\{ X_{t}\\}\\)가 ARMA(p, q) 모형을 따른다고 하자. 이 시계열이 인과적(causal)이라는 것은, 현재 값 \\(\\{ X_{t}\\}\\)가 과거의 백색잡음 \\(\\{ Z_{t - j}\\}\\)들의 가중합으로 표현될 수 있음을 의미한다.즉, 다음을 만족하는 상수 \\(\\{\\psi_{j}\\}\\)가 존재해야 한다.\n\\[X_{t} = \\overset{\\infty}{\\sum_{j = 0}}\\psi_{j}Z_{t - j},\\overset{\\infty}{\\sum_{j = 0}}|\\psi_{j}| &lt; \\infty\\]\n이 조건은 현재 값이 오직 과거 값들로부터만 영향을 받음을 뜻한다.\n인과성의 수학적 조건\n인과성은 AR 다항식 \\(\\phi(z) = 1 - \\phi_{1}z - \\cdots - \\phi_{p}z^{p}\\)가 \\(|z| \\leq 1\\)인 모든 z에 대해 0이 아닌 경우와 동치이다. 이는 AR 방정식의 근이 단위원 바깥에 있어야 한다는 의미다.\n이 조건을 만족하면, ARMA 모형을 무한 MA(무한 이동평균) 형태로 전개할 수 있다. 인과성이 보장되지 않으면, 현재 값이 미래의 충격(오차)에 의존하게 되므로, 실제 예측 불가능한 모형이 된다.\n가역성 invertibility\n【정의】 ARMA(p, q) 과정 \\(\\{ X_{t}\\}\\)가 가역적이라고 하려면, \\(\\overset{\\infty}{\\sum_{j = 0}}|\\pi_{j}| &lt; \\infty\\)인 상수들 \\(\\{\\pi_{j}\\}\\)가 존재하고 \\(Z_{t} = \\overset{\\infty}{\\sum_{j = 0}}\\pi_{j}X_{t - j}\\)가 모든 t에 대해 성립해야 한다. 가역성은 다음 조건과 동치이다:\n\\[\\theta(z) = 1 + \\theta_{1}z + \\cdots + \\theta_{q}z^{q} \\neq 0\\text{for all}|z| \\leq 1\\]\n가역성은 MA(이동평균) 과정이 동일한 자기회귀(AR) 과정으로 유일하게 표현될 수 있는 성질을 말한다. 쉽게 말해, MA 모형은 원래 과거의 오차항(\\(Z_{t}\\))들을 설명변수로 쓰지만, 실제 분석에서는 오차항을 직접 알 수 없다. 가역성이 있으면 이 오차항들을 과거 관측값의 무한 선형결합으로 표현할 수 있기 때문에, 오차항 없이도 모형을 재작성하고 추정할 수 있다.\n이 성질이 없으면, 동일한 관측값 시계열을 설명하는 MA 모형이 여러 개 존재하게 되어 모형이 비식별(non-identifiable) 문제가 발생한다. 그래서 추정 안정성과 해석 가능성을 위해 MA 모형에는 반드시 가역성 조건을 부여한다.\n\n\n\n3. ARMA 모델 추정 과정\n① 데이터 사전 진단\nARMA 모형을 적용하기 전, 데이터의 특성을 파악하여 정상성 여부와 모형 적합 가능성을 평가한다.\n\n시간도표) 분석: 시계열을 시간축에 따라 시각화하여 추세, 계절성, 변동성의 존재 여부를 직관적으로 확인한다. 정상성 데이터라면 평균과 분산이 일정해야 한다.\n백색잡음 검정: 시계열이 완전 무작위인지 여부를 확인한다. 데이터가 백색잡음이면 예측 가능성이 없으므로 ARMA 모형을 적용할 필요가 없다.\n정상성 검정: ADF(augmented Dickey–Fuller)나 KPSS 검정을 통해 정상성 여부를 통계적으로 판정한다.\n등분산성 검정: 시간에 따라 분산이 변하지 않는지 확인한다. 필요하면 변환(log, Box–Cox 등)을 통해 분산을 안정화시킨다.\n\n② 모형 식별 (Model Identification)\n정상성 확보 후, ACF와 PACF 분석을 통해 AR(p), MA(q) 차수를 결정한다.\n\nACF(Autocorrelation Function): 시차(lag)별 상관관계를 측정하여 MA 차수 식별에 활용한다.\nPACF(Partial Autocorrelation Function): 시차별 순수 자기상관을 계산하여 AR 차수 식별에 사용한다.\n계절성 진단: 계절성이 있으면 계절 차분(Seasonal Differencing)을 고려하고, 계절 AR/MA 항을 포함한 SARIMA 모형으로 확장한다.\n\n③ 모형 추정 (Parameter Estimation)\n식별된 차수(p, q)에 맞춰 ARMA 모형을 설정하고, 회귀계수를 추정한다.\n\n추정 방법: 최대우도추정(MLE, Maximum Likelihood Estimation) 또는 최소제곱법(OLS, Ordinary Least Squares) 사용.\n계절성 데이터 처리: 계절성이 있으면 주기(s)만큼 차분한 시계열을 이용해 계수를 추정한다.\n\n④ 모형 진단 (Model Diagnostics)\n추정된 모형이 적절한지 평가한다.\n\n계수 유의성 검정: 각 계수의 t-통계량과 p값을 확인해 통계적으로 유의한지 판단한다.\n잔차 분석: 모형의 잔차가 백색잡음(평균 0, 분산 일정, 자기상관 없음)인지 검정한다. 잔차에 패턴이 남아 있으면 모형이 데이터를 충분히 설명하지 못한 것이다.\nLjung–Box Q 검정: 잔차의 자기상관이 유의한지 평가한다.\n\n⑤ 예측 모형 적용 (Forecasting)\n모형 진단을 통과한 최종 ARMA 모형을 사용하여 미래 값을 예측한다.\n\n단기/중기 예측: 정상성 시계열에서는 장기 예측보다 단기 예측에서 오차가 적다.\n예측 결과와 신뢰구간(Confidence Interval)을 함께 제시하여 불확실성을 평가한다.\n\n\n\n4. ACF와 PACF\n\n(1) 자기공분산함수(ACF)\n정의\n자기상관계수(Autocorrelation Coefficient)는 시계열 데이터가 서로 다른 시점에서 얼마나 유사한지를 나타내는 척도로 시차 j에서의 자기상관계수 \\(\\rho(j)\\)는 다음과 같이 정의한다.\n\\(\\rho(h) = \\frac{\\text{COV}(X_{t},X_{t - h})}{V(X_{t})}\\). 즉, 시점 \\(t\\)와 \\(t - h\\)의 값이 얼마나 비슷한지를 분산으로 표준화한 값이다.\n성질\n\n\\(\\rho(h)\\)는 상관계수이므로 항상 \\(- 1 \\leq \\rho(h) \\leq 1\\)범위에 존재한다.\n\\(h = 0\\) : \\(Y_{t}\\)와 \\(Y_{t}\\)의 상관계수이므로 항상 1이다.\n\\(\\rho(h) = \\rho( - h)\\)\n정상 백색잡음 과정에서는 \\(\\rho(h) = 0\\) (단, \\(h \\neq 0\\))\n정상 시계열에서는 \\(\\rho(h)\\)가 시차 \\(h\\)에만 의존하고 시간 \\(t\\)에는 의존하지 않는다.\nAR(p) 모형의 경우 PACF가 p시차에서 절단되고, ACF는 지수적으로 감소한다.\nMA(q) 모형의 경우 ACF가 q시차에서 절단되고, PACF는 지수적으로 감소한다.\n\n\n\n(2) 모형 식별\n시계열 데이터의 특성을 분석하여 적합한 모형의 형태와 차수를 결정하는 단계이다. 부분자기상관함수(PACF)와 함께 이용하여 AR, MA, ARMA 모형의 차수를 추정한다.\n계절성이 존재하는 경우 주기별 패턴을 추가로 분석하여 계절 차수를 식별한다. 예를 들어 ACF가 일정 시점에서 절단되는 경우 MA 모형의 후보이며, PACF가 일정 시점에서 절단되는 경우 AR 모형의 후보이다. ACF와 PACF가 모두 서서히 감소하는 경우 ARMA 모형의 후보이다.\n\n\\(X_{t} = Z_{t} \\sim WN\\) ACF\n\n\\[\\rho(h) = 0,forh &gt; 0\\]\n\n\\(X_{t} = Z_{t} + \\theta_{1}Z_{t - 1} + \\cdots + \\theta_{q}Z_{t - q} \\sim MA(q)\\) ACF\n\n\\(\\gamma(h) = \\{\\begin{matrix}\n\\sigma^{2}\\overset{q - |h|}{\\sum_{j = 0}}\\theta_{j}\\theta_{j + |h|}, & \\text{if}|h| \\leq q, \\\\\n0, & \\text{if}|h| &gt; q.\n\\end{matrix}\\)4\n\n\\(X_{t} - \\phi_{1}X_{t - 1} = Z_{t} \\sim AR(1)\\) ACF\n\n\\[\\rho(h) = \\phi_{1}^{|h|},h = 0,1,2,\\ldots\\]\n\n\\(X_{t} - \\phi_{1}X_{t - 1} - \\phi_{2}X_{t - 2} = Z_{t} \\sim AR(2)\\) ACF\n\n\\[\\rho(h) = \\phi_{1}\\rho(h - 1) + \\phi_{2}\\rho(h - 2),h \\geq 2\\]\n\\[$\\rho(0) = 1$, $\\rho(1) = \\frac{\\phi_{1}}{1 - \\phi_{2}}\\]\nAR(2) ACF는 지수적 감소(실수 근 2개), 감쇠 진동(복소수 근 전재), 또는 이들의 혼합 패턴을 보인다.\n\n\\(X_{t} - \\phi_{1}X_{t - 1} = Z_{t} + \\theta_{1}Z_{t - 1} \\sim ARMA(1,1)\\) ACF\n\n\\[\\rho(0) = 1\\]\n\\[\\rho(1) = \\frac{(1 + \\theta_{1}\\phi_{1})(\\theta_{1} + \\phi_{1})}{1 + 2\\theta_{1}\\phi_{1} + \\theta_{1}^{2}}\\]\n\\[\\rho(h) = \\phi_{1}^{h - 1}\\rho(1),h \\geq 2\\]\n\\(AR(1)\\)모형처럼 지수적 감소 형태를 보인다. 단, \\(\\rho(1)\\)값이 \\(\\theta_{1}\\)값에 의해 조정되므로, \\(AR(1)\\)보다 첫 번째 시차에서의 상관이 더 크거나 작을 수 있다. \\(h \\geq 2\\)에서는 \\(AR(1)\\)과 동일하게 \\(\\phi_{1}\\)의 거듭제곱에 따라 감소한다.\nACF와 PACF 이용한 모형 식별\n\n\n\n\n\n\n\n\n모형\nACF\nPACF\n\n\n\n\nAR(p)\n지수적으로 감소한다.\n시차 \\(p\\) 이후 0으로 떨어진다.\n\n\nMA(q)\n시차 \\(q\\) 이후 0으로 떨어진다.\n지수적으로 감소한다.\n\n\nARMA(p, q)\n지수적으로 감소한다.\n지수적으로 감소한다.\n\n\n\n\n\n\n5. 사례분석\n\n(1) 데이터 사전 진단 및 전처리\n시간도표\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom datetime import datetime\n\n# 예제 데이터 불러오기\ndf = sns.load_dataset(\"flights\")\ndf['date'] = df.apply(lambda x: datetime.strptime(f\"{x['year']}-{x['month']}\", '%Y-%b'), axis=1)\n\n# date를 인덱스로 설정\ndf = df.set_index('date')\n\n# 시간도표 그리기\nplt.figure(figsize=(12,6))\nplt.plot(df.index, df['passengers'], marker='o', linestyle='-')\nplt.title(\"Time Plot of Monthly Passengers (1949–1960)\")\nplt.xlabel(\"Date\")\nplt.ylabel(\"Number of Passengers\")\nplt.grid(True)\nplt.show()\n\n\n\n\n\n1. 추세 존재\n1949년부터 1960년까지 승객 수가 전반적으로 증가하는 뚜렷한 상승 추세를 보이고 있다. 이는 장기적으로 항공 여행 수요가 꾸준히 늘어났음을 의미한다.\n2. 계절성 뚜렷\n매년 일정한 주기(12개월)를 두고 승객 수가 반복적으로 증감하는 패턴이 나타난다. 여름철과 연말에 승객 수가 높은 피크를 보이고, 겨울·초봄에는 감소하는 경향이 있다.\n3. 변동폭 확대(Heteroscedasticity)\n시간 경과에 따라 계절 변동폭(peak와 trough의 차이)이 점점 커지고 있다. 이는 비정상 시계열에서 자주 나타나는 특징으로, 분산이 일정하지 않다는 것을 시사합니다.\n4. 비정상성 가능성\n평균과 분산이 시간에 따라 변동하므로 정상성(stationarity)을 만족하지 않을 가능성이 크다. ARMA 모형 적용 전에 차분(differencing) 또는 변환(log transformation) 등을 통해 정상화를 시도해야 할 필요가 있다.\n백색잡음 진단: Ljung-Box and Box-Pierce 통계량\n\n귀무가설 : 분석대상 시계열 데이터는 백색 잡음이다. \\(Y_{t} = e_{t}\\) &lt;=&gt; 모형인식 불가능\n대립가설 : 백색 잡음이 아니다. &lt;=&gt; 패턴이 존재한다. &lt;=&gt; ARMA 모형 인식이 가능하다.\n\n#백색잡음 진단\nimport statsmodels.api as sm\nsm.stats.acorr_ljungbox(df['passengers'],lags=[20],boxpierce=True)\n\n\n\n\n\n\n\n\n\nlb_stat\nlb_pvalue\nbp_stat\nbp_pvalue\n\n\n1434.148907\n5.300473E-292\n1328.532248\n2.291495E-269\n\n\n\n첫 행은 Ljung-Box 통계량(1456)과 유의확률, 두번째 행은 Box-Pierce 통계량(1381)과 유의확률이다. 두 방법 모두 유의확률이 &lt;0.001이므로 백색잡음이 아니다. boxpierce=True을 제외하면 Ljung-Box 통계량만 출력된다.\n해당 시계열 데이터에는 자기상관이 강하게 존재하며, 단순한 무작위 잡음이 아니다. 즉, ARMA 등 자기상관 구조를 반영한 모형을 적용할 필요가 있다.\n단위근 unit root 검정\n귀무가설 : 단위근을 갖는다. 단위근 unit root 모형 &lt;=&gt; random walk 모형\n대립가설 : 단위근 문제가 없다.\n단위근 문제가 발생하면 차분으로 문제를 해결한다.\n#Augmented Dickey-Fuller 단일근 검정\nfrom statsmodels.tsa.stattools import adfuller\nadf_result = adfuller(df['passengers'])\nprint(\"ADF Statistic:\", adf_result[0])\nprint(\"p-value:\", adf_result[1])\nADF Statistic: 0.8153688792060498\np-value: 0.991880243437641\nADF 검정의 귀무가설(H₀)은 ”시계열이 단위근을 가진 비정상 과정이다”이다. p-value가 0.99로 매우 크기 때문에 귀무가설을 기각할 수 없다. 즉, 이 시계열은 비정상 시계열이며, 추세 제거(차분) 또는 변환 후에 ARMA 모형을 적용해야 한다.\n등분산 검정(ARCH test)\n\n귀무가설(H₀): 잔차는 등분산성을 가진다.\n대립가설(H₁): 잔차는 이분산성을 가진다.\n\n# 등분산성 검정 (ARCH test)\nfrom statsmodels.stats.diagnostic import het_arch\n\n# flights 데이터에서 'passengers' 컬럼 사용\nstat, p_value, _, _ = het_arch(df['passengers'])\n\nprint(f\"ARCH LM 통계량: {stat}\")\nprint(f\"p-value: {p_value}\")\n\nif p_value &lt; 0.05:\n    print(\"귀무가설 기각: 이 데이터는 이분산성을 가질 가능성이 큼\")\nelse:\n    print(\"귀무가설 채택: 이 데이터는 등분산성을 가짐\")\nARCH LM 통계량: 126.0645062581934\np-value: 2.961233760503082e-22\n귀무가설 기각: 이 데이터는 이분산성을 가질 가능성이 큼\np-value가 0.05 미만이므로 귀무가설을 기각한다. 따라서, 이 데이터는 이분산성을 가질 가능성이 매우 높다고 결론낼 수 있다. 즉, flights 데이터의 승객 수 시계열은 분산이 시간에 따라 일정하지 않고, 변동성이 시점에 따라 달라진다고 해석된다.\n로그변환, 제곱근 변환, Box-Cox 변환이 있는데 가장 많이 사용되는 로그변환을 사용한다.\n전처리후 ADF 검정\nfrom statsmodels.tsa.stattools import adfuller\nimport numpy as np\n\n# 로그 변환\ndf['log_passengers'] = np.log(df['passengers'])\n\n# 1차 차분\ndf['log_diff1'] = df['log_passengers'].diff()\n\n# NaN 제거 후 ADF 테스트\nadf_result = adfuller(df['log_diff1'].dropna())\nprint(\"ADF Statistic:\", adf_result[0])\nprint(\"p-value:\", adf_result[1])\nADF Statistic: -2.717130598388114\np-value: 0.07112054815086184\n유의수준 5%에서 단위근 문제없다. (정상성 프로세스)\n\n\n(2) 모형진단\nfrom statsmodels.tsa.stattools import adfuller\nimport numpy as np\n\n# 로그 변환\ndf['log_passengers'] = np.log(df['passengers'])\n\n# 1차 차분\ndf['log_diff1'] = df['log_passengers'].diff()\n\n# NaN 제거 후 ADF 테스트\nadf_result = adfuller(df['log_diff1'].dropna())\nprint(\"ADF Statistic:\", adf_result[0])\nprint(\"p-value:\", adf_result[1])\n\n\n\n\n\n1. ACF\n\n대부분의 시차에서 값이 신뢰구간(파란 음영 영역) 안에 있음.\n특정 시차(예: 1, 12)에서만 약간의 유의한 값이 있지만, 전반적으로 빠르게 소멸하는 형태.\n이는 잔여 상관 구조가 거의 없음을 의미합니다.\n\n2. PACF\n\nPACF도 비슷하게, 거의 모든 시차에서 값이 유의하지 않음.\n시차 1, 12 부근에 약간의 봉이 보이지만 강하지 않음.\nAR 계수가 크게 필요하지 않음을 시사.\n\n3. 잠재 모형\n비계절 부분\n\nACF: 시차 1에서만 살짝 유의 → MA(1) 가능성\nPACF: 시차 1에서만 살짝 유의 → AR(1) 가능성 → p, q 둘 다 0~1 범위에서 시도\n\n계절 부분 (주기 12)\n\nACF: 시차 12 부근 약간 유의 → 계절 MA(1) 가능성\nPACF: 시차 12 부근 약간 유의 → 계절 AR(1) 가능성 → P, Q 둘 다 0~1 범위에서 시도\n\n\n\n\n\n\n\n\n모형\n비계절 차수 (p,d,q)\n계절 차수 (P,D,Q,s)\n\n\n모델 1\n(0,1,1)\n(0,1,1,12)\n\n\n모델 2\n(1,1,0)\n(0,1,1,12)\n\n\n모델 3\n(1,1,1)\n(0,1,1,12)\n\n\n모델 4\n(0,1,1)\n(1,1,0,12)\n\n\n모델 5\n(0,1,1)\n(1,1,1,12)\n\n\n\n\n\n\n(3) 최적 모형\n#모형 비교\ncandidates = [\n    ((0,1,1),(0,1,1,12)),\n    ((1,1,0),(0,1,1,12)),\n    ((1,1,1),(0,1,1,12)),\n    ((0,1,1),(1,1,0,12)),\n    ((0,1,1),(1,1,1,12)),\n]\n\nrows = []\nfor order, seas in candidates:\n    m = sm.tsa.statespace.SARIMAX(df[\"log_passengers\"], order=order, seasonal_order=seas,\n                                  enforce_stationarity=False, enforce_invertibility=False)\n    r = m.fit(disp=False)\n    rows.append({\"order\": order, \"seasonal_order\": seas, \"AIC\": r.aic, \"BIC\": r.bic})\n\npd.DataFrame(rows).sort_values(\"AIC\")\n\n\n\n\n\n\n\n\n\n차수\n계절차수\nAIC\nBIC\n\n\n(1, 1, 0)\n(0, 1, 1, 12)\n-437.52582\n-429.21377\n\n\n(0, 1, 1)\n(1, 1, 0, 12)\n-437.11592\n-428.77855\n\n\n(0, 1, 1)\n(0, 1, 1, 12)\n-435.44352\n-427.157\n\n\n(1, 1, 1)\n(0, 1, 1, 12)\n-433.78518\n-422.73649\n\n\n(0, 1, 1)\n(1, 1, 1, 12)\n-428.86966\n-417.82097\n\n\n\n\nAIC 최솟값: -437.525820 → 차수=(1,1,0), 계절차수=(0,1,1,12)\nBIC 최솟값: -429.213766 → 동일하게 차수=(1,1,0), 계절차수=(0,1,1,12)\n\n모형 추정 및 검정\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport statsmodels.api as sm\n\n# 1) 데이터 로드 & 인덱스 설정\ndf = sns.load_dataset(\"flights\")\ndf[\"date\"] = pd.to_datetime(df[\"year\"].astype(str) + \"-\" + df[\"month\"].astype(str))\ndf = df.set_index(\"date\").sort_index()\n\n# 2) 로그 변환(열로 보관)\ndf[\"log_passengers\"] = np.log(df[\"passengers\"])\n\n# 3) SARIMA 적합: (p,d,q)×(P,D,Q,12)\nmodel = sm.tsa.statespace.SARIMAX(\n    df[\"log_passengers\"],\n    order=(1,1,0),\n    seasonal_order=(0,1,1,12),\n    enforce_stationarity=False,\n    enforce_invertibility=False,\n)\nres = model.fit()\nprint(res.summary())\n\n# 1) 잔차 추출\nresiduals = res.resid\n# 2) ACF/PACF 그래프\nfig, ax = plt.subplots(2, 1, figsize=(12,6))\nsm.graphics.tsa.plot_acf(residuals, lags=30, ax=ax[0])\nsm.graphics.tsa.plot_pacf(residuals, lags=30, ax=ax[1])\nplt.tight_layout()\nplt.show()\n\n# 3) Ljung-Box 검정 (잔차 독립성)\nlb_test = acorr_ljungbox(residuals, lags=[12,24], return_df=True)\nprint(\"Ljung-Box test:\\n\", lb_test)\n\n# 4) ARCH 검정 (이분산성 확인)\narch_test = het_arch(residuals)\nprint(\"\\nARCH Test: LM stat=%.4f, p=%.4f\" % (arch_test[0], arch_test[1]))\n\n# 5) 정규성 검정 (Jarque-Bera/Anderson-Darling)\nnorm_test = normal_ad(residuals)\nprint(\"\\nNormality test (Anderson-Darling): stat=%.4f, p=%.4f\" % (norm_test[0], norm_test[1]))\n모형 계수 유의성\n회귀계수 유의성 매우 높음\n\n\n\n\n\n추정식: \\(Y_{t} = - 0.380Y_{t - 1} - 0.553\\varepsilon_{t - 12} + \\varepsilon_{t},\\sigma^{2} = 0.0014)\\)\n\\[(1 + 0.380L)(1 - L)(1 - L^{12})X_{t} = (1 - 0.553L^{12})\\varepsilon_{t}\\]\n잔차 진단 결과\n1. ACF/PACF\n\n대부분의 시차에서 자기상관이 95% 신뢰구간 내에 있음.\n다만 lag=12 근처에서 뚜렷한 음(-) 피크가 나타남 → 계절적 효과가 다 설명되지 않았을 가능성.\n\n2. Ljung-Box test (Q-stat)\n\n12시차: p=0.0001, 24시차: p=0.022 → 잔차가 백색잡음 가정을 위배 (독립성 결여).\n즉, 모형이 데이터의 자기상관을 완전히 설명하지 못함.\n\n3. ARCH test\n- p=1 → 조건부 이분산성(ARCH 효과) 없음.\n4. 정규성 검정 (Anderson-Darling)\n\np=0.0000 → 잔차가 정규성을 따르지 않음.\n분포가 비정규적(꼬리 두꺼움, 왜도 존재)임을 시사.\n\n추정된 SARIMA(1,1,0)×(0,1,1,12) 모형은 데이터의 전반적인 추세와 계절성을 잘 설명하였으나, 잔차에 계절적 자기상관(특히 lag=12)이 남아 있음 → 추가 개선 필요.\n잔차는 이분산성 문제는 없지만, 정규성 및 독립성 가정을 위배 → 예측 신뢰구간 해석에 주의 필요.\n5. 개선 방향 제안\n\n계절 차수를 더 늘려서 SARIMA(1,1,0)×(0,1,2,12) 또는 SARIMA(1,1,1)×(0,1,1,12) 시도. ➟ 시도 했으나 모형 적합도 낮고 여전히 잔차 진단 결과 문제점 해결되지 않음\n혹은 외생 변수(X) 포함한 SARIMAX 또는 구조적 시계열 모형(Trend+Seasonal decomposition) 검토.\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport statsmodels.api as sm\n\n# 1) SARIMA 적합: (p,d,q)×(P,D,Q,12)\nmodel = sm.tsa.statespace.SARIMAX(\n    df[\"log_passengers\"],\n    order=(1,1,0),\n    seasonal_order=(0,1,1,12),\n    enforce_stationarity=False,\n    enforce_invertibility=False,\n)\nres = model.fit()\n\n# 2) 12개월 예측 (로그 스케일의 평균과 신뢰구간)\nforecast_res = res.get_forecast(steps=12)\nforecast_mean = forecast_res.predicted_mean         # Series (index가 미래 월)\nforecast_ci   = forecast_res.conf_int()             # DataFrame\n\n# 잘못된 변수명 수정\nprint(\"12개월 예측(로그 스케일):\")\nprint(forecast_mean)\n\n# 3) SSE 계산 (원단위 복원 + NaN 정리)\ny_true = df[\"passengers\"]\ny_pred_in_sample = np.exp(res.fittedvalues)         # 원단위\n# 관측치와 예측치의 공통 구간만 사용\ny_true_aligned  = y_true.loc[y_pred_in_sample.index].dropna()\ny_pred_aligned  = y_pred_in_sample.loc[y_true_aligned.index]\nSSE = float(((y_true_aligned - y_pred_aligned) ** 2).sum())\n\n# 4) 그래프 시각화\nplt.figure(figsize=(12, 6))\nplt.plot(df.index, y_true, label=\"Observed\")\nplt.plot(y_pred_aligned.index, y_pred_aligned, label=\"Fitted\")\n\n# 예측(원단위, 로그→exp 복원)\nfc_idx = forecast_mean.index\nplt.plot(fc_idx, np.exp(forecast_mean), label=\"Forecast\", color=\"red\")\nplt.fill_between(\n    fc_idx,\n    np.exp(forecast_ci.iloc[:, 0]),\n    np.exp(forecast_ci.iloc[:, 1]),\n    color=\"pink\", alpha=0.3\n)\n\nplt.legend()\nplt.title(f\"SARIMA(1,1,0)×(0,1,1,12) Forecast  |  SSE={SSE:,.2f}\")\nplt.xlabel(\"Date\"); plt.ylabel(\"Passengers\")\nplt.tight_layout(); plt.show()\n\n\n\n\n\nres.fittedvalues는 칼만필터의 diffuse 초기화(불확실한 초기 상태) 때문에 처음 몇 달 값이 비정상적으로 튀기도 합니다. 그래서 1949~1950 초반에 오렌지선이 바늘처럼 치솟죠. 데이터/모형 문제라기보다 시작값 처리 방식 때문입니다.\n\n초기 1차+계절차분 만큼의 웜업 구간을 버리고(보통 13~24개월) 그 이후부터 적합치를 사용\nres.fittedvalues 대신 res.get_prediction(start=...)로 안정화된 구간만 가져오기\nSSE도 같은 구간에서 계산\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport statsmodels.api as sm\n\n# 데이터 로드 & 빈도 지정\ndf = sns.load_dataset(\"flights\")\ndf[\"date\"] = pd.to_datetime(df[\"year\"].astype(str) + \"-\" + df[\"month\"].astype(str))\ndf = df.set_index(\"date\").sort_index().asfreq(\"MS\")\ndf[\"log_passengers\"] = np.log(df[\"passengers\"])\n\n# 1) SARIMA 적합\nmodel = sm.tsa.statespace.SARIMAX(\n    df[\"log_passengers\"],\n    order=(1,1,0),\n    seasonal_order=(0,1,1,12),\n    enforce_stationarity=False,\n    enforce_invertibility=False,\n)\nres = model.fit(disp=False)\n\n# 추정 결과 요약 출력\nprint(res.summary())\n\n# 2) 안정화 이후 fitted values (초기 스파이크 방지)\nwarmup = 24\npred_in = res.get_prediction(start=df.index[warmup], end=df.index[-1])\nmu  = pred_in.predicted_mean\nvar = getattr(pred_in, \"var_pred_mean\", pred_in.se_mean**2)\nfitted_stable = np.exp(mu + 0.5*var)\ny_true = df[\"passengers\"].loc[fitted_stable.index]\n\n# SSE 계산\nSSE = float(((y_true - fitted_stable) ** 2).sum())\nprint(\"\\n[모형 적합도]\")\nprint(f\"SSE = {SSE:,.2f}\")\n\n# 3) 12개월 예측\nfc = res.get_forecast(steps=12)\nmu_f  = fc.predicted_mean\nvar_f = getattr(fc, \"var_pred_mean\", fc.se_mean**2)\nfc_mean = np.exp(mu_f + 0.5*var_f)\nfc_ci   = np.exp(fc.conf_int())\n\nprint(\"\\n[12개월 예측치]\")\nprint(fc_mean)\n\n# 4) 시각화\nplt.figure(figsize=(12,6))\nplt.plot(df.index, df[\"passengers\"], label=\"Observed\", color=\"C0\")\nplt.plot(fitted_stable.index, fitted_stable, label=f\"Fitted (after {warmup}m warmup)\", color=\"C1\")\nplt.plot(fc_mean.index, fc_mean, label=\"Forecast (12m)\", color=\"C3\")\nplt.fill_between(fc_mean.index, fc_ci.iloc[:,0], fc_ci.iloc[:,1], color=\"C3\", alpha=0.25)\nplt.title(f\"SARIMA(1,1,0)×(0,1,1,12) | SSE={SSE:,.2f}\")\nplt.xlabel(\"Date\"); plt.ylabel(\"Passengers\"); plt.legend(); plt.tight_layout(); plt.show()"
  },
  {
    "objectID": "notes/intro_stat/goodness_of_fits.html",
    "href": "notes/intro_stat/goodness_of_fits.html",
    "title": "기초통계 5. 분포 적합성검정",
    "section": "",
    "text": "chapter 1. 적합성 검정\n\n1. 개념\n\n(1) 필요이유\n분포 적합성 검정은 관측된 자료가 특정 이론적 분포를 따른다는 가설의 타당성을 평가함으로써, 통계적 모형의 전제가 충족되는지를 확인하고 추론 결과의 신뢰성을 확보하기 위해 수행된다.\n통계 모형의 전제 검토\n많은 통계 분석 기법(회귀분석, 분산분석, 최대우도법 등)은 특정한 확률 분포(정규분포, 포아송분포 등)를 전제로 한다. 이러한 가정이 충족되지 않으면 분석 결과는 왜곡되거나 신뢰할 수 없는 결론을 초래할 수 있다. 따라서 분포 적합성 검정은 분석 전, 자료가 전제된 분포를 따르는지 확인하는 데 핵심적인 절차이다.\n이론 모형의 타당성 평가\n통계적 실험이나 조사에서 수립한 이론적 분포 모델(예: 유전 비율, 고객 유입 패턴 등)이 실제 데이터에 적합한지 검정하는 것은 모형 자체의 타당성을 판단하는 데 필수적이다. 예를 들어, 멘델의 유전 법칙에 따라 3:1 비율이 예측된다면, 실제 결과가 이와 유의하게 다른지를 검정해야 한다.\n모수 추정의 신뢰도 확보\n분포가 적절하게 지정되지 않은 상태에서 수행된 추정은 불편하거나 비효율적인 추정량을 유도할 수 있다. 분포 적합성 검정은 분석자가 설정한 분포 가정이 얼마나 정당한지를 사전 확인함으로써, 추정과 추론의 타당성을 보장해준다.\n모의실험, 품질관리 등 응용 분야에서의 판단 기준\n산업 통계에서는 생산품의 불량률이 이항분포를 따른다고 가정하고 관리도를 구성함 → 이 분포 적합성이 무너지면 관리 기준 자체가 무의미하다. 서비스센터의 전화 수신량이 포아송 분포를 따른다는 가정을 기반으로 인력 배치 → 적합성 검정이 필요하다.\n\n\n(2) 적합성 검정 개념\n분포 적합성 검정은 관측된 데이터가 특정한 이론적 분포(모형 분포)에 잘 부합하는지를 검정하는 통계적 방법이다. 즉, 어떤 모집단에서 추출된 표본이, 가정한 분포—예를 들어 이항분포, 포아송분포, 정규분포 등—로부터 생성되었을 가능성을 평가한다. 이 검정은 다음과 같은 상황에 적용된다.\n\n주어진 자료가 특정 분포에 따르는지 검정하고자 할 때\n실험 결과가 이론적으로 기대되는 확률 분포와 일치하는지 검정할 때\n\n모집단 확률밀도함수 \\(f(x;\\theta)\\)로부터 얻은 확률표본 데이터의 확률밀도함수는 모집단 확률밀도함수와 동일하므로 확률표본데이터로부터 확률밀도함수를 얻고 이를 이용하여 모집단 확률밀도함수에 대한 가설을 검정할 수 있다.\n귀무가설이 “설정한 이론 분포를 따른다”이므로 귀무가설이 기각되면 데이터가 어떤 분포를 따르는지 알 수 없다. 하여, 적합성 검정은 귀무가설이 채택되는 것이 추론의 관심이다.\n\n\n\n\n\n\n\n(3) \\(\\chi^{2}\\) 적합성 검정\n확률표본 데이터로부터 히스토그램을 구한다. 이산형 데이터는 가질 수 있는 개별 값(포아송분포와 같이 이론적으로 무한 값을 갖는 경우는 마지막 구간의 빈도가 5 이상이 되도록 구간 수를 결정한다.)이 구간이고 연속형은 데이터의 범위(=최대값-최소값)를 10~12개 구간 개수가 되도록 동일 폭으로 나눈 구간으로 정의된다.\n\n\n\n\n\n확률표본 데이터로부터 빈도표를(이를 관측 빈도) 만들고 (histogram과 동일) 모집단이 따를 것 같은 이론적 분포로부터(예:정규분포) 빈도표(이를 기대 빈도)를 만들어 비교하면 빈도의 차이가 거의 없으면 모집단은 그 분포를 따른다고 하자 그렇지 않으면 기각한다. 표본 분포가 설정한 모집단 분포와 동일하다면 관측빈도와 기대빈도는 동일할 것이다.\n통계적 가설\n\n귀무가설 : 확률표본 데이터는 가정된 이론분포를 따른다.\n대립가설 : 확률표본 데이터는 가정 분포를 따르지 않는다. 그러나 어떤 분포를 따르는지는 알 수 없다.\n\n검정통계량\n\\(\\chi^{2} = \\overset{k}{\\sum_{i = 1}}\\frac{(O_{i} - E_{i})^{2}}{E_{i}} \\sim \\chi^{2}(df = k - 1 - m)\\), 여기서 \\(O_{i}\\)는 범주 \\(i\\)에서의 관측도수, \\(E_{i}\\)는 해당 범주에서의 이론적 분포로부터 계산되는 기대도수이며 \\(k\\)는 범주의 수이다. 그리고 \\(m\\)은 모수 추정에 사용된 자유도 수이다.\n\n\n\n2. 이산형 확률모형\n주사위는 공평한가? (균일분포)\n보유한 주사위가 공정한지 fair 알아보기 위하여 1,000번을 던저 나온 결과를 정리한 것이다. 주사위가 공정한지 유의수준 5%에서 검정하시오.\n\n\n\n\n\n\n\n\n\n\n\n\n눈금\n1\n2\n3\n4\n5\n6\n\n\n빈도 \\(O_{i}\\)\n150\n160\n165\n155\n170\n200\n\n\n\n\n귀무가설 : 주사위는 공정하다. 각 눈금이 나올 확률은 \\(\\frac{1}{6}\\)이다.\n대립가설 : 주사위는 공정하지 않다.\n기대빈도 \\(E_{i}\\) : 귀무가설이 옳다는 가정 하에 계산되는 빈도이다. 그러므로 1~6 각 눈금의 기대빈도는 1000/6-166.7으로 동일하다.\n검정통계량 \\(ts = \\sum_{i}\\frac{(O_{i} - E_{i})^{2}}{E_{i}} \\sim \\chi^{2}(df = k - c - 1)\\), \\(k =\\)셀의 크기, \\(c =\\)추정된 모수의 개수,\\(TS = 9,49 \\sim \\chi^{2}(5)\\)\n유의확률 : 0.091이므로 귀무가설이 채택되어 주사위는 공평하다고 할 수 있다.\n\nfrom scipy.stats import chisquare\n\n# 관측 빈도\nobserved = [150, 160, 165, 155, 170, 200]\n\n# 총 시행 횟수\nn = sum(observed)\n\n# 기대 확률: 공정한 주사위 → 각 면의 확률은 1/6\nexpected = [n / 6] * 6  # [166.67, 166.67, ..., 166.67]\n\n# 카이제곱 적합성 검정 수행\nchi2_stat, p_value = chisquare(f_obs=observed, f_exp=expected)\n\n# 출력\nprint(\"카이제곱 통계량:\", round(chi2_stat, 3))\nprint(\"p-value:\", round(p_value, 4))\nprint(expected)\n카이제곱 통계량: 9.5  p-value: 0.0907  [166.66666666666666, 166.66666666666666, 166.66666666666666, 166.66666666666666, 166.66666666666666, 166.66666666666666]\n성비는 동일한가? (이항분포)\n우리나라 출생 아이의 성비가 동일한지 알아보기 위하여 자녀가 3인인 1,000 가구의 남아 수를 조사한 자료이다. 이를 이용하여 성비가 동일한지 검정하시오.\n\n\n\n\n\n\n\n\n\n\n남자아이 수\n0\n1\n2\n3\n\n\n가구수\n100\n350\n400\n150\n\n\n\n\n귀무가설 : 성비는 동일하다.\n대립가설 : 성비는 동일하지 않다.\n\n기대빈도 : 성비가 동일하다면 남아 비율은 1/2이므로 남자아이 수(\\(X\\))는 이항분포(n=3, p=1/2)을 따르므로 \\(P(X = 0|X \\sim B(3,0.5))\\), \\(P(X = 1|X \\sim B(3,0.5))\\)… 기대확률에 \\(n = 1000\\)을 곱해 구하면 된다. [125, 375, 375, 125]\n검정통계량 : \\(TS = 13.33 \\sim \\chi^{2}(4 - 1 = 3)\\)\n유의확률 : 0.004로 유의수준 0.05보다 작으므로 귀무가설을 기각하여 남녀 아이 성비는 다르다. 남자아이 2명이 가구가 기대빈도 375 &lt; 관측빈도 400 많고 3명도 관측빈도가 많으므로 여자 아이보다 남자 아이가 더 많다. 우리나라는 남아을 선호한다.\nfrom scipy.stats import chisquare\n\n# 관측 도수: 남자아이 수에 따른 가구 수\nobserved = [100, 350, 400, 150]\n\n# 기대 도수(이론빈도): 이항분포(B(n=3, p=0.5)) × 총 가구 수 (1000)\nfrom scipy.stats import binom\n\nn = 3\np = 0.5\ntotal = 1000\n\n# 이항분포 기반 기대도수 계산\nexpected = [binom.pmf(k, n, p) * total for k in range(4)]\n\n# 카이제곱 적합성 검정 수행\nchi2_stat, p_value = chisquare(f_obs=observed, f_exp=expected)\n\n# 결과 출력\nprint(\"카이제곱 통계량:\", round(chi2_stat, 3))\nprint(\"p-value:\", round(p_value, 4))\nprint(expected)\n카이제곱 통계량: 13.333  p-value: 0.004  [np.float64(125.0), np.float64(375.0), np.float64(375.0), np.float64(125.0)]\n교통사고 건수는 포아송분포를 따르나?\nD지역 1일 교통사고 건수를 알아보기 위하여 300일 동안 매일 고통사고 건수를 조사한 자료이다.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n건수\n0\n1\n2\n3\n4\n5\n6\n7\n\n\n요일수\n20\n54\n74\n67\n45\n25\n11\n4\n\n\n\n\n귀무가설 : D 지역 1일 교통사건 건수는 포아송분포를 따른다.\n대립가설 : 포아송분포를 따르지 않는다.\n\n포아송 모수 추정 : 포아송 분포는 평균을 모수로 가지고 있으므로 (건수*요일) 합을 300일로 나누면 하루 평균 교통사고 건수 추정치이다. \\(\\widehat{\\lambda} = 2.673\\)\n기대빈도: \\(\\widehat{p(j)} = P(X = j|X \\sim P(\\lambda = 2.67))\\), 단 건수=7일 때는 4개로 기대빈도가 5미만이므로 6과 7을 합하여 셀의 개수는 7개이다. [20, 55, 74, 66, 44, 23, 14]\n검정통계량 : \\(TS = 0.186 \\sim \\chi^{2}(7 - 1 - 1 = 5)\\), 포아송 분포 모수를 1개 추정했으므로 자유도는 1 더 줄어든다.\n유의확률은 0.99이므로 귀무가설을 채택되어 교통사고 건수는 모수가 2.67(건수/1일)인 포아송분포 따른다.\nfrom scipy.stats import poisson, chisquare\nimport numpy as np\n\n# 관측도수\nobserved = np.array([20, 54, 74, 67, 45, 25, 11, 4])\ntotal_days = observed.sum()\n\n# 평균 λ 추정: 포아송 분포의 모수는 관측된 평균과 같다고 가정\nvalues = np.arange(len(observed))\nlambda_hat = np.sum(values * observed) / total_days\n\n# 기대도수 계산 (포아송분포 기반)\nexpected = poisson.pmf(values, mu=lambda_hat) * total_days\n\n# 기대도수가 너무 작은 셀은 마지막에 병합 (R 규칙: 기대도수 &lt; 5 합치기)\n# 여기서는 6건, 7건 이상을 병합\nobserved_adj = np.append(observed[:-2], observed[-2:].sum())\nexpected_adj = np.append(expected[:-2], expected[-2:].sum())\n\n# Ensure that the sums of observed and expected frequencies match\nexpected_adj = expected_adj * observed_adj.sum() / expected_adj.sum()  # Adjust expected_adj\n\n# 카이제곱 검정\nchi2_stat, p_value = chisquare(f_obs=observed_adj, f_exp=expected_adj)\n\n# 결과 출력\nprint(\"추정된 λ (평균 사고 건수):\", round(lambda_hat, 4))\nprint(\"카이제곱 통계량:\", round(chi2_stat, 4))\nprint(\"p-value:\", round(p_value, 4))\n추정된 λ (평균 사고 건수): 2.6733  카이제곱 통계량: 0.186  p-value: 0.9999  [20.83690818 55.7040012 74.4576816 66.35006738 44.3439617 23.70923819 14.59814175]\n\n\n3. 연속형 확률모형\n개념\n연속형 확률모형 적합성 검정은 관측된 연속형 데이터가 특정 이론적 확률분포(예: 정규분포, 지수분포, 감마분포 등)를 따르는지 여부를 검정하는 통계적 절차이다. 이는 통계적 추정이나 추론, 예측 모형의 기반이 되는 분포 가정의 타당성을 확인하기 위한 전 단계로 매우 중요하다.\n귀무가설과 대립가설\n귀무가설: 자료는 지정된 연속형 분포를 따른다.\n대립가설: 자료는 해당 분포를 따르지 않는다.\n\n\n\n\n\n\n\n\n검정 방법\n설명\n적용 분포\n\n\nKolmogorov–Smirnov (K–S) 검정\n표본 누적분포함수(ECDF)와 이론 누적분포 간의 최대 거리(D)를 검정\n일반적 (정규, 지수 등)\n\n\nAnderson–Darling 검정\nK–S보다 분포 꼬리 부분을 더 민감하게 반영\n특히 정규성 검정\n\n\nShapiro–Wilk 검정\n표본 분산의 구조를 통해 정규성을 직접 검정\n정규성에 특화\n\n\nLilliefors 검정\nK–S 검정의 변형으로, 모수(평균, 분산)가 추정값일 때 사용\n정규/지수분포\n\n\n\n시각화 도구\n연속형 확률분포의 적합성 검정에서는 수치 기반의 정량적 검정뿐 아니라, 분포 형태를 직관적으로 이해할 수 있는 시각화 기법도 중요하게 활용된다. 대표적인 도구로 확률–확률(P–P) 그래프와 분위–분위(Q–Q) 그래프가 있다. 두 방법 모두 표본 데이터가 특정 이론 분포를 얼마나 잘 따르는지를 시각적으로 비교한다는 공통점을 가지지만, 비교에 사용하는 척도에서 차이가 있다.\nP–P 그래프는 누적확률(cumulative probability)을 기준으로, 표본 데이터의 누적분포함수(CDF) 값과 이론 분포의 CDF 값을 비교한다.\nQ–Q 그래프는 분위(quantile)를 기준으로, 표본 데이터의 분위 값과 이론 분포의 분위 값을 비교한다.\n일반적으로 P–P 그래프는 분포 전 구간에서의 적합도를 균형 있게 보여주는 데 유리하며, Q–Q 그래프는 특히 꼬리 부분의 적합성 차이를 확인하는 데 효과적이다. 따라서 분석 목적과 관심 구간에 따라 두 그래프를 적절히 선택하거나 병행하여 사용하는 것이 바람직하다.\nProbability-Probability plot\nP–P plot은 관측값의 누적확률(경험누적분포)과 이론 누적분포함수(주로 정규분포를 사용함)의 값을 직접 비교하여 각 관측값에 대해 다음의 점을 그린다.\n\\((x_{i}:{\\widehat{F}}_{n}(x_{i}) = \\frac{i}{n + 1},F(x_{i}))\\), 여기서 \\({\\widehat{F}}_{n}(x_{i})\\)는 표본 누적분포,\\(F(x_{i})\\)는 이론 누적분포이다. 45도 대각선을 기준으로 점들이 근접해 있으면 분포 적합성 양호하다고 판단한다.\n\n\n\n\n\nQ–Q Plot (Quantile–Quantile Plot)\nQ–Q plot은 표본 분위수와 이론 분위수를 직접 비교하는 도구이다. 각 \\(i\\)-번째 관측값에 대해 다음을 비교한다.\n\\((\\text{이론 분위수},\\text{표본 분위수}) = (F^{- 1}(p_{i} = \\frac{i - 0.5}{n}),x_{(i)})\\), 여기서\\(F^{- 1}(p_{i})\\)는 이론 분포(정규분포를 주로 사용함)의 \\(p_{i}\\) 분위수,\\(x_{(i)}\\)는 표본의 \\(i\\)-번째 순서통계량이다.\n\n\n\n\n\n#P-P plot\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport scipy.stats as stats\nimport seaborn as sns\n\n# 타이타닉 데이터 불러오기\ntitanic = sns.load_dataset(\"titanic\")\n\n# age 변수 추출 및 결측치 제거\nage_data = titanic[\"age\"].dropna()\nage_sorted = np.sort(age_data)\nn = len(age_sorted)\n\n# 경험 누적확률 (empirical CDF)\nempirical_cdf = np.arange(1, n + 1) / (n + 1)\n\n# 이론 누적확률 (정규분포 기준 CDF)\nmean_age = np.mean(age_sorted)\nstd_age = np.std(age_sorted, ddof=1)\ntheoretical_cdf = stats.norm.cdf(age_sorted, loc=mean_age, scale=std_age)\n\n# P–P Plot 그리기\nplt.figure(figsize=(6, 5))\nplt.plot(empirical_cdf, theoretical_cdf, 'o', label='Data Points')\nplt.plot([0, 1], [0, 1], 'r--', label='y = x')\nplt.xlabel('Empirical CDF')\nplt.ylabel('Theoretical CDF (Normal)')\nplt.title('P–P Plot of Titanic Age (vs Normal)')\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n#Q-Q plot\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport scipy.stats as stats\nimport seaborn as sns\n\n# 타이타닉 데이터 불러오기 및 전처리\ntitanic = sns.load_dataset(\"titanic\")\nage_data = titanic[\"age\"].dropna()\n\n# Q–Q plot (정규분포와 비교)\nplt.figure(figsize=(6, 5))\nstats.probplot(age_data, dist=\"norm\", plot=plt)\nplt.title(\"Q–Q Plot of Titanic Age (vs Normal)\")\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\nP-P plot 결과는 Titanic 승객의 나이(age) 변수가 정규분포보다 오른쪽으로 치우쳐져 있으며, 이는 양의 왜도 특성을 지닌 우로 치우진 분포임을 시사한다.\n\n(3) 적합성 검정방법\nShapiro Wilk W-통계량(정규성 검정)\nShapiro-Wilk 검정(Shapiro & Wilk, 1965)은 주어진 데이터가 정규분포를 따르는지를 검토하기 위한 방법으로, 특히 소표본(\\(n \\leq 50\\))에서 강력한 검정력을 가지며, 현재는 1000개 이하의 표본에도 널리 사용된다.\n통계량: \\(W = \\frac{\\left( \\sum_{i = 1}^{n}a_{i}x_{(i)} \\right)^{2}}{\\sum_{i = 1}^{n}(x_{i} - \\overline{x})^{2}}\\), 여기서 \\(x_{(i)}\\)는 순서통계량, \\(\\overline{x}\\)는 표본 평균, \\(a_{i}\\)는 정규분포에서의 기대 순위값을 기반으로 계산된 계수이다. \\(W\\)값은 일반적으로 0과 1 사이의 값을 가지며, 값이 1에 가까울수록 데이터가 정규분포에 가깝다는 것을 의미한다.\n\n귀무가설: 데이터는 정규분포를 따른다.\n대립가설: 데이터는 정규분포를 따르지 않는다.\n\nimport seaborn as sns\nfrom scipy.stats import shapiro\nimport matplotlib.pyplot as plt\n# 데이터 불러오기\ntitanic = sns.load_dataset(\"titanic\")\n# age 변수에서 결측치 제거\nage_data = titanic['age'].dropna()\n# Shapiro-Wilk 검정 수행\nw_stat, p_value = shapiro(age_data)\nprint(f\"W-통계량 = {w_stat:.4f}, p-value = {p_value:.4f}\")\nW-통계량 = 0.9815, p-value = 0.0000\nShapiro-Wilk 검정 결과와 시각화를 종합하면, Titanic 데이터의 age 변수는 정규성을 충족하지 않는 것으로 판단된다.\nKolmogorov-Smirnov 검정과 D-통계량\nKolmogorov-Smirnov(K-S) 검정은 관측된 누적분포함수가 주어진 이론적 누적분포함수와 일치하는지를 검정하는 방법이다. 이는 연속형 분포에 대한 적합도 검정으로, 데이터의 분포가 특정 분포(예: 정규분포, 지수분포 등)를 따르는지 평가하는 데 사용된다.\n통계량: \\(D = \\sup_{x}\\left| F_{n}(x) - F(x) \\right|\\), 여기서, \\(F_{n}(x)\\)는 표본 누적분포함수,\\(F(x)\\)는 비교 대상이 되는 이론 누적분포함수,\\(\\sup_{x}\\)는 모든 \\(x\\)에 대해 최대 절댓값을 취하는 것이다. D-통계량은 두 누적분포함수 간의 최대 거리를 의미하며, 분포 간의 차이가 클수록 D 값이 커진다.\nfrom scipy.stats import kstest, norm\nimport seaborn as sns\n# Titanic 데이터셋에서 age 변수\ntitanic = sns.load_dataset(\"titanic\")\nage_data = titanic['age'].dropna()\n# 평균과 표준편차로 정규분포 생성\nmean = age_data.mean()\nstd = age_data.std()\n# Kolmogorov-Smirnov 검정 수행\nd_stat, p_value = kstest(age_data, 'norm', args=(mean, std))\nprint(f\"D-통계량 = {d_stat:.4f}, p-value = {p_value:.4f}\")\nD-통계량 = 0.0646, p-value = 0.0050→정규분포를 따르지 않는다.\n\n\n\n\n\n\n\n\n분포 종류\n설명\nPython 함수 예시 (scipy.stats)\n\n\n정규분포\n평균과 표준편차로 정의되는 연속 분포\nnorm', args=(μ, σ)\n\n\n지수분포\n사건 간의 시간 간격 모델링\nexpon', args=(loc, scale)\n\n\n균등분포\n모든 구간에서 같은 확률\nuniform', args=(a, b-a)\n\n\n감마분포\n대기 시간이나 생존 분석에서 자주 사용\ngamma', args=(shape, loc, scale)\n\n\n베타분포\n0~1 사이 확률 변수 모델링에 적합\nbeta', args=(α, β)\n\n\n로지스틱분포\nS자형 누적분포, 로지스틱 회귀와 관련\nlogistic', args=(loc, scale)\n\n\n카이제곱분포\n분산 분석, 독립성 검정 등에서 사용\nchi2', args=(df,)\n\n\n레이리분포\n신호 처리, 물리학에서 사용\nrayleigh', args=(loc, scale)\n\n\n정규 로그분포\n지수적으로 증가하는 변수에 적합\nlognorm', args=(σ, loc, scale)\n\n\n\nTitanic 데이터셋의 age 변수는 정규성을 만족하지 않으며, 히스토그램과 정규성 검정 결과를 종합할 때 우측으로 치우친 형태의 비대칭 연속형 분포로 해석할 수 있다. Kolmogorov-Smirnov 적합도 검정 결과, 이 변수는 로그정규분포나 감마분포에 가장 가까운 분포 특성을 보이며, 분석 시 이러한 분포적 특성을 고려한 접근이 필요하다.\nfrom scipy.stats import norm, expon, lognorm, gamma, kstest\n\n# 데이터\ndata = age_data\n# 정규분포\nnorm_params = norm.fit(data)\nks_norm = kstest(data, 'norm', args=norm_params)\n# 지수분포\nexpon_params = expon.fit(data)\nks_expon = kstest(data, 'expon', args=expon_params)\n# 로그정규분포\nlognorm_params = lognorm.fit(data)\nks_lognorm = kstest(data, 'lognorm', args=lognorm_params)\n# 감마분포\ngamma_params = gamma.fit(data)\nks_gamma = kstest(data, 'gamma', args=gamma_params)\n\n# 결과 출력\nprint(f\"[정규분포]     D = {ks_norm.statistic:.4f}, p = {ks_norm.pvalue:.4f}\")\nprint(f\"[지수분포]     D = {ks_expon.statistic:.4f}, p = {ks_expon.pvalue:.4f}\")\nprint(f\"[로그정규분포] D = {ks_lognorm.statistic:.4f}, p = {ks_lognorm.pvalue:.4f}\")\nprint(f\"[감마분포]     D = {ks_gamma.statistic:.4f}, p = {ks_gamma.pvalue:.4f}\")\n[정규분포] D = 0.0646, p = 0.0050  [지수분포] D = 0.2964, p = 0.0000  [로그정규분포] D = 0.0565, p = 0.0202  [감마분포] D = 0.0575, p = 0.0172  모두 유의확률이 0.05보다 작아 최적의 이론분포는 발견하지 못했지만 로그정규분포가 가장 근접한 분포이다.\nAnderson–Darling 검정과 AD 통계량\nAnderson–Darling 검정은 표본 데이터가 특정 이론분포(주로 정규분포)를 따르는지 검정하는 정규성 검정 기법 중 하나로, Kolmogorov–Smirnov 검정(K–S 검정)의 일반화된 형태이다. 이 검정은 누적분포함수의 전체 영역에서의 차이뿐만 아니라 꼬리 부분에서의 차이도 민감하게 반영하도록 설계되어 있다.\n즉, 정규분포에서의 꼬리부 적합성을 보다 엄격하게 평가할 수 있는 검정으로, 특히 K–S 검정보다 정규성 검정에서 더 강력한 검정력을 가지는 것으로 알려져 있다.\n\\[A^{2} = - n - \\frac{1}{n}\\overset{n}{\\sum_{i = 1}}\\left\\lbrack (2i - 1) \\cdot \\left( \\ln F(x_{(i)}) + \\ln\\left( 1 - F(x_{(n + 1 - i)}) \\right) \\right) \\right\\rbrack\\]\n검정통계량 \\(A^{2}\\) 값이 작을수록 이론 분포와 적합하며, 사전에 정의된 임계값과 비교하여 가설을 기각할지 여부를 결정한다.\nfrom scipy.stats import anderson\nimport seaborn as sns\n# Titanic 데이터셋에서 age 변수\ntitanic = sns.load_dataset(\"titanic\")\nage_data = titanic['age'].dropna()\n# Anderson-Darling 검정 (정규성 기준)\nresult = anderson(age_data, dist='norm')\n\nprint(f\"AD 통계량 = {result.statistic:.4f}\")\nfor cv, sig in zip(result.critical_values, result.significance_level):\n    print(f\"유의수준 {sig}% →임계값: {cv:.4f}\")\nAD 통계량 = 3.8230  유의수준 15.0% →임계값: 0.5730  유의수준 10.0% →임계값: 0.6520  유의수준 5.0% →임계값: 0.7830  유의수준 2.5% →임계값: 0.9130  유의수준 1.0% →임계값: 1.0860 → 유의수준 1% 임계값보다 통계량이 3.823으로 크므로 정규성은 기각된다.\nKolmogorov–Smirnov 검정은 다양한 연속 분포에 적용할 수 있는 범용 적합도 검정으로, 간편하게 분포 적합성을 판단할 수 있다는 장점이 있다. 반면, Anderson–Darling 검정은 특히 정규분포에 대한 검정력에서 더 강력하며, 꼬리 부분의 차이까지 민감하게 반영할 수 있어 정규성 검정 등에서 더 신뢰할 수 있는 판단 근거를 제공한다.\n\n\n\n\n\n\n\n\n션 이름\n설명\n사용 예\n\n\nnorm'\n정규분포 (default)\nanderson(data, 'norm')\n\n\nexpon'\n지수분포\nanderson(data, 'expon')\n\n\nlogistic'\n로지스틱분포\nanderson(data, 'logistic')\n\n\ngumbel'\n극값분포 I형 (Gumbel, 극단값 이론)\nanderson(data, 'gumbel')\n\n\ngumbel_l', 'gumbel_r'\n왼쪽/오른쪽 Gumbel 분포 (SciPy에 따라)\nanderson(data, 'gumbel_r')\n\n\n\nLilliefors 검정(정규성 검정)\nK–S 검정은 비교 대상 분포(예: 정규분포)의 모수, 즉 평균과 표준편차가 사전에 주어져 있어야 한다는 제약이 있다. 실제 분석에서는 대부분 모수를 알 수 없고 표본으로부터 추정하기 때문에, 이러한 상황을 반영한 검정 방법이 필요하다. 이러한 배경에서 Lilliefors 검정이 제안되었다.\nLilliefors 검정은 Kolmogorov–Smirnov 검정의 일종이지만, 이론 정규분포의 평균과 표준편차를 표본 데이터로부터 추정한 경우에 적용할 수 있도록 수정된 형태이다. 통계량은 Kolmogorov–Smirnov 검정과 동일하게 계산되며 다음과 같다.\n\\[D = \\sup_{x}\\left| F_{n}(x) - F(x;\\widehat{\\mu},\\widehat{\\sigma}) \\right|\\]\nfrom statsmodels.stats.diagnostic import lilliefors\nimport seaborn as sns\n\n# Titanic 데이터셋 중 age 변수\ntitanic = sns.load_dataset(\"titanic\")\nage_data = titanic['age'].dropna()\n\n# Lilliefors 검정 (정규성 검정)\nstat, p_value = lilliefors(age_data)\n\nprint(f\"Lilliefors 통계량 D = {stat:.4f}, p-value = {p_value:.4f}\")\nLilliefors 통계량 D = 0.0646, p-value = 0.0010\nLilliefors 검정은 Kolmogorov–Smirnov 검정을 기반으로 하면서, 정규분포의 모수(평균, 표준편차)를 표본에서 추정한 상황에 맞게 수정된 정규성 검정 방법이다. 정규분포에 대한 사전 모수 정보 없이도 검정이 가능하며, 실제 데이터 분석에서 널리 활용된다. p-value를 기반으로 귀무가설을 기각할 수 있으며, Shapiro–Wilk 검정보다 덜 민감하지만 모수 추정 상황에서는 더 현실적인 방법이다.\nJarque–Bera 검정 (JB 검정)\n많은 통계적 추론과 회귀분석에서는 잔차항이 정규분포를 따른다는 가정이 필수적이다. 이 가정이 충족되지 않으면 회귀계수의 유의성 검정, 신뢰구간 추정 등에서 오류가 발생할 수 있다. Jarque–Bera 검정은 이러한 정규성 가정을 수학적으로 검토하는 방법 중 하나로, 왜도와 첨도에 근거하여 정규성을 판단하는 모멘트 기반 검정이다.\n\\(\\text{JB} = \\frac{n}{6}\\left( S^{2} + \\frac{(K - 3)^{2}}{4} \\right)\\), 여기서 \\(S\\)는 표본 왜도, \\(K\\)는 표본 첨도이다. 정규분포일 경우 왜도는 0, 첨도는 3이 되어 JB 통계량이 0에 가까워진다. JB 통계량은 자유도 2인 카이제곱 분포를 따른다.\nfrom scipy.stats import jarque_bera\nimport seaborn as sns\n\n# Titanic 데이터셋에서 age 변수 사용\ntitanic = sns.load_dataset(\"titanic\")\nage_data = titanic['age'].dropna()\n\n# Jarque–Bera 검정\nstat, p_value = jarque_bera(age_data)\n\nprint(f\"JB 통계량 = {stat:.4f}, p-value = {p_value:.4f}\")\nJB 통계량 = 18.7876, p-value = 0.0001\n유의확률이 매우 작으므로, 귀무가설 기각 → 데이터는 정규분포를 따른다고 보기 어렵다.\n\n\n\n\nchapter 2. 함수 적합\n\n1. 함수적합 개념\n함수 적합(function fitting)은 주어진 데이터가 어떤 함수 형태를 따르는지를 분석하고, 그 함수를 가장 잘 설명할 수 있는 모수나 구조를 추정하는 과정을 말한다. 이는 통계학, 수학, 공학, 데이터 과학 전반에서 널리 사용되는 기본적인 분석 기법으로, 관측값과 이론적 모델 간의 관계를 규명하고 예측력을 확보하는 데 중요한 역할을 한다.\n이론적으로 함수 적합은 다음과 같은 배경에 기반한다.\n모형 설정: 분석 대상이 되는 현상에 대해, 데이터가 따를 것으로 가정되는 함수 형태(선형, 비선형, 다항식, 지수함수, 로그함수 등)를 설정한다. 이 과정은 이론적 배경, 선행연구, 혹은 경험적 관찰에 의해 결정된다.\n모수 추정: 설정한 함수의 형태를 기준으로, 주어진 데이터와 함수 간의 오차를 최소화하는 모수(parameter)를 추정한다. 가장 널리 쓰이는 방법은 최소제곱법(least squares)이며, 확률적 오차 구조를 고려하는 경우 최대우도추정법(maximum likelihood estimation) 등이 사용된다.\n적합도 평가: 추정된 함수가 데이터를 얼마나 잘 설명하는지를 평가한다. 결정계수(R²), 평균제곱오차(MSE), 평균절대오차(MAE)와 같은 지표가 사용되며, 확률분포 가정이 있는 경우에는 적합성 검정(예: 카이제곱 검정, Kolmogorov–Smirnov 검정)도 활용된다.\n통계적 가정과 한계: 함수 적합에는 오차의 독립성, 등분산성, 정규성 등의 가정이 포함되는 경우가 많다. 이러한 가정이 위배되면 모수 추정의 효율성과 타당성이 떨어질 수 있다. 따라서 적합 후 잔차 분석(residual analysis)을 통해 가정 위배 여부를 점검하는 것이 중요하다.\n활용과 확장: 적합된 함수는 예측(prediction), 보간(interpolation), 외삽(extrapolation), 변수 간 관계 해석 등에 활용된다. 최근에는 전통적 모수적 방법뿐 아니라, 스플라인(spline) 적합, 커널 회귀(kernel regression), 머신러닝 기반 비모수적 함수 추정 등으로 확장되고 있다.\n\n\n2. 함수적합 원리\n함수 적합의 수학적 원리는 관측된 데이터가 특정한 함수 형태를 따른다고 가정하고, 해당 함수의 모수를 추정하여 데이터와 함수 간의 차이를 최소화하는 데 있다. 이 과정은 주로 최적화(optimization) 문제로 표현된다.\n\n문제 설정\n\n관측된 데이터가 \\(\\{(x_{i},y_{i})|i = 1,2,\\ldots,n\\}\\) 와 같이 주어졌다고 하자. 우리는 어떤 함수 형태 \\(y = f(x;\\theta)\\)를 가정한다. 여기서 \\theta는 추정해야 하는 모수(parameter)들의 집합이다.\n\n오차 정의\n\n함수 적합에서는 모델이 예측한 값 \\({\\widehat{y}}_{i} = f(x_{i};\\theta)\\)와 실제 관측값 \\(y_{i}\\)의 차이를 오차(error) 또는 잔차(residual)라고 한다. \\(e_{i} = y_{i} - {\\widehat{y}}_{i}\\)\n\n목적 함수 설정\n\n가장 일반적인 접근법은 잔차의 제곱합을 최소화하는 최소제곱법(Least Squares Method)이다.\n\\[S(\\theta) = \\overset{n}{\\sum_{i = 1}}\\left( y_{i} - f(x_{i};\\theta) \\right)^{2}\\]\n최적의 모수 \\(\\widehat{\\theta}\\)는 다음을 만족한다. \\(\\widehat{\\theta} = \\arg\\min_{\\theta}S(\\theta)\\)\n\n모수 추정\n\n【선형 함수 적합】 만약 \\(f(x;\\theta)\\)가 \\(\\theta\\)에 대해 선형이라면, 해를 닫힌 형태(closed form)로 구할 수 있다. 예) 단순 선형회귀 \\(y = \\beta_{0} + \\beta_{1}x\\) 최적 해는 \\(\\widehat{\\beta} = (X^{\\top}X)^{- 1}X^{\\top}y\\)이다. 여기서 X는 설계행렬(design matrix)이다.\n【비선형 함수 적합】 \\(f(x;\\theta)\\)가 모수에 대해 비선형이면, 해석적 해를 구하기 어렵기 때문에 수치적 최적화 기법(예: Gauss–Newton, Levenberg–Marquardt 알고리즘)을 사용한다.\n\n확률론적 해석\n\n만약 오차항 \\(\\varepsilon_{i}\\)가 평균 0, 분산 \\(\\sigma^{2}\\)인 독립 동일분포(\\(iid\\)) 정규분포를 따른다고 가정하면, 최소제곱법은 최대우도추정(MLE)와 동일한 해를 준다. 즉, \\(y_{i} = f(x_{i};\\theta) + \\varepsilon_{i},\\varepsilon_{i} \\sim N(0,\\sigma^{2})\\)의 경우,\n\\(\\widehat{\\theta} = \\arg\\max_{\\theta}\\overset{n}{\\prod_{i = 1}}\\frac{1}{\\sqrt{2\\pi\\sigma^{2}}}\\exp\\left( - \\frac{(y_{i} - f(x_{i};\\theta))^{2}}{2\\sigma^{2}} \\right)\\)은 최소제곱해와 동일하다.\n\n적합도 평가\n\n모형이 데이터를 얼마나 잘 설명하는지 평가하기 위해 다음과 같은 지표를 사용한다.\n결정계수: \\(R^{2} = 1 - \\frac{SS_{\\text{res}}}{SS_{\\text{tot}}}\\), \\(SS_{\\text{res}} = \\overset{n}{\\sum_{i = 1}}(y_{i} - {\\widehat{y}}_{i})^{2}\\), 여기서 \\(SS_{\\text{tot}} = \\overset{n}{\\sum_{i = 1}}(y_{i} - \\overline{y})^{2}\\), \\({\\widehat{y}}_{i}\\)는 예측값, \\(y_{i}\\)는 관측값, \\(\\overline{y}\\)는 y의 평균이다.\n평균제곱오차(MSE): \\(\\text{MSE} = \\frac{1}{n}\\overset{n}{\\sum_{i = 1}}(y_{i} - {\\widehat{y}}_{i})^{2}\\)\n평균절대오차(MAE): \\(\\text{MAE} = \\frac{1}{n}\\overset{n}{\\sum_{i = 1}}|y_{i} - {\\widehat{y}}_{i}|\\)\n적합성 검정(카이제곱 검정, Kolmogorov–Smirnov 검정 등)\n\n\n3. 선형함수 linear finction 예제\n모형 & 데이터\n\\(Y_{i} = \\alpha + \\beta X_{i} + e_{i}\\), \\(i = 1\\)\n#함수 설정\ndef func(x, a, b):\n    return a*x + b\n\n#설명변수 X [0~10]  정수를 100개 동일 구간으로 나누어 저장\nimport numpy as np\nx = np.linspace(0, 10, 100)\n\n\n\n\n\ny=func(x, 1, 2)+0.9*np.random.normal(size=len(x))\n\\(y=1+2 \\times x\\) 함수값에 난수 생성(\\(0.9 \\times N(0,1)\\)) 값을 더한다.\n가장 적합한 함수 구하는 규칙 Least Sqaure Methods\n관측치 \\(y_i\\)와 적합치 \\(\\hat y_i = \\hat a + \\hat b \\times x\\)의 차이의 제곱합이 최소가 되는 모수 \\((\\hat a, \\hat b)\\) 구하는 방법이다.\n\\(min_{\\alpha,\\beta} \\sum_i^n (y_i-\\alpha-\\beta x_i)^2\\)\nfrom scipy.optimize import curve_fit\nbeta, est_cov = curve_fit(func,x,y)\nbeta에는 OLS 추정값가 저장되고 est_cov에는 추정분산이 출력된다. 대각행렬이 각 모수의 추정분산이 된다.\n \n회귀분석과 결과 비교\nfrom scipy import stats\nimport numpy as np\nslope, intercept, r_value, p_value, std_err=stats.linregress(x,y)\n\n\n\n\n\nimport statsmodels.api as sm\nmodel=sm.OLS(y,sm.add_constant(x))\nfit=model.fit()\nfit.summary()\n\n\n\n\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\nplt.scatter(x, y, marker='.')\nplt.plot(x, 2+1*x, linewidth=2)\nplt.plot(x, func(x,*popt), color='red', linewidth=2)\nplt.legend(['Original', 'Best Fit'], loc=2)\nplt.show()\n\n\n\n\n\n\n\n3. 비선형함수 non-linear finction 예제\n추정함수: \\(y_{i} = aexp(\\frac{- (x - b)^{2}}{2c^{2}})\\)\nfrom scipy.optimize import curve_fit\n\ndef func(x, a, b, c):\n    return a*np.exp(-(x-b)**2/(2*c**2))\nx = np.linspace(0, 10, 100)\ny = func(x, 1, 5, 2) # 답인 y들과\ny_gen = y + 0.2*np.random.normal(size=len(x)) # noise\nbeta,est_cov=curve_fit(func, x, y_gen)\nbeta\n\n\n\n\n\nplt.scatter(x, y_gen, marker='.')\nplt.plot(x, y, linewidth=2, color='blue')\nplt.plot(x, func(x, *beta), color='red', linewidth=2)\nplt.legend(['Original', 'Best Fit'], loc=2)\nplt.show()"
  },
  {
    "objectID": "notes/intro_stat/anova.html",
    "href": "notes/intro_stat/anova.html",
    "title": "기초통계 8. 분산분석",
    "section": "",
    "text": "chapter 1. 실험설계 기초\n\n1. 개념\n실험 설계는 기록된 반응을 바탕으로 치료나 집단 간 비교가 가능하도록 연구의 틀을 마련하는 과정이다. 연구는 자연 상태에서의 관찰처럼 환경을 방해하지 않고 수행될 수도 있고, 실험실처럼 요인을 인위적으로 통제하는 방식으로 이루어질 수도 있다.\n예를 들어, 학교 유형별 2학년 읽기 점수 차이는 자연 관찰 연구에 해당하지만, 온도와 습도가 진드기 수명 주기에 미치는 영향은 통제가 가능한 실험실에서 연구된다. 통제는 특정 요인의 효과를 명확히 파악하는 데 도움이 되지만, 지나치게 인위적이면 실제 자연 상태를 반영하는 정도가 떨어진다. 따라서 실험의 유용성을 위해서는 조건 통제와 현실성 사이의 균형이 필요하다.\n실험설계는 데이터를 어떻게 만들지에 관한 단계이고, 분산분석(ANOVA)은 만들어진 데이터를 어떻게 해석할지에 관한 단계다. 즉, 실험설계에서 처리(treatment) 구조, 반복(replication), 무작위화(randomization), 블록화(blocking) 같은 요소를 미리 결정하면, 그 결과 데이터는 분산분석 모형에 자연스럽게 들어갈 수 있는 형태로 구성된다. 예를 들어, 요인별 수준과 배치 방법이 명확히 설계되어 있으면, ANOVA에서는 요인별 제곱합(SS)을 나누어 변동의 원인을 추적할 수 있다.\n\n\n2. 실험설계 용어\n절대실험과 비교실험\n절대실험은 관심 있는 대상의 현재 상태나 특성을 파악하기 위해 요인을 조작하지 않고 관찰이나 조사를 통해 정보를 얻는 방식이다. 예를 들어, 3G 서비스에 대한 고객 만족도를 조사하는 경우가 이에 해당한다. 이러한 실험은 원인과 결과의 관계를 직접적으로 검증하기보다는, 현재 나타나고 있는 현상을 기술하는 데 초점을 둔다.\n반면 비교실험은 관심 있는 현상에 영향을 미칠 수 있는 요인을 변화시키고, 그에 따른 반응의 변화를 분석하는 실험이다. 예를 들어, 기존 마케팅 전략과 새로 개발한 마케팅 전략을 각각 적용하여 매출이나 고객 반응의 차이를 비교하는 방식이다. 이 경우 요인의 효과를 통계적으로 검정할 수 있으며, 분산분석(ANOVA)은 이러한 비교실험을 분석하는 대표적인 방법이다.\n설계된 실험(designed experiment)\n여기서는 일반적인 단어 사용과, 우리가 실험 설계 구조에서 부여하는 의미를 구분하고자 한다. 간단히 말해, 설계된 실험이란 미리 정해진 틀 안에서 집단을 관찰·측정·평가하는 연구를 뜻한다. 이때 관찰·측정은 특정한 반응(결과 변수)에 대해 이루어진다. 연구자는 실험 중 해당 틀의 요소들을 통제하여, 관심 있는 집단 간에 타당한 비교가 가능하도록 데이터를 수집한다. 이러한 통제를 통해 통계적 추론이 가능해진다.\n요인(factors)\n물고기 사료 (기존 사료A, 새로 개발된 사료=B)와 수온 온도(20도, 25도)에 따른 몸무게 증가 효과를 보기 위하여 어항 4개(성장 환경 동일), 물고기 12마리(몸무게 유사, 성장 속도 유사)를 랜덤하게 선택하여, 각 어항에 3마리 물고기 배정 후, (사료x온도) 어항 넣은 후 2주 후에 몸무게를 측정하였다.\n\n사료 종류 – A-기존사료, B-새로운 사료 (2수준)\n수온 – 20도, 25도 (2수준)\n\n사료 종류와 수온은 연구자가 통제하는 요인이다. 반응변수는 물고기 몸무게(g)이며, 이는 측정값(measurement)으로 기록되지만 연구자가 통제하지는 않는다.\n처리(treatments) 및 설계 형태\n처리는 요인의 수준을 조합하여 구성된다. 위 실험에서는 2수준(tk료) × 2수준(수온) = 4개의 처리가 가능하며, 설계형태는 다음과 같이 정리된다.\n\n만약 요인이 하나만 있다면, 예를 들어 비료 종류만 비교한다면, 이는 일원배치(one-way classification)이다.\n두 요인을 모두 고려하면 요인배치 설계(factorial treatment design)가 되며, 분산분석(ANOVA)을 통해 비료 효과, 관개 효과, 그리고 두 요인의 상호작용 효과까지 평가할 수 있다.\n\n대조 처리(control treatment)\n대조 처리는 다른 처리들의 효과를 비교하는 기준이 되는 특별한 처리 유형이다. 대조 처리가 특히 중요한 경우는 세 가지다.\n첫째, 실험 조건이 일반적으로 효과적인 처리라도 그 효과를 보여주기 어렵게 만드는 경우이다. 이때, 아무 처리를 하지 않는 대조 처리를 포함하면, 실험 조건이 처리 간 효과 차이를 드러내지 못한다는 점을 보여줄 수 있다. 예를 들어, 토마토 재배 실험에서 토양 비옥도가 이미 매우 높다면, 질소 비료를 추가해도 모든 수준이 비슷하게 보일 수 있다. 그러나 ’질소를 전혀 주지 않는’ 대조 처리를 포함하면, 토양 자체의 높은 비옥도가 드러나고, 대조 처리의 효과가 질소 처리와 비슷하다는 점이 확인될 수 있다.\n둘째, 기존에 잘 확립된 표준 방법과 새로운 방법들을 비교하는 경우다. 여러 새로운 절차가 제안될 때, 기존 표준 처리를 대조군으로 두어 비교한다.\n셋째, 플라세보(placebo) 대조다. 이는 피실험자가 실험 중 단순히 처치 과정을 받는 것만으로 반응을 보일 수 있는 경우에 사용된다. 예를 들어, 환자가 의사를 만나 치료를 처방받는 것만으로 통증이 일시적으로 감소할 수 있다. 이 경우, 활성 성분이 없는 가짜 약(플라세보)을 환자에게 알리지 않고 투여하여, 활성 성분이 있는 실제 치료와 비교함으로써 그 진짜 효과를 평가할 수 있다.\n실험 단위(experimental unit)\n실험 단위는 처리를 무작위로 할당받는 물리적 개체, 혹은 처리 집단 중 하나에서 무작위로 선택된 실험 대상이다. 예를 들어, 물고기 몸무게 실험에서는 “어항” 가 실험 단위이다.\n다른 예로, 연구자가 실험실 쥐에게 신약의 다양한 용량(처리 수준)을 시험한다고 하자. 각 쥐에게 무작위로 한 번의 약물 투여를 할당하면, 각 쥐가 실험 단위가 된다.\n반복(replication)\n처리가 한 실험 단위에 할당되면 이를 해당 처리의 한 번의 반복이라고 한다. 일반적으로는 각 처리를 여러 실험 단위에 무작위로 배정하여, 각 처리에 대해 여러 번의 반복을 수행한다. 이렇게 하면 특정 처리에 대한 여러 개의 독립 관측값을 확보할 수 있다.\n측정 단위(measurement unit)\n실험 단위(experimental unit)와 구별되는 개념으로, 측정 단위는 실제로 측정을 수행하는 물리적 개체를 말한다. 예를 들어, 물고기 몸무게 실험에서는 “물고기” 가 측정 단위이다. 그러나, 대부분의 실험에서는 실험 단위와 측정 단위가 동일하다.\n실험 오차(experimental error)\n실험 오차란, 동일한 처리를 받고 동일한 실험 조건에서 관측된 실험 단위들 간 반응의 변동을 말한다. 실험 오차가 0이 되지 않는 이유에는 다음이 포함된다.\n(a) 처리를 받기 전부터 존재하는 실험 단위 간의 자연적인 차이\n(b) 측정 장치의 변동(측정 기기 오차)\n(c) 처리 조건을 설정하는 과정에서의 변동\n(d) 처리 요인이 아닌 외부 요인들이 반응 변수에 미치는 영향\n즉, 실험 오차는 의도한 요인 외에 존재하는 모든 변동이며, 분산분석에서 오차항(Residual, Error MS)로 나타나게 됩니다.\n실험 오차의 분산(variance of experimental error)\n연구 가설을 검정하고 처리 집단 평균의 함수에 대한 신뢰구간을 만들기 위해서는, 실험 오차 분산의 추정값이 필요하다. 실험설계, 처리 설계, 반복(replication) 횟수는 모두 이 추정에 영향을 준다.\n가장 기본적인 실험(단일 요인, t개의 수준, 실험 단위와 측정 단위가 동일)에서는, 동일한 처리를 받은 실험 단위들 간 반응값의 풀링된 분산(pooled variance)이 실험 오차 분산의 추정값이 된다.\n예를 들어, 쥐를 대상으로 약물 농도 실험을 할 때, 네 가지 처리(농도 수준)에 각각 10마리의 쥐를 무작위 배정했다고 하자. 각 농도 수준에서 10마리 쥐의 혈중 농도 반응값 분산을 계산하고, 네 수준의 분산을 합쳐서 하나의 풀링된 분산 추정값을 얻는다. 이 값이 실험 오차 분산의 추정치가 된다.\n대조군과 통제군\n통제집단은 비교의 기준이 되는 집단으로, 예를 들어 기존 사료 A를 먹인 물고기 집단이 이에 해당한다. 반면 실험집단은 새로운 처리를 적용하는 집단으로, 새로 개발된 사료 B를 먹인 물고기 집단이 여기에 해당한다. 이렇게 두 집단을 설정하면, 동일한 조건에서 사료 종류만 달리했을 때 몸무게 변화가 어떻게 다른지 비교할 수 있다.\n또한 실험 설계에서 사전·사후 검정의 목적에 따라 분석 방법이 달라진다. 만약 개별 물고기의 실험 전과 후 몸무게 차이에 관심이 있다면, 동일 개체를 두 번 측정하므로 짝진 표본 검정을 사용한다. 그러나 실험의 주된 목적이 사료의 효과를 비교하는 것이라면, 실험 전 측정한 몸무게는 주효과가 아니라 공변량(covariate)으로 처리하여, 사료 이외의 사전 차이를 통계적으로 제거한 뒤 순수한 사료 효과를 추정하는 것이 적절하다.\nPlacebo 효과\n플라세보 효과는 실제로 치료 성분이 없거나 효과가 없는 처치를 받았음에도 불구하고, 환자가 치료를 받았다고 믿기 때문에 증상이 호전되는 현상을 말한다. 예를 들어, 과거 위염 치료에서 인공적으로 가스를 주입했을 때, 환자들이 통증이 완화된 것처럼 느껴 한동안 내과에서 사용된 사례가 있다. 이는 실제 약리 효과가 아니라, ”치료를 받았다”는 믿음에서 비롯된 심리적·생리적 반응이다. 노인 환자들이 병원을 자주 찾는 이유 중에도 이와 유사한 측면이 있는데, 이는 의사와 상담을 하고 처치를 받았다는 사실만으로도 병이 나아진다고 느끼기 때문이다.\n이러한 플라세보 효과를 배제하기 위해 블라인드(blind) 실험을 실시한다. 블라인드 실험에서는 환자가 위약(placebo)과 실제 치료약을 구별하지 못하도록 하여, 심리적 기대 효과를 최소화한다. 그러나 실험자가 어느 환자가 어떤 처치를 받았는지 알고 있다면, 실험자의 무의식적 행동이나 태도가 환자 반응에 영향을 줄 수 있다. 이를 방지하기 위해 이중 블라인드(double blind) 실험을 적용하는데, 이 방법에서는 환자와 실험자 모두 위약과 치료약을 구별할 수 없도록 하여 보다 객관적인 효과 검증이 가능하게 한다.\n\n\n3. 실험오차 제어\n앞에서 살펴본 것처럼, 실험 오차는 여러 잠재적 원인에서 발생할 수 있다. 실험 오차의 분산이 크면 추론의 정밀성이 크게 떨어진다. 처리 평균의 추정치는 표준편차가 커지고, 그 결과 신뢰구간이 넓어져 추정이 부정확해진다. 또한 가설검정에서 제2종 오류(Type II error)의 가능성이 커진다. 따라서 실험 오차를 줄이는 모든 기법은 더 나은 실험과 더 정밀한 추론으로 이어진다.\n연구자는 실험 오차의 많은 잠재적 원인을 통제할 수 있다. 주요 원인으로는\n\n실험이 수행되는 절차\n실험 단위와 측정 단위의 선택\n측정이 이루어지고 기록되는 절차\n실험 단위의 블록화(blocking)\n실험 설계 유형\n공변량(covariates)의 사용\n\n실험 절차\n실험 절차가 세심하고 일관되게 이루어지지 않으면 반응변수의 분산이 커지고, 경우에 따라 처리 평균 추정에 편향이 생길 수 있다. 이를 방지하려면 실험 인력에 대한 충분한 훈련, 예산 범위 내 정밀 장비 사용, 장비의 주기적 유지·보정, 그리고 실험 조건의 일정한 유지가 필수적이다.\n실험 단위와 측정단위\n실험 단위가 반응변수에 영향을 줄 수 있는 특성에서 서로 비슷하지 않으면, 실험 오차 분산이 커진다. 연구의 주요 목적 중 하나는 서로 다른 처리를 받은 실험 단위들의 평균 반응 차이를 확인하는 것이다. 이를 위해 연구자는 관심 있는 실험 단위의 모집단을 정하고, 그 모집단에서 실험 단위를 무작위로 뽑아 처리를 무작위로 배정하는 것이 이상적이다.\n블록화를 통한 실험오차 줄이기\n블록화(blocking)은 실험 단위 집단 내에서 중요한 특성 차이가 클 때, 실험 오차 분산을 줄이는 데 효과적인 방법이다. 반응변수에 영향을 줄 수 있는 특성에 따라 실험 단위를 비슷한 것끼리 묶어 블록을 만들면, 각 블록 내부는 균질해지고 전체적으로는 다양한 특성을 포함할 수 있다. 이후 각 블록 안에서 처리를 무작위로 배정하여, 블록 내 비교를 통해 큰 차이로 인한 변동이 처리 효과 분석을 가리지 않게 한다. 이 과정에서 블록화 기준이 된 특성에 의한 변동은 실험 오차에서 분리된다.\n블록화 기준은 다양하다. 예를 들어, 피험자의 나이·체중·성별·건강상태·교육 수준과 같은 신체적 특성, 쌍둥이나 같은 배에서 태어난 동물처럼 관련 있는 개체, 인접 토지나 실험대 위 식물 위치 같은 공간적 위치, 요일처럼 실험 시점(환경이 날마다 달라질 수 있음), 그리고 측정·조작 방식에 차이가 있을 수 있는 실험자 등이 있다.\n중요한 점은, 각 블록화 기준의 모든 수준에서 모든 처리를 관찰하려는 것이다. 예를 들어, 세 개의 조립 라인에서 발생하는 주요 결함 차량 수를 비교하는 연구라면, 요일을 블록화 변수로 삼아 주 5일 동안 모든 조립 라인을 비교하도록 설계해야 한다.\n공변량을 이용한 변동성 줄이기\n공변량(covariate)은 반응변수와 관련이 있는 변수로, 실험 오차의 변동을 줄이는 데 활용된다. 실험 단위의 물리적 특성을 기준으로 균질한 블록을 만드는 블록화와 달리, 공변량은 해당 특성을 연속형 변수 그대로 모델에 포함하여 처리 효과를 비교할 때 반응변수를 보정한다.\n예를 들어, 새로운 식이요법과 대조 식이요법의 체중 감량 효과를 비교하는 개 실험에서 개의 나이가 112세로 다양하다면, 블록화는 이를 3세 미만, 38세, 8세 초과의 세 그룹으로 나누어 분석한다. 반면 공변량 기법은 개의 나이를 정확히 기록한 뒤, 처리 효과 분석 시 나이에 따른 영향을 통계적으로 제거한 상태에서 식이요법 효과를 비교한다. 이렇게 하면 연령 차이에 의한 불필요한 변동이 줄어, 더 정밀한 비교가 가능하다.\n공변량이 되기 위해서는 세 가지 조건을 만족해야 한다. 첫째, 반응변수와 관계가 있어야 하고, 둘째, 측정이 가능해야 하며, 셋째, 처리에 의해 영향을 받지 않아야 한다. 대부분의 경우 공변량은 처리를 적용하기 전에 실험 단위에서 측정한다. 예로는 토양 비옥도, 원자재 불순물 함량, 실험 단위의 초기 무게, 학생의 SAT 점수, 대상자의 콜레스테롤 수치, 포장 내 곤충 밀도 등이 있다.\n\n\n4. 랜덤화와 블록화\n\n(1) 랜덤화(실험단위에 처리배치)\n앞 절에서 논의했듯이, 통계 절차는 실험에서 얻은 데이터가 정규분포를 따르는 반응값 모집단에서 추출한 무작위 표본과 동등하다는 조건을 전제로 한다. 만약 실험이 사전에 정의된 처리 집단에서 실험 단위를 무작위로 선택하는 방식이라면, 이 조건이 타당한지를 실제로 확인할 수 있다. 그러나 특정 기준을 충족하는 실험 단위를 선정하거나, 농업 시험장의 토지 구획처럼 이미 주어진 실험 단위를 사용하는 경우에는 이러한 반응값이 특정 모집단의 무작위 표본이라는 가정이 다소 의문스러울 수 있다.\n랜덤화는 실험 단위의 배정과 실험 수행 순서를 무작위로 결정하여 실험의 객관성을 보장하는 절차이다. 완전임의배치법(CRD)에서는 이러한 랜덤화를 통해 관심 요인 이외의 기타 원인들이 실험 결과에 영향을 주는 가능성을 최소화한다. 특히, 시간의 흐름에 따라 변할 수 있는 인자나 경향이 있다면, 실험을 서로 다른 시간대에 균등하게 배치함으로써 이러한 시간 효과를 약화시킬 수 있다. 이를 통해 실험 결과가 특정 시점이나 조건에 치우치지 않도록 하여, 처리 효과를 보다 정확하게 추정할 수 있다.\n비료 종류(A, B, C)에 따른 벼 수확량의 차이를 알아보기 위하여 실험을 한다고 하자. 반복 수는 각 비료에 대해 4번을 한다면 실험을 위해 총 12곳의 경작지가 필요하다. 12 경작지가 땅의 비옥도가 동일하다면 CRD 방법을 이용하여 실험하면 된다. 난수표를 이용하여 임의로 비료를 배정할 수도 있지만 총 실험 수가 많지 않으므로 12 장의 종이에 각 번호를 적고 던진 후 하나씩 선택하여 실험을 배정하면 된다. 만약 1, 5, 2, 7, 12, 10 …, 3 순으로 선택하였다면 다음과 같다.\n\n\n\n\n\n\n\n(2) 블록화\n블록화는 실험 단위를 무작위로 배정하기 어려운 상황에서 실험의 정밀도를 높이기 위해 사용하는 방법이다. 성질이 유사한 단위들을 묶어 블록을 구성하면, 각 블록 내부에서는 실험 환경이 균일하게 유지되어 처리 효과를 보다 정확하게 비교할 수 있다. 이를 위해 실험 전체를 시간적 또는 공간적으로 나누어 블록을 설정하며, 블록은 실험 계획 단계에서 또 다른 독립변수(요인)로 간주한다. 예를 들어, 실험이 이틀에 걸쳐 수행된다면, 실험 날짜 자체가 블록 요인이 되어 각 날짜 내에서 처리 배정을 수행하게 된다.\n만약 경작지 아래 개천이 흐른다면 물로부터 거리에 따라 땅의 비옥도의 차이는 있을 것이므로 CRD 방법은 적합하지 않다. 물의 위치에 따라 경작지를 나누고(블록화:block) 각 블록 내에서 각 비료를 하나씩 임의(randomized)로 배정하는 실험 계획을 실시하면 된다. 이를 Randomized Block Design이라 한다. 행이 (물에서 먼 정도) 블록이 되는 RBD 설계의 예이다.\n\n\n\n\n\n\n\n\n5. 반복(replication)과 반복 측정(repetition)\n반복은 동일한 처리를 서로 다른 독립적인 실험 단위에 적용하는 것을 의미한다. 이렇게 하면 실험 조건을 처음부터 다시 세팅하여 처리 효과를 여러 번 관찰할 수 있으므로, 실험 오차를 추정하고 결과의 재현성을 검증할 수 있다. 또한 표본 수가 늘어나 통계적 검정력이 향상되며, 분산분석에서는 오차항 계산에 직접 활용된다. 예를 들어, 사료 A를 주는 어항을 5개 두고 각각에서 물고기의 성장률을 측정하는 경우가 반복에 해당한다.\n반복측정은 동일한 실험 단위에서 동일한 조건으로 여러 번 측정하거나 관찰하는 것을 말한다. 이는 측정 과정에서 발생하는 변동을 줄이고 정밀도를 높이기 위한 방법이다. 같은 실험 단위에서 얻은 값은 통계적으로 독립이 아니므로 실험 오차 추정에 직접 사용되지는 않으며, 보통 평균값을 계산해 분석에 사용한다. 예를 들어, 한 어항의 수온을 하루 동안 세 번 측정한 뒤 평균을 내는 경우가 반복측정에 해당한다.\n결국 반복은 실험의 신뢰성과 오차 추정을 위한 핵심 설계 요소이고, 반복측정은 측정값의 정밀도를 높이는 보조적 방법이다.\n\n\n6. 반복수 결정\n실험에서 반복(replication) 횟수는 처리 평균의 추정 정확도와, 처리 평균 차이에 대한 가설검정의 검정력을 결정하는 핵심 요소이다. 일반적으로 반복 횟수가 많을수록 추정값의 정확도가 높아지고, 신뢰구간이 좁아지며, 가설검정의 검정력도 커진다.\n그러나 반복 횟수는 실험 비용, 많은 실험 단위 관리에 필요한 시간, 그리고 실험 단위 확보 가능성 등의 제약을 받는다. 따라서 연구자는 추정 정확도나 검정력 측면에서 합리적인 목표를 충족할 최소 반복 횟수를 결정해야 한다.\n\n(1) 처리평균 신뢰수준 활용방법\n처리 평균에 대한 100(1−α)% 신뢰구간의 폭이 원하는 수준이 되도록 반복 횟수를 결정할 수 있다. 실험 단위 수가 충분하다고 가정하면, 반복 횟수 r는 다음 공식으로 근사 계산할 수 있다.\n\\(r = \\frac{(z_{\\frac{\\alpha}{2}})^{2}{\\widehat{\\sigma}}^{2}}{E^{2}}\\), 여기서 \\(z_{\\frac{\\alpha}{2}}\\): 신뢰수준 100(1−α)%에 해당하는 표준정규분포 임계값으로 보통(95% 신뢰수준, 1,95 사용), \\(\\widehat{\\sigma}\\)은 실험 표준편차(사전 조사나 과거 실험에서 추정), 그리고 \\(E\\)는 추정값의 허용 오차(precision)이다. \\(E\\)는 연구자가 결정하며, \\(\\widehat{\\sigma}\\)은 과거 데이터를 이용하여 \\(\\widehat{\\sigma} = \\frac{x_{max} - x_{min}}{4}\\)을 사용한다.\n연구자는 네 가지 사료 처리 수준에서 물고기 몸무게를 측정하고자 한다. 목표는 각 처리 평균(μ₁, μ₂, μ₃, μ₄)에 대한 추정치가 실제 평균값에서 ±4 파운드 이내에 있을 확률이 95%가 되도록 반복 수를 정하는 것이다.\n과거 수확량 범위: 40 ~ 70 파운드\n표준편차 추정치: \\(\\widehat{\\sigma} = \\frac{70 - 40}{4} = 7.5\\)\n신뢰수준: 95% →\\(z_{0.025} = 1.96\\)\n원하는 정밀도(허용 오차): \\(E = 4\\) 파운드\n\\(r = \\frac{(z_{\\frac{\\alpha}{2}})^{2}{\\widehat{\\sigma}}^{2}}{E^{2}} = \\frac{(1.96)^{2}(7.5)^{2}}{(4)^{2}} = 13.51\\), 결론적으로 각 처리 수준별로 14회 반복을 수행해야 원하는 정밀도를 달성할 수 있다.\n\n\n(2) F 검정의 검정력을 이용한 반복 수 결정\nt개의 처리가 있는 실험에서 반복 수를 결정할 때, 연구 목표 중 하나는 다음 가설을 검정하는 것이다. \\(H_{0}:\\mu_{1} = \\mu_{2} = \\cdots = \\mu_{t}\\)\nF 통계량은 \\(F = \\frac{MST}{MSE}\\) 로 계산되며, 여기서 MST는 처리 평균제곱, MSE는 오차 평균제곱이다. 반복 수 \\(r\\)은 모든 처리에서 동일하다고 가정하며(\\(r_{1} = r_{2} = \\cdots = r_{t}\\)), 다음 요소를 지정해야 한다.\n유의수준(α) – 제1종 오류 허용 확률\n효과 크기(D) – 실제적으로 의미 있는 두 처리 평균의 최소 차이\n제2종 오류(β)와 검정력(1−β) – 효과 크기가 D 이상일 때 귀무가설을 기각하지 못할 확률과 이를 보완하는 검정력\n실험 오차 분산(σ²) – 과거 실험, 예비 연구, 문헌 등에서 추정\n비중심 F 분포를 이용해 제2종 오류 확률 β(λ)를 계산하며, 비중심성 모수 λ는 \\(\\lambda = \\frac{r\\sum_{i = 1}^{t}(\\mu_{i} - \\overline{\\mu})^{2}}{\\sigma^{2}}\\)이고, 평균 차이가 D 이상인 최소 경우에는 \\(\\lambda = \\frac{rD^{2}}{2\\sigma^{2}}\\)로 단순화된다. λ와 처리 개수 t를 사용해 \\(\\phi = \\sqrt{\\frac{\\lambda}{t}}\\)를 구하고, 검정력 표나 파워 곡선을 통해 원하는 검정력(예: 0.90)을 만족하는 최소 \\(r\\)을 찾는다. 이렇게 하면 실험 설계 시 요구되는 반복 수를 정량적으로 산출할 수 있다.\n연구자가 4가지 질소 비료 처리 수준에 따른 피칸(pecan) 수확량을 비교하는 실험을 설계하는 상황이다. 연구자는 평균 수확량 차이가 15파운드 이상이면 경제적으로 유의하다고 판단하며, 유의수준 α=0.05에서 F 검정을 통해 이러한 차이를 90% 확률(검정력=0.90)로 검출할 수 있도록 필요한 반복(replication) 수를 계산하고자 한다.\n기존 실험에서 수확량은 40~70파운드였으므로, 표준편차 추정치는 \\(\\widehat{\\sigma} = \\frac{70 - 40}{4} = 7.5\\)이고, 처리 개수 t = 4, 자유도 \\(\\nu_{1} = t - 1 = 3,\\nu_{2} = N - t = 4(r - 1)\\)이다.\n효과 크기 D = 15일 때, \\(\\phi = \\frac{\\sqrt{r}D}{\\sqrt{2t}\\widehat{\\sigma}} = 0.707\\sqrt{r}\\)가 된다.\n파워 곡선 표를 이용하여 시도-오류(trial & error) 방식으로 r 값을 찾는다.\nr=6 → φ=1.73, ν₂=20 → 파워=0.75 (부족)\nr=10 → φ=2.24, ν₂=36 → 파워=0.96 (충분하지만 여유 있음)\nr=9 → φ=2.12, ν₂=32 → 파워=0.93 (목표 충족)\n따라서, r=9가 적절한 반복 수로 결정된다. 이는 총 36개의 실험 단위(4처리 × 9반복)가 필요함을 의미한다.\n\n\n\n\n\n\n\n\n\nchapter 2. 완전 랜덤화 설계 Completely Randomized Design\n\n1. 완전 랜덤화 설계 개념\n단일 요인 완전랜덤화 설계(CRD)는 t개의 모집단(처리) 평균 \\(\\mu_{1},\\mu_{2},\\ldots,\\mu_{t}\\)를 비교하는 데 초점을 맞춘다.\n여기서 t개의 서로 다른 모집단이 존재하며, 각각으로부터 독립적인 무작위 표본을 크기 \\(n_{1},n_{2},\\ldots,n_{t}\\)로 추출한다고 가정한다.\n실험계획법 용어로는, 총 \\(n_{1} + n_{2} + \\ldots + n_{t}\\)개의 동질적인 실험단위(측정이 이루어지는 사람이나 물체)가 있다고 본다. 각 처리는 무작위로 실험단위에 배정되며, 예를 들어 \\(n_{1}\\)개의 단위에는 처리 1, \\(n_{2}\\)개의 단위에는 처리 2가 적용되는 식이다. 이 실험의 목적은 각 처리(모집단) 평균에 대해 통계적 추론을 수행하는 것이다.\n\n\n\n\n\n\n(1) 모형 및 가정\n\\(y_{ij} = \\mu + \\alpha_{i} + e_{ij}\\), 가정 \\(e_{ij} \\sim N(0,\\sigma^{2})\\)\n(가정) 오차항은 독립이며 정규분포를 따르고 분산은 동일하다.\n\n\\(i\\) : 요인A의 수준 첨자 \\(i = 1,2,...,k\\), \\(k\\)는 요인의 수준 수\n\\(j\\) : \\(i\\)-번째 요인의 반복 첨자\n\\(y_{ij}\\) : 요인 \\(\\alpha\\)의 수준 \\(i\\) 의 처리를 받은 \\(j\\)-번째 실험단위 관측치\n\\(\\epsilon_{ij}\\): \\(i\\)번째 처리의 \\(j\\)번째 실험단위에서 발생한 임의 오차\n\\(n_{i}\\): \\(i\\)번째 처리의 반복수\n총 데이터 크기 : \\(\\sum\\sum n_{ij} = n\\)\n\n\n\n(2) 추정\n모수\n\n\\(\\mu\\) : 모집단 총평균\n\\(\\alpha_{i}\\) : 요인 \\(\\alpha\\)는 주효과를 나타내는 알파벳으로 실험 처리효과라고도 한다. \\(i\\)-번째 수준의 주효과는 \\(\\alpha_{i}\\)로 표현한다.\n\\(\\mu + \\alpha_{i} = \\mu_{i}\\) : 수준 \\(i\\) 모집단 평균\n모수 : \\(\\mu,\\mu_{1},\\mu_{2},...,\\mu_{k}\\) (\\(k + 1\\)개)\n\n추정량 (MVUE)\n총평균 점추정치 : \\(\\widehat{\\mu} = \\frac{\\sum_{i}\\sum_{j}y_{ij}}{n} = \\overline{\\overline{y}}\\)\n요인 수준 \\(i -\\)(모집단)- 평균 점추정치: \\({\\widehat{\\mu}}_{i} = \\frac{\\sum_{j}^{}y_{ij}}{n_{i}} = \\overline{y_{i}}\\)\nOLS(최소자승합) 추정치 \\(min_{\\mu,A_{i}}\\sum\\sum(y_{ij} - \\widehat{y_{ij}})^{2}\\) ⬌ \\(\\widehat{\\underset{¯}{\\beta}} = (X'X)^{- 1}X'\\underset{¯}{y}\\)\n요인 \\(A\\)의 수준 3개, 반복 2인 데이터의 모형은 다음과 같다.\n\\(\\left\\lbrack \\begin{array}{r}\ny_{11} \\\\\ny_{12} \\\\\ny_{21} \\\\\ny_{22} \\\\\ny_{31} \\\\\ny_{32}\n\\end{array} \\right\\rbrack = \\begin{bmatrix}\n1 & 1 & 0 & 0 \\\\\n1 & 1 & 0 & 0 \\\\\n1 & 0 & 1 & 0 \\\\\n1 & 0 & 1 & 0 \\\\\n1 & 0 & 0 & 1 \\\\\n1 & 0 & 0 & 1\n\\end{bmatrix}\\left\\lbrack \\begin{array}{r}\n\\mu \\\\\nA_{1} \\\\\nA_{2} \\\\\nA_{3}\n\\end{array} \\right\\rbrack + \\left\\lbrack \\begin{array}{r}\ne_{11} \\\\\ne_{12} \\\\\ne_{21} \\\\\ne_{22} \\\\\ne_{31} \\\\\ne_{32}\n\\end{array} \\right\\rbrack\\)⬄ \\(\\underset{¯}{y} = X\\underset{¯}{\\beta} + \\underset{¯}{e}\\)\n모수(\\(\\underset{¯}{\\beta}\\))는 3개이나 \\(rank(X) = 3\\) (독립인 방정식 개수)이므로 모수 4개를 모두(\\(\\mu,\\alpha_{1},\\alpha_{2},\\alpha_{3}\\)) 추정할 수 없다. 하여, 모수의 수를 줄이는 제약 조건 하에서 모수를 추정한다. \\(\\sum\\mu_{i} = 0\\) 가정 하에 모수를 추정한다.\n\n\n(3) 검정가설\n\\(H_{0}:\\mu_{1} = \\mu_{2} = \\ldots = \\mu_{t}\\) ⇔ \\(H_{0}:\\alpha_{1} = \\alpha_{2} = \\ldots = \\alpha_{t} = 0\\)\n\\(H_{a}:\\text{적어도 하나의}\\mu_{i}\\text{가 다르다}\\) ⇔ \\(H_{a}:\\text{적어도 하나의}\\tau_{i} \\neq 0\\)\n\n\n(4) 총변동, 요인(집단) 변동, 오차변동\n변동분해\n총변동(③)=집단간 변동(②)+집단내 변동(①)\n③ 반응변수의 총변동(SST, Total Sum of Squares) \\(SST = \\sum_{i}\\sum_{j}(y_{ij} - \\overline{\\overline{y}})^{2}\\), 여기서 \\(\\overline{\\overline{y}} = \\sum_{i}\\sum_{j}y_{ij}/n\\)이다.\n② 요인(집단간, 설명) 변동 (SSBetween)\\(SSB = \\sum_{i}\\sum_{j}({\\overline{y}}_{i} - \\overline{\\overline{y}})^{2}\\), 여기서 \\(\\overline{y_{i}} = \\sum_{j}y_{ij}/n_{i}\\)이다.\n① 오차(집단내) 변동 (SSError, SSWithin) \\(SSE = \\sum_{i}\\sum_{j}(y_{ij} - {\\overline{y}}_{i})^{2}\\)\n평균변동 Mean Sum of Squares\n변동(Sum of Squares) 값을 자유도로 나눈 값, 변동의 평균적 개념이다. 자유도는 데이터가 가진 정보로 데이터가 \\(n\\)개인 경우 데이터가 가진 정보는 \\(n\\)개이다. 만약 이 데이터에서 평균을 계산하여 주어졌다면 다른 하나의 값을 없애도 알 수 있으므로 데이터가 가진 정보는 \\((n - 1)\\)이다.\n\n총변동(\\(SST\\))의 경우 총 데이터 크기가 \\(n_{+ +}\\)이고 \\(\\overline{\\overline{y}}\\)를 하나 추정했으므로 \\((n - 1)\\)이 자유도이다.\n요인변동(\\(SSB\\))의 경우 요인 수준 개수(집단 개수 \\(k\\)) 평균에서 \\(\\overline{\\overline{y}}\\)을 하나 추정했으므로 \\((k - 1)\\)이 자유도이다.\n그리고 Cochran 정리(변동 자유도의 합의 동일하다)에 의해 오차변동(\\(SSE,SSW\\)) 자유도는 \\((n - k)\\)이다.\n평균 요인변동(Mean Squared Between): \\(MSB = \\frac{SSB}{k - 1}\\)\n평균오차변동(Mean Squared Error): \\(MSE = \\frac{SSE}{n - k}\\)\n\n\n\n(5) 변동의 분포\n오차의 가정 \\(e_{ij} \\sim N(0,\\sigma^{2})\\)으로부터 \\(y_{ij} \\sim N(\\mu + \\alpha_{i},\\sigma^{2})\\)이므로 다음이 증명된다.\n\\(\\frac{SSB}{\\sigma^{2}} \\sim \\chi^{2}(k - 1)\\), \\(\\frac{SSE}{\\sigma^{2}} \\sim \\chi^{2}(n - k)\\)\n오차 분산의 추정치 : \\(\\widehat{\\sigma^{2}} = MSE\\)\n변동비의 분포\n\\(\\frac{SSB}{\\sigma^{2}} \\sim \\chi^{2}(k - 1)\\), \\(\\frac{SSE}{\\sigma^{2}} \\sim \\chi^{2}(n - k)\\)이므로 서로 독립인 카이제곱 분포의 비는 \\(F\\)-분포를 따르므로\n\\(TS = \\frac{SSB/(k - 1)}{SSE/(n - k)} \\sim F(k - 1,n - k)\\)가 성립한다.\n평균변동 기대값\n\\[E(MSE) = \\sigma^{2}\\]\n\\[E(MSB) = \\sigma^{2} + \\frac{\\sum n_{i}(\\mu_{i} - \\mu)^{2}}{k - 1}\\]\n만약 모든 집단의 평균이 동일하다면 \\((\\mu_{i} - \\mu) = 0\\)이므로 \\(E(MSE) = E(MSB)\\)가 된다. 그러므로 \\(TS = \\frac{SSB/(k - 1)}{SSE/(n - k)} = 1\\)이 된다. 만약 집단 간 평균의 차이가 커지면 \\(TS\\)값은 커지게 된다.\n분산분석표\n\n귀무가설 : 요인 수준별 평균은 동일하다(집단 평균은 동일하다). \\(H_{0}:\\mu_{1} = \\mu_{2} = ... = \\mu_{k}\\)\n대립가설 : 적어도 하나의 집단 평균은 다르다. &lt;=&gt; 모든 집단 평균이 동일한 것은 아니다.\n\n\n\n\n\n\n\n\n\n\n\n요인\n자승합\nSum of Squares\n자유도\ndf\n평균자승합\nMean Squares\nF-통계량\n\n\n\n\n집단간 변동\n처리변동\nBetween\nSSB\n\\[\\sum_{i}\\sum_{j}({\\overline{y}}_{i} - \\overline{\\overline{y}})^{2}\\]\n\\[k - 1\\]\n\\[MSB = \\frac{SSB}{k - 1}\\]\n\\[F = \\frac{MSB}{MSE}\\]\n\n\n오차변동\nError\nSSE, SSW\\(\\sum_{i}\\sum_{j}(y_{ij} - {\\overline{y}}_{i})^{2}\\)\n\\[n - k\\]\n\\[MSE = \\frac{SSE}{n - k}\\]\n\\[\\sim F(k - 1,n - k)\\]\n\n\n수정총합\nCorrected Total\nSST\n\\[\\sum_{i}\\sum_{j}(y_{ij} - \\overline{\\overline{y}})^{2}\\]\n\\[n - 1\\]\n\n\n\n\n\n독립인 두 모집단 추론과 하나의 요인 수준 2인 분산분석과 동일함\n검정통계량 : \\(TS = \\frac{SSB/(k - 1)}{SSE/(n - k)} \\sim F(1,n - 2) = t(n - 2)^{2}\\), 분자의 자유도가 1인 F-분포는 \\(t\\)-분포의 제곱과 동일하다.\n\n\n\n2. 사후 검정 (Post-hoc test)\n\n(1) 개념\n분산분석에서의 F-검정은 여러 집단 평균의 동질성을 검정하는 절차로, 귀무가설 \\(H_{0}:\\mu_{1} = \\mu_{2} = \\ldots = \\mu_{t}\\)이 참인지 여부를 평가한다. 이 검정에서 귀무가설이 기각되면, 적어도 하나의 집단 평균이 다른 집단과 차이가 있다는 사실은 알 수 있지만, 구체적으로 어떤 집단이 서로 다른지를 파악할 수는 없다. 예를 들어, \\(\\mu_{1}\\)이 \\(\\mu_{2}\\)와 다른지, 혹은 \\(\\mu_{3}\\)이 \\(\\mu_{2},\\mu_{4},\\mu_{5}\\)의 평균과 다른지, 또는 처리 평균 값들 간에 증가 추세가 있는지와 같은 구체적인 질문에는 답하지 못한다.\n이러한 구체적인 차이를 확인하기 위해서는 사후 검정(Post-hoc test) 또는 다중 비교(Multiple Comparison) 절차가 필요하다. 다중 비교에서는 집단 간 모든 가능한 쌍체 비교(pairwise comparison)나 특정 선형 결합 대비(contrast)를 검정하게 되며, 이 과정에서 여러 개의 가설을 동시에 검정하는 만큼 유의수준의 조정이 필수적이다.\n다중 비교에서는 여러 개의 가설을 동시에 검정하므로 유의수준을 조정해야 한다. 이를 조정된 실험 유의수준 (controlled experimental error rate)이라 하고 \\(1 - (1 - \\alpha)^{c}\\)이다. 여기서 \\(c\\)는 가설 수를 의미한다. \\(c = 1\\)인 경우 유의수준은 \\(\\alpha = 0.05\\)이지만, 요인의 수준 개수가 \\(k = 4\\)개인 경우 쌍체 비교 시 검정되어야 하는 귀무가설 개수는 \\({}_{4}C_{2} = 6\\)개이므로 조정된 유의수준은 \\(1 - (1 - 0.05)^{6} = 0.264\\)로 매우 높다. 즉, 6개 귀무가설을 동시에 검증하는 경우 유의수준은 5%가 아니라 26.4%가 된다.\n한편, 데이터 분석에서 주의해야 할 문제 중 하나는 데이터 스누핑(data snooping) 또는 데이터 드레징(data dredging)이다. 이는 데이터를 관찰한 뒤 흥미롭게 보이는 비교만을 선택하여 분석하는 행위로, 원래 계획된 비교가 아닌 사후 비교이므로 사전에 설정한 신뢰수준 \\((1 - \\alpha)\\)를 보장하지 못한다. 예를 들어, 여섯 개의 평균 비교에서 가장 큰 평균과 가장 작은 평균을 데이터 확인 후 선택해 비교하는 경우, 신뢰계수는 계획 시점의 값과 달라진다.\n이러한 문제를 해결하는 방법으로는 두 가지 접근이 있다. 첫째, 관찰 후 가능한 모든 비교를 포함할 수 있는 다중 비교 절차를 사용하여 사후에도 신뢰수준을 유지하는 방법이다. 둘째, 데이터 스누핑 결과를 탐색적 가설 생성(exploratory hypothesis generation)의 자료로 활용하여, 이후의 독립된 실험에서 해당 가설을 검증하는 것이다. 이 경우 데이터 스누핑 단계에서는 최종 결론을 내리지 않고, 후속 연구를 통해 가설을 확인(confirm)하거나 기각하는 절차를 거친다.\n분산분석의 F-검정은 단지 귀무가설 즉 전체적인 차이를 검정하는 것이다. 그러므로 수준별 차이(pairwise: 예:)가 있는지 혹은 수준의 선형 결합 대비(contrast: 예: )의 차이가 있는지 검정할 필요가 있는데 이를 사후 검정 혹은 다중 비교(대비 포함)라 한다. 사후 검정이므로 비록 F-검정 결과와 관계없이 (귀무가설을 채택하더라도) 시행하게 된다.\n\n\n(2) 선형 대비(Linear Contrast)\n선형 대비는 \\(t\\)개의 모집단 평균 \\(\\mu_{1} = \\mu_{2} = ... = \\mu_{t}\\)사이의 특정 비교를 나타내는 선형 결합 형태로, 계수들의 합이 0이 되도록 설정한다. \\(l = \\sum_{i}a_{i}\\mu_{i} = a_{1}\\mu_{1} + a_{2}\\mu_{2} + ... + a_{t}\\mu_{t},\\sum_{i}a_{i} = 0\\)\n예를 들어 \\(\\mu_{1}\\)과 \\(\\mu_{2}\\)를 비교하고자 한다면, \\(l = \\mu_{1} - \\mu_{2}\\)와 같이 쓸 수 있으며, 이때 \\(a_{1} = 1,a_{2} = - 1\\), 나머지 모두 0이 된다. 또는 \\(\\mu_{1}\\)과 \\(\\mu_{2},\\mu_{3}\\)의 평균을 비교할 때는 \\(l = \\mu_{1} - (\\mu_{2} + \\mu_{3})/2\\)와 같이 표현하며, 계수는 \\(a_{1} = 1,a_{2} = a_{3} = - 1/2\\), 나머지는 0이다.\n선형 대비의 추정량(\\(\\widehat{l}\\))은 \\(l = \\sum_{i}a_{i}{\\overline{y}}_{i.}\\)이고 분산 추정량은 \\(\\widehat{V}(\\widehat{l}) = MSE\\left\\lbrack \\frac{a_{1}^{2}}{n_{1}} + \\frac{a_{2}^{2}}{n_{2}} + \\cdots + \\frac{a_{t}^{2}}{n_{t}} \\right\\rbrack = MSE\\overset{t}{\\sum_{i = 1}}\\frac{a_{i}^{2}}{n_{i}}\\)이다.\n직교 선형대비\n두 개의 선형 대비, \\(l_{1} = \\sum_{i}a_{i}{\\overline{y}}_{i.}\\), \\(l_{2} = \\sum_{i}b_{i}{\\overline{y}}_{i.}\\)가 직교(orthogonal)하려면, 표본 수가 동일할 때 \\(\\sum_{i}a_{i}b_{1} = 0\\)을 만족해야 한다. 모든 쌍이 직교 관계에 있으면 이를 상호 직교(mutually orthogonal) 대비 집합이라 한다. 즉, 선형 대비는 단순 평균 비교를 넘어, 요인 수준 간의 특정 패턴이나 조합 차이를 통계적으로 명확히 검정할 수 있는 도구이며, 직교 대비를 사용하면 독립적인 정보를 효율적으로 추출할 수 있다.\n두 대비가 직교(orthogonal)하다는 것은, 한 대비가 다른 대비에 대한 정보를 전혀 주지 않는다는 의미한다. 직교성을 만족하면 t개의 처리 평균으로부터 t-1개의 상호 직교 대비를 만들 수 있으며, 각 대비는 서로 독립적인 변동을 설명한다. 이 t-1개의 대비 제곱합을 합하면 처리 제곱합과 같아진다.\n선형대비가 설명하는 변동의 크기는 대비 제곱합(SSC)으로 계산하는데 대비 추정치의 제곱을, 표본 크기와 대비 계수의 제곱비로 나눈 값으로 정의되며, 해당 대비가 처리 평균 차이 중 어느 정도를 설명하는지를 나타낸다. \\(SSC = \\frac{\\left( \\sum{i = 1}^{t}a_{i}\\overline{y}i \\right)^{2}}{\\sum{i = 1}^{t}\\left( \\frac{a_{i}^{2}}{n_{i}} \\right)} = \\frac{{\\widehat{l}}^{2}}{\\sum_{i = 1}^{t}\\left( \\frac{a_{i}^{2}}{n_{i}} \\right)}\\) 만약 처리수준 반복수 \\(n_{i}\\)가 동일하면 \\(SSC = \\frac{n(\\widehat{l})^{2}}{\\sum_{i = 1}^{t}a_{i}^{2}}\\)이다.\n선형대비 가설검정\n귀무가설: \\(H_{0}:l = a_{1}\\mu_{1} + a_{2}\\mu_{2} + \\cdots + a_{t}\\mu_{t} = 0\\)\n대립가설: \\(H_{a}:l = a_{1}\\mu_{1} + a_{2}\\mu_{2} + \\cdots + a_{t}\\mu_{t} \\neq 0\\)\n검정통계량: \\(F = \\frac{SSC}{MS_{\\text{Error}}} \\sim F(1,n - t)\\)\n\n\n(3) 다중비교 방법\n유의수준 결정에 대하여\n연구자가 t개의 모집단 평균을 m개의 대비(contrast)로 비교한다고 가정하자. 각 대비는 F-검정을 통해 동일한 유의수준 α로 검정되는데, 여기서 α는 개별 비교 유의수준 \\(\\alpha_{i}\\)로, 각 검정에서 제1종 오류가 발생할 확률이다. 그러나 m개의 검정을 모두 수행하면, 적어도 하나의 귀무가설을 잘못 기각할 확률이 커진다. 이를 실험 전체 유의수준\\(\\alpha_{E}\\)이라 하며, \\(\\alpha_{E}\\)는 여러 검정을 동시에 고려한 제1종 오류 확률이다.\n만약 모든 검정이 독립이고 MS_Error의 자유도가 매우 크다고 가정하면, \\(\\alpha_{E} = 1 - (1 - \\alpha_{I})^{m}\\)으로 계산된다. 그러나 실제 연구에서는 검정들이 완전히 독립이 아니므로 위 식이 \\(\\alpha_{E}\\)의 상한이다. 따라서 목표 \\(\\alpha_{E}\\)에 맞추어 다음과 같이 \\(\\alpha_{i}\\)를 조정해야 한다. 예를 들어, m=8, t=20, \\(\\alpha_{E}\\)≤ 0.05를 원하면, 개별 유의수준은\\(\\alpha_{i} = 1 - (1 - 0.05)^{\\frac{1}{8}} \\approx 0.0064\\)로 설정해야 한다.\n이 방법은 실험 전체 오류율을 보수적으로 통제하지만, 지나치게 작은 αᵢ를 사용하게 되어 제2종 오류(β)가 커질 수 있다는 단점이 있다.\nFisher’s Least Significant Difference\nFisher의 최소유의차(LSD, Least Significant Difference) 방법은 분산분석에서 전체 평균이 같다는 귀무가설을 기각한 이후, 구체적으로 어떤 집단 평균이 서로 다른지를 알아보기 위해 고안된 사후(pairwise) 비교 절차이다.\nR.A. Fisher(1949)가 제안한 이 방법은 두 평균 간 차이를 t-검정을 통해 검정하되, 전체 집단 수 t에 대해 가능한 모든 쌍을 비교하는 형태로 진행된다. LSD의 유의수준 α는 독립적(직교, orthogonal) 비교나 사전에 계획된 비교(preplanned comparison)일 때만 올바르게 유지된다. 그러나 LSD는 계산이 간단해 많은 연구자들이 모든 가능한 쌍체 비교에 적용하는 경향이 있어, 실험 후 흥미로운 비교를 선택해 분석하는 경우(Type I error 증가 위험)에는 적합하지 않다.\n이러한 문제를 완화하기 위해, 연구자들은 먼저 분산분석의 F-검정을 수행하여 전체 효과가 유의하다고 판정된 경우에만 LSD를 적용하는 절차를 권장하며, 이를 Fisher의 보호된 LSD(Protected LSD) 라고 부른다. Cramer와 Swanson(1973)의 시뮬레이션 연구에 따르면, 보호된 LSD는 실험 단위에서의 제1종 오류율이 F-검정의 α 수준과 유사하게 유지되는 것으로 나타났다.\n\\[LSD_{ij} = t_{\\frac{\\alpha}{2}}\\sqrt{MSE\\left( \\frac{1}{n_{i}} + \\frac{1}{n_{j}} \\right)}\\]\n요인 수준 \\(i\\)와 \\(j\\)의 평균 차이가 위의 값 이상이어야 유의하다고 판단한다. pairwise (두 수준별 평균 비교) 검정에 사용하나 이는 다중 비교에 해당되지는 않는다. 독립인 두 모집단 평균차이 검정과 동일하나 통합분산 대신 \\(MSE\\)(집단 통합분산 개념)을 사용한다.\nTukey HSD(honestly significant difference) procedure\nTukey의 W 절차는 다중비교에서 개별 비교 오류율을 통제할 때 발생하는 주요 단점을 해결하기 위해 제안된 방법이다. Fisher의 비보호 LSD처럼 개별 비교 오류율이 작더라도, 여러 평균을 동시에 비교하면 최소 한 쌍 이상의 평균 차이가 유의하게 나타날 확률이 높아진다. 이러한 문제를 피하기 위해, 다른 오류율을 통제하는 다중비교 방법들이 개발되었는데, 그 중 하나가 Tukey(1953)가 제안한 **Studentized 범위 분포(Studentized range distribution)**를 이용한 방법이다. 보수적인(귀무가설 기각하지 않음) 방법으로 자연 과학에서 가장 많이 이용한다.\n검정통계량: \\(\\frac{\\overline{y}\\text{largest} - \\overline{y}\\text{smallest}}{\\sqrt{MSE/n}}\\)\n첫째, t개의 표본 평균을 크기 순서대로 나열한다.\n둘째, 두 모집단 평균 \\(\\mu_{i}\\)와 \\(\\mu_{j}\\)의 차이를 검정할 때, 다음 조건이 만족되면 두 평균이 서로 다르다고 판단한다. \\(|{\\overline{y}}_{i} - {\\overline{y}}_{j}| \\geq W\\), 여기서 \\(W = q_{\\alpha}(t,\\nu)\\sqrt{\\frac{MSE}{n}}\\)이다. \\(q_{\\alpha}(t,\\nu)\\)는 t개의 모집단 평균 비교 시 사용하는 Studentized 범위 분포의 상한 임계값으로 Tukey에 의해 표로 주어져 있다.\nStudent–Newman–Keuls(SNK) ⇔ Duncan Multiple range test\nSNK 절차 역시 Studentized 범위 통계량을 사용하지만, 비교되는 평균들 간의 간격(steps)의 수에 따라 서로 다른 임계값을 적용한다는 점에서 차이가 있다. 예를 들어, 주어진 다섯 개 처리 평균을 크기순으로 배열하면 다음과 같다고 하자. \\(\\overline{y}(1) = 1.175,\\overline{y}(2) = 1.293,\\overline{y}(3) = 1.328,\\overline{y}(4) = 1.415,{\\overline{y}}_{(5)} = 1.500\\)\nTukey W 절차에서는 모든 쌍별 비교에 대해 동일한 Studentized 범위 임계값 \\(q_{\\alpha}(t,\\nu)\\)를 사용한다. 반면 SNK 절차에서는 t개의 표본평균을 낮은 순서에서 높은 순서로 배열했을 때, 평균 간의 간격이 r단계 떨어져 있는 경우 다음의 임계값을 사용한다. \\(W_{r} = q_{\\alpha}(r,\\nu)\\sqrt{\\frac{MSE}{n}}\\). 예를 들어, \\(\\overline{y}(1) = 1.175,\\overline{y}(4) = 1.415\\)을 비교할 때는 \\(W_{5}\\)대신 \\(W_{4}\\)를 사용하여 간격별로 다른 임계값을 적용해 비교의 유연성을 높인 방법이다.\n많은 연구와 실험에서는 비교를 위해 대조군(control treatment)을 포함합니다. 대조군은 다른 모든 처치와 비교하는 기준이 되며, 실험 조건에 따라 그 필요성이 달라집니다. 예를 들어, 해충 밀도가 지나치게 높을 경우 일반적으로 효과적인 살충제라도 눈에 띄는 효과를 내기 어려운데, 이때 대조군 살포를 통해 실제 해충 수준을 파악할 수 있습니다.\n또 다른 예로는, 어떤 처치가 주어지면 그 자체로 긍정적인 반응이 나타나는 플라시보 효과 상황입니다. 이러한 경우 대조군은 플라시보를 받으며, 실험 참가자들은 무작위로 대조군 또는 실험군에 배정되고, 처리 방법은 동일하게 유지됩니다. 의학·임상시험에서는 새로운 약이나 치료법의 효과를 측정하기 위해 플라시보 대조군을 자주 사용합니다. 또한, 대조군은 현재 사용 중인 표준 치료나 절차를 대표할 수도 있습니다.\nDunnett’s Procedure\nDunnett(1955) 방법은 대조군과 각 실험군의 평균을 비교할 때 실험 단위 제1종 오류율을 통제하는 절차입니다. 각 실험군 평균과 대조군 평균의 차이를 계산하여, 이를 임계값과 비교합니다.\n\\(D = d_{\\alpha}(k,v)\\sqrt{\\frac{2MSE}{n}}\\), 여기서 \\(k = t - 1\\)(t는 요인 개수), \\(n_{c} = n_{1} = \\cdots = n_{t - 1} = n\\), 그리고 \\(v\\)는 오차변동의 자유도이다.\nScheffe’s S method\n앞서 소개된 다중비교 절차는 모두 t개의 모집단 평균 간의 쌍별(pairwise) 비교를 위해 개발된 방법이다. 이에 비해 Scheffé(1953)가 제안한 절차는 t개의 모집단 평균 사이에서 가능한 모든 비교를 수행할 수 있는 보다 일반적인 방법이다.\n검정통계량: \\(\\widehat{l} = a_{1}{\\overline{y}}_{1} + a_{2}{\\overline{y}}_{2} + \\cdots + a_{t}{\\overline{y}}_{t}\\)\n기각역: \\(S = \\sqrt{\\widehat{V}(\\widehat{l})} \\cdot \\sqrt{(t - 1)F_{\\alpha,\\text{df}_{1},\\text{df}_{2}}}\\), 여기서 \\(\\widehat{V}(\\widehat{l}) = MSE\\sum_{i}\\frac{a_{i}^{2}}{n_{i}}\\)이다.\n대비 (contrast) : 집단 간 차이 분석\n\\(Q = \\overset{k}{\\sum_{i}}c_{i}\\mu_{i},\\sum c_{i} = 1\\), 만약 두 집단 \\((i,j)\\) 평균 비교인 경우에는 \\(c_{i} = 1,c_{j} = - 1\\)이고 나머지는 \\(c_{i} = 0\\)이다. \\(Q = c_{i} - c_{j}\\), 만약 집단이 3개이고 처음 2개집단의 평균과 3번째 집단의 평균 차이를 비교한다면, \\(c_{1} = 1/2,c_{2} = 1/2,c_{3} = - 1\\)으로 설정한다.\n검정통계량 : \\(TS = \\frac{n(\\sum c_{i}{\\overline{y}}_{i})^{2}/\\sum c_{i}^{2}}{MSE} \\sim F(m - 1,n - k)\\), \\(m =\\)\\(c_{i}\\)가 0이 아닌 개수\n\n\n\n3. 사례 실습\n분꽃 3종(Versicolor, Setosa, Virginica)에 대한 sepal(꽃받침 조각) 길이, 넓이, petal(꽃잎) 길이, 넓이 데이터이다.\n#분꽃 데이터 불러오기\nimport pandas as pd\nimport seaborn as sns\niris = sns.load_dataset('iris')\niris.info()\n\n\n\n\n\n히스토그램 그리기\n#히스토그램 그리기\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nf, ax = plt.subplots(figsize=(11,9))\nsns.distplot(iris.loc[iris['species']=='setosa','petal_length'], ax=ax, label='setosa')\nsns.distplot(iris.loc[iris['species']=='versicolor','petal_length'], ax=ax, label='versicolor')\nsns.distplot(iris.loc[iris['species']=='virginica','petal_length'], ax=ax, label='virginica')\nplt.title('Histogram')\nplt.legend()\nplt.show()\n\n\n\n\n\n나무상자 그리기\n#히스토그램 그리기\nimport seaborn as sns\nax=sns.boxplot(x='species',y='petal_length',hue='species',data=iris)\n\n\n\n\n\n요인별 기초통계량\n품종별 꽃잎 길이는 virginica종이 가장 길고(평균 0.552) setpsa종이(평균 0.462) 가장 짧다.\n#기초통계량 표\niris.pivot_table(index=['species'],values=['petal_length'],aggfunc=['mean','std'])\n\n\n\n\n\n분산분석 anova\nF-검정통계량=1180, 유의확률&lt;0.001로 귀무가설이 기각되어 품종간 꽃잎 길이는 유의한 차이가 있다.\n#분산분석\nimport statsmodels.api as sm\nfrom statsmodels.formula.api import ols\ny=iris['petal_length']\nresults=ols('y~C(species)',data=iris).fit()\nsm.stats.anova_lm(results,typ=2)\n\n\n\n\n\n모형 \\(y_{ij} = \\mu + A_{i} + e_{ij}\\)\nsetosa 꽃잎 길이 평균=절편 1.462(기초 통계량 평균과 동일)\nversicolor 품종 평균 길이 : 1.462+2.798=4.26,\nvirginica 꽃잎 평균 길이 : 1.462+4.09=5.552이다.\n#추정결과\nresults.summary()\n\n\n\n\n\n사후검정 : 튜키 다중비교 검정\nSE종과 VE종 간 꽃잎 길이 차이(meandiff +이므로 VE종의 꽃잎 길이 큼)는 유의함(True) - 첫째 행 둘째, 셋째 모두 유의한 차이가 있으므로 VI종(5.55) &gt; VE종(4.26) &gt; SE종(1.46) 순으로 꽃잎의 길이가 유의한 차이를 보이고 있다.\n\n#다중비교 튜키검정\nfrom statsmodels.stats.multicomp import pairwise_tukeyhsd\nfrom statsmodels.stats.multicomp import MultiComparison\nmc=MultiComparison(iris['petal_length'],iris['species'])\nmc_results = mc.tukeyhsd()\n\n\n\n\n\n분석결과표\n품종 이름 옆 알파벳(윗첨자)은 다중비교 결과를 표시한 것으로 유의한 차이가 있는 품종 간에는 동일 알파벳을 공유하지 않게 설정한다.\n\n\n\n\n\n\n\n\n\n품종\n평균\n표준편차\nF-통계량\n(유의확률)\n\n\n\n\nsetosaa\n\n1.46\n\n\n0.174\n\n\n1180\n\n\n\nversicolorb\n\n4.26\n\n\n0.470\n\n&lt;0.001\n\n\nvirginicac\n\n5.55\n\n\n0.552\n\n\n\n\n\n\n\n\nchapter 3. 블록설계 Block Design\n\n1. 개념\n\n\n\n\n\n각 논 구역에는 동일한 조건을 갖춘 세 구간이 있으며, 일부 구간은 물이 있는 상태를 나타낸다(그림 속 오리 표시는 물이 있는 구간임). 만약 세 가지 비료(A, B, C)를 아홉 개 구간에 무작위로 배정한다면, 왼쪽 그림과 같은 무작위 배정 계획이 나올 수 있다.\n이 설계에서 각 비료 처리별로 세 개의 관측값을 얻을 수 있지만, 비료 간 수확량 차이가 실제로는 논 구역별 토양 상태, 일조량, 또는 물의 유무와 같은 환경 차이에 기인할 수 있다. 즉, 논 구역(블록) 요인과 비료 종류 요인이 혼동(confounded)되어 있기 때문에, 수확량 차이가 비료 자체의 효과인지, 아니면 논 구역 조건 차이 때문인지 구분할 수 없다.\n이러한 경우 완전 무작위 설계는 적절하지 않다. 대신, 실험 단위 간의 환경 차이를 통제하기 위해 무작위 완전 블록 설계(Randomized Complete Block Design, RCBD)를 사용한다. RCBD에서는 각 블록(논 구역) 내에 모든 비료 종류가 한 번씩 배정되도록 한다. 오른쪽 그림은 각 블록에 A, B, C 세 비료가 한 번씩 배정된 예시이다.\n이 방법을 사용하면 논 구역에 따른 환경 차이(물의 유무 포함)로 인한 변동을 제거하고, 비료 간 수확량 비교가 더 정밀해진다. 예를 들어,\n\\(H_{0}:\\mu_{A} - \\mu_{B} = 0\\), \\(H_{a}:\\mu_{A} - \\mu_{B} \\neq 0\\)라는 가설을 검정했을 때, 귀무가설을 기각하면 A와 B의 수확량 차이는 비료 효과에 의한 것이며, 논 구역 조건 차이에 의한 것이 아니다.\nRCBD는 외생적 변동 요인(블록)이 존재할 때 t개의 처리 평균을 비교하는 데 사용하는 실험 설계이다. b개의 블록이 있을 경우, 각 블록 내에 t개의 처리를 무작위로 배정하되, 각 처리는 블록마다 정확히 한 번씩 나타난다.\n\n\n2. 모형 및 가정\n\n\n\n\n\n\\[y_{ij} = \\mu + \\alpha_{i} + \\beta_{j} + \\varepsilon_{ij}\\]\n\n\\(y_{ij}\\): j번째 블록에서 i번째 처리를 받은 실험 단위의 관측값\n\\(t\\)는 처리개수, \\(b\\)는 블록개수, 그리고 \\(n = tb\\)은 총 실험개수이다. \\(i = 1,2,..,t\\), \\(j = 1,2,..,b\\)\n\\(\\mu\\): 전체 평균(알 수 없는 상수)\n\\(\\alpha_{i}\\): i번째 처리 효과(알 수 없는 상수)\n\\(\\beta_{j}\\): j번째 블록 효과(알 수 없는 상수)\n\\(\\varepsilon_{ij}\\): j번째 블록에서 i번째 처리를 받은 실험 단위의 반응값에 대한 오차항\n평균 0, 분산 \\(\\sigma_{\\varepsilon}^{2}\\)를 가지는 정규분포를 따른다고 가정\n모든 오차항은 서로 독립이어야 함\n\n위 가정을 따르면, \\(y_{ij}\\)는 평균 \\(E(y_{ij}) = \\mu + \\alpha_{i} + \\beta_{j}\\)과 분산 \\(\\sigma_{\\varepsilon}^{2}\\)를 갖는 정규분포를 따른다.\n\n\n3. 추정과 가설검정\n추정\n\n전체평균 \\(\\widehat{\\mu} = {\\overline{y}}_{..} = \\frac{1}{n}\\sum_{i}\\sum_{j}y_{ij}\\)\n처리 \\(i\\)의 표본평균: \\({\\widehat{\\mu}}_{i} = {\\overline{y}}_{i \\cdot} = \\frac{1}{b}\\overset{b}{\\sum_{j = 1}}y_{ij}\\), \\({\\widehat{\\alpha}}_{i} = {\\overline{y}}_{i \\cdot} - {\\overline{y}}_{\\cdot \\cdot}\\)\n블록 \\(j\\)의 표본평균: \\({\\widehat{\\mu}}_{j} = {\\overline{y}}_{\\cdot j} = \\frac{1}{t}\\overset{t}{\\sum_{i = 1}}y_{ij}\\), \\({\\widehat{\\beta}}_{j} = {\\overline{y}}_{\\cdot j} - {\\overline{y}}_{\\cdot \\cdot}\\)\n\n변동분해\n\n총변동: \\(SST = \\overset{t}{\\sum_{i = 1}}\\overset{b}{\\sum_{j = 1}}(y_{ij} - {\\overline{y}}_{\\cdot \\cdot})^{2}\\)\n처리변동: \\(SSTr = b\\overset{t}{\\sum_{i = 1}}({\\overline{y}}_{i \\cdot} - {\\overline{y}}_{\\cdot \\cdot})^{2}\\)\n블록변동: \\(SSBl = t\\overset{b}{\\sum_{j = 1}}({\\overline{y}}_{\\cdot j} - {\\overline{y}}_{\\cdot \\cdot})^{2}\\)\n오차변동: \\(SSE = \\overset{t}{\\sum_{i = 1}}\\overset{b}{\\sum_{j = 1}}(y_{ij} - {\\overline{y}}_{i \\cdot} - {\\overline{y}}_{\\cdot j} + {\\overline{y}}_{\\cdot \\cdot})^{2}\\)\n\n분산분석표\n\n\n\n\n\n\n귀무가설 \\(H_{0}:\\alpha_{1} = \\alpha_{2} = \\ldots = \\alpha_{t} = 0\\) (모든 처리효과가 0 → 처리 간 차이가 없다)\n검정통계량: \\(F = \\frac{MST}{MSE}\\), \\(E(MSE) = \\sigma_{\\varepsilon}^{2}\\)\n\n\\(E(MST) = \\sigma_{\\varepsilon}^{2} + b\\theta_{T}\\), \\(\\theta_{T} = \\frac{1}{t - 1}\\sum_{i}\\alpha_{i}^{2}\\). 만약, 귀무가설이 만족한다면 \\(\\sum_{i}\\alpha_{i}^{2} = 0\\)이 되어 \\(E(MST) = \\sigma_{\\varepsilon}^{2}\\)가 되어 거정통계량 \\(F = 1\\)이다.\n블록효과 검정\n상대효율은 블록화 설계가 완전임의배치설계에 비해 처리 평균을 얼마나 더 정밀하게 추정할 수 있는지를 나타내는 척도이다. 블록효과 자체를 통계적으로 검정하기보다, 블로킹이 실험 단위의 변동을 줄여 주었는지를 평가하는 데 주로 사용된다.\n랜덤화 완전블록설계(RCBD)에서 처리 평균의 분산은 평균제곱오차(MSE)를 블록 수 b로 나눈 값으로 추정되며, 완전임의배치설계(CR)에서는 MSE를 반복수 r로 나눈 값으로 추정된다. 두 설계가 동일한 정밀도를 갖도록 하려면, 완전임의배치설계의 반복수 r과 블록 수 b 사이에 \\(\\frac{MSECR}{r} = \\frac{MSERCB}{b}\\)의 관계가 성립해야 한다. 이를 변형하면 \\(\\frac{MSECR}{MSERCB} = \\frac{r}{b}\\)가 되고, 이 비율 r/b가 바로 상대효율이다.\n상대효율 값이 1보다 크면, RCBD가 동일한 정밀도를 얻기 위해 CRD보다 적은 수의 실험 단위만으로도 충분하다는 뜻이다. 이는 블로킹이 변동을 효과적으로 줄였음을 의미한다. 반대로 상대효율이 1에 가까우면, 블로킹의 효과가 거의 없으며 두 설계의 효율 차이가 미미하다는 것을 나타낸다.\n실험에서 RCBD의 효율을 CR 설계와 비교하려면, 두 설계를 모두 수행해 MSE를 비교하는 것이 이상적이다. 그러나 실제로는 CR 설계를 따로 하지 않으므로 직접 비교가 불가능하다. 대신 RCBD의 분산분석(ANOVA) 결과에서 얻은 블록 평균제곱(MSB)과 오차 평균제곱(MSE)을 활용해 상대효율을 계산할 수 있다.\n\\[RE(RCB,CR) = \\frac{MSECR}{MSERCB} = \\frac{(b - 1)MSB + b(t - 1)MSE}{(bt - 1)MSE}\\]\n이 식은 CR 설계에서의 MSE를 RCBD에서의 MSE로 환산한 형태이며, \\((b - 1)MSB\\)항이 블록화로 인한 변동 감소 효과를 반영한다. 만약 RE 값이 1보다 훨씬 크다면, 이는 r (CR 설계에서 동일 정밀도를 얻기 위한 반복 수)가 b보다 크다는 의미이며, 블로킹이 변동을 효과적으로 줄였음을 시사한다. 다시 말해, 동일한 정밀도를 얻기 위해 CR 설계에서는 RCBD보다 훨씬 더 많은 관측이 필요하다는 뜻이다.\n\n\n4. 사례 실습\ndiamonds 데이터는 총 53,940개의 관측값을 포함하며, 미국에서 판매된 다이아몬드 기록을 기반으로 가공된 예제 자료이다. 원래는 R의 ggplot2 패키지에서 시각화와 회귀, 분류 실습을 위해 제작되었으며, 가격과 물리적 특성 간의 관계를 탐구하는 목적으로 자주 활용된다.\n데이터에는 연속형 변수와 범주형 변수가 혼합되어 있다. 연속형 변수에는 무게를 나타내는 carat, 깊이 비율인 depth, 상부 평면의 폭 비율인 table, 그리고 가격(price)과 물리적 치수(x, y, z)가 포함된다. 범주형 변수에는 연마 품질을 나타내는 cut, 색상 등급인 color, 투명도를 나타내는 clarity가 있으며, 이들 범주형 변수는 모두 품질이나 특성의 등급 순서를 가진다. 가격은 캐럿, 컷, 색상, 투명도와 같은 요인에 따라 크게 변동하는 특징을 보인다.\ndiamonds 데이터는 블록 설계의 예시 자료로 활용할 수 있다. 처리 요인(treatment)으로는 다이아몬드의 연마 품질 등급인 cut을 설정할 수 있으며, 블록 요인(block)으로는 색상 등급(color)이나 투명도 등급(clarity)을 선택할 수 있다. 반응변수(response)는 가격(price)으로 두어, 처리와 블록 요인에 따른 가격 차이를 분석하는 형식으로 모형을 구성할 수 있다.\n다만, 이 데이터는 실제 실험 설계로 수집된 자료가 아니라 판매 기록을 기반으로 한 관측 자료이므로, RCBD 분석에서 요구하는 무작위 배치나 독립성과 같은 기본 가정이 충족되지 않을 가능성이 있다.\nimport seaborn as sns\nimport statsmodels.api as sm\nfrom statsmodels.formula.api import ols\n\n# 1. 데이터 불러오기\ndiamonds = sns.load_dataset(\"diamonds\")\n\n# 2. RCBD 모형 적합 (cut: 처리, color: 블록)\nmodel = ols(\"price ~ C(cut) + C(color)\", data=diamonds).fit()\nanova_table = sm.stats.anova_lm(model, typ=2)\n\nprint(anova_table)\n            sum_sq       df           F         PR(&gt;F)\n C(cut) 9.699679e+09 4.0 159.106485 1.288117e-135  C(color) 2.550704e+10 6.0 278.932685 0.000000e+00  Residual 8.219243e+11 53929.0 NaN NaN\n# 3. mean_sq 직접 계산\nanova_table[\"mean_sq\"] = anova_table[\"sum_sq\"] / anova_table[\"df\"]\n\n# 4. MSB, MSE, RE 계산\nMSB = anova_table.loc[\"C(color)\", \"mean_sq\"]      # 블록 평균제곱\nMSE = anova_table.loc[\"Residual\", \"mean_sq\"]     # 오차 평균제곱\nt = diamonds[\"cut\"].nunique()                    # 처리 수\nb = diamonds[\"color\"].nunique()                  # 블록 수\n\nRE = ((b-1)*MSB + b*(t-1)*MSE) / ((b*t - 1)*MSE)\nprint(\"상대효율(RE):\", RE)\n상대효율(RE): 50.046944451222345\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(8,6))\nsns.boxplot(data=diamonds, x=\"cut\", y=\"price\", palette=\"pastel\")\nplt.title(\"(cut) price (Boxplot)\", fontsize=14)\nplt.xlabel(\"Cut (treatment)\")\nplt.ylabel(\"price\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nchapter 4. 이원 분산분석 two-way ANOVA\n\n1. 모형 및 가정\n\\(y_{ijk} = \\mu + \\alpha_{i} + \\beta_{j} + (\\alpha\\beta)_{ij} + e_{ijk}\\),\n\n\\(y_{ijk}\\): A 요인의 i수준, B 요인의 j수준을 받은 k번째 실험단위의 반응값\n\\(\\mu\\): 전체 평균(모르는 상수)\n\\(\\alpha_{i}\\): 요인 A의 i수준 효과, \\(i = 1,2,...,a\\)\n\\(\\beta_{j}\\): 요인 B의 j수준 효과, \\(j = 1,2,...,b\\)\n\\(\\alpha\\beta_{ij}\\): 요인 A의 i수준과 요인 B의 j수준 간 상호작용 효과\n\\(\\varepsilon_{ijk}\\): 무작위 오차항 (평균 0, 분산 \\(\\sigma_{\\varepsilon}^{2}\\)의 정규분포, 독립), \\(k = 1,2,...,n\\)\n\n모수\n\n\\(\\mu\\) : 모집단 총평균\n주효과 main effect : \\(\\alpha_{i},\\beta_{j}\\)\n상호효과 interaction effect : \\(\\alpha\\beta_{ij}\\)\n\n\n\n2. 추정 및 가설검정\n모평균 추정\n\\(\\widehat{\\mu} = {\\overline{y}}_{...} = \\frac{1}{abn}\\sum_{ijk}y_{ijk}\\), 표본 총평균\n주효과 추정\n\n주효과 \\(\\alpha_{i}\\) 의 점추정치: \\(\\alpha_{i} = {\\overline{y}}_{i..} - {\\overline{y}}_{...}\\)\n주효과 \\(\\beta_{j}\\) 의 점추정치: \\(\\beta_{j} = {\\overline{y}}_{.j.} - {\\overline{y}}_{...}\\)\n상호효과 \\(\\alpha\\beta_{ij}\\) 의 점추정치: \\(\\alpha\\beta_{ij} = {\\overline{y}}_{ij.} - {\\overline{y}}_{i..} - {\\overline{y}}_{.j.} + {\\overline{y}}_{...}\\)\n\n\n\n\n\n\n변동분해\n\n총변동: \\(SST = \\sum_{ijk}(y_{ijk} - {\\overline{y}}_{\\cdots})^{2}\\)\n요인 \\(A\\) 변동: \\(SSA = bn\\sum_{i}({\\overline{y}}_{i..} - {\\overline{y}}_{...})^{2}\\)\n요인 \\(B\\) 변동: \\(SSB = an\\sum_{i}({\\overline{y}}_{.j.} - {\\overline{y}}_{...})^{2}\\)\n요인 \\(AB\\) 변동 \\(SSAB = n\\sum_{i}\\sum_{j}({\\overline{y}}_{ij.} - {\\overline{y}}_{i..} - {\\overline{y}}_{.j.} + {\\overline{y}}_{...})^{2}\\)\n잔차 변동 \\(SSE = \\sum_{ijk}(y_{ijk} - {\\overline{y}}_{ij.})^{2}\\)\n\n분산분석표\n\n\n\n\n\n\n\n\n\n\n요인\n자승합\nSum of Squares\n자유도\ndf\n평균자승합\nMean Squares\nF-통계량\n\n\n\n\n요인 A 주효과\n\\[SSA\\]\n\\[a - 1\\]\n\\[MSA\\]\n\\[\\frac{MSA}{MSE}\\]\n\n\n요인 B 주효과\n\\[SSB\\]\n\\[b - 1\\]\n\\[MSB\\]\n\\[\\frac{MSB}{MSE}\\]\n\n\n요인 AB 상호효과\n\\[SSAB\\]\n\\[(a - 1)(b - 1)\\]\n\\[MSAB\\]\n\\[\\frac{MSAB}{MSE}\\]\n\n\n오차변동\n\\[SSE\\]\n차이\n\\[MSE\\]\n\n\n\n수정총합\nCorrected Total\n\\[SST\\]\n\\[n - 1\\]\n\n\n\n\n\n\n\n4. 사례분석\nSEABORN MPG 데이터 : 자동차 연비(mpg)에 영향을 미치는 요인으로 실린더 개수와 생산국가를 고려하였다.\n\n생산국 origin : usa, japan, europe\n실린더 개수 cylinders : 3, 4, 5, 6, 8 (4, 6개만 사용)-생산국\n\n#mpg 데이터 불러오기\nimport pandas as pd\nimport seaborn as sns\ndata= sns.load_dataset('mpg')\ndata=data[(data['cylinders']==4)|(data['cylinders']==6)]\ndata.info()\nColumn Non-Null Count Dtype\n 0 mpg 288 non-null float64  1 cylinders 288 non-null int64  2 displacement 288 non-null float64  3 horsepower 282 non-null float64  4 weight 288 non-null int64  5 acceleration 288 non-null float64  6 model_year 288 non-null int64  7 origin 288 non-null object  8 name 288 non-null object\nBox Plot 그리기\n#나무상자 그림\nimport seaborn as sns\nsns.boxplot(x=\"cylinders\",y=\"mpg\",hue='origin',notch=True,data=data)\nplt.show()\n\n\n\n\n\n평균, 표준편차\n#요인별 기초통계량\ndata.pivot_table(index=['origin'],columns=['cylinders'],values=['mpg'],aggfunc=['mean','std'],margins=True)\n\n\n\n\n\n생산국가 : 생산국가에 따른 연비차이는 매유 유의함(유의확률 &lt;0.001), 생산국가 일본 &gt; 유럽 &gt; 미국 순으로 연비가 높다. 사후검정 결과 3개 생산국가 간 각각 유의한 차이가 있다.\n실린더 개수 : 실린더 개수에 따른 연비차이는 매유 유의함(유의확률 &lt;0.001) 실린더 수가 많을수록 연비는 낮아진다.\n\n(생산국가)*(실린더개수) 상호효과도 매유 유의함\n\n#이원 분산분석\nimport statsmodels.api as sm\nfrom statsmodels.formula.api import ols\ny=data['mpg']\nresults=ols('y~C(cylinders)+C(origin)+C(cylinders):C(origin)',data=data).fit()\nsm.stats.anova_lm(results,typ=2)\n\n\n\n\n\n평균도표\nimport matplotlib.pyplot as plt\n\nmeans = (data\n         .groupby(['cylinders','origin'])['mpg']\n         .mean()\n         .reset_index())\n\npivot = means.pivot(index='cylinders', columns='origin', values='mpg').sort_index()\n\nplt.figure()\nfor b in pivot.columns:\n    plt.plot(pivot.index.astype(str), pivot[b], marker='o', label=f'origin={b}')\nplt.xlabel('Factor A: cylinders')\nplt.ylabel('Mean response (mpg)')\nplt.title('Interaction plot (means)')\nplt.legend(title='Factor B: origin')\nplt.show()\n\n\n\n\n\n\n\n\nchapter 5. 공분산분석 Analysis of Covariance\n\n1. 공분산분석 개념\n공분산분석에서의 공변량은 다른 변수(요인)들 간의 관계를 조절하기 위해 사용되는 변수인 공변량은 독립변수(요인)와 종속변수 사이의 관계를 더 정확하게 검증하거나 그룹 간의 차이를 더 정확하게 비교하기 위해 사용된다.\n공변량을 사용하는 주요 목적은 통제(Control)입니다. 실험에서는 종종 변수들 간의 관계를 이해하거나 종속 변수에 영향을 미치는 다른 요인들을 고려하게 되는데 이 때 공변량을 사용하여 이러한 외생적인 요인들의 영향을 줄일 수 있다. 예를 들어, 어떤 약의 효과를 연령대에 따라 측정한다고 가정해 보자. 이 때 약의 효과를 측정하는 것 외에도 나이가 약의 효과에 영향을 줄 수 있는데 이 경우 나이를 공변량으로 사용하여 나이의 영향을 제거하고 약의 효과를 더 정확하게 측정할 수 있다. 즉, 공변량은 관심의 대상이 아니라 요인의 유의성 검정을 정확하기 위하여 고려하는 설명변수이다.\n\n\n2. 모형 및 가정\n\\(y_{ijk} = \\mu + A_{i} + B_{j} + (AB)_{ij} + x_{ijk} + e_{ijk}\\), 가정 \\(e_{ij} \\sim N(0,\\sigma^{2})\\)\n\n\\(i\\) : 요인 A의 수준 첨자, \\(j\\) : 요인 B의 수준 첨자, \\(k\\) : 반복 첨자\n요인 A 수준 개수 \\(a\\), 요인 B 수준 개수 \\(b\\)\n\\(y_{ijk}\\) : 요인 \\(A\\) \\(i\\) 수준, 요인 \\(B\\)의 수준 \\(j\\)의 \\(k\\)-번째 종속변수 관측치\n\\(x_{ijk}\\) : 요인 \\(A\\) \\(i\\) 수준, 요인 \\(B\\)의 수준 \\(j\\)의 \\(k\\)-번째 공변량 관측치\n\n\n\n3. 이원 공분산분석 사례\nseaborn 예제 데이터(운동)\n\n종속변수 : 운동 후 맥박 수\n요인 : 운동시간 3개 층(1 min=96, 15 min=117, 30 min=126, 매우 유의), 비만 유형 2개 층(no fat=121, low fat=105, 매우 유의)\n공변량 : 운동 전 맥박 수(매우 유의) 운동 후 맥박은 운동 전 맥박에 영향을 받으므로 요인의 검증을 위한 통제변수이므로 유의성 검증 대상이 아니다.\n\n#운동 데이터 불러오기\nimport pandas as pd\nimport seaborn as sns\ndata= sns.load_dataset('exercise')\n\n#데이터 전처리\nrest=pd.melt(data[data['kind']=='rest'],id_vars=['id','diet','time'],value_vars=['pulse'],value_name='rest_pulse')\nrun=pd.melt(data[data['kind']=='running'],id_vars=['id','diet','time'],value_vars=['pulse'],value_name='run_pulse')\ndf=pd.concat([rest,run],axis=1).iloc[:,[1,2,4,9]]\ndf.info()\nColumn Non-Null Count Dtype  0 diet 30 non-null category  1 time 30 non-null category  2 rest_pulse 30 non-null int64  3 run_pulse 30 non-null int64\n#요인별 기초통계량\ndf.pivot_table(index=['time'],columns=['diet'],values=['run_pulse'],aggfunc=['mean','std'],margins=True)\n\n\n\n\n\n#공변량 분석\nimport statsmodels.api as sm\nfrom statsmodels.formula.api import ols\ny=df['run_pulse']\nresults=ols('y~C(diet)+C(time)+C(diet):C(time)+rest_pulse',data=df).fit()\nsm.stats.anova_lm(results,typ=2)\n\n\n\n\n\n운동 후 맥박(run_pulse)에 대해 식이요법(diet)과 측정 시점(time)의 영향을 공변량(rest_pulse, 안정 시 맥박)을 통제한 상태에서 살펴본 결과, 식이요법과 측정 시점 모두에서 유의한 차이가 나타났다. 즉, 집단 간 식이요법의 차이가 운동 후 맥박에 영향을 주었으며, 운동 후 경과 시간에 따라서도 맥박은 뚜렷하게 달라졌다.\n또한 식이요법과 시간 간에는 상호작용 효과가 나타나, 운동 직후에는 두 집단 간 차이가 크지 않았으나 시간이 지남에 따라 무지방(no fat) 집단은 맥박이 꾸준히 증가한 반면, 저지방(low fat) 집단은 증가 폭이 상대적으로 작아졌다. 이로 인해 30분 시점에는 두 집단 간의 차이가 크게 벌어졌다. 반면, 안정 시 맥박(rest_pulse)은 운동 후 맥박에 통계적으로 유의한 영향을 주지 않았다.\n따라서 본 연구에서는 저지방 식이요법이 운동 후 시간이 지남에 따라 맥박 상승을 억제하는 효과가 있음을 확인할 수 있다. 이는 식이 관리가 운동에 따른 생리적 반응에도 중요한 조절 요인이 될 수 있음을 시사한다."
  },
  {
    "objectID": "notes/intro_stat/concept_of_stat.html",
    "href": "notes/intro_stat/concept_of_stat.html",
    "title": "기초통계 1. 통계학 개념",
    "section": "",
    "text": "chapter 1. 통계학이란?\n통계학은 단순히 수를 더하거나, 그래프를 그리고, 타율·완료율·실업률과 같은 비율을 계산하는 것만을 의미하지 않는다. 통계학은 사회와 자연의 수치적 현상을 기술하고, 불완전하고 변동성이 있는 정보를 바탕으로 모집단과 과정에 대한 결론을 도출하는 과학적 원리와 기법의 집합이다. 즉, 통계학은 데이터로부터 배우는 과학이다.\n현대 사회에서 거의 모든 사람, 기업 경영자, 마케팅 담당자, 사회과학자, 공학자, 의학 연구자, 소비자 등 데이터를 다룬다. 데이터는 분기별 매출액, 청소년 범죄 증가율, 수질 샘플의 오염도, 환자의 치료 생존율, 인구조사 결과, 혹은 자동차 구매 결정에 필요한 정보 등 다양한 형태로 존재한다.\n\n1. 개념\n\n(1) 정의\n통계학은 수학의 한 분야로, 숫자 데이터를 수집하고 정리하며, 이를 분석하고 표현하는 일련의 과정에 관한 학문이다.\nKendall과 Stuart에 따르면, 통계학은 관심의 대상이 되는 집단(모집단)의 성질을 세거나 측정하여 얻어진 데이터를 다루는 과학의 한 분야이며, 여기서 이러한 성질을 확률변수라 한다.\nOtt는 통계학을 단순히 ’데이터에 관한 학문’이라고 정의하였고, 어떤 익명의 정의에서는 통계학을 ’미지에 대한 가이드’라고 표현하였다.\nFisher (1935)는 통계학은 불확실한 상황에서 의사결정을 내리기 위해 데이터를 수집, 요약, 해석하는 과학이다.\nTukey (1962)는 통계학은 데이터를 더 잘 이해하고 설명하기 위한 학문이며, 단순한 데이터 요약을 넘어 새로운 통찰을 제공하는 방법론이다.\nAmerican Statistical Association (ASA)에서는 통계학은 데이터로부터 지식을 추출하고, 그 과정에서 불확실성을 정량화하며, 이를 바탕으로 의사결정을 지원하는 과학이자 예술이다.\nDodge & Romig (1959)은 통계학은 현상의 변동성을 이해하고 이를 바탕으로 합리적인 결정을 내리는 데 필요한 기술과 방법을 다루는 학문이다.\nCroxton & Cowden (1939)은 통계학은 사실을 수집, 분류, 요약, 해석하여 합리적인 결론을 도출하고 예측을 가능하게 하는 과학이다.\n마지막으로, 필자는 통계학을 ’Statistics is Art’라고 표현하며, 이를 단순한 수치 해석을 넘어 창의적 해석과 통찰의 영역으로 확장한다.\n\n\n(2) 왜 통계학이 필요한가?\n통계학이 필요한 이유는 다양하다. 첫째, 발표된 수치 자료를 평가할 수 있는 능력을 갖추기 위해서이다. 우리는 제조업체의 제품 주장, 사회·소비자·정치 여론조사 결과, 과학 연구 논문의 수치 결과 등에 일상적으로 노출된다. 이러한 결과 중 다수는 표본조사에 기초한 추론이며, 일부는 타당하지만 일부는 그렇지 않다. 표본 규모가 충분한 경우도 있으나 부족한 경우도 있다. 그럼에도 불구하고 이들 수치는 그럴듯하게 보인다. 일부 사람(특히 통계학자)은 통계를 이용하면 거의 모든 주장을 뒷받침할 수 있다고 말하고, 다른 사람은 통계가 진실을 왜곡하기 쉽다고 말한다. 두 주장 모두 사실이다. 표본조사 결과를 제시할 때는 고의이든 아니든 비전문가에게 잘못된 인상을 줄 위험이 존재한다.\n둘째, 직업이나 업무 수행 과정에서 표본조사(설문조사 또는 실험) 결과를 해석하거나, 통계적 분석 방법을 활용하여 결론을 도출해야 할 필요가 있다. 예를 들어, 의사는 신약의 효능을 알리는 광고를 자주 접한다. 광고에는 신약과 기존 약물의 효과를 비교한 수치가 제시되지만, 그 차이가 실제 약효의 차이인지, 아니면 단순히 실험 측정의 무작위 변동 때문인지 판단해야 한다.\n최근 법정 재판에서도 증거의 신뢰성을 평가하기 위해 확률과 통계적 추론의 활용이 증가하고 있다. 사회과학, 자연과학, 물리과학에서 통계는 필수적이다. 이들 학문은 자연현상 관찰, 표본조사, 실험을 통해 새로운 이론을 개발·검증하며, 통계 방법은 매출·이익 예측, 품질 관리, 회계 감사 등 다양한 분야에서 활용된다.\n결론적으로 통계학은 과학, 산업, 경영 전반에서 중요한 역할을 수행한다. 이러한 분야에서 일하는 사람은 통계의 기본 개념, 강점, 한계를 반드시 이해해야 한다.\n\n\n(3) 통계학자는 무엇을 하는가?\n데이터로부터 배우는 과정에서 통계학자는 연구 또는 실험 설계, 분석을 위한 데이터 준비(그래프·수치 요약 포함), 데이터 분석, 분석 결과 보고의 모든 단계에 관여한다. 데이터 수집 방법에는 좋은 방법과 나쁜 방법이 있으며, 통계학자는 기존 조사기법과 과학적 연구 설계 지식을 활용하거나 새로운 기법을 개발하여 올바른 데이터 수집 방법을 제시한다.\n데이터가 수집되면 의미 있는 해석을 위해 요약이 필요하다. 통계학자는 그래프·표·수치 요약을 통해 데이터를 정리하는 방법을 추천·적용하며, 평균값(대표값)과 범위·산포도 측정치는 데이터 해석의 중요한 기초가 된다.\n통계학의 목적은 표본에서 얻은 정보를 바탕으로 모집단에 대한 추론을 하는 것이다. 예를 들어, 시장조사에서 일부 소비자의 반응을 조사하여 신제품에 대한 전체 잠재 소비자의 반응을 추정한다. 표본조사가 계획·실행이 잘 되면, 표본에서의 반응은 모집단의 반응과 대체로 일치한다고 볼 수 있다. 확률 개념은 이러한 추론을 가능하게 하며, 최적 추정치와 추정치의 오차 범위를 함께 제시할 수 있게 한다.\n예를 들어, 감사인이 25,000개 계정 중 무작위로 2,000개를 조사하여 84개(4.2%)에서 오류를 발견했다고 하자. 이를 바탕으로 전체 25,000개 계정의 오류율을 추정할 수 있다. 추정치가 4.2%이고, 오차 범위가 ±0.9%라면, 실제 오류율은 이 범위 내에 있을 가능성이 높다. 이 ±값을 추론의 추정 오차(probable error)라 한다. 확률 개념은 이 추정 오차를 계산하게 해준다.\n데이터 분석 단계에서 통계학자는 기존 기법을 적용하거나, 수학·확률이 결합된 새로운 분석 방법을 개발할 수 있다. 결과 보고 단계에서 통계학자는 분석 결과를 청중이 이해할 수 있도록 시각 자료·표·수치와 함께 전달한다. 비공식 대화에서는 데이터의 의미가 왜곡되거나 누락되기 쉽기 때문에, 공식 보고서는 비전문가도 혼동 없이 해석할 수 있는 방식으로 작성해야 한다.\n통계학자의 역할은 모든 연구나 실험에 해당하지만, 학위 소지 통계학자는 드물다. 많은 조직은 전문 통계학자가 없거나 소수만 고용한다. 따라서 연구 설계, 데이터 요약 및 분석, 결과 보고는 비전문가가 수행하는 경우가 많다. 전문 통계학자가 참여하더라도 연구팀 전원이 통계 설계와 분석의 기본 개념을 이해해야 하며, 각자의 전문성을 바탕으로 연구 설계와 결과 전달 방안을 함께 결정해야 한다.\n\n\n(4) 통계학과 빅데이터\n통계학은 관심 있는 분야에서 존재하는 불확실성을 설명하기 위해, 목적에 맞는 데이터를 계획적으로 수집하고 이를 분석하여 가치 있는 정보를 추출하는 과학이다.\n반면, 빅데이터는 실시간으로 발생하는 대용량·고속·복잡 데이터를 수집하고 분석하는 기술적·방법론적 접근을 의미한다. 빅데이터는 사전에 이론이나 명확한 목적을 설정하지 않고 수집된 경우가 많으며, 따라서 분석 과정에서 사후적으로 가치와 의미를 추출하는 특성을 가진다.\n빅데이터 연구문제는 어떻게 처리하나?\n빅데이터 연구문제는 그 특성상 다양한 분야 전문가의 협업을 필요로 한다. 연구문제와 직접 관련된 분야의 전문가는 물론, 데이터 수집·처리 기술을 담당하는 컴퓨터 프로그래머, 대규모 데이터 분석 기법을 다루는 머신러닝 전문가, 그리고 분석의 과학적 타당성을 확보하는 통계 전문가가 반드시 필요하다.\n통계 전문가는 빅데이터로부터 가치 있는 정보를 추출하는 데 필수적인 역할을 한다. 빅데이터 분석에서 다음과 같은 이슈들은 특히 중요하며, 데이터 규모가 클수록 그 심각성이 커진다.\n\n데이터 품질과 결측치 문제\n데이터 관측 특성 및 매개효과 등 인과 추론에서의 혼동 요인\n예측과 모형의 불확실성 정량화 문제\n\n과학적 통계학은 이러한 문제를 다루기 위해 정교한 기술과 모델을 제공한다. 통계학자는 과학적 질문을 통계적 질문으로 변환하는 데 핵심적인 역할을 하며, 여기에는 데이터 구조 파악, 데이터를 생성한 기본 모델의 구축, 그리고 모수 추정이나 예측 과정이 포함된다.\n빅데이터 시대 통계는 기회이다.\n빅데이터 시대는 통계학에 새로운 기회를 제공한다. 빅데이터는 그 규모와 복잡성으로 인해, 저차원적이고 단순한 분석 상황에서 주로 사용되던 기존의 통계기법이나 단순 블랙박스 기반 계산만으로는 충분히 처리하기 어렵다. 따라서 데이터의 특성과 연구 목적에 부합하는 맞춤형 통계적 접근이 필수적이다.\n통계학자는 데이터에서 편향을 발견하고 이를 수정하는 데 능숙하며, 불확실성을 정량적으로 측정하고 해석할 수 있는 능력을 갖추고 있다. 또한, 실험 설계와 표본 설계를 통해 신뢰성 있는 자료를 수집하고, 데이터 품질을 평가하며, 연구의 한계와 잠재적 오류를 명확히 드러내는 데 중요한 역할을 수행한다.\n나아가, 결측 데이터나 비표본오차를 처리하고, 복잡한 데이터 구조를 설명하기 위한 적절한 모형을 개발하며, 인과 추론과 비교 효과 분석을 위한 통계적 방법을 설계한다. 이와 더불어, 중복되거나 정보 가치가 낮은 변수를 제거하고, 다양한 출처로부터의 정보를 통합하며, 분석 결과를 효과적으로 전달할 수 있는 시각화 기법을 선택하고 구현함으로써 빅데이터 분석의 품질과 신뢰성을 높인다.\n\n\n(5) 가설 및 모델 hypothesis and modeling\n연구는 자연 현상이나 사회 현상에 대한 의문을 명확한 문제로 정의하고, 이를 자료에 기반하여 체계적으로 검증하는 과정이다. 연구문제는 대개 이론적 배경이나 경험적 관찰에서 출발하며, 과학적 설명이 필요한 구체적인 질문의 형태로 정리된다.\n연구자는 먼저 분석의 대상이 되는 모집단을 설정하고, 그 안에서 관심을 두는 특성을 측정하기 위한 변수(측정 항목)를 정의한다. 이어서 데이터 수집 방식을 결정하고, 연구문제를 통계적으로 표현할 수 있는 구조로 전환한다. 이를 통해 단순한 질문을 수학적으로 다룰 수 있는 통계문제로 변환하고, 검증 가능한 통계적 가설을 설정하게 된다..\n예를 들어, “두 교육 방법 중 어느 쪽이 더 효과적인가?”라는 질문은 다음과 같은 통계적 가설로 구체화한다:\n\n귀무가설: 두 방법의 평균 성적 차이는 없다.\n대립가설: 두 방법의 평균 성적은 서로 다르다.\n\n이처럼 통계 가설은 연구문제를 수량화하고, 검정 가능하게 만드는 통계적 사고의 핵심 도구이다.\n한편, 최근 각광받는 빅데이터 분석에서는 이러한 전통적 문제 정의 절차가 명확하게 드러나지 않는 경우가 많다. 이미 존재하는 대규모 데이터를 바탕으로 탐색적 분석을 수행하는 경우, 연구문제나 가설이 사전에 엄밀히 설정되기보다는 분석 과정에서 문제를 발견하거나 결과를 토대로 가설을 재구성하는 방식이 흔하다. 이에 따라 통계학에서는 전통적인 가설 기반 접근뿐 아니라, 데이터 중심의 유연한 사고방식과 모형화 능력이 점점 더 중요해지고 있다.\n\n\n\n2. 역사\n\n(1) 기술통계학\n기술통계는 인간 사회의 조직과 행정을 위해 수천 년 전부터 활용되어 왔다. 통계라는 개념이 학문적으로 정립되기 이전에도, 사람들은 집단의 크기를 파악하거나 자원을 관리하고 세금을 부과하는 등의 목적을 위해 수치 정보를 체계적으로 기록하고 분석해왔다.\n통계의 가장 오래된 기록은 성경에서 찾아볼 수 있다. 구약 성경의 『민수기(Numbers)』에서는 이스라엘 백성이 광야 생활을 시작하기 전과 후, 두 차례에 걸쳐 인구 조사를 실시한 내용이 등장한다. 이는 단순한 인구 파악을 넘어서, 부족 간 질서 유지와 사회적 구조의 관리를 위한 수단이었다(민수기 1장과 26장에서 각각 출애굽 전후 이스라엘 12지파 인구 조사가 이루어짐).\n고대 로마 제국에서도 인구 조사는 중요한 국가 행정 수단으로 활용되었다. 로마 황제 툴리우스(Tullius)는 세금 징수를 위해 5년마다 인구 조사를 실시하였으며, 훗날 시저(Caesar)는 이를 제국 전역으로 확대하였다. 로마의 이러한 전수조사는 이후 ’센서스’라는 현대 통계 용어의 어원이 되었으며, 이는 라틴어 censura에서 유래된 것으로 ’세금, 평가, 권한’라는 의미를 갖는다(Servius Tullius(기원전 6세기경), 인구조사 제도화한 최초의 인물로 평가됨. 로마 시민권자 등록, 세금, 병역 의무 기반 마련. 율리우스 시저 시대(기원전 1세기), 인구조사가 제국 전역에 적용되며 행정적 통치 도구로 활용됨).\n서기 74년 로마 제국이 쇠퇴한 이후에도, 인구 조사의 전통은 중단되지 않았다. 영국과 스페인을 포함한 유럽 각국은 식민지 지역까지 인구 조사를 확대하였으며, 이는 오늘날 미국, 페루 등지의 초기 센서스 기록으로도 남아 있다(영국은 1801년부터, 미국은 1790년부터 인구총조사 시작. 스페인은 식민지인 페루에서 16~17세기에 교회 중심으로 인구조사 수행).\n’통계’라는 단어 역시 라틴어 status에서 유래되었으며, 본래는 ’국가의 상태)’를 뜻하였다. 이 용어는 국가의 행정 상태를 수치로 기술하려는 시도에서 출발하여, 점차 오늘날과 같은 과학적 분석의 의미로 발전하였다(독일어 Statistik에서 유래, 초기에는 국가 행정 상태를 기술하는 의미. 라틴어 status = ’state(상태, 국가)’).\n17세기에는 영국에서 출생률과 사망률에 대한 체계적인 조사가 시작되었다. 이 시기 통계는 단순한 집계 수준을 넘어서, 자료를 정리하고 시각화하는 기술적 방법으로 발전하기 시작하였다(John Graunt(1620–1674)이 런던의 사망통계(Bills of Mortality)를 체계적으로 분석함. 근대 통계학의 효시). 특히 플로렌스 나이팅게일(Florence Nightingale)은 통계학의 초기 발전에 기여한 인물로 잘 알려져 있다. 그녀는 크림 전쟁 당시 사망 원인을 분류하고 이를 시각적으로 표현하기 위해 폴라 다이어그램(polar diagram)을 사용하였으며, 이는 보건 행정 개선에 결정적인 영향을 미쳤다(Florence Nightingale이 사용한 ”coxcomb diagram” 또는 polar area chart는 오늘날의 인포그래픽 시각화의 초기 형태로 인정받음. 의료 현황 개선에 영향).\n\n\n\n\n\n통계학이 국가 운영과 인구 관리라는 행정적 목적에서 출발했다면, 사회조사는 사회 구조와 인간 행동에 대한 이해를 목적으로 발전해 왔다. 사회조사는 사회학, 정치학, 경제학 등 다양한 사회과학 분야에서 연구의 핵심 도구로 사용되며, 특히 인간의 태도, 가치관, 생활조건 등을 체계적으로 측정하고 분석하는 데 필수적인 방법론이다.\n사회조사의 초창기 형태는 19세기 후반 유럽에서 등장하였다. 대표적인 사례로 칼 마르크스(Karl Marx)는 1880년에 약 25,000명의 프랑스 노동자들을 대상으로 우편 조사를 실시하였다. 이 조사는 노동자의 정치적 성향과 태도를 파악하기 위한 것으로, 계급 구조에 대한 그의 이론적 관심을 실증 자료로 뒷받침하려는 시도의 일환이었다.\n비슷한 시기, 막스 베버(Max Weber)는 관찰과 질적 조사를 병행하며 노동자들의 심리, 직업관, 사회적 태도를 연구하였다. 그는 사회 구조의 변화가 개인의 가치와 의식에 어떤 영향을 미치는지를 파악하기 위해, 직접적인 사회조사 방법을 적극 활용하였다. 베버의 접근은 단순한 수치 수집을 넘어 사회 현상의 맥락적 이해에 초점을 두었고, 이후 질적 조사 방법의 토대를 제공하였다.\n20세기에 들어서면서 사회조사 방법론은 미국을 중심으로 급격히 발전하였다. 미국 사회학자들은 계량적 방법론을 바탕으로, 다양한 사회 현상을 분석하기 위한 이론적・실증적 틀을 정교화해 나갔다. 특히 **미국 통계국(Bureau of the Census)은 인구조사 경험을 바탕으로, 표본추출 이론과 조사 설계 기법을 발전시켰으며, 이는 후속 연구에 큰 영향을 미쳤다.\n이와 함께, Gallup(갤럽)과 Roper(로퍼)와 같은 여론조사 기관들이 등장하면서, 사회조사는 대중의 의견과 태도를 측정하는 핵심 도구로 자리 잡게 되었다. 이들은 대표성 있는 표본 구성, 질문지 설계, 반응 편향의 최소화 등 조사 방법론을 지속적으로 개선하며, 현대적 여론조사의 표준을 형성하였다.\n\n\n(2) 추론통계학\n통계학은 단순히 데이터를 요약하는 기술통계를 넘어서, 데이터를 바탕으로 일반화하거나 예측하는 추론적 사고로까지 확장되어 왔다. 이와 같은 추론 통계의 기원은 수학, 특히 확률이론과 게임이론, 그리고 논리적 사고에 기반한 과학적 방법론과 밀접하게 연결되어 있다.\n1645년경 피에르 드 페르마(Pierre de Fermat)와 블레즈 파스칼(Blaise Pascal)은 확률이론의 시초로 평가받는 도박 문제를 수학적으로 해석하면서 중요한 전환점을 만들었다. 두 사람은 친구인 드 메레(Chevalier de Méré)의 질문, ”두 명이 중단된 게임에서 상금을 공정하게 나누려면 어떻게 해야 하는가?“에 답하기 위해 서신을 주고받으며 확률 계산에 대한 기초를 마련하였다. 이 과정에서 파스칼은 나중에 ’파스칼의 삼각형’으로 알려지게 된 이항계수 배열을 활용하였다(파스칼의 삼각형은 그보다 수백 년 전 중국에서도 알려져 있었지만, 서양 확률론에서는 본격적 도입의 계기가 되었음).\n그 후 야코프 베르누이(Jacob Bernoulli)는 『Ars Conjectandi(추측의 기술, 1713)』에서 법칙의 대수(the Law of Large Numbers)를 제시하여 확률과 통계 간 연결을 정립하였다. 에이브러햄 드 무아브르(Abraham de Moivre)는 확률 분포에 대한 근사 이론을 통해 정규분포 개념을 수학적으로 정교화하였고, 안드레이 콜모고로프(Andrey Kolmogorov)는 20세기에 들어 확률론을 엄밀한 공리계 체계로 재구성하여 현대 확률이론의 기초를 확립하였다.\n정규분포는 추론통계에서 가장 중요한 분포 중 하나로, 그 기원은 이항실험에 대한 근사 문제에서 시작되었다. 예를 들어 동전을 여러 번 던져 앞면이 나오는 횟수를 반복 실험하면, 그 결과는 점점 종 모양의 분포를 따른다. 이는 드 무아브르가 1733년에 제안한 정규근사 이론으로 발전되었으며, 이후 피에르-시몽 라플라스(Pierre-Simon Laplace)가 중심극한정리를 수학적으로 확장하였다.\n정규분포의 수학적 공식 자체는 카를 프리드리히 가우스(Carl Friedrich Gauss)에 의해 제안되었다. 그는 천체의 위치 측정 오차 분포를 분석하던 중 대부분의 오차가 평균을 중심으로 대칭적으로 분포한다는 사실을 발견하고, 이를 바탕으로 오늘날 우리가 ”정규분포”라 부르는 분포의 밀도함수를 유도하였다. 이 때문에 정규분포는 가우시안 분포(Gaussian distribution)라고도 불린다.\n20세기 초에는 소표본 자료의 추론 문제가 주요 관심사로 떠올랐다. 윌리엄 셜리어스 고셋(William S. Gosset)은 당시 기네스 맥주 공장의 연구 책임자로 근무하면서, 소규모 실험에서도 유의미한 추론을 하기 위한 새로운 분포를 개발하였다. 그는 ”Student”라는 필명으로 1908년 t-분포를 발표하였으며, 이는 이후 소표본 평균의 분포와 추론에 핵심적으로 활용되고 있다.\n같은 시기 프랜시스 골턴(Francis Galton)은 유전 연구를 수행하면서, 부모와 자식의 키 사이의 관계를 분석하였고, 이를 통해 오늘날의 회귀분석 개념의 기초를 마련하였다. 골턴은 ’회귀’라는 용어를 사용하여 값이 평균으로 수렴하려는 경향성을 설명하였다. 그의 통계적 접근은 이후 칼 피어슨(Karl Pearson)에 의해 수리적으로 정교화되었고, 상관계수, 회귀직선, 분산 분석 등 현대 통계학의 주요 기법으로 발전하였다.\n마지막으로, 통계학의 현대적 응용을 이끈 인물로는 로널드 피셔(Ronald A. Fisher)가 있다. 그는 농업 실험의 효율성을 높이기 위해 실험 설계와 분산분석(ANOVA) 개념을 도입하였고, 이는 실험 통계학의 혁신적 전환점을 이끌었다. 피셔의 업적은 통계적 추론 이론뿐 아니라 실용적 적용 방법론 측면에서도 깊은 영향을 남겼다.\n\n\n\n\nchapter 2. 통계분석\n\n1. 통계분석 종류\n\n(1) 기술통계학 descriptive statistics\n기술통계는 수집된 데이터를 정리하고 요약하여, 그 속에 담긴 정보를 간단명료하게 표현하는 통계학의 가장 기초적인 분야이다. 복잡하고 방대한 자료를 한눈에 파악할 수 있도록 해주며, 이후의 심층 분석이나 추론 통계로 나아가기 위한 출발점 역할을 한다.\n보다 구체적으로, 기술통계란 관측된 자료 전체(모집단 또는 표본)에 대해 숫자나 그래프와 같은 요약 도구를 활용하여 데이터를 설명하거나 시각화하는 방법을 말한다. 이를 통해 어떤 현상에 대해 수집된 데이터에서 특징적인 값과 패턴을 파악하고, 이를 정량적으로 기술할 수 있다.\n기술통계의 주요 목적은 다음과 같이 정리된다.\n데이터의 중심 위치 파악: 평균, 중앙값 등 대표값 산출\n자료의 산포 정도 파악: 범위, 분산, 표준편차 등 변동성 측정\n분포의 형태 파악: 비대칭성(왜도), 뾰족함(첨도) 등 분포 특성 확인\n시각적 패턴 파악: 그래프와 도표를 통한 직관적 이해\n즉, 기술통계는 데이터의 전반적 모습을 요약하여 정보를 명확히 전달하고, 이후 분석의 기초 자료를 제공하는 필수적인 단계라 할 수 있다.\n\n\n\n\n\n\n\n(2) 추론통계학 inferential statistics\n추론통계는 관측된 자료(표본)를 바탕으로 전체 모집단에 대한 일반적인 결론을 도출하거나 미래를 예측하는 통계 방법이다. 즉, 수집된 일부 데이터를 근거로 전체에 대한 의사결정을 내릴 수 있도록 돕는 도구이다.\n보다 구체적으로, 추론통계란 표본 데이터로부터 모집단의 특성을 추정하거나 가설을 검정하는 통계적 방법론을 말한다. 모든 대상을 조사하는 전수조사는 시간, 비용, 접근성 등 현실적 제약으로 인해 불가능하거나 비효율적인 경우가 많다. 이러한 한계를 극복하기 위해 통계학은 표본에서 얻은 정보를 활용하여 모집단을 추론하며, 이 과정에서 확률 이론이 기반이 되고 오차와 불확실성을 수치화하는 것이 특징이다.\n추론통계가 필요한 이유는 다음과 같다.\n\n전수조사의 한계: 시간, 비용, 접근성 측면에서 비현실적일 수 있음\n빅데이터 시대에도 유효성 유지: 데이터가 방대해도 표본이 아닌 ’편향된 관측값 모음’일 수 있음\n정확도와 신뢰도 평가 필요: 단순한 기술통계만으로는 결과가 우연인지 여부를 판단할 수 없음\n\n따라서 추론통계는 표본을 기반으로 모집단의 특성을 과학적으로 예측·검증하고, 결과의 불확실성을 정량적으로 제시함으로써 합리적인 의사결정을 가능하게 한다.\n\n\n\n\n\n\n\n\n항목\n설명\n예시\n\n\n모수 추정\n모집단의 평균, 비율 등을 표본으로부터 추정\n평균 키의 신뢰구간 추정\n\n\n가설 검정\n어떤 주장(가설)에 대해 데이터로 판단\nA와 B 그룹의 평균 차이 검정\n\n\n신뢰구간\n모수에 대한 추정값의 오차 범위 제시\n“평균은 65~75cm 사이”\n\n\np값과 유의수준\n우연에 의한 결과인지 판단 기준\np &lt; 0.05 → 통계적으로 유의\n\n\n\n\n\n\n2. 통계분석 접근법\n통계 분석은 크게 두 가지 접근 방식으로 나눌 수 있다. 첫째, 확증적 연구(Confirmatory Research)는 사전에 이론이나 가설을 명확히 설정하고, 이를 검증하기 위해 계획적으로 데이터를 수집·분석하는 방식이다. 이 접근은 가설 검정, 추론통계, 실험설계 등과 밀접하며, 분석 과정에서 통계적 유의성 검토와 불확실성 평가가 핵심이 된다.\n둘째, 탐색적 연구(Exploratory Research)는 데이터 자체를 면밀히 살펴 숨겨진 구조나 패턴, 관계를 발견하는 데 초점을 둔다. 이 경우 가설은 사전에 정해지지 않거나 매우 느슨하게 설정되며, 분석 과정에서 발견된 특징이 이후 새로운 가설로 발전하기도 한다.\n두 방식은 철학적 배경과 절차에서 차이를 보이지만, 실제 연구에서는 상호 보완적으로 활용되며, 분석 목적과 상황, 그리고 데이터의 성격에 따라 적절히 선택·결합된다.\n\n(1) 연역적 접근: 확증적 연구\n\n\n\n\n\n확증적 연구(confirmatory research)는 연역적 추론에 기반한 연구 방식으로, 기존의 이론이나 가설에서 출발하여 필요한 데이터를 수집하고, 이를 통해 가설의 타당성을 검증하는 절차를 거친다. 이러한 접근은 과학적 연구의 전통적인 방법으로 널리 활용되어 왔으며, 특히 통계학의 가설검정 이론과 밀접한 관련이 있다.\n철학자 칼 포퍼(Karl Popper)는 1955년 발표한 저술에서 ”과학 이론은 논리적으로 반증 가능해야 하며, 이론은 직관에 의해서만 얻어질 수 있다”고 주장하며 연역적 방법의 중요성을 강조하였다. 그의 반증 가능성 원리는 가설검정 절차의 철학적 토대가 되었으며, 과학적 이론이 경험적 자료에 의해 검증·거부될 수 있어야 함을 뒷받침한다.\n예를 들어 ”온라인 강의는 대면 강의보다 만족도가 낮을 것이다”라는 가설을 세운 뒤, 이를 검증하기 위해 설문조사를 실시하고, 두 집단의 평균 만족도 차이를 통계적으로 검정하는 과정은 확증적 연구의 전형적인 사례라 할 수 있다.\n\n\n(2) 귀납적 접근: 탐색적 연구\n\n\n\n\n\n탐색적 연구(exploratory research)는 귀납적 추론에 기반한 접근으로, 사전에 명확한 가설을 세우지 않은 채 데이터 자체를 관찰하고 요약하며, 그 속에 존재하는 패턴이나 관계를 발견하려는 과정이다. 이러한 방식은 특히 연구의 초기 단계나 구조가 복잡한 데이터를 다룰 때 유용하다.\n이 개념은 통계학자 **존 튜키(John W. Tukey)**가 1977년 저서 Exploratory Data Analysis에서 체계적으로 정립하였다. 튜키는 기존의 가설검정 중심 분석에 대해 비판을 제기하며, 데이터를 먼저 충분히 탐색하고 이해한 뒤에야 검증 가능한 가설이 설정될 수 있다고 주장했다.\n탐색적 데이터 분석(EDA)의 대표적 방법에는 다음이 포함된다.\n\n데이터 구조 요약: 평균, 사분위수, 범위 등 수치 요약 도구\n데이터 시각화: 히스토그램, 박스플롯, 산점도 등 분포와 특성 파악을 위한 그래프\n데이터 재표현(변환): 로그 변환, 표준화 등 분석을 용이하게 만드는 변환 기법\n\n이러한 방법론은 이후 데이터 마이닝(Data Mining)으로 발전했고, 오늘날의 빅데이터 분석으로까지 확장되었다. 특히 사전 가설이 없거나 기존 이론으로 설명하기 어려운 복잡하고 방대한 자료를 다룰 때, 탐색적 접근은 필수적인 분석 전략이 된다.\n\n\n\n3. 통계분석 절차\n통계분석은 단순히 수치를 계산하는 과정이 아니라, 현실의 문제를 수치화하고 해석하며 결론을 도출하는 일련의 사고 절차이다. 이 절차는 문제 정의에서부터 최종 해석과 보고에 이르기까지 체계적으로 이루어지며, 각 단계는 서로 밀접하게 연결되어 있다.\n\n(1) 연구문제 정의\n현실에서 관찰한 현상이나 관심 있는 주제를 바탕으로 명확한 연구 질문을 설정한다. 이 단계에서는 분석의 목적이 기술인지, 예측인지, 또는 비교인지 구체화되어야 한다.\n예: ”남성과 여성의 스마트폰 사용 시간에 차이가 있는가?”\n\n\n(2) 분석 대상과 변수 설정\n모집단(population)을 정의하고 측정할 변수를 명확히 구분한다. 이때 변수의 유형(정성/정량)과 측정수준(명목/서열/등간/비율)도 함께 고려해야 한다.\n예: 종속변수 – 하루 스마트폰 사용 시간 (연속형), 독립변수 – 성별 (명목형)\n모집단 population 표본 sample\n통계학의 가장 기본적인 개념 중 하나는 모집단과 표본의 구분이다. 통계분석은 모집단 전체를 대상으로 직접 관측하는 것이 아니라, 일부를 추출한 표본을 통해 모집단의 특성을 추론하는 데 목적이 있다.\n\n\n\n\n\n모집단의 정의: 모집단이란, 분석 대상이 되는 모든 관찰 단위(subject 또는 individual)의 모임으로, 연구자가 관심을 갖는 전체 집단을 의미한다. 모집단은 사람일 수도 있고, 제품, 기관, 사건, 또는 시간 단위 등 다양한 형태를 가질 수 있다. 이들은 특정 공통 특성이나 조건을 만족하는 개체들로 구성된다. 예를 들면, 전국 고등학교의 학교 평균 시험 성적, 한국 내 제조기업의 연간 매출 등이다.\n모집단을 수학적으로 표현할 때, 보통 각 개체의 관심 특성을 \\(X_{1},X_{2},\\ldots,X_{N}\\)으로 나타낸다. 여기서 \\(X_{i}\\)는 \\(i\\)번째 모집단 개체의 측정값이며, 모집단의 크기 \\(N\\)은 보통 매우 크거나 알려져 있지 않은 경우가 많다.\n\\(X_{i} \\sim f(x;\\theta)\\)\n\\((x_{1},x_{2},...,x_{n}) \\sim f(x)\\)\n표본은 모집단에서 일부 개체만을 선택하여 관측한 자료의 집합이다. 현실적으로 전체 모집단을 모두 조사하기에는 시간, 비용, 자원의 제약이 크기 때문에, 통계학은 적절한 표본을 추출하여 분석을 수행하고, 이를 통해 모집단에 대한 결론을 일반화하려 한다.\n표본은 보통 \\(n\\)개의 개체로 구성되며, 각 표본값은 \\(x_{1},x_{2},\\ldots,x_{n}\\)으로 표현된다. 즉, \\(\\text{표본} = \\{ x_{1},x_{2},\\ldots,x_{n}\\},n \\ll N\\)\n확률표본이란, 모집단에 속한 모든 개체가 동일한 확률로 선택될 수 있는 조건하에 추출된 표본이다. 이러한 표본은 표본의 대표성을 확보할 수 있어, 신뢰할 수 있는 통계 추론을 가능하게 한다. 확률표본의 대표적 방법에는 단순무작위추출, 층화추출, 군집추출 등이 있다.\n모수 parameter와 통계량 statistic\n통계학에서는 분석의 대상이 되는 전체 집단을 모집단이라 하고, 그 모집단에 속한 개체들이 가지고 있는 수치적 특성을 모수라고 한다. 그러나 모집단 전체를 조사하는 것은 현실적으로 어렵기 때문에, 일부를 추출한 확률표본을 통해 필요한 정보를 얻는다. 이때 표본으로부터 계산된 값은 통계량이라고 부른다.\n모수란, 연구자가 관심을 갖는 모집단 전체의 특성을 수치로 요약한 값이다. 이는 보통 관찰할 수 없는 값이며, 표본조사를 통해 추정하게 된다. 모수는 정해진 불변의 값으로 간주되며, Greek letter(예: \\(\\mu,\\sigma,p\\)) 등으로 표현되는 경우가 많다. 이러한 수치는 모집단을 모두 조사하지 않으면 알 수 없지만, 통계 분석의 목적은 바로 이러한 모수를 추정하거나 검정하는 데 있다.\n통계량이란, 표본 데이터를 바탕으로 계산된 수치로서, 모수에 대한 정보를 제공하는 요약값이다. 통계량은 표본에 따라 값이 변하는 확률변수이며, 알파벳 기호(예:\\(\\overline{x},s,\\widehat{p}\\))로 표현된다. 대표적인 통계량에는 표본평균, 표본비율, 표본분산, 최댓값, 중앙값 등이 있다.\n\n\n(3) 데이터 수집\n통계 분석은 항상 데이터를 바탕으로 이루어진다. 데이터는 현실의 현상을 수치화하거나 분류하여 표현한 것으로, 통계학에서는 이를 통해 현상의 구조를 파악하고 일반화된 결론을 도출한다.\n데이터란, 표본 개체의 관심 특성(변수)을 측정하거나 관측하여 얻은 수치 또는 문자 값들의 모음이다. 통계학에서는 데이터가 단순한 숫자 그 이상의 의미를 가진다. 각 데이터는 하나의 개체에 대해 특정 변수를 측정한 결과이며, 분석의 단위가 되는 기본 요소이다.\n관측값이란, 각 표본 개체에 대해 실제로 관측되거나 기록된 값 하나하나를 의미한다. 예를 들어, 학생의 하루 공부 시간을 조사한 경우, ”3시간”, ”5시간”, ”2.5시간” 등은 각각의 관측값이다. 여러 개의 변수(예: 키, 몸무게, 성별 등)를 동시에 측정할 경우, 하나의 표본 개체에 대한 여러 개의 관측값이 함께 주어진다.\n변수란, 각 개체의 특성을 나타내는 관심의 대상이 되는 속성으로, 변화하거나 서로 다른 값을 가질 수 있는 항목이다. 변수는 크게 다음과 같이 나눌 수 있다.\n질적 변수(qualitative): 범주 또는 속성을 나타냄 (예: 성별, 전공, 혈액형 등)\n양적 변수(quantitative): 수치로 표현되는 변수 (예: 키, 몸무게, 점수 등)\n단변량(univariate) vs. 이변량(bivariate) 데이터\n통계 분석에서는 변수의 수에 따라 분석의 성격이 달라진다.\n단변량 데이터: 하나의 변수만을 측정한 자료 → 예: 학생들의 수학 점수만을 조사한 경우\n이변량 데이터: 두 개의 변수에 대한 관측값이 **쌍(pair)**으로 구성된 자료 → 예: 학생들의 영어 점수와 수학 점수를 동시에 조사한 경우 → 또는 치료 전과 후 같은 동일 개체에 대해 두 번 측정한 경우도 이변량 데이터로 본다.\n이변량 자료는 두 변수 간의 관계를 분석하는 데 주로 활용된다. 예를 들어, 키와 몸무게 간의 상관관계, 공부 시간과 시험 성적 간의 관계 등은 이변량 데이터를 통해 분석된다.\n\n\n(4) 데이터 정리와 전처리\n현대의 데이터 분석에서는 데이터를 수집한 이후의 정리와 정제 과정, 즉 전처리(data preprocessing)가 전체 분석에서 가장 많은 시간과 노력이 소요되는 단계로 알려져 있다. 전처리는 모델링을 하기 위한 준비 작업으로, 데이터의 신뢰성과 해석력을 높이는 데 핵심적인 역할을 한다. 단순하고 지루해 보일 수 있지만, 전처리를 제대로 수행하지 않으면 분석 결과가 왜곡되거나 모델의 성능이 떨어지는 위험이 크기 때문에, 분석의 성공을 좌우하는 결정적인 단계라고 할 수 있다.\n데이터 전처리는 다음과 같은 흐름으로 진행된다\n① 데이터셋 확인\n분석 대상이 되는 데이터셋 전체의 구조와 변수의 속성을 파악하는 단계이다.\n\n독립변수(예측변수)와 종속변수(목표변수)를 구분한다.\n각 변수의 데이터 유형을 확인한다. 범주형 vs. 수치형, 날짜, 문자열 등\n변수들의 측정 수준(명목, 서열, 등간, 비율)을 이해하고 적절한 처리 방법 고려한다.\n\n② 결측치(missing value) 처리\n결측치란 어떤 관측값에서 특정 변수의 값이 누락된 상태를 의미한다. 결측값이 포함된 데이터를 그대로 모델에 사용할 경우, 변수 간 관계가 왜곡되거나 분석 자체가 불가능해질 수 있다. 주요 처리 방법은 다음과 같다.\n\n삭제: 결측값이 많은 변수/행 제거\n대체(imputation): 평균, 중앙값, 또는 예측모델 기반 대체\nFlag 변수 생성: 결측 유무를 나타내는 변수 추가 (ML 모델용)\n\n③ 이상값(outlier 또는 anomaly) 처리\n이상값이란 데이터의 전반적인 분포와 크게 벗어난 관측값으로, 통계량(평균, 분산 등)과 모델의 계수 추정에 왜곡을 일으킬 수 있다. 처리 방법은 다음과 같다.\n\n식별: Boxplot, Z-score, IQR, Mahalanobis distance 등\n삭제 또는 대체\n빅데이터에서는 이상값이 ’주목 대상’임 → 다른 개체와 행동 패턴이 현저히 다르면 스몰 개체 탐색에 유용 (ex. 금융 사기, 희귀 질병 탐지 등)\n\n④ 특성공학 (Feature Engineering)\n특성공학은 기존의 변수를 조작하거나 가공하여, 모델링에 도움이 되는 정보를 추가하거나 불필요한 노이즈를 제거하는 작업이다. 이는 단순한 정리가 아니라, 분석 성능을 향상시키는 창의적 과정이다. 주요 기법은 다음과 같다.\n\n변수 변환: 로그변환, 표준화, 정규화 등\n더미 변수(가변수) 생성: 범주형 변수의 수치화\n연속형 변수의 범주화(binning)\n차원 축소: PCA, 변수 선택 기법\n중요 변수 선택: 종속변수와의 관계 기반 선택 (예: 상관계수, 회귀계수, 트리 기반 중요도 등)\n\n\n\n(5) 데이터 요약\n데이터가 수집되고 정비된 이후에는, 분석에 앞서 그 전체적인 분포와 구조를 파악하기 위한 요약 작업이 필요하다. 이러한 요약은 크게 두 가지 방식으로 이루어진다. 하나는 숫자로 요약하는 수치적 방법이고, 다른 하나는 시각적으로 표현하는 그래프 요약이다. 이 두 방법은 서로 보완적으로 작용하며, 데이터의 패턴이나 이상값, 관계 구조 등을 파악하는 데 기초 자료가 된다.\n숫자 요약 numerical summary\n수치 요약은 각 변수의 중심 위치나 변동 정도, 비율 등을 하나의 값 또는 지표로 나타내는 방법이다. 주요 요약 지표로는 다음과 같은 것들이 있다:\n\n표본평균: 자료 전체의 평균값으로, 자료의 중심 위치를 나타낸다. 예를 들어, 학생 200명의 하루 평균 공부 시간이 3.7시간이라면, 이는 해당 집단의 대표적인 공부 시간 수준을 의미한다.\n표본비율: 전체 중에서 특정 범주에 속하는 비율을 나타낸다. 예를 들어, 전체 조사 대상자 중 흡연자의 비율이 18%라면, 이는 건강행동에 관한 분석에서 중요한 지표가 된다.\n최솟값, 최댓값, 중앙값, 사분위수 등: 자료의 범위와 분포 폭, 한쪽으로의 치우침 등을 파악하는 데 유용하다.\n회귀모형의 계수 추정값: 수치 요약의 보다 확장된 형태로, 예를 들어 공부 시간과 시험 점수 사이의 관계를 회귀모형으로 추정하면, 절편과 기울기 계수는 데이터가 나타내는 경향을 수치화한 요약이 된다.\n\n그래프 요약 (시각화 요약, Graphical Summary)\n그래프는 복잡한 수치를 직관적으로 이해할 수 있게 해주며, 특히 분포의 형태, 이상값, 변수 간 관계 등을 한눈에 파악하는 데 탁월한 도구이다. 주요 그래프 유형은 다음과 같다:\n\n막대그래프(bar chart): 범주형 변수의 빈도나 비율을 표현할 때 사용되며, 각 범주의 크기를 쉽게 비교할 수 있다. 예를 들어 학년별 학생 수나 성별 분포를 표현할 수 있다.\n히스토그램(histogram): 연속형 자료를 구간별로 나누어 빈도를 시각화한 그래프로, 자료의 분포 형태(대칭성, 치우침 등)를 파악하는 데 유용하다.\n박스플롯(boxplot): 중앙값, 사분위수, 이상값 등을 한 눈에 보여주며, 자료의 분포와 산포도를 비교할 수 있게 해준다. 특히 여러 집단 간 비교에 효과적이다.\n산점도(scatter plot): 두 변수 간의 관계를 점의 형태로 표현한 그래프로, 이변량 자료의 상관 관계를 시각적으로 확인할 수 있다. 예를 들어 공부 시간과 시험 점수 사이의 관계를 표현할 수 있다.\n\n\n\n(6) 데이터 분석\n데이터 분석은 단순한 수치 요약을 넘어, 표본에서 얻은 정보를 바탕으로 모집단에 대한 결론을 도출하는 과정이다. 이 과정은 크게 두 가지로 구분된다. 하나는 모수나 모형의 값을 추정하는 것, 다른 하나는 어떤 주장(가설)이 통계적으로 옳은지를 검정하는 것이다.\n추정 (Estimation)\n추정이란, 표본 자료로부터 모집단의 특성을 나타내는 모수 또는 모형의 계수를 점추정하는 과정이다. 이때 사용하는 표본 통계량은 단순한 계산값이 아니라, 모수의 값을 추정하기 위한 도구로 간주된다. 통계학에서는 추정에 사용되는 통계량을 추정량이라 하고, 그 계산된 결과 값을 추정값이라 부른다.\n모수 추정: H대학교 전체 재학생의 흡연율 \\(p\\), H대학교 학생들의 주당 평균 공부시간 \\(\\mu\\) →이러한 값들은 직접 조사할 수 없기 때문에, 무작위로 추출한 표본으로부터 표본비율\\(\\widehat{p}\\), 표본평균 \\(\\overline{x}\\)등을 통해 추정하게 된다.\n모형 추정: 복수의 변수들 간의 관계를 수식으로 설명하는 통계 모형이 사용될 경우, 그 모형 내의 계수들도 표본을 통해 추정해야 한다. 예를들면, 학습시간을 독립변수, GPA를 종속변수로 하는 선형 회귀모형 \\(\\text{GPA}_{i} = \\beta_{0} + \\beta_{1} \\cdot \\text{학습시간}_{i} + \\varepsilon_{i}\\) → 이때 \\(\\beta_{0},\\beta_{1}\\)은 모형의 모수이며, 표본 자료를 통해 추정한 값 \\({\\widehat{\\beta}}_{0},{\\widehat{\\beta}}_{1}\\)이 추정값이다.\n가설 검정 (Hypothesis Testing)\n가설 검정은 어떤 주장이 데이터에 근거해 통계적으로 지지될 수 있는지를 판단하는 방법이다. 연구자가 설정한 주장에 대해, 표본에서 계산된 통계량이 이를 뒷받침하는지를 평가한다. 이때 사용하는 통계량은 검정통계량이라고 하며, 통계적으로 유의미한 차이 혹은 효과가 있는지를 판단하는 기준이 된다. 검정은 두 가지 가설을 설정하여 비교하는 방식으로 이루어진다:\n\n귀무가설: 변화나 차이가 없다는 기본 가정\n대립가설: 연구자가 입증하고자 하는 주장\n\n\n\n\n\nchapter 3. 통계의 이해\n\n1. 확률에 대한 이해\n확률 개념\n확률이란 특정 사건이 발생할 가능성을 수치로 표현한 것이다. 우리는 일상생활에서 ”오늘 비가 올 거야” 또는 ”정오에 만나자”처럼 확정적인 표현을 자주 사용하지만, 사실 대부분의 사건은 불확실성을 내포하고 있다. 예기치 못한 변수들—날씨 변화, 교통 체증, 예외 상황 등—로 인해 실제 결과는 예측과 다르게 나타날 수 있다.\n그러나 우리는 이러한 불확실성을 간편하게 말하고 행동하기 위해 간과하거나 단순화하며, 확률을 정량적으로 인식하는 데 익숙하지 않다. 이로 인해 확률을 오해하거나 과소평가하는 일이 흔히 발생한다.\n관찰 편향과 확률 추정의 오류\n사람들은 직접 경험하거나 반복적으로 노출된 사건을 실제보다 자주 발생하는 것으로 오해하는 경향이 있다. 이를 관찰 편향이라 한다. 예를 들어, 많은 사람들이 비행기를 두려워하지만, 미국에서 연간 자동차 사고 사망자는 34,000명에 달하는 반면, 항공기 사고로 인한 사망자는 5명에 불과했다(2013년 기준). 즉, 운전이 항공기보다 약 7,000배 위험함에도 불구하고, 뉴스에 자주 보도되는 비행기 사고로 인해 위험 인식을 왜곡받는다.\n이와 유사하게, ”부자들은 세금을 덜 낸다”는 인식도 반복 노출에 의해 형성된 경우가 많다. 그러나 미국 의회예산국(CBO)의 자료에 따르면, 소득 상위 1%는 소득의 약 29%를, 하위 20%는 약 2%만을 소득세로 납부하고 있다. 즉, 사실과 인식 사이에는 간극이 존재한다.\n반복성과 확률의 누적 효과\n확률이 작다고 해서 무시할 수 있는 것은 아니다. 예를 들어, **지구에 1년 내 소행성이 충돌할 확률이 0.0003%라고 하더라도, 이 확률이 10만 년, 100만 년 동안 반복되면 충돌 가능성은 각각 약 3%, 30%, 96%로 증가한다. 이는 곧 확률이 매우 작아도 시도 횟수가 충분하면 결국 발생한다는 원리를 보여준다.\n동일한 원리는 ’생일 역설’에서도 확인된다. 한 사람과 생일이 겹칠 확률은 0.27%에 불과하지만, 100명이 함께 있는 방에서는 그 중 누군가가 나와 생일이 같을 확률이 약 24%에 이른다.\n결론\n확률은 불확실성을 다루기 위한 도구이며, 직관만으로 접근하면 판단 착오를 유발할 수 있다. 특히 뉴스, 경험, 반복된 메시지에 기반한 편향된 확률 인식을 경계하고, 통계적 사고를 통해 사실 기반의 합리적인 판단을 기르는 것이 중요하다.\n\n\n2. 확률 해석의 함정\n통계학은 객관적 수치를 바탕으로 현상을 설명하는 강력한 도구이지만, 통계 자체는 아무 말도 하지 않는다. 의미는 그것을 해석하는 사람에 의해 부여되며, 이 과정에서 오류와 오해가 빈번히 발생한다. 여기서는 통계 해석에서 흔히 발생하는 오류들을 살펴본다.\n평균의 오해: 숫자보다 중요한 맥락\n사람들은 종종 평균값 하나만으로 전체를 판단하는 실수를 저지른다. 예를 들어, 한 세대 전 동유럽의 1인당 소득은 $3,400, 아시아는 $1,600이었다고 하자. 이 수치만 보고 ”동유럽인의 소득이 더 높다”고 결론짓는 것은 전체 인구 규모를 무시한 오류이다. 동유럽이 1억 명, 아시아가 40억 명이라면, 총소득은 아시아가 훨씬 크다.\n또한, 평균은 분포의 특성을 설명하지 않는다. 개인의 실제 소득은 평균보다 훨씬 크거나 작을 수 있으며, 표준편차 없이 평균만으로는 전형적인 사례를 파악할 수 없다.\n상관관계와 인과관계의 혼동\n”상관관계는 인과관계를 의미하지 않는다”는 사실은 잘 알려져 있다. 예컨대, 과거 미국에서 태양 흑점 수와 공화당 상원 점유율 사이에 높은 상관관계가 있었지만, 단지 우연의 일치일 뿐 인과성은 없었다.\n또한 인과관계가 존재한다고 하더라도, 그 방향이 잘못 해석되는 경우가 많다. 예를 들어, 천식환자가 많은 지역은 공기가 깨끗한 경우가 많은데, 이는 천식환자가 그런 지역으로 이주했기 때문이지, 깨끗한 공기가 천식을 유발한 것이 아니다. 이러한 오류를 역인과(reverse causality)라 한다.\n제3의 변수: 교란변수(confounding)의 위력\n때때로 두 변수는 동시에 변화하지만, 실제 원인은 제3의 변수일 수 있다. 예를 들어, 교회 수와 범죄율 사이의 양의 상관관계는 인구 규모라는 공통 원인 변수로 설명될 수 있다. 이러한 경우, 단순한 상관분석만으로 인과성을 주장하는 것은 매우 위험하다. 교회 수와 범죄율 사이에 양의 상관관계가 있다”는 관찰은, 실제로는 인구 수(교란 변수)가 많을수록 두 수치가 모두 증가하기 때문일 수 있다.\n집계의 편향: aggregation bias\n집계 통계량(예: 평균, 중위수)은 전체 경향을 요약하는 데 유용하지만, 표본의 구성이 무작위가 아니거나 변화한다면 심각한 왜곡을 초래할 수 있다. 예를 들어, 미국 중위소득은 수십 년간 정체된 것처럼 보이지만, 해마다 소득층의 구성원이 바뀌는 횡단면 분석의 한계 때문이다.이와 달리, 패널 데이터는 동일한 개인을 장기간 추적하므로 더 정밀한 분석이 가능하다.\n측정 지표의 해석에 주의하라\n지표가 무엇을 나타내는지를 정확히 이해하는 것이 중요하다. 예를 들어, 실업률은 구직을 포기한 사람을 제외하므로, 고용 상황이 나빠졌음에도 실업률은 낮아질 수 있다. 또한, 소비자물가지수는 평균 소비를 반영하므로, 고령자처럼 특정 항목(예: 의약품)에 지출이 많은 계층의 체감 물가와는 괴리가 클 수 있다.\n\n\n3. 두 집단 비교분석 의미\n”이 두 모집단은 같은가, 다른가?“라는 질문에 답해야 한다. 예를 들어, 어떤 신약이 기존 약보다 효과가 있는지 알고 싶을 때, 두 그룹 간 차이를 비교한다. 그러나 이러한 판단은 확정적이 아니라 언제나 확률적이다.\n평균의 차이: 진짜인가, 우연인가?\n예를 들어, 위약 그룹의 평균 체중 감소가 2파운드이고, 신약 그룹은 4파운드라고 하자. 표면적으로는 신약이 더 효과적인 것처럼 보이지만, 그 차이가 단순히 무작위 변동에 의한 것일 수도 있다. 통계는 여기서 다음 두 가지 질문에 답한다.\n관측된 차이가 실제로 얼마나 큰가?\n그 차이가 우연히 발생할 가능성은 얼마나 적은가?\n이 질문에 대한 답은 주로 p값과 신뢰구간, 그리고 통계적 유의성이라는 개념을 통해 제공된다.\n통계적 유의성과 실질적 의미는 다르다.\n통계적으로 유의미한 차이라 하더라도, 그 차이가 현실에서 의미 있는 정도인지 판단하는 것은 별개의 문제이다. 예를 들어, 두 도시의 평균 기온이 0.1도 차이가 나고 p &lt; 0.05라면 통계적으로는 유의하지만, 일상 생활에서는 체감할 수 없는 차이일 수 있다. 이를 실질적(또는 실용적) 유의성과 구분해야 한다.\n효과 크기와 표준오차\n두 집단 간의 차이를 해석할 때는 단순한 차이값이 아니라, 그것이 표준오차에 비해 얼마나 큰가를 봐야 한다. 예를 들어, 차이가 2파운드이고 표준오차가 0.5파운드라면, 이는 4배 차이이며 z-통계량 또는 t-통계량으로 측정할 수 있다. 이런 비교를 통해 우리는 단순한 차이가 아닌 통계적으로 신뢰할 수 있는 차이를 판단하게 된다.\n무작위 표본과 일반화 가능성\n통계적 검정은 무작위 추출을 전제로 한다. 만약 표본이 편향되어 있다면, 아무리 p값이 작고 유의미한 차이가 나타나더라도, 그 결과를 전체 모집단에 일반화하기는 어렵다. 예컨대, 운동선수만을 대상으로 한 체중 감량 약 실험 결과를 일반 성인에게 그대로 적용할 수는 없다.\np값에 대한 올바른 이해\np값은 흔히 오해되는 개념이다. 정확히 말하면 p값은 다음과 같은 의미를 가진다. ”귀무가설이 참일 때, 관측된 차이보다 크거나 같은 차이가 나올 확률”. 즉, p &lt; 0.05는 ”이 차이가 단지 우연히 발생했을 가능성이 5% 미만이다.”를 의미하지만, 귀무가설이 5% 확률로 참이라는 뜻은 아니다. 마찬가지로 대립가설이 95% 확률로 맞다는 뜻도 아니다. 이는 매우 흔한 오해이므로 주의가 필요하다.\n\n\n4. 인과관계와 상관관계\n통계의 중요한 목적 중 하나는 **인과관계(causality)**를 밝히는 것이다. 즉, 하나의 변수(요인)가 다른 변수에 영향을 미치는지를 확인하는 것이다.\n\n”최저임금이 실업률에 영향을 미치는가?”\n”교육 수준이 소득에 영향을 미치는가?”\n”운동이 체중에 영향을 주는가?”\n\n이런 질문에 답하려면 단순한 상관관계(correlation) 이상의 것이 필요하다.\n상관관계는 인과관계가 아니다.\n통계에서 가장 널리 알려진 격언 중 하나는 바로 이것이다. 단지 두 현상이 같이 움직인다고 해서, 그 중 하나가 다른 하나의 원인이라는 보장은 없다.\n아이스크림 판매량과 익사 사고 수는 함께 증가한다. → 하지만 이는 여름이라는 제3 요인에 의한 것이다.\n커피 소비량과 심장 질환의 상관관계가 있다고 해도, 커피가 원인인지, 생활 습관이 원인인지 알기 어렵다.\n따라서, 두 변수 사이의 관계가 **단순한 공변성(co-movement)**인지, 아니면 실제 원인과 결과 관계인지 구분하는 것이 중요하다.\n인과관계를 밝히는 3가지 방법\n통계학자들은 인과성을 추론하기 위해 다음 세 가지 접근법을 사용한다:\n1. 무작위 대조 실험 (Randomized Controlled Trials, RCTs)\n\n실험군과 대조군을 무작위로 나눈다.\n한 그룹에는 처치를 하고, 다른 그룹에는 하지 않는다.\n두 그룹 간 결과의 차이를 측정한다.\n\n이 방식은 골드 스탠다드로 간주된다. 하지만 비용이 많이 들고, 모든 상황에서 시행하기 어려운 단점이 있다.\n\n신약 임상 시험\n교육 방법 비교 실험\n\n2. 자연 실험 (Natural Experiments)\n자연적으로 무작위에 가까운 배정이 일어나는 경우를 분석한다.\n\n어떤 주에서는 최저임금이 인상되었고, 인접한 주에서는 그대로인 경우 → 두 지역 간 실업률 변화를 비교\n자연 실험은 실험의 통제가 불가능할 때 대안으로 유용하다.\n\n3. 회귀 분석 (Regression Analysis)\n\n통제 변수들을 포함해 두 변수 간의 관계를 통계적으로 추정\n단순히 상관관계를 넘어서, 다른 변수들을 고정한 상태에서 한 변수의 변화가 다른 변수에 미치는 영향을 평가\n\n(예) 교육 수준, 나이, 경력, 거주지 등을 통제한 후 교육이 소득에 미치는 효과를 추정\n예시: 최저임금과 실업률\n어떤 통계 분석 결과는 ”최저임금이 오르면 실업률도 오른다”는 양의 상관관계를 보여줄 수 있다. 하지만 이것만으로 ”최저임금 인상이 실업을 초래한다”고 단정할 수 없다. 우리는 다음과 같은 질문을 던져야 한다:\n\n이 관계가 다른 요인에 의한 것은 아닌가?\n소매업이나 외식업처럼 특정 산업에 국한된 현상은 아닌가?\n이 관계가 청년층 또는 저숙련 노동자에게만 국한되는 것은 아닌가?\n\n따라서 단순한 상관관계는 불충분하며, 회귀 분석이나 자연 실험, 무작위 대조 실험 등을 통해 인과성의 정밀한 확인이 필요하다.\n도전 과제: 역인과성과 누락 변수\n역인과성(reverse causality)\n\nA가 B를 유발한 것이 아니라 B가 A를 유발한 것일 수 있음\n예: 건강한 사람이 운동을 더 많이 한다 → 운동이 건강을 유발한 것처럼 보일 수 있음\n\n누락 변수 편향(omitted variable bias)\n\nA와 B 사이의 관계가 제3의 변수 C 때문일 수 있음\n\n예: 교육과 소득 사이의 관계 → 실제로는 지능이나 부모의 사회경제적 지위 등 다른 요인이 영향을 줄 수 있음\n결론: 인과성은 신중하게 판단해야 한다.\n”하나가 다른 것에 영향을 준다”는 결론을 내리려면 단순한 수치 이상의 근거가 필요하다.\n\n적절한 분석 설계\n가능한 통제 변수\n인과성이 실제 존재하는지에 대한 논리적 설명\n\n이런 요소들이 함께 고려되어야 한다. 인과적 추론은 사회과학에서 특히 중요하지만, 오해도 가장 많다. 통계는 이러한 인과관계를 분석하는 도구일 뿐이며, 판단의 책임은 분석가에게 있다.\n\n\n5. 인과 관계에 대한 해석\n통계학의 핵심 목적 중 하나는 인과관계(causal relationship)를 밝히는 것이다. 즉, 하나의 요인이 다른 결과에 직접적인 영향을 주는지를 확인하는 일이다. 예를 들어, 다음과 같은 질문들은 모두 인과적 추론을 요구한다:\n\n최저임금 인상이 실업률에 영향을 주는가?\n교육 수준이 소득 수준을 결정하는가?\n운동이 체중 감소에 기여하는가?\n\n이러한 질문은 단순한 상관관계(correlation)를 넘어서는 분석을 필요로 한다.\n상관관계와 인과관계는 다르다.\n”상관관계는 인과관계가 아니다”는 통계학의 기본 원칙이다. 두 변수의 값이 함께 움직인다고 해서, 한 변수가 다른 변수의 원인이라고 볼 수는 없다.\n예를 들어, 아이스크림 판매량과 익사 사고 수가 여름에 함께 증가한다고 해서, 아이스크림이 익사의 원인이라고 말할 수는 없다. 이는 제3의 요인, 즉 기온에 의한 공통 변화일 수 있다.\n인과관계를 밝히는 대표적 방법\n\n무작위 대조 실험 (RCT): 실험 대상자들을 무작위로 두 그룹으로 나누고, 한 그룹에만 처치를 가해 차이를 관찰하는 방법이다. 가장 신뢰도 높은 방법이지만, 현실적 제약(비용, 윤리 문제)으로 인해 항상 적용할 수 있는 것은 아니다.\n자연 실험: 정책 변화, 제도 개편 등 자연적으로 무작위성이 주어지는 상황을 활용해 인과관계를 분석한다. 예를 들면, 어떤 주에서는 최저임금을 인상했고, 인접 주는 유지한 경우 → 실업률 변화를 비교\n회귀 분석: 여러 변수의 영향을 동시에 고려하여, 특정 변수(예: 교육 수준)의 변화가 결과 변수(예: 소득)에 독립적으로 미치는 영향을 추정하는 방법이다. 이는 통제 변수를 활용해 인과 구조에 접근한다.\n\n인과 분석의 도전 과제\n\n역인과성(reverse causality): 결과라고 생각한 것이 실제로는 원인일 수 있다. 예: 건강한 사람이 운동을 많이 하는 경우, 운동이 건강을 유발한 것처럼 보일 수 있다.\n누락 변수 편향(omitted variable bias): 실제 인과에 영향을 주는 제3의 변수가 빠질 경우, 두 변수 간 관계는 잘못 해석될 수 있다. 예: 교육과 소득의 관계에 지능이나 가정환경이 개입하는 경우"
  },
  {
    "objectID": "notes/survey/delphi_ahp_conjoint.html",
    "href": "notes/survey/delphi_ahp_conjoint.html",
    "title": "조사방법론. 7. 델파이, AHP, 컨조인트분석",
    "section": "",
    "text": "chapter 1. 델파이 조사\n\n1. 개념\n델파이 방법(Delphi Method)은 의사결정과 미래 예측을 위해 전문가의 의견을 체계적이고 신뢰할 수 있는 방식으로 수렴하는 방법이다. 이 방법은 다양한 분야에 걸쳐 활용되며, 특히 불확실한 미래 상황을 전망하는 데 있어 중요한 도구로 자리 잡고 있다. 델파이 방법은 전문가들의 의견을 익명으로 수집하고, 반복적인 피드백 과정을 거쳐 점진적으로 의견의 합의를 도출하는 방식으로 이루어진다.\n델파이 방법은 1948년 미국 RAND 연구소에서 처음 개발되었다. 제2차 세계대전 이후 급변하는 기술 환경과 군사 전략 수립의 필요성에 대응하기 위해, 미래의 전쟁 양상과 기술 발전 방향을 예측할 목적으로 고안된 것이다. 초기에는 핵전쟁 가능성, 무기 체계 개발, 군사 전략 등의 국방 관련 주제가 주로 다루어졌지만, 1960년대 이후에는 정보기술, 연구개발(R&D), 교육, 사회과학 등 다양한 분야로 적용 영역이 확대되었다.\n델파이 방법은 종종 브레인스토밍 회의나 일반 설문 조사와 혼동되지만, 이들과는 중요한 차이를 지닌다. 브레인스토밍은 참가자 간의 자유로운 의견 개진을 장점으로 하지만, 발언권이 강한 소수의 의견에 전체 논의가 왜곡될 위험이 존재한다. 반면, 델파이 방법은 응답자의 익명성을 보장함으로써 이러한 영향을 최소화하고, 개별 전문가의 독립적인 판단을 유지한 채 집단적인 통찰을 이끌어낸다.\n일반 설문 조사와도 구별된다. 설문 조사는 대체로 현재 상황이나 개인의 만족도를 파악하는 데 초점이 맞춰져 있다. 이에 반해, 델파이 방법은 전문가의 미래 예측 능력을 활용하여 향후 변화 가능성과 대응 방안을 도출하는 데 중점을 둔다. 예를 들어, 단순히 “현재 서비스에 대한 만족도”를 묻는 질문은 델파이 방식에 부적절하다. 대신, “향후 10년 내 이 산업에서 핵심적인 변화 요인은 무엇인가?”와 같은 전망 중심의 질문이 보다 적합하다.\n결국 델파이 방법은 단순한 의견 수렴이나 자료 조사 이상의 성격을 가진다. 반복적이고 구조화된 피드백 과정을 통해 전문가들의 견해를 정제하고, 특정 개인의 영향력에서 벗어난 객관적 합의를 도출함으로써 미래를 예측하는 데 효과적인 수단이 된다. 이는 브레인스토밍이나 일반 설문과는 본질적으로 다른 방식이며, 특히 전략 수립과 정책 기획, 기술 예측 등 고차원의 의사결정 상황에서 그 진가를 발휘한다.\n\n\n2. 델파이 방법의 핵심 개념\n전문가 의견 탐색 및 수렴\n델파이 방법은 특정 주제에 대해 전문가들의 의견을 체계적으로 탐색하고 반복적으로 수렴하는 과정을 통해 최적의 결론을 도출하는 기법이다. 이 과정은 단순히 여러 사람의 의견을 모으는 것이 아니라, 각 전문가의 독립적인 판단을 익명으로 수집하고, 피드백을 제공하며, 반복적으로 의견을 조정해 나가는 구조를 가진다.\n이 방법은 “다수의 전문가 의견이 단일 전문가보다 더 신뢰할 수 있다”는 가정에 기반하고 있으며, 특히 개별 의견 간의 편향이나 극단적 견해를 완화하고 집단 지성을 활용함으로써 보다 객관적이고 실현 가능성 높은 결론을 도출하는 데 유리하다. 따라서 델파이 방법은 정답이 명확하지 않거나 예측이 어려운 문제 상황에서 합리적 집단 합의에 도달하는 데 매우 효과적인 도구로 활용된다.\n설문지를 통한 의견 수렴\n델파이 방법에서는 전문가들이 일련의 설문지를 통해 반복적으로 의견을 제시하고 조정한다. 이 설문은 한 번으로 끝나는 것이 아니라, 2차, 3차에 걸쳐 반복적으로 시행되며, 각 라운드 후에는 집단의 응답 경향이 요약된 피드백이 제공된다. 이를 통해 전문가들은 다른 이들의 응답을 참고하면서 자신의 견해를 수정하거나 유지할 수 있는 기회를 갖게 된다.\n이 과정은 익명으로 진행되어, 특정 개인의 사회적 지위나 영향력이 결과에 영향을 미치는 것을 방지한다. 결과적으로, 개인의 주관적 판단보다는 집단적이고 합리적인 판단에 기반한 의견 수렴이 이루어지며, 이는 보다 신뢰성 높은 합의에 도달하도록 돕는다.\n통제된 피드백 과정\n델파이 방법의 핵심 중 하나는 통제된 피드백이다. 각 라운드가 끝난 후, 연구자는 참여 전문가들에게 다른 응답자들의 응답 분포나 평균, 중앙값 등 요약된 통계 정보를 제공한다. 이를 통해 전문가들은 자신의 견해가 전체 집단의 의견과 어느 정도 일치하거나 차이가 나는지를 확인할 수 있다.\n이러한 피드백은 단순한 정보 제공이 아니라, 의견의 수정과 재조정을 유도하는 통제된 메커니즘이다. 특히 극단적이거나 소수의견은 다수의 경향을 참고하면서 점차 조율되고, 집단 전체가 보다 합리적이고 신뢰할 수 있는 합의점으로 수렴하게 된다. 이 과정은 무작위적인 토론이 아닌, 체계적이고 점진적인 의견 정제의 역할을 한다.\n익명성 보장\n델파이 방법은 모든 응답 과정을 익명으로 진행하여, 특정 개인의 의견이 다른 전문가들의 판단에 직·간접적으로 영향을 미치는 것을 차단한다. 이는 집단 토론에서 흔히 발생할 수 있는 편승 효과(Bandwagon Effect)나 후광 효과(Halo Effect)와 같은 심리적 편향을 줄이는 데 중요한 역할을 한다.\n예를 들어, 명망 있는 전문가의 발언이 다른 참여자들에게 과도한 영향을 미치는 것을 방지함으로써, 각 전문가가 자신의 전문성과 판단에 근거한 독립적인 응답을 할 수 있도록 돕는다. 이러한 익명성은 델파이 방법의 객관성과 신뢰성을 높이는 핵심 요소 중 하나다.\n반복적 절차\n델파이 방법은 단회성 조사에 그치지 않고, 여러 차례 반복되는 의견 수렴 과정을 통해 전문가들의 판단을 점진적으로 정제해 나가는 특징을 갖는다. 각 회차에서는 이전 라운드의 요약 결과와 통계적 피드백이 제공되며, 전문가들은 이를 참고하여 자신의 응답을 유지하거나 수정할 수 있다.\n이러한 반복적 절차를 통해 극단적인 의견은 완화되고, 전문가 집단 내 합의에 가까운 결론에 도달하게 된다. 이 과정은 보통 2~4회 정도 반복되며, 응답의 변화 폭이 줄어들고 의견이 수렴되면 종료된다. 즉, 반복적 피드백과 응답 조정을 통해 보다 신중하고 타당한 합의된 전망이나 판단을 얻을 수 있다.\n\n\n3. 델파이 방법 절차\n\n(1) 문제 정의\n델파이 방법의 첫 단계는 연구 주제를 명확히 설정하고, 그 목적을 구체화하는 것이다. 이는 전체 조사 과정의 방향을 결정짓는 핵심 단계로, 무엇을 예측하고자 하는지, 어떤 결론을 도출하려는지를 명확히 해야 한다.\n예를 들어, “향후 10년간 인공지능이 고등교육에 미치는 영향”이라는 주제를 설정할 경우, 단순한 기술 동향을 넘어 교육 제도, 교사 역할 변화, 학습 방식의 진화 등 다양한 측면을 고려할 필요가 있다.\n문제 정의가 불명확하거나 포괄적이면, 전문가들이 일관된 방향으로 의견을 제시하기 어려워지고, 의견 수렴도 분산될 수 있다. 따라서 명확하고 구체적인 조사 목적과 기대 성과를 설정하는 것이 성공적인 델파이 조사의 출발점이다.\n\n\n(2) 응답 패널 구성\n델파이 방법에서 응답 패널의 구성은 조사 결과의 질을 좌우하는 핵심 요소이다. 패널은 보통 15~35명 규모로 구성되며, 최소 4명의 전문가는 반드시 포함되어야 한다. 이는 다양한 관점을 반영함과 동시에 의견의 신뢰성을 확보하기 위함이다.\n패널 구성을 위해서는 예상 응답률(일반적으로 60~80%)을 고려하여 여유 있는 인원을 사전에 확보해야 한다. 예를 들어, 25명의 유효 응답을 목표로 할 경우, 최소 30~40명의 잠재 패널 명단을 준비하는 것이 바람직하다.\n또한 패널 참여자는 조사 목적을 정확히 이해하고 있어야 하며, 해당 분야에서의 전문성과 경험을 갖춘 인물이어야 한다. 이는 단순한 의견이 아닌, 전문적 통찰이 반영된 정제된 응답을 유도하기 위해서다.\n패널 구성 시 연구자는 다음 기준을 고려해야 한다.\n\n다양한 하위 분야 또는 시각을 대표할 수 있는 구성\n이질성과 동질성 간의 균형 유지\n반복적 참여가 가능한 일정 여유\n\n적절하게 구성된 패널은 의견 수렴 과정의 신뢰성을 높이고, 보다 정확한 미래 예측과 정책 제언을 가능하게 한다.\n\n\n(3) 설문지 구성 및 문항 검증\n델파이 방법에서 설문지는 전문가의 의견을 체계적이고 명확하게 수렴하기 위한 핵심 도구이다. 따라서 설문지는 문제 정의에 부합하면서도 응답자의 이해를 돕고 부담을 줄이는 방향으로 설계되어야 한다.\n문항 구성 시 다음의 원칙을 따른다.\n\n개방형 문항: 선택 보기 구성이 어려운 탐색적 질문이나 주관적인 의견을 수렴할 때는 개방형 문항을 사용한다. 이는 전문가의 자유로운 사고를 유도하고, 다양한 아이디어를 수집하는 데 유리하다.\n선택형 문항: 선택지가 명확한 경우, 객관식 문항을 사용하여 응답자의 부담을 줄이고 응답 일관성을 높인다. 반복 회차에서는 초기 응답을 바탕으로 선택지를 구성할 수 있다.\n\n설문지는 사전 검토 및 문항 검증 절차를 반드시 거쳐야 한다. 이를 위해 보통 5인 이내의 관련 분야 전문가를 대상으로 사전 조사(pilot test)를 실시하고, 다음과 같은 사항을 점검한다.\n\n문항이 모호하거나 중복되지는 않는지\n질문의 의도와 어휘가 명확한지\n문항 순서가 논리적이며 응답 흐름에 적절한지\n불필요한 문항은 없는지 또는 중요한 문항이 누락되지 않았는지\n\n이 과정을 통해 설문지의 타당성(validity)과 명확성(clarity)을 확보할 수 있으며, 응답자의 이해도와 응답률을 높이는 데 기여한다. 궁극적으로 정제된 설문지는 반복 회차마다 신뢰도 높은 의견 수렴을 가능하게 하고, 델파이 연구의 질을 결정짓는 핵심 요소가 된다.\n\n\n\n\n\n\n\n\n\n\n\n\n(4) 1차 설문조사 실시 및 분석\n1차 설문조사는 전문가 패널에게 최종적으로 확정된 설문지를 송부함으로써 시작된다. 이때 조사의 목적, 회차 구성, 응답 방식, 일정(예: 2주 이내 회신 요청) 등이 명확하게 안내되어야 하며, 전자메일, 우편, 또는 온라인 플랫폼을 통해 진행될 수 있다.\n1. 설문 실시 방법\n\n설문 참여에 대한 동의를 얻고, 응답의 익명성과 기밀성을 보장한다.\n설문 기간과 회신 방법을 구체적으로 안내한다.\n온라인 설문 시스템(예: Google Forms, LimeSurvey 등)을 활용하면 관리가 용이하다.\n\n2. 응답 자료 분석\n\n빈도분석을 통해 각 문항의 응답 분포를 확인하고, 중복되거나 비효율적인 문항 제거를 고려한다.\n개방형 문항의 응답은 내용분석(content analysis)을 통해 핵심 키워드와 공통된 의견을 도출하고, 이를 선택형 보기로 재구성하여 다음 회차 설문에 반영한다.\n\n3. 응답 일치도 분석 기준\n델파이 방법은 의견 수렴의 정도(합의 정도)를 확인하는 데 중점을 둔다. 이를 위해 다음과 같은 통계 지표를 활용하여 응답의 일치도를 판단한다: 응답 일치도\n\n리커트 척도 : IQR 1이하, 변동계수 (=표준편차/평균) 0.5 이하\n비율 척도 : 변동계수 0.5 이하\n객관식 선택 문항 : 50%~75% 이상\n\nIQR(Interquartile Range)는 중앙 집중성을 확인하는 지표로, 값이 작을수록 의견이 모여 있음을 의미한다. 변동계수(CV)는 상대적 분산 정도를 나타내며, 평균에 대한 표준편차의 비율로 계산된다. 객관식 선택 문항은 특정 선택지에 응답이 집중되어 있는지를 통해 합의 여부를 판단할 수 있다.\n\n\n\n\n\n\n\n\n\n\n\n\n(5) 2차 설문지 구성\n2차 설문지는 1차 조사 결과를 바탕으로 정교하게 구성된다. 이 과정에서는 응답 일치도가 낮은 문항을 중심으로 표현 방식이나 선택지를 조정하여 응답자의 이해를 돕는다. 특히 개방형 문항에서 도출된 주요 응답을 기반으로 객관식 보기 항목을 구성함으로써, 명확하고 구조화된 응답을 유도할 수 있다.\n또한 1차 설문 결과 요약을 함께 제공하여, 전문가들이 다른 응답자들의 평균적 견해를 참고한 뒤 자신의 응답을 조정할 수 있도록 한다. 이를 통해 응답 간의 극단적 편차를 줄이고, 보다 높은 합의 수준에 도달할 수 있다. 2차 설문은 단순한 반복이 아니라, 의견 수렴을 위한 정제와 피드백 조정의 핵심 단계로 작용한다.\n\n\n\n\n\n\n\n\n\n\n\n\n(6) 2차 설문조사 실시\n2차 설문은 수정된 문항을 반영하여 전문가 패널에게 재송부하는 단계로, 설문은 전자메일이나 우편 등의 방식으로 배포되며, 조사 목적과 일정이 명확하게 안내된다. 이 단계에서는 전문가들이 1차 조사 결과를 바탕으로 자신의 의견을 재검토하고 조정할 기회를 갖는다.\n회수된 응답은 통계적으로 분석되어 응답 일치도를 다시 평가하며, 의견의 수렴 정도를 확인한다. 특히, 의견 분산이 줄어들었는지, 특정 문항에 대한 합의가 형성되었는지를 중점적으로 파악한다. 필요에 따라 추가적인 반복 조사를 고려할 수 있다.\n\n\n(7) 최종 결과 보고서 작성\n파이 조사의 마지막 단계는 전체 조사 과정을 정리하고, 응답 분석 결과 및 전문가 의견의 수렴 과정을 체계적으로 요약한 최종 보고서를 작성하는 것이다. 이 보고서에는 설문 설계 및 조사 절차, 각 라운드에서 수집된 응답의 통계적 분석 결과, 의견 변화의 양상, 최종적으로 도출된 합의 내용을 포함해야 한다.\n작성된 결과 보고서는 정책 결정, 전략 수립, 연구 기획, 기술 예측 등 다양한 분야에서 의사결정의 근거 자료로 활용될 수 있다. 델파이 결과 제시방법은 다음과 같다.\n\n중요도 척도 : 최빈값 &gt; 중위값과 IQR (Inter Quartile Range)\n비율척도 : 중위값 &gt; 평균, IQR (Inter Quartile Range)\n선택 보기문항의 경우 빈도 백분율 (%) 표시\n\n\n\n\n\n\n\n\n\n4. 한계\n첫째, 미래에 대한 평가 절하 현상이 자주 발생한다. 인간의 사고방식은 현재의 상황을 과도하게 중요하게 여기고, 미래의 변화 가능성을 충분히 반영하지 못하는 경향이 있다. 전문가들조차도 현재를 기준으로 미래를 바라보기 때문에 혁신적이거나 급격한 변화보다는 점진적인 변화를 예상하는 경우가 많다. 따라서 델파이 방법을 사용할 때는 미래를 단순한 연장선으로만 평가하지 않도록 유도할 필요가 있다.\n둘째, 단순화 경향이 나타난다. 전문가들은 복잡한 사회·경제적 요인을 충분히 반영하기보다 특정 변수나 트렌드만을 독립적으로 분석하는 경향이 있다. 이는 예측의 실용성을 높이는 장점도 있지만, 실제로 상호작용하는 다양한 요소들을 간과하게 만들어 예측의 정확성을 떨어뜨릴 수 있다. 특히, 시스템적인 사고 없이 개별 요소만 고려하면 현실과 동떨어진 결과가 나올 가능성이 높다.\n셋째, 전문성의 한계와 비현실적인 전망도 문제로 작용할 수 있다. 델파이 방법이 전문가의 집단 지성을 활용하는 방식이긴 하지만, 모든 전문가가 동일한 수준의 통찰력을 갖고 있는 것은 아니다. 일부 전문가들은 자신의 경험과 직관에 의존하여 근거가 부족한 예측을 제시하기도 하고, 반대로 지나치게 이상적인 전망을 내놓기도 한다. 이러한 의견이 전체 결과에 영향을 미칠 경우, 실질적으로 활용하기 어려운 예측이 도출될 위험이 있다. 따라서 전문가 선정 과정에서 균형 잡힌 시각과 충분한 경험을 갖춘 인물을 포함하는 것이 중요하다.\n마지막으로, 질문 형식의 명확성과 목적성이 핵심적인 역할을 한다. 델파이 방법에서 활용되는 질문은 예측의 방향성을 결정하는 중요한 요소이므로, 모호한 표현을 피하고 하나의 질문이 하나의 명확한 주제를 다루도록 구성해야 한다. 질문이 불명확하면 전문가들의 응답이 일관성을 잃거나 지나치게 광범위해질 수 있으며, 이로 인해 최종적인 합의 도출이 어려워질 수 있다.\n결론적으로, 델파이 방법이 효과적인 예측 도구가 되기 위해서는 미래의 변화 가능성을 충분히 고려하고, 단순화를 경계하며, 전문가 의견의 신뢰성을 검토하고, 질문 설계를 체계적으로 진행해야 한다. 이를 보완할 수 있다면 델파이 방법은 보다 정교하고 신뢰도 높은 미래 예측을 가능하게 할 것이다.\n\n\n\nchapter 2. AHP 방법\nAHP(Analytic Hierarchy Process)는 Thomas Saaty(1980)가 제안한 계량적 의사결정 기법으로, 가치 평가 및 복잡한 의사결정 문제 해결을 위한 방법론이다. 이 기법은 계층구조 원리, 우선순위 결정 원리, 일관성 원리를 기반으로 의사결정 대안을 평가하며, 다양한 대안을 다수의 목표와 비교하여 최적의 선택을 도출하는 데 활용된다. AHP는 두 가지 핵심 과정을 통해 문제를 분석한다.\n첫째, 정성적 혹은 무형적 특성을 상대적 비율 척도를 이용해 수량화하여 평가할 수 있도록 한다.\n둘째, 복잡한 문제를 점차 작은 요소로 분해하여 이원 비교를 수행함으로써 보다 단순한 형태로 의사결정을 진행한다.\n이를 위해 의사결정 대안을 평가할 수 있는 요소들을 계층 구조로 구성하며, 각 계층의 요소들은 평가자의 지식, 경험, 직관을 바탕으로 상대적으로 비교된다. 이러한 비교 과정을 통해 의사결정 대안을 수량화하여 최적의 대안을 선택할 수 있도록 지원하는 것이 AHP의 핵심 원리이다.\n\n1. AHP 주요 특징\nAHP는 복잡한 의사결정 문제를 체계적으로 분석하고 해결하기 위한 방법론으로, 여러 대안 중에서 최적의 선택을 도출하기 위해 다음과 같은 절차적 특징을 갖는다.\n우선, AHP는 문제를 구조화하기 위해 계층적 구조를 구성한다. 이는 가장 상위 수준의 의사결정 목표에서 출발하여, 그 아래 기준(criteria), 하위기준(sub-criteria), 그리고 최종적으로 대안(alternatives)으로 이어지는 방식이다. 이러한 계층 구조는 문제를 명확하게 시각화하고, 각 구성 요소 간의 관계를 체계적으로 분석할 수 있도록 한다.\n다음으로, AHP는 쌍대비교(pairwise comparison)를 통해 각 요소의 상대적 중요도를 평가한다. 이때 Saaty가 제안한 1~9의 정수 척도를 사용하여 두 요소 간 우선순위를 수치화하며, 이를 통해 각 요소의 가중치(weight)를 산출한다. 이 과정은 정성적 판단을 정량적으로 변환하는 데 핵심적인 역할을 한다.\n이렇게 도출된 비교 결과는 판단 행렬(judgment matrix)로 구성되며, 이에 대해 고유값 분석(eigenvalue analysis)을 실시하여 일관성을 검토한다. 특히 일관성 지수(Consistency Index, CI)와 일관성 비율(Consistency Ratio, CR)을 계산하여 응답의 논리적 일관성이 유지되고 있는지를 평가하고, 수용 가능한 범위(CR &lt; 0.1)를 넘을 경우 재검토가 요구된다.\n마지막으로, 계층 구조의 각 수준에서 산출된 가중치를 종합하여 각 대안의 종합 점수를 계산한다. 이 점수를 바탕으로 가장 높은 평가를 받은 대안을 최적의 선택으로 결정하게 된다. 이를 통해 AHP는 복수의 기준과 복잡한 평가 요소를 고려한 합리적 의사결정을 가능하게 한다.\n\n\n2. AHP 기본 전제\nAHP는 의사결정 과정에서 계층 구조 원리, 우선순위 결정 원리, 일관성 원리를 기본 전제로 한다.\n먼저, 계층 구조 원리는 복잡한 의사결정 문제를 보다 작은 요소로 분해하여 구조화하는 개념이다. 이를 통해 목표, 평가 기준, 하위 기준, 대안 등을 체계적으로 정리할 수 있다.\n둘째, 우선순위 결정 원리는 평가 요소 간 상대적인 중요도를 비교하여 가중치를 산출하는 과정이다. 이를 위해 이원 비교를 수행하며, 각 요소의 중요도를 수량화하여 최적의 의사결정을 도출할 수 있도록 한다.\n셋째, 일관성 원리는 의사결정 과정에서 논리적 일관성을 유지하도록 검증하는 절차이다. AHP는 일관성 비율을 활용하여 평가자의 판단이 논리적으로 타당한지 확인하고, 신뢰할 수 있는 결과를 도출할 수 있도록 한다.\n이러한 원리를 기반으로 AHP 방법을 적용하면, 정성적 평가 요소에 대한 가중치를 산정할 수 있으며, 이를 통해 평가 요소들의 우선순위를 체계적으로 결정할 수 있다.\n\n\n3. AHP 절차\nAHP는 의사결정 문제를 체계적으로 분석하고 최적의 대안을 도출하기 위해 다음과 같은 절차를 따른다.\n① 먼저, 의사결정 문제를 계층화하는 과정이 이루어진다. 이를 위해 의사결정과 관련된 평가 요소들을 목표, 기준, 하위 기준, 대안 등으로 구조화하여 계층적 모델을 구축한다. 이를 통해 복잡한 문제를 보다 명확하게 정리하고 분석할 수 있는 기반을 마련한다.\n② 다음 단계에서는 전문가 설문조사를 통해 평가 요소 간의 상대 비교 데이터를 수집한다. 평가자는 계층 내 각 요소들을 쌍(pair)으로 비교하여 상대적 중요도를 판단하며, 이 데이터를 바탕으로 상대 비교 행렬이 생성된다.\n③ 이후, 상대 비교 행렬을 이용하여 평가자의 응답 일관성을 검토하고 상대적 가중치를 계산한다. 이를 위해 일관성 지수(Consistency Index, CI)와 일관성 비율(Consistency Ratio, CR)을 활용하여 응답의 논리적 일관성을 평가한다. 평가자가 2인 이상인 경우, 일관성을 갖춘 응답자들의 상대 비교 행렬을 종합하여 단일 상대 비교 행렬을 도출하는 과정이 추가로 수행된다.\n④ 마지막으로, 평가 대상이 되는 여러 대안들에 대한 우선순위를 산정한다. 이를 위해 각 의사결정 요소의 상대적 가중치를 종합하여 최종적으로 가장 적합한 대안을 선정할 수 있도록 한다.\n\n\n4. 계층화\nAHP 방법에서 가장 중요한 단계는 의사결정과 관련된 요소들을 계층화하는 과정이다. 계층화란 시스템을 구성하는 각 특성이나 속성을 기준으로 분할된 집단을 형성하는 과정으로, 하나의 집단이 특정한 하위 집단에만 영향을 주고, 동시에 상위 집단으로부터만 영향을 받는 구조를 의미한다.\n계층 구조의 최상층에는 가장 포괄적인 의사결정 목표가 위치하며, 그 아래에는 목표 달성에 영향을 미치는 다양한 기준과 요소들이 하위 계층으로 배치된다. 계층이 낮아질수록 요소들은 보다 구체적인 특성을 가지게 되며, 각 계층 내 요소들 간에는 상호 비교가 가능해야 한다.\n계층 설정 시 고려해야 할 사항\n첫째, 계층의 완전성과 비완전성을 고려해야 한다. 하위 계층의 모든 요소가 직계 상위 계층의 모든 항목과 관련될 경우 완전한 계층이라 하며, 그렇지 않을 경우 비완전한 계층이라고 한다. Ramanujam과 Saaty(1981)는 AHP를 활용할 때 모든 계층을 반드시 완전하게 구조화할 필요는 없다고 주장하였으며, 일부 계층이 비완전하더라도 의사결정 과정에 큰 영향을 미치지 않는다고 보았다.\n둘째, 계층 내 평가 요소의 개수가 적절하게 설정되어야 한다. 인간은 동시에 9개의 대상을 비교할 수 있는 인지적 한계를 가지며, 비교 항목의 개수가 10개 이상이 되면 응답의 일관성을 유지하기 어려워진다. 따라서 Saaty(1980)는 계층 내 요소의 개수를 5개~9개 정도로 유지하는 것이 적절하며, 평가 요소 간 상대적 중요도를 비교할 때는 9점 척도를 사용하는 것이 바람직하다고 제안하였다.\n계층화의 기본 원칙과 단계\n최상위 계층에는 의사결정의 목표를 설정한다.\n\n중간 계층에는 목표 달성에 영향을 미치는 주요 평가 요소(기준 및 하위 기준)를 배치한다.\n최하위 계층에는 최종적으로 평가할 여러 대안을 포함한다.\n계층이 낮을수록 요소들은 보다 구체적이어야 하며, 계층 내 요소들 간에는 비교가 가능해야 한다.\n각 계층의 요소들은 직계 하위 집단에만 영향을 미치며, 동시에 상위 계층으로부터만 영향을 받는다.\n\n이러한 계층화 과정을 통해 복잡한 의사결정 문제를 구조적으로 정리할 수 있으며, 이를 기반으로 AHP를 활용한 체계적인 평가와 의사결정이 가능해진다.\n\n\n5. 상대비교 행렬 및 일관성 비율\nAHP에서 계층 구조가 완성되면, 각 계층 내 평가 요소들의 상대적 중요도를 평가하기 위해 상대 비교 행렬을 작성한다. 상대 비교 행렬은 대칭 행렬의 형태를 가지며, 행렬의 차수는 평가 요소의 개수를 의미한다.\n절차\n\n상대 비교 행렬을 구성하여 평가 요소 간 상대적 중요도를 비교한다.\n상대 비교 행렬의 대각 원소는 1이며, 상·하 대칭 원소는 역수 관계를 가진다.\n응답자의 판단 일관성을 검토하기 위해 일관성 지수(CI)를 계산한다.\n난수 지수(RI)를 활용하여 일관성 비율(CR)을 구하고, CR이 10% 미만이면 일관성이 확보된 것으로 판단한다.\n\n상대비교 행렬\n평가 요소를 \\(i,j\\)쌍으로 비교할 때, 평가 요소 \\(i\\)의 상대적 중요도를 \\(w_{i}\\), 평가 요소 \\(j\\)의 상대적 중요도를 \\(w_{j}\\)라 하면, 상대 비교 행렬의 원소는 \\(a_{ij} = \\frac{w_{i}}{w_{j}}\\)로 정의된다. 이때, 상대 비교 행렬은 다음과 같은 형태를 가진다.\n\\[A = \\begin{bmatrix}\na_{11} & a_{12} & \\cdots & a_{1p} \\\\\na_{21} & a_{22} & \\cdots & a_{2p} \\\\\na_{31} & a_{32} & \\cdots & a_{3p} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{p1} & a_{p2} & \\cdots & a_{pp}\n\\end{bmatrix}\\]\n대각 행렬 원소: \\(a_{ii} = 1\\) (자기 자신과의 비교는 항상 1)\n상대 비교 값: \\(a_{ij}(i \\neq j)\\)는 기준 평가 요소 \\(i\\)가 평가 요소 \\(j\\)에 비해 중요하다고 판단되는 정도를 의미하며, Saaty(1980)가 제안한 1~9 척도를 사용하여 값을 입력한다. (예: 1/9, 1/8, …, 1, 2, …, 9)\n대응 원소 관계: \\(a_{ji} = \\frac{1}{a_{ij}}\\)\n일관성 지수\n상대 비교를 통한 평가에서 응답자의 판단에는 다소간의 논리적 오류가 발생할 수 있다. 이를 보완하기 위해 일관성 지수를 활용하여 응답의 일관성을 검토한다.\n상대 비교 행렬이 완전히 일관성을 가지려면 \\(a_{ij} \\cdot a_{jk} = a_{ik}\\) 관계가 성립해야 하며, 상대 비교 행렬의 최대 고유치(\\(\\lambda_{\\max}\\))는 평가 요소의 개수 \\(n\\)과 동일하게 된다. 이를 활용하여 Saaty(1980)는 일관성 지수를 다음과 같이 정의하였다.\n\\(CI = \\frac{\\lambda_{\\max} - n}{n - 1}\\).\n일관성이 높은 경우: 응답자의 판단이 논리적으로 일관됨을 의미하며, 결과의 신뢰도가 높다.\n일관성이 낮은 경우: 응답자의 판단 과정에서 일관성이 유지되지 않았음을 의미하며, 결과의 신뢰도가 낮아진다.\n일관성 비율 (Consistency Ratio, CR)\nSaaty(1980)는 상대 비교를 9점 척도로 수행할 경우, 평가 요소의 개수에 따라 사용할 수 있는 난수 지수를 제시하였다. 이를 활용하여 일관성 비율을 계산할 수 있다.\n\\(CR = \\frac{CI}{RI}\\): CR 값이 10% 미만이면 일관성이 확보된 응답으로 간주한다.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n차수 (n)\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\nRI\n0\n0\n0.58\n0.9\n1.12\n1.24\n1.32\n1.41\n1.45\n\n\n\n\n\n6. 상대 중요도 계산\n상대 비교 행렬을 통해 계층 내 평가 요소들의 상대적 중요도를 평가하고 일관성을 검증한 후, 최종적으로 상대적 중요도(가중치)를 산출하는 과정이 진행된다.\n먼저, 상대 비교 행렬로부터 최대 고유치(\\(\\lambda_{\\max}\\))를 구하고, 이에 대응하는 고유 벡터를 도출한다. 고유 벡터의 각 원소는 평가 요소의 상대적 중요도를 나타내며, 모든 원소의 합이 1이 되도록 정규화하여 최종 가중치를 산출한다. 이를 위해 고유 벡터의 각 원소를 원소 합으로 나누어 정규화된 중요도 값을 얻는다.\n계층이 2개 이상인 경우\n의사결정 과정에서 일반적으로 계층은 2개 이상으로 구성된다. 이 경우, 각 계층별로 동일한 방법을 반복하여 평가 요소들의 가중치를 산출한다. 최하위 계층의 평가 요소에 대한 최종 가중치는 자신이 속한 계층의 가중치와 상위 계층의 평가 요소 가중치를 곱하여 계산된다. 즉, 상위 계층의 가중치는 하위 계층으로 전달되며, 이러한 과정이 반복되어 최종적으로 평가 요소들의 가중치가 결정된다. 이를 활용하여 의사결정의 대안들을 평가하고 최적의 선택을 도출할 수 있다.\n다수의 평가자가 존재하는 경우\n의사결정 과정에서 평가자가 2명 이상인 경우, 개별적으로 도출된 상대 비교 행렬을 종합하여 단일 그룹 상대 비교 행렬을 생성하는 과정이 필요하다. 이때 사용되는 대표적인 방법은 다음과 같다.\n1. 그룹 평가 방법(Delphi 방법)\n평가자들의 의견을 종합하여 하나의 상대 비교 행렬을 직접 작성하는 방법이다.\n전문가 의견을 반복적으로 조정하여 합의된 평가 결과를 도출하는 것이 특징이다.\n2. 개별 평가 후 그룹 전체 상대 비교 행렬 산출 방법\n각 평가자의 상대 비교 행렬을 개별적으로 작성한 후, 이를 바탕으로 그룹 전체의 단일 상대 비교 행렬을 생성하는 방식이다.\nSaaty(1980)는 이 방법에서 각 평가자의 상대 비교 행렬의 원소에 대해 기하 평균(geometric mean)을 적용하여 단일 비교 행렬을 생성하는 방법을 제안하였다.\nSaaty(1980)의 기하 평균 방법은 계산이 간편하며, 행렬의 역수성을 유지할 수 있다는 장점이 있어 가장 널리 사용된다. 이후 개별 상대 비교 행렬로부터 그룹 전체 상대 비교 행렬을 얻는 다양한 방법이 연구되었지만, Saaty(1995)는 각 방법 간 결과 차이가 거의 없음을 주장하였다.\n결론적으로, AHP를 통한 상대 중요도 산출 과정에서는 일관성 검증을 거쳐 신뢰성을 확보한 후, 고유 벡터를 이용하여 가중치를 계산하며, 다수 평가자의 의견을 반영할 경우 기하 평균을 활용하는 방법이 가장 효과적으로 평가된다.\n\n\n7. AHP 활용 사례\n네트워크 환경에서 발생하는 사이버 위협의 위협 수준을 정량적으로 산정하기 위해 계층 분석 기법을 적용할 수 있다. 사이버 위협 평가는 주로 정성적 요소를 기반으로 이루어지며, 주요 평가 항목으로는 감염 대상 획득, 감염 경로, 감염 시 증상, 방어 조치 난이도, 피해 자산 유형 등이 포함될 수 있다. 이러한 평가 요소들은 객관적인 정량화가 어려운 특성을 지니므로, AHP를 활용하여 계층적 구조를 설정하고 상대적 가중치를 산출함으로써 체계적인 위협도 평가가 가능하다.\n최상위 계층: 사이버 위협도 산정을 위한 핵심 평가 요소(감염 대상 획득, 감염 경로, 감염 시 증상, 방어 조치 난이도, 피해 자산 유형)\n중간 계층: 각 5개 평가 요소별 하위 평가 기준\n최하위 계층: 사이버 위협의 개별 사례(대안), 즉 위협 발생 사례별 위협도 점수\n여기서는 최상위 계층의 평가 요소 5개에 대한 상대적 가중치를 부여하는 방법을 설명한다. 동일한 방법을 적용하여 하위 계층에서도 각 항목의 가중치를 산정할 수 있다.\n\n\n\n\n\n사이버 위협 관련 전문가 3명을 대상으로 설문 조사를 실시하여 상대비교 행렬을 얻었다. ID 1번, 2번 응답자의 일관성 비율이 10% 미만이었으므로, 이들만 응답의 일관성을 유지하였음을 알 수 있다.\n\n\n\n\n\n평가 일관성을 유지한 ID 1번, 2번 응답자의 상대비교 행렬을 이용하여 단일 상대비교 행렬을 구하면 아래와 같다. 단일 상대비교 행렬은 Saaty(1980)가 제안한 기하평균 방법을 이용하여 구해졌다.\n\n\n\n\n\n단일 상대비교 행렬의 최대 고유치는 5.127이므로 일관성 지수 CI=0.032이고, 일관성 비율 CR=2.8%이다. 상대비교 행렬의 최대 고유치에 대응하는 고유벡터는 다음과 같다. 각 평가요소의 가중치는 고유벡터의 합을 구하고, 대응하는 평가요소의 고유벡터 값을 합으로 나눈 값이다. 결과를 해석해 보면, 네트워크상에서 사이버 위협이 발생했을 때 피해예상자산이 감염경로에 비해 2배 더 위협적이라는 할 수 있다.\n\n\n\n\n\n\n\n\nchapter 3. 컨조인트 분석\n컨조인트 분석(Conjoint Analysis)은 소비자의 선호를 평가하고 예측하기 위해 설계된 다변량 통계 기법이다. 이는 제품 또는 서비스의 속성과 각 속성의 수준에 대한 소비자의 선호도를 분석하는 방법으로, 특히 마케팅, 제품 기획, 가격 결정 등의 분야에서 널리 활용된다.\n컨조인트 분석의 기본 개념은 소비자가 제품을 개별 속성 단위가 아니라, 속성들의 조합(conjoint)으로 인식하고 평가한다는 점에 있다. 따라서 이 기법은 소비자의 실제 선택 과정과 유사한 환경을 모형화하여, 개별 속성이 소비자의 선택에 미치는 영향을 분해하고 추정하는 데 초점을 맞춘다.\n\n1. 컨조인트 분석 개념\n\n(1) 목적\n컨조인트 분석은 제품 및 서비스 기획에서 중요한 의사결정을 내리는 데 활용되며, 주요 목적은 다음과 같다.\n\n독립변수(속성)의 상대적 중요도를 분석하여 특정 제품이나 서비스의 평가 요소를 도출한다.\n종속변수(소비자 선호도)에 미치는 영향을 정량적으로 측정하여 소비자가 무엇을 중요하게 여기는지를 파악한다.\n최적의 속성 조합을 찾아 기업의 제품 기획 및 마케팅 전략에 반영하여 경쟁력을 강화한다.\n\n\n\n(2) 개념\n컨조인트 분석은 소비자가 제품의 다양한 속성을 어떻게 평가하고 선택하는지를 분석하기 위한 방법이다. 이 기법은 평가자들이 제시된 대안에 대해 응답한 결과를 바탕으로 각 속성 수준의 효용(utility)을 추정할 수 있도록, 요인 설계(factorial design)를 기반으로 분석을 수행한다.\n실제 조사에서는 제품이나 서비스의 대안을 여러 속성의 조합으로 구성하여 제시하고, 소비자가 이들 조합 중 어떤 방식을 선호하는지를 평가하게 한다. 이를 통해 최적의 속성 조합을 도출하고, 제품 개발이나 마케팅 전략 수립에 활용할 수 있다.\n’컨조인트(Conjoint)’라는 용어는 ’Consider’와 ’Jointly’의 합성어로, 소비자가 여러 속성을 함께 고려하여 평가한다는 개념을 반영하고 있다.\n컨조인트 분석과 다차원척도법(Multidimensional Scaling) 비교\n컨조인트 분석과 다차원척도법은 모두 소비자의 심리적 판단을 정량적으로 측정하기 위한 기법이라는 공통점을 가진다. 그러나 두 방법은 평가 대상을 제시하는 방식과 분석 초점에서 차이를 보인다.\n\n컨조인트 분석은 제품이나 서비스의 속성을 요인 설계 방식으로 조합한 후, 응답자가 어떤 조합을 선호하는지 평가하게 함으로써 속성별 효용(utility)을 추정한다. 즉, 사전에 정의된 속성 조합을 바탕으로 선호 구조를 분석하는 데 중점을 둔다.\n다차원척도법(MDS)은 소비자가 느끼는 제품이나 브랜드 간의 유사성 또는 선호도 차이를 바탕으로, 그 관계를 저차원의 공간에 시각화하는 방법이다. 평가자는 구체적인 속성보다는 전체적 인상이나 거리감을 기준으로 판단하게 되며, 분석 결과는 점 간의 거리로 나타난다.\n\n요약하면, 컨조인트 분석은 속성 기반의 선택 분석에, MDS는 심리적 거리 기반의 포지셔닝 분석에 적합한 방법이다.\n\n\n(3) 고려 사항\n\n종속변수 측정상의 문제: 응답자의 선호나 선택을 어떻게 정량화할 것인지 명확히 설정해야 한다. 예를 들어, 순위(rank), 평점(rating), 선택(choice) 방식 중 어떤 방식을 택할지에 따라 분석 기법이 달라지고, 결과 해석에도 영향을 미친다.\n독립변수 결합상의 문제: 독립변수(속성)들을 어떤 방식으로 조합해 제시할 것인지가 분석의 핵심이다. 전수조합(full-profile design)은 정보가 많지만 부담이 크고, 부분요인 설계(fractional factorial design)는 효율적이지만 정보 손실 위험이 있다. 따라서 연구 목적과 응답자의 인지 부담을 균형 있게 고려해 설계해야 한다.\n\n\n\n(4) 기본원리\n제품이나 서비스는 여러 속성의 조합으로 구성되며, 각 속성은 여러 수준을 가질 수 있다. 예를 들어, 스마트폰을 고려할 경우 다음과 같은 속성과 수준이 존재할 수 있다.\n\n\n\n\n\n\n\n속성(Attribute)\n수준(Levels)\n\n\n화면 크기\n5인치, 6인치, 7인치\n\n\n배터리 용량\n3000mAh, 4000mAh, 5000mAh\n\n\n가격\n50만원, 70만원, 90만원\n\n\n\n\n\n\n2. 컨조인트 분석 관련 용어 정리\n1. Attribute (속성)와 Level (수준)\n속성: 제품이나 서비스가 가지는 독립변수로, 소비자가 고려하는 주요 특성이다.\n수준: 각 속성이 취할 수 있는 값. 컨조인트 분석에서는 최소 두 개 이상의 수준을 가져야 하며, 일반적으로 4~5개 이내로 설정하는 것이 적절하다.\n예제: 휴대폰의 속성이 3개(화면크기, 배터리 용량, 가격)이고, 각 속성이 3 개의 수준을 가진다면, 가능한 제품 조합 수는 \\(3 \\times 3 \\times 3 = 27\\)개이다.\n2. 종속변수 dependent variable\n응답자의 선호도: 소비자가 특정 속성 조합을 선호하는 정도를 나타내는 값으로, 컨조인트 분석에서 최적의 제품 설계를 위한 핵심 정보가 된다.\n3. 주효과 main effects\n독립변수(속성)가 종속변수에 미치는 직접적인 영향을 의미한다. 특정 속성이 전체 선호도에 어느 정도 기여하는지 평가할 수 있다.\n\n\n3. 컨조인트 분석 모형\n\n(1) 이론적 배경\n컨조인트 분석의 이론적 기초는 효용 이론과 선택 모델에 기반을 둔다. 주요한 이론적 요소는 다음과 같다.\n(1) 효용 이론(Utility Theory)\n컨조인트 분석은 선택 대안의 속성이 개인의 효용에 미치는 영향을 분석하는데, 이는 경제학과 행동과학에서 널리 연구된 효용 이론을 바탕으로 한다. 효용 이론에서 소비자는 주어진 대안 중 최대 효용을 제공하는 대안을 선택한다고 가정한다. 즉, 제품이나 서비스의 속성이 소비자에게 부여하는 효용 값을 평가하고, 이를 통해 소비자의 선호도를 모델링 한다.\n(2) 분해적 접근(Decompositional Approach)\n컨조인트 분석은 전체적인 선택을 기반으로 개별 속성이 미치는 영향을 역추정 하는 분해적 접근을 따른다. 즉, 소비자가 직접적으로 속성의 중요도를 평가하는 것이 아니라, 속성이 조합된 여러 대안을 비교하면서 나타나는 선택 행태를 분석하여 속성별 중요도를 추론한다.\n(3) 다속성 효용 모형(Multi-Attribute Utility Model)\n컨조인트 분석에서는 다속성 효용 모형을 사용하여 개별 속성이 전체 효용에 미치는 기여도를 평가한다. 가장 일반적인 효용 함수는 선형 가법적 형태로 표현된다.\n\n\n(2) 전통적 컨조인트 분석\n전통적 컨조인트 분석은 다속성 효용 모형을 기반으로 하며, 소비자가 제품을 선택할 때 각 속성이 독립적으로 효용을 제공한다고 가정한다.\n(1) 가법적 효용 모형(Additive Utility Model)\n\\[U(X) = \\beta_{0} + \\beta_{1}X_{1} + \\beta_{2}X_{2} + \\ldots + \\beta_{k}X_{k} + \\varepsilon\\]\n\\(U(X)\\): 소비자가 선택한 대안 \\(X\\)의 총 효용값\n\\(X_{1},X_{2},\\ldots,X_{k}\\): 제품의 속성들\n\\(\\beta_{0}\\): 상수항\n\\(\\beta_{1},\\beta_{2},\\ldots,\\beta_{k}\\): 각 속성의 가중치(효용 값)\n\\(\\varepsilon\\): 오차항(random error)\n위 모형은 각 속성이 독립적으로 효용을 제공하며, 그 합이 전체 제품의 효용을 결정한다는 가정을 따른다.\n(2) 수행 절차\n① 속성 및 수준 선정\n연구자가 제품의 주요 속성과 각 속성의 수준을 결정한다. 예를 들어, 스마트폰의 속성을 다음과 같이 설정할 수 있다.\n\n\n\n\n\n\n\n속성(Attribute)\n수준(Levels)\n\n\n가격\n100만원, 120만원, 140만원\n\n\n배터리 용량\n3000mAh, 4000mAh, 5000mAh\n\n\n브랜드\n삼성, 애플, 샤오미\n\n\n\n② 실험 설계\n총 조합 개수는 27개가 되어 가능한 모든 조합을 평가하는 것은 비효율적이므로, 부분(1/3) 요인 설계를 사용하여 9개의 대표적인 제품 프로필을 선정한다.\n\n\n\n\n\n\n\n\n가격\n배터리 용량\n브랜드\n\n\n100만원\n3000mAh\n삼성\n\n\n100만원\n4000mAh\n애플\n\n\n100만원\n5000mAh\n샤오미\n\n\n120만원\n3000mAh\n샤오미\n\n\n120만원\n4000mAh\n삼성\n\n\n120만원\n5000mAh\n애플\n\n\n140만원\n3000mAh\n애플\n\n\n140만원\n4000mAh\n샤오미\n\n\n140만원\n5000mAh\n삼성\n\n\n\n③ 데이터 수집\n적정 응답자 수: \\(n \\geq \\frac{1000 \\times c}{a \\times t}\\) Johnson & Orme (1996)의 경험적 공식 (예제) \\(n \\geq \\frac{1000 \\times 3}{3 \\times 9} = \\frac{3000}{27} = 111.1 \\approx 112\\)\n\\(c\\): 각 속성의 최대 수준 개수\n\\(a\\): 하나의 제품 프로필에 포함된 속성 개수\n\\(t\\): 한 명의 응답자가 평가하는 제품 프로필 수\n종속변수: 응답자는 10점 척도나 선호여부(0, 1)로 응답한다.\n독립변수: \\(X_{1} = \\text{가격 120만원},X_{2} = \\text{가격 140만원},\\)\\(X_{3} = \\text{배터리 4000mAh},X_{4} = \\text{배터리 5000mAh},\\)\\(X_{5} = \\text{브랜드 애플},X_{6} = \\text{브랜드 샤오미}\\).\n④ 모형 추정 및 해석\n10점 척도는 OLS 추정, 선호 여부는 로짓회귀로 추정한다. 추정 결과가 다음과 같다면 소비자가 제품의 어떤 속성을 가장 중요하게 생각하는지 정량적으로 헤석할 수 있다.\n\n\n\n\n\n\n\n\n속성\n수준\n효용 값 (추정)\n\n\n가격\n120만원\n0.3333\n\n\n가격\n140만원\n-1.3333\n\n\n배터리\n4000mAh\n2\n\n\n배터리\n5000mAh\n2.5\n\n\n브랜드\n애플\n-1.3333\n\n\n브랜드\n샤오미\n-0.6667\n\n\n\n\n가격이 높아질수록 선호도가 감소 (120만원까지만 소폭 증가)\n배터리 용량이 커질수록 선호도 증가\n삼성 브랜드가 가장 선호됨\n최적의 제품 조합: 120만원 X 5000mAh X 삼성 브랜드\n\n\n\n(3) 적응형 컨조인트 분석(Adaptive Conjoint Analysis, ACA)\n적응형 컨조인트 분석은 응답자의 선택에 따라 설문이 동적으로 조정되는 방식의 컨조인트 분석 기법으로 전통적 컨조인트 분석과 달리, 모든 응답자에게 동일한 제품 프로필을 제시하는 것이 아니라, 응답자의 초기 선호도를 기반으로 이후 질문이 맞춤형으로 제시되는 방식을 사용한다.\n응답자의 피로도를 줄일 수 있음 (불필요한 속성 조합을 제외)\n높은 차원의 속성을 포함할 수 있음 (속성이 많아도 설문이 복잡해지지 않음)\n개인 맞춤형 분석이 가능 (응답자마다 다른 질문을 받을 수 있음)\n따라서 제품의 속성 개수가 많거나, 개인별 선호도 차이가 클 경우 적응형 컨조인트 분석이 더욱 효과적이다.\n① 이론적 모형\n전통적 컨조인트 분석과 동일한 가법적 효용 모형을 사용한다.\n\\[U(X) = \\beta_{0} + \\beta_{1}X_{1} + \\beta_{2}X_{2} + \\ldots + \\beta_{k}X_{k} + \\varepsilon\\]\n전통적 컨조인트 분석에서는 모든 응답자가 동일한 속성 조합을 평가하는 반면, 적응형 컨조인트 분석에서는 응답자의 이전 응답을 반영하여, 이후 질문이 동적으로 조정한다. 각 응답자에게 가장 의미 있는 질문을 집중적으로 제시하여 보다 효율적인 데이터를 수집할 수 있다.\n② 설문방식\n1. 속성별 중요도 평가\nQ1. 가격이 얼마나 중요한가요? 5점 척도\nQ2. 배터리 용량이 중요한가요? 5점 척도\nQ3. 브랜드 선호도가 있는가요? 브랜드 3개\n2. 속성 수준 비교 질문: 응답자의 답변을 바탕으로 맞춤형 제품 조합을 생성하여 비교 질문을 제시한다.\nQ. 다음 두 가격 수준 중 어느 것이 더 선호됩니까?\nA: 100만원 B: 120만원\nQ. 배터리 용량 중 어느 수준을 선호합니까?\nA: 4000mAh B: 5000mAh\n3. 제품 프로필 비교: 이전 응답을 바탕으로, 응답자가 비교하기 쉬운 2개의 제품 프로필을 생성하여 선택하도록 한다.\n예: ”다음 두 제품 중 하나를 선택하세요.”\nA 제품: 120만원, 5000mAh, 삼성\nB 제품: 100만원, 3000mAh, 애플\n개별 응답자의 선호도에 따라 불필요한 속성 조합은 제거되므로 응답 피로도가 낮고 개별 응답자의 선호도를 정밀하게 측정할 수 있어 개인 맞춤형 마케팅이 가능하나 설문 설계가 복잡하고, 분석이 어렵다.\n③ 추정방법\n(방법1) 전통적 컨조인트 분석 OLS(최소 자승법) 동일\n(방법2) 계층적 베이지안 추정 (Hierarchical Bayesian, HB)\n개별 응답자의 효용 값을 베이지안 추론을 이용하여 추정하고 모집단 평균과 개별 차이를 반영하여 신뢰성 높은 효용 값을 도출한다.\n\n\n(4) 선택기반 컨조인트 분석(Choice-Based Conjoint, CBC)\n선택기반 컨조인트 분석은 소비자가 여러 개의 제품 옵션 중 하나를 선택하는 방식으로 데이터를 수집하는 컨조인트 분석 기법이다. 이는 소비자의 실제 구매 행동과 가장 유사한 방법으로 설계되었으며, 시장 점유율 예측, 최적의 제품 조합 분석, 가격 민감도 측정 등 다양한 마케팅 전략 수립에 활용된다.\nCBC의 가장 큰 특징은 응답자가 단순히 제품의 속성에 점수를 부여하거나 순위를 매기는 것이 아니라, 실제 구매 결정을 내리는 것처럼 여러 제품 중 하나를 선택하는 방식으로 응답한다는 점이다. 따라서 CBC는 소비자의 실질적인 선택 행동을 반영하는 데 유리하며, 보다 현실적인 소비자 선호 데이터를 제공할 수 있다.\n추정은 이산 선택 모델을 기반으로 하며, 소비자가 특정 제품을 선택할 확률을 예측하는 방식으로 이루어진다. 이를 위해 다항 로짓 모형 또는 혼합 로짓 모형과 같은 확률적 선택 모델이 적용될 수 있다. 이러한 모델들은 응답자의 선택 데이터를 바탕으로 개별 속성들이 소비자의 최종 선택에 미치는 영향을 분석하며, 제품의 가격, 기능, 브랜드 등의 속성 간 상대적인 중요도를 정량적으로 평가할 수 있도록 한다.\n① 이론적 배경\n이산 선택 모형을 기반으로 하며, 소비자가 여러 개의 대안 중 하나를 선택하는 방식을 따른다. 이는 경제학에서 사용하는 랜덤 효용 모형을 적용하여, 소비자가 가장 높은 효용(Utility)을 제공하는 제품을 선택한다고 가정합니다.\n효용 함수: 소비자가 제품 \\(i\\) 를 선택할 확률은 효용 함수를 통해 결정됩니다.\n\\(U_{i} = V_{i} + \\varepsilon_{i}\\), 여기서 \\(U_{i}\\): 제품 \\(i\\) 의 총 효용, \\(V_{i}\\): 관측 가능한 속성들의 가중 합, \\(\\varepsilon_{i}\\): 오차항 ~ (정규 분포 또는 로지스틱 분포 가정)\n특히, 효용 값 \\(V_{i}\\) 는 제품의 속성 값과 해당 속성의 가중치의 선형 조합으로 표현됩니다.\n\\(V_{i} = \\beta_{0} + \\beta_{1}X_{1} + \\beta_{2}X_{2} + \\ldots + \\beta_{k}X_{k}\\), 여기서\n\\(X_{1},X_{2},\\ldots,X_{k}\\): 제품 속성 값\n\\(\\beta_{1},\\beta_{2},\\ldots,\\beta_{k}\\): 속성별 효용 계수\n② 설문방식\n사례에서 27개 조건 혹은 9개 조건 중 3개를 임의로 선정하여 응답자에게 보여주고 선호하는 제품을 선택한다.\n③ 추정방법: 다항 로짓 모형(Multinomial Logit Model, MNL)\n이 모형은 소비자가 특정 제품을 선택할 확률을 예측하는 방식으로 동작한다.\n\\(P(i) = \\frac{e^{V_{i}}}{\\sum_{j}^{}e^{V_{j}}}\\), 여기서 \\(P(i)\\): 소비자가 제품 i 를 선택할 확률, \\(e^{V_{i}}\\): 제품 \\(i\\) 의 효용 값을 지수 함수로 변환한 값, \\(\\sum_{j}^{}e^{V_{j}}\\): 모든 제품의 효용 값의 합이다."
  },
  {
    "objectID": "notes/survey/nonresponse.html",
    "href": "notes/survey/nonresponse.html",
    "title": "조사방법론. 3. 무응답과 대체",
    "section": "",
    "text": "chapter 1. 무응답 개요\n설문조사에서 무응답(nonresponse)은 표본으로 선정된 일부 응답자가 질문에 답하지 않는 현상을 의미한다. 이러한 무응답이 발생하면, 추정값이 편향될 수 있으며 표본이 모집단을 제대로 대표하지 못하게 되어 분석 결과의 신뢰성이 떨어질 가능성이 높다.\n무응답자가 응답자와 다른 특성을 가진다면, 응답자만을 대상으로 계산한 추정값은 모집단 전체의 실제 값과 차이를 보일 수 있다. 특히 무응답이 일정한 방향이나 경향성을 가지는 경우, 이는 단순한 오류를 넘어서 설계 전체에 체계적인 편향을 초래할 수 있으며, 이를 무응답 편향(nonresponse bias)이라고 한다.\n무응답이 특정 집단에 집중될 경우, 표본이 모집단의 다양한 특성을 충분히 반영하지 못하게 된다. 예를 들어, 소득이 높은 사람이나 바쁜 직장인처럼 일정한 특성을 가진 집단의 응답률이 낮다면, 이들의 의견은 과소 대표되거나 완전히 누락될 수 있다. 그 결과 표본의 대표성이 손상된다.\n또한 무응답이 무작위로 발생한다면 통계적으로 큰 문제가 되지 않을 수 있지만, 현실에서는 특정 문항이나 주제에 따라 일관된 응답 회피가 발생하는 경우가 많다. 예를 들어 정치적 성향에 대한 질문에서 특정 지지층이 응답을 회피한다면, 결과는 특정 방향으로 왜곡될 수 있다.\n무응답으로 인해 유효한 응답 수가 줄어들면, 전체 표본의 크기가 감소하게 되어 통계 추정의 분산이 커지고 표본 오차가 증가한다. 이는 결과의 정확성을 떨어뜨리고, 신뢰구간을 넓혀 결과 해석에 불확실성을 더하게 된다.\n결과적으로 무응답이 많을수록 조사 결과를 일반화하기 어려워지고, 정책 결정이나 연구 활용 시 신뢰할 수 없는 정보를 제공하게 될 가능성이 커진다. 따라서 설문조사에서 무응답을 최소화하고, 발생한 무응답을 적절히 보정하는 절차가 중요하다.\n단위 무응답 unit nonresponse\n무응답이 발생하는 방식 중 하나는 단위 무응답(unit nonresponse)으로, 이는 표본으로 선정된 사람이 조사에 전혀 응답하지 않는 경우를 의미한다. 이 경우, 조사 대상자는 모든 질문에 답하지 않기 때문에 해당 응답자는 전체적으로 누락된다. 예를 들어 어떤 조사 대상자가 전화를 받자마자 “나는 설문조사에 절대 참여하지 않는다. 다시 전화하지 말라”고 말하며 응답을 거부하는 경우, 이는 전형적인 단위 무응답에 해당한다.\n항목 무응답 item nonresponse\n무응답이 전체 문항이 아닌 일부 질문에서만 발생하는 경우를 항목 무응답(item nonresponse)이라고 한다. 이는 응답자가 특정 질문에 대해서만 답변을 하지 않거나 회피하는 상황을 말한다.\n예를 들어 조사자가 “작년 총 가구 소득이 얼마였습니까?”라고 묻자, 응답자가 “모르겠다. 아내가 그런 기록을 관리한다”고 답하며 해당 항목에 대한 응답을 유보하는 경우, 이는 항목 무응답에 해당한다.\n\n\nchapter 2. 단위 무응답 유형\n무응답은 여러 가지 이유로 발생할 수 있으며, 크게 세 가지 유형으로 구분할 수 있다.\n첫째, 조사 요청 전달 실패는 조사자가 표본으로 선정된 대상자에게 조사 요청을 전달하지 못하는 경우를 의미한다. 이는 조사 대상자를 물리적으로 찾지 못하거나, 우편으로 보낸 설문지가 반송되는 상황 등이 포함된다. 이러한 경우에는 조사 자체가 이루어지지 않으므로 단위 무응답으로 처리된다.\n둘째, 응답 거부는 조사 대상자가 조사 요청을 인지했음에도 불구하고 설문 참여를 명확히 거절하는 경우에 해당한다. 전화나 방문을 통해 접촉이 이루어졌지만, 개인적인 이유, 조사에 대한 불신, 시간 부족 등의 이유로 응답을 거부하는 경우가 여기에 포함된다.\n셋째, 응답 불가능은 조사 대상자가 설문 문항을 이해하지 못하거나, 인지적 또는 신체적인 이유로 응답할 수 없는 경우를 의미한다. 예를 들어 고령자나 언어적 장벽이 있는 사람이 질문의 의미를 제대로 파악하지 못해 답변을 제공하지 못하는 상황이 해당된다.\n\n1. 조사 요청 전달 실패로 인한 단위 무응답\n비접촉 또는 조사 요청 전달 실패로 인한 무응답은 조사 대상자가 특정한 데이터 수집 방식으로는 접근이 어려운 경우에 발생한다. 이와 같은 상황에서 중요한 개념은 접근 가능성이며, 이는 조사자가 표본으로 선정된 대상자에게 실제로 연락하거나 조사를 수행할 수 있는지를 의미한다. 조사 대상자의 연락처 정보가 부정확하거나, 반복적인 접촉 시도에도 불구하고 연결되지 않는 경우, 설문 참여 여부와 무관하게 조사가 이루어지지 않게 된다.\n가구 설문조사에서 접근 가능성 문제\n조사자가 응답자가 집에 머무는 시간을 알고 있다면, 단 한 번의 시도로도 성공적으로 접촉할 수 있다. 그러나 실제 조사에서는 표본 대상자의 접근 가능한 시간이 사전에 알려져 있는 경우가 드물다. 이로 인해 조사자는 동일한 대상자에게 여러 차례 연락을 시도해야 하며, 이는 조사 비용과 시간이 증가하는 원인이 되기도 한다.\n접근이 어려운 사례\n조사 요청 전달이 실패하는 주요 원인 중 하나는 물리적인 접근의 제한이다. 이는 외부인의 접촉을 차단하는 환경에서 자주 발생하며, 예를 들어 출입이 통제된 아파트 건물이나 자동 응답 전화 시스템이 설치된 경우 조사자가 대상자에게 직접 접근하기 어렵다.\n우편을 통한 설문조사에서는 설문지가 대상자에게 도달하더라도, 발신자를 알 수 없는 우편물을 열어보지 않고 폐기하는 사람들은 응답에서 자연스럽게 누락될 가능성이 높다.\n전화 조사에서도 비슷한 문제가 발생한다. 예를 들어 집에 머무는 시간이 거의 없는 사람들은 조사자가 여러 차례 전화를 시도하더라도 연결되지 않을 가능성이 크다. 또한 발신자 번호를 차단하거나 필터링하는 서비스를 이용하는 경우, 조사원의 연락 시도 자체가 인지되지 않아 응답으로 이어지지 않을 수 있다.\n첫번 째 시도에서 조사 성공률이 가장 높다.\n성공적인 연락률은 통화 시도 횟수가 늘어날수록 점차 감소하는 경향이 있으며, 이러한 감소는 종종 지수적인 형태를 따른다. 특히 가구를 대상으로 한 설문조사에서는 첫 번째 접촉을 성사시키기 위해 몇 차례의 전화 시도가 필요한지를 결정하는 데 다음과 같은 두 가지 요인이 주요하게 작용한다.\n첫째, 통화가 이루어지는 시간대는 응답률에 영향을 미친다. 일반적으로 저녁 시간이나 주말에 걸려온 전화는 평일 낮 시간에 비해 응답률이 더 높은 것으로 나타난다. 이는 많은 사람들이 해당 시간대에 집에 머물 가능성이 높기 때문이다.\n둘째, 모집단의 특성에 따라 접근 가능성이 다르게 나타날 수 있다. 예를 들어 직업, 생활 패턴, 주거 형태 등에 따라 어떤 모집단은 조사자와의 연락이 상대적으로 더 용이한 반면, 또 다른 모집단은 접근이 더 어려울 수 있다. 이러한 차이는 조사 설계와 연락 전략에 반영될 필요가 있다.\n우편, 이메일, 웹 설문조사\n인터뷰어가 직접 접촉하지 않는 방식의 조사는, 조사 요청이 표본 대상자에게 지속적으로 노출될 수 있도록 설계되어야 한다. 예를 들어 우편 설문조사의 경우, 설문지가 일단 가구에 도착하면 응답 여부와 관계없이 일정 기간 동안 가구 내에 그대로 남아 있게 된다. 이로 인해 가구 구성원은 자신이 원하는 요일이나 시간대에 설문에 응답할 수 있다.\n이러한 방식에서는 전화나 방문 조사와 달리, 조사자가 능동적으로 접촉을 시도하지 않더라도 응답이 이루어질 가능성이 생긴다. 따라서 직접적인 접촉을 필요로 하는 조사 방식과는 달리, 표본 단위가 첫 접촉에 이르기까지 필요한 시도 횟수나 시간의 분포가 서로 다른 양상을 보인다.\n\n\n2. 응답거부로 인한 단위 무응답\n설문조사의 성공 여부는 응답자가 낯선 조사자의 요청에 자발적으로 응할 의사가 있는지에 크게 좌우된다. 응답자가 설문에 참여하기 위해서는 몇 가지 심리적 조건이 충족되어야 한다.\n우선, 조사자로부터 신체적 또는 경제적 피해를 입을 것이라는 두려움이 없어야 한다. 응답자는 자신에게 어떠한 해도 가해지지 않을 것이라는 확신을 가져야 한다.\n또한, 응답 과정에서 자신의 평판이 손상될 가능성을 걱정하지 않아야 하며, 조사 참여가 사회적 이미지에 부정적인 영향을 미치지 않을 것이라고 느껴야 한다.\n설문 참여로 인해 심리적 스트레스를 겪을 수 있다는 불안도 응답을 가로막는 요인이 될 수 있다. 따라서 조사는 응답자에게 부담을 주지 않는 방식으로 진행되어야 한다.\n기밀 보장에 대한 신뢰 역시 중요한 요소이다. 응답자는 조사자가 제공한 정보 보호 약속을 믿을 수 있어야 하며, 자신의 응답 내용이 외부로 유출되지 않을 것이라는 확신을 가져야 한다.\n마지막으로, 응답자는 자신의 의견을 자유롭게 표현할 수 있으며, 솔직한 정보를 제공하더라도 불이익을 받거나 위험에 처하지 않을 것이라고 믿을 때 비로소 진정한 응답이 이루어진다. 이러한 심리적 안정이 확보되어야만 응답자는 설문조사에 적극적으로 참여하게 된다.\n\n(1) 설문조사와 타 조사의 차이\n설문조사 요청은 사람들이 일상생활에서 경험하는 다양한 외부 요청들과 비교해볼 수 있으며, 이러한 비교는 설문 응답 행태를 이해하는 데 중요한 시사점을 제공한다. 대중이 낯선 사람으로부터 받는 요청은 일반적으로 판매 전화, 업무 또는 서비스 관련 연락, 기부 요청, 정치 활동, 그리고 설문조사 등으로 나눌 수 있다. 이들은 여러 측면에서 서로 다른 특성을 보인다.\n첫째, 사람들의 경험 빈도는 접촉 방식에 따라 달라진다. 과거에는 방문 판매가 흔했으나, 최근에는 텔레마케팅이 그 자리를 대신하고 있다. 이로 인해 낯선 사람으로부터 걸려오는 전화나 우편, 이메일 메시지는 설문조사보다는 상품이나 서비스 판매를 목적으로 하는 경우가 훨씬 더 많다. 이러한 경험의 불균형은 응답자가 설문조사 요청을 다른 상업적 요청과 혼동할 가능성을 높인다.\n둘째, 대중의 인식 수준에서도 차이가 나타난다. 기업이나 널리 알려진 자선단체, 정치 단체의 경우 많은 사람들이 해당 조직의 이름에 익숙할 수 있다. 반면, 설문조사를 실시하는 주체가 대학이나 정부 기관인 경우에도 일부 응답자는 사전에 해당 기관을 인식하고 있을 수 있지만, 그렇지 않은 경우에는 요청자가 낯설게 느껴질 수 있다.\n셋째, 인센티브의 제공 여부도 다르다. 설문조사는 응답에 대한 감사의 표시로 소정의 금전적 보상이나 선물을 제공하는 경우가 있으나, 판매나 서비스 요청에서는 일반적으로 이러한 유인책이 제공되지 않는다. 이는 설문 응답 동기를 유도하는 중요한 차별점이 될 수 있다.\n넷째, 연락의 지속성 측면에서 판매 및 모금 요청은 대체로 반복적인 시도를 하지 않는 경향이 있다. 망설이는 대상자를 설득하기보다는 새로운 대상을 찾는 것이 더 효율적이기 때문이다. 이에 반해, 설문조사 특히 확률 표본조사는 표본의 대표성을 확보해야 하므로 동일 가구에 대해 여러 차례 연락을 시도하는 경우가 많다.\n다섯째, 요청의 성격에서도 차이가 있다. 판매는 일반적으로 상품이나 서비스에 대한 금전적 지불을 요구하지만, 설문조사는 응답자의 시간과 정보를 제공받는 것이 목적이다. 설문은 응답자의 자발적인 참여가 아닌 조사자의 접근으로 시작되기 때문에, 응답자는 설문 참여보다는 자신의 원래 활동으로 돌아가는 것을 더 중요하게 여길 수 있다.\n이러한 반복적인 경험은 사람들로 하여금 설문 요청에 대해 일정한 반응을 형성하게 만든다. 즉, 응답자의 반응은 과거의 유사한 경험에서 비롯된 습관적인 대응일 수 있다. 설문 요청이 상업적 전화와 혼동되어 거절되는 경우도 자주 발생하는데, 이를 해결하기 위해서는 반복적인 연락을 통해 조사 목적을 명확히 설명하는 것이 중요하다. 또한 신뢰할 수 있는 기관이 주관하는 조사라면, 조사원이 해당 기관의 이름을 강조함으로써 상업적 목적이 아님을 분명히 할 수 있다.\n\n\n(2) 응답거절 단위 무응답 발생 요인\n설문조사에서 응답률을 높이고 무응답 편향을 줄이기 위해서는 응답 거절로 인한 단위 무응답 발생 요인을 이해하는 것이 중요하다. 이러한 요인은 조사자가 통제할 수 없는 요인과 조정 가능한 요인으로 나눌 수 있다.\n먼저, 조사자가 조정하기 어려운 요인으로는 사회적 환경과 개인 수준의 특성이 있다. 예를 들어, 대도시 지역에서는 가구 단위 설문에서 응답 거절 비율이 상대적으로 높게 나타난다. 이는 도시 생활의 익명성, 바쁜 생활 패턴, 낯선 사람에 대한 경계심 등이 영향을 줄 수 있다. 또한, 가구 구성원이 여러 명인 가정은 1인 가구보다 응답에 협조할 가능성이 높으며, 이는 집단 내의 의견 조율이나 설문 참여에 대한 심리적 부담이 분산되기 때문이다. 개인 수준에서는 남성이 여성보다 설문에 응하지 않을 가능성이 더 높은 경향이 관찰된다.\n한편, 조사자가 조정할 수 있는 요인도 있다. 대표적인 예로는 조사원 개인의 능력과 설문 설계 방식이 있다. 경험이 풍부한 조사원은 응답자의 반응에 능숙하게 대응하고 신뢰를 구축하는 데 유리하므로, 경험이 적은 조사원보다 더 높은 응답 협조율을 이끌어낼 수 있다. 또한, 응답자에게 소정의 인센티브를 제공하는 것은 응답률을 높이는 데 효과적이며, 이는 설문 참여에 대한 긍정적인 동기를 부여하는 역할을 한다.\n이와 같은 요인들을 종합적으로 고려하여 설문조사를 설계하고 실행한다면, 단위 무응답을 줄이고 보다 신뢰성 있는 결과를 얻을 수 있다.\n\n\n(3) 응답거절 관련 이론적 가설\n응답 거절을 설명하는 데 활용되는 이론적 가설은 설문조사의 응답률과 무응답 편향을 이해하는 데 중요한 통찰을 제공한다. 다음의 네 가지 가설은 응답자의 행동을 설명하는 대표적인 이론적 틀로 제시된다.\n첫째는 기회비용 가설이다. 이 가설에 따르면 바쁜 사람일수록 인터뷰나 설문조사 참여를 거부할 가능성이 높다. 이들은 일상에서 다른 활동에 할애할 시간이 부족하므로, 설문 참여를 시간 낭비로 인식하거나 부담스럽게 느낀다.\n둘째는 사회적 고립 개념이다. 이는 사회경제적 계층의 양극단에 위치한 사람들이 설문 요청에 응하지 않을 가능성이 높다는 주장이다. 지나치게 부유하거나 매우 빈곤한 계층의 경우, 사회적 제도나 기관과의 관계가 약해 설문 요청 자체를 거부하거나 무관심하게 대할 수 있다.\n셋째는 주제 관심 가설이다. 이 가설은 특정 주제에 대해 관심이 있는 사람들만이 설문에 적극적으로 응하면서, 표본이 해당 주제에 편향된 집단으로 구성될 가능성이 있다는 점을 지적한다. 이러한 경우 통계적 결과에 무응답 오류가 발생할 수 있으며, 이는 전체 모집단을 반영하지 못하는 왜곡된 결과로 이어질 수 있다.\n넷째는 과도한 설문조사 개념이다. 반복적이고 빈번한 설문 요청은 응답자의 피로도를 높이고, 결과적으로 설문 참여를 꺼리게 만드는 요인이 될 수 있다. 이는 특히 동일한 대상자에게 유사한 설문이 반복적으로 도달하는 상황에서 더욱 두드러진다.\n이러한 가설들은 설문조사 과정에서 나타나는 응답 거절 현상을 보다 정교하게 이해하고, 이를 줄이기 위한 설계 전략을 수립하는 데 유용하게 활용될 수 있다.\n\n\n(4) 레버리지-현저성 이론 leverage-salience theory\n레버리지-현저성 이론(leverage-salience theory)은 사람들이 설문 요청의 여러 속성을 서로 다르게 중요하게 여긴다는 점에 주목한다. 설문 주제, 인터뷰에 소요되는 시간, 후원 기관, 수집된 데이터의 활용 목적 등이 이에 해당하며, 응답자는 각 속성을 긍정적으로 평가할 수도 있고 부정적으로 받아들일 수도 있다.\n조사자는 사전에 어떤 속성이 응답자에게 중요한지를 알기 어렵지만, 설문이 진행되는 과정에서 특정 속성이 강조되면 응답자의 응답 여부에 영향을 미칠 수 있다. 즉, 강조된 요소가 응답자의 가치관이나 관심과 부합하면 설문에 응할 가능성이 높아지며, 반대로 불편함이나 거부감을 유발하는 경우에는 응답을 거절할 가능성이 높아진다.\n예를 들어, 첫 번째 응답자는 설문 주제에 높은 관심을 가지지만 시간이 많이 걸리는 점을 부담스럽게 느낄 수 있고, 두 번째 응답자는 주제에는 관심이 없지만 제공되는 인센티브에 긍정적인 반응을 보일 수 있다. 이때 조사자가 설문의 후원 기관이나 보상에 대해 강조한다면, 각 응답자에게 해당 요소가 설문 참여 여부를 결정짓는 기준으로 작용할 수 있다. 이러한 상황에서는 주제에 긍정적인 첫 번째 응답자가 설문에 응할 가능성이 더 높아질 수 있다.\n이 이론이 주는 주요 시사점은 설문조사에서 응답자의 다양성을 고려한 맞춤형 접근이 필요하다는 것이다. 응답자가 설문 요청을 수락하거나 거절하는 이유는 사람마다 다르고, 조사자는 이러한 이유를 조사 초기에는 파악하기 어렵다. 따라서 단일한 접근 방식만으로는 다양한 응답자의 요구와 우려를 효과적으로 해결하기 어렵다. 대신, 응답자가 긍정적으로 인식할 수 있는 속성을 파악하고 이를 적절히 강조하는 전략을 통해 응답률을 높이는 것이 효과적이다.\n\n\n\n3. 요청된 데이터를 제공할 수 없는 경우의 단위 무응답\n단위 무응답은 응답자가 설문에 응할 의사가 있음에도 불구하고, 요청된 데이터를 제공할 수 없는 경우에도 발생할 수 있다. 이러한 상황은 표본 대상자와의 접촉이 성공적으로 이루어졌더라도, 다양한 제약으로 인해 응답이 불가능한 경우를 포함한다.\n예를 들어, 일부 응답자는 설문이 제공되는 언어를 전혀 이해하지 못하거나, 질문의 내용을 이해하거나 기억에서 필요한 정보를 불러오는 것이 어려울 수 있다. 정신적인 부담이나 인지적 제약이 있는 경우도 여기에 해당된다. 또한, 건강 문제로 인해 응답이 어려운 경우도 있으며, 문해력에 제한이 있어 설문지를 읽거나 해석하는 데 어려움을 겪는 경우도 무응답으로 이어질 수 있다. 기업을 대상으로 한 조사에서는 조사 형식이 적절하지 않거나 조사에 할애할 시간이 부족하여 필요한 정보를 제공하지 못하는 경우가 발생하기도 한다.\n이와 같이 응답 불가능의 원인이 다양하기 때문에, 단위 무응답이 통계 분석에 미치는 영향도 상황에 따라 달라진다. 예를 들어, 인구의 건강 상태를 조사하는 설문에서는 건강상의 이유로 응답이 이루어지지 않는 경우가 무응답 편향을 유발할 수 있다. 건강이 좋지 않은 사람들이 조사에서 빠지게 되면, 전체 인구의 건강 수준이 실제보다 더 양호하게 추정될 가능성이 있다. 반면, 동일한 조사에서 정치적 태도나 사회적 인식을 측정하는 경우에는 같은 무응답이 결과에 미치는 영향이 상대적으로 작을 수 있다.\n따라서 이러한 유형의 무응답은 단순한 데이터 누락 이상의 의미를 가지며, 조사 주제와 분석 목적에 따라 그 영향을 면밀히 평가하고 보정할 필요가 있다.\n\n\n\nchapter 3. 응답률 계산\n\n1. 무응답이 통계 품질에 미치는 영향\n무응답으로 인해 발생하는 편향은 무응답의 원인이 조사 대상이 되는 통계적 특성과 연관될 때 더욱 심각해진다. 예를 들어, 혼자 사는 가구의 비율을 조사하는 경우를 생각해보면, 1인 가구는 집에 머무는 시간이 상대적으로 짧아 조사자가 연락을 시도했을 때 접촉이 어려울 수 있다. 이 경우 단 한 번의 연락 시도만으로는 해당 가구가 표본에서 누락될 가능성이 높으며, 결과적으로 혼자 사는 사람의 비율이 실제보다 낮게 추정될 수 있다. 반면, 반복적인 연락 시도를 통해 이러한 가구를 조사에 포함시키면 오히려 과도하게 반영되어 실제보다 높은 추정값이 나올 수도 있다. 이는 무응답이 가구의 거주 시간과 밀접하게 관련되어 있기 때문이다.\n이와 달리, 정치적 관심도와 같은 주제를 조사하는 경우에는 무응답이 통계에 미치는 영향이 상대적으로 작을 수 있다. 따라서 무응답이 초래하는 통계적 편향은 조사 주제와 무응답의 원인 간의 관련성에 따라 달라질 수 있다.\n거절 무응답 오류는 응답자가 설문에 응하지 않는 이유가 해당 통계와 관련이 있을 때 발생한다. 예를 들어, 어떤 설문조사에서 특정 인센티브가 특정 집단의 응답률을 높이는 데는 효과적일 수 있으나, 동시에 응답자의 구성이나 특성에 영향을 주어 조사 결과 자체에 변화를 일으킬 가능성도 있다.\n무응답 편향이 발생하는지를 판단하기 위해서는 응답 여부에 영향을 미치는 인과 메커니즘을 고려해야 한다. 어떤 경우에는 무응답의 원인이 조사 대상과 직접적인 관련이 없어 통계적으로 큰 문제가 되지 않기도 한다. 예를 들어, 가구의 전기 사용량을 조사할 때 집에 머무는 시간이 응답 성향에는 영향을 줄 수 있지만, 이 성향이 전기 사용량과 직접적인 관련을 가지지 않는다면 무응답은 비교적 무해한 것으로 간주할 수 있다.\n반면, 무시할 수 없는 인과 메커니즘이 작용하는 경우도 있다. HIV 유병률 조사에서는 응답자의 감염 여부 자체가 설문 참여 여부에 영향을 줄 수 있다. 이러한 경우 무응답은 단순한 비응답이 아닌 통계의 과소추정을 초래하는 심각한 편향 요인이 될 수 있다. 실제로 미국에서 HIV 유행 초기 공중보건 당국은 혈액 채취를 포함한 전국적 유병률 조사를 실시하고 금전적 보상까지 제공했으나, HIV 감염자들의 응답률은 여전히 낮았다. 기록 대조 연구에 따르면 이는 사회적 낙인으로 인해 감염자들이 설문 참여를 기피했기 때문이며, 그 결과 조사된 HIV 감염률이 실제보다 낮게 추정되는 과소추정이 발생한 것으로 나타났다.\n무응답의 영향은 조사 주제와 맥락에 따라 다르게 나타난다. 어떤 조사에서는 무응답률이 높든 낮든 결과에 큰 차이를 주지 않는 경우도 있으며, 이로 인해 일부 연구자들은 무응답률을 중요하지 않다고 간주하기도 한다. 이는 무응답률이 반드시 조사 결과의 질을 저해한다는 가정이 항상 성립하지는 않는다는 점을 보여준다.\n그러나 무응답의 원인이 조사 대상 속성과 밀접하게 연관된 경우에는 통계적 추정에 심각한 왜곡을 초래할 수 있다. 문제는 대부분의 경우 연구자가 무응답이 어떤 특성과 연관되어 발생하는지를 명확히 파악하기 어렵다는 데 있다. 이러한 불확실성 속에서 무응답의 영향을 정확히 측정하거나, 최적의 응답률을 판단하는 것은 현실적으로 쉽지 않다. 따라서 대부분의 연구에서는 조사 예산이 허용하는 범위 내에서 가능한 한 높은 응답률을 확보하려는 전략이 일반적으로 사용된다.\n\n\n2. 응답률 계산\n\n(1) 무응답 편향 계산 공식\n무응답 편향nonresponse bias 계산식: \\({\\overline{y}}_{r} - {\\overline{y}}_{s} = \\frac{m_{s}}{n_{s}}({\\overline{y}}_{r} - {\\overline{y}}_{m})\\)\n\n\\({\\overline{y}}_{s}\\): 특정 표본에서 선택된 전체 응답의 평균\n\\({\\overline{y}}_{r}\\): 해당 표본 내 응답자의 평균\n\\({\\overline{y}}_{m}\\): 해당 표본 내 무응답자의 평균\n\\(n_{s}\\): 해당 표본의 총 표본 수\n\\(r_{s}\\): 해당 표본 내 응답자의 수\n\\(m_{s}\\): 해당 표본 내 무응답자의 수\n\n무응답자의 평균, \\({\\overline{y}}_{m}\\)을 알 수 없으나 확률적인 특성을 가질 수 있으므로 이를 반영한 보다 일반적인 편향 공식은 다음과 같다.\n\\[bias({\\overline{y}}_{r}) = \\text{Cov}(r_{i},Y_{i}) + E\\left\\lbrack \\left( \\frac{m_{s}}{n_{s}} \\right)({\\overline{y}}_{r} - \\overline{Y}) \\right\\rbrack\\]\n응답 확률(\\(r_{i}\\))과 관심 변수(\\(Y_{i}\\)) 간의 공분산: 이는 응답자가 될 확률과 조사 대상 변수 간의 상관관계를 의미하며, 응답자가 될 가능성이 높은 사람이 특정한 특성을 가질 경우 편향이 발생할 수 있다.\n기대 무응답률과 전체 평균의 차이: 두 번째 항은 기대 무응답률 \\(\\frac{m_{s}}{n_{s}}\\)과 응답자 평균(\\({\\overline{y}}_{r})\\) 과 모집단 평균(\\(\\overline{Y}\\))의 차이의 곱이다. 이는 무응답자의 특성이 전체 모집단과 다를 경우 발생하는 편향을 나타낸다.\n무응답자의 특성이 더욱 뚜렷해지고 무응답률이 낮아지더라도, 무응답 오류가 반드시 감소하는 것은 아니다. 이는 무응답자의 특성이 전체 모집단과 크게 다를 경우, 단순히 응답률을 높이는 것만으로는 통계적 편향을 효과적으로 줄이기 어렵다는 점을 시사한다. 다시 말해, 응답률이 개선되었다고 하더라도 응답하지 않은 집단의 특성이 여전히 표본에서 제대로 반영되지 않는다면, 결과적으로 무응답 편향은 여전히 존재하거나 오히려 심화될 수도 있다. 따라서 단순한 응답률 향상보다는, 무응답자의 특성과 응답자의 차이를 파악하고 적절한 보정 방법을 병행하는 노력이 필요하다.\n\n\n(2) 응답율 계산 주요 이슈\n무응답률을 단순히 \\(\\frac{m_s}{n_s}\\) 의 비율로 계산하는 방식은 그 계산에 내포된 복잡성을 감추는 결과를 낳을 수 있다. 무응답률은 무응답 오류를 구성하는 주요 요소 중 하나이며, 때로는 조사 결과의 신뢰도를 높여 보이기 위해 의도적으로 무응답률을 낮게 산정하려는 경향도 나타난다. 하지만 무응답률의 정확한 계산에는 여러 가지 실질적인 문제가 존재한다.\n첫째, 표본 프레임에는 조사 대상이 아닌 단위가 포함될 수 있기 때문에, 조사 대상자의 적격성을 사전에 선별하는 과정이 필요하다. 예를 들어, 가구를 대상으로 하는 전화 조사에서 표본 프레임에 업무용 전화번호가 포함되어 있는 경우, 해당 번호의 수신자가 조사 대상에 해당하는지를 확인하기 어렵다. 이로 인해 응답률 계산 시 분모에 어떤 대상을 포함할 것인지에 대한 기준이 불확실해질 수 있다.\n둘째, 일부 표본은 클러스터 단위로 구성되어 있어, 표본 추출 단계에서 개별 조사 대상자의 수를 알기 어려운 경우가 있다. 예를 들어, 학교를 표본 단위로 설정한 후 그 안에서 학생을 조사하는 경우, 전체 클러스터가 무응답으로 분류될 때 실제 몇 명이 응답하지 않은 것인지 명확히 알기 어렵다. 이는 무응답률 산정 시 분자의 해석에 불확실성을 더한다.\n셋째, 표본 프레임에 포함된 요소들이 동일한 선택 확률을 갖지 않는 경우, 응답률 계산에 가중치를 반영해야 하는지 여부에 대한 고민이 필요하다. 예를 들어, 특정 소수 민족 집단을 초과 표집한 경우, 응답률 산정 시 이들의 과대표집 비율을 그대로 반영할 것인지 아니면 가중치를 조정할 것인지에 대한 기준이 모호할 수 있다.\n이러한 이유로 인해 응답률은 단순한 비율 수치 이상의 의미를 가지며, 무응답률의 해석과 활용에 있어 보다 세심한 접근이 요구된다.\n\n\n(3) 응답률 계산\n첫 번째와 두 번째 이슈를 해결하는 방법 중 하나는 분모의 값을 추정하는 것이다. 이때 외부 정보 또는 다른 사례에서 얻은 정보를 활용할 수 있으므로 응답률은 다음과 같이 계산될 수 있다.\n\\(\\frac{I}{I + R + NC + O + e(UH + UO)}\\), 여기서 \\(I\\)은 조사 완료자 수, \\(R\\)은 거절 및 중단, \\(NC\\)은 미접촉, \\(O\\)은 기타 적격 사례, \\(UH\\)은 조사대상자 여부가 불분명한 사례, \\(UO\\)은 기타 적격 여부가 불분명한 사례, \\(e\\)은 적격 여부가 불분명한 사례 중 적격으로 추정되는 비율이다. 변수 \\(e\\)의 추정치는 현재 진행 중인 설문조사에서 얻을 수 있다.\n\\[e = \\frac{I + R + NC + O}{I + R + NC + O + \\text{샘플에 포함된 부적격 사례}}\\]\n만약 \\(e\\)에 대한 신뢰할 수 있는 추정치를 얻을 수 없다면 \\((UH + UO)\\)을 계수 없이 분모를 사용하거나 분모에서 이를 제외하는 방식이다.\n응답률을 추정할 때 선택 확률이 불균등한 경우, 단순한 계산 방식만으로는 정확한 응답률을 산정하기 어렵다. 예를 들어, 행정 서비스에 대한 사회조사를 실시하면서 저소득층 거주 지역을 다른 지역에 비해 두 배 높은 비율로 표본추출한 경우를 생각해보자. 이와 같은 설계에서는 응답률과 관련해 다음과 같은 두 가지 주요 문제가 발생할 수 있다.\n첫째는 층별 응답률 비교이다. 저소득층 지역과 비저소득층 지역 간의 응답률을 각각 구해 비교하는 것은 일반적인 접근 방식이며, 두 집단 간의 평균 차이를 분석하는 데 유용하다. 이 경우에는 각 층 내에서의 응답률을 별도로 계산하고, 단순 비교를 통해 응답 특성을 이해할 수 있다. 이러한 분석에서는 앞서 설명한 표준적인 응답률 계산 방식이 적용 가능하다.\n둘째는 전체 표본의 응답률을 계산하는 경우이다. 만약 분석의 초점이 전체 모집단의 평균에 있다면, 응답률을 계산할 때 표본 추출 시의 불균등한 선택 확률을 반영해야 한다. 즉, 각 표본 요소의 선택 확률에 따라 가중치(\\(w_i\\))를 부여한 후 이를 활용하여 전체 응답률을 산정해야 한다. 이러한 가중 응답률 계산은 전체 표본이 모집단을 얼마나 잘 대표하는지를 보다 정확히 평가할 수 있도록 도와준다.\n이처럼 불균등 표집 설계에서는 응답률 계산 시 분석 목적에 따라 적절한 방법을 선택해야 하며, 층별 비교와 전체 평균 추정에서 서로 다른 계산 방식이 요구된다.\n다양한 응답률 지표의 활용\n응답률을 해석하고 활용할 때는 조사 목적과 구조에 따라 적절한 지표를 선택하는 것이 중요하다. 단순한 전체 응답률 외에도 다양한 유형의 응답률 지표가 존재하며, 이는 조사 설계와 분석 목적에 따라 다르게 적용될 수 있다.\n우선, 거절률은 설문 요청을 받은 대상자 중에서 응답을 거부한 비율을 의미하며, 일반적으로 \\(R / (I + R)\\) 의 공식으로 계산된다. 여기서 \\(R\\) 은 거절 수, \\(I\\)는 실제 응답 수를 나타낸다. 또한, 처음에는 응답을 거절했지만 이후에 설문에 참여한 경우를 반영하는 거절 변환율도 함께 고려할 수 있다. 이러한 지표는 조사 접근 방식의 효과성을 평가하는 데 유용하다.\n다음으로, 포괄률은 특히 기업을 대상으로 한 조사에서 사용되며, 전체 조사 대상 집단 중 실제 응답한 단위가 차지하는 비율을 의미한다. 이는 예를 들어 생산량이나 고용 규모와 같은 정보를 추정할 때 중요하게 고려된다. 이때 대형 유통업체인 이마트가 응답하지 않는 것과, 소규모 동네 편의점이 빠지는 경우는 조사 결과에 미치는 영향이 다르기 때문에 단순한 개수 기준의 응답률보다 포괄률이 더 적절한 지표가 될 수 있다.\n또한, 복합 응답률은 수행평가조사와 같이 다단계 표본추출이 이루어지는 구조에서 활용된다. 예를 들어, 학교와 학생이라는 두 수준에서 각각 무응답이 발생하는 경우, 이들 각 수준의 응답률을 결합하여 전체 응답률을 계산하는 방식이다. 이는 조사 참여가 여러 단계를 거치는 설계에서 보다 정확한 응답 수준을 반영할 수 있도록 한다.\n결론적으로, 응답률은 단일한 개념으로 이해하기보다는, 조사 대상, 설계 구조, 분석 목적에 따라 다양한 형태로 정의되고 활용될 수 있으며, 상황에 맞는 지표를 적절히 선택하는 것이 중요하다.\n\n\n\n\nchapter 4. 항목 무응답\n\n1. 항목 무응답 정의\n항목 무응답은 설문조사에서 응답자가 전체 설문에 참여하였음에도 불구하고 특정 질문에 대해서만 응답하지 않는 경우를 의미한다. 예를 들어, 소비자 조사의 응답자가 대부분의 문항에 성실히 응답하였더라도, 조사자가 지난 1년간의 가족 소득을 묻는 질문에 대해서는 답변을 거부할 수 있다. 항목 무응답은 단위 무응답과 마찬가지로 통계에 편향을 일으킬 수 있으나, 그 영향은 해당 항목의 데이터를 활용한 통계에만 국한된다.\n항목 무응답이 발생하는 원인은 단위 무응답과는 성격이 다를 수 있다. 단위 무응답은 설문 참여 여부를 결정하는 초기 단계에서 발생하는 반면, 항목 무응답은 설문이 진행되는 도중 개별 문항에 대한 응답 결정 과정에서 나타난다.\n항목 무응답의 주요 원인에는 다음과 같은 요인이 포함된다. 첫째, 응답자가 질문의 의도를 충분히 이해하지 못할 경우 답변을 생략할 수 있다. 둘째, 질문에 대한 정보를 기억해내기 어렵거나, 정확한 수치를 제공하기 어려운 상황도 무응답으로 이어질 수 있다. 셋째, 응답자가 질문 내용이 민감하다고 느끼거나, 정보를 공개할 동기나 의지가 부족한 경우에도 응답을 거부할 가능성이 있다.\n또한, 일부 응답자는 자신이 제공할 수 있는 답변이 정확하지 않다고 판단할 때 해당 항목을 생략하기도 한다. 이러한 상황에서는 질문의 형식을 조정하여 응답률을 개선할 수 있다. 예를 들어, 소득을 구체적인 금액이 아니라 범위로 제시하면, 응답자가 보다 부담 없이 응답할 수 있어 항목 무응답을 줄이는 데 도움이 될 수 있다.\nBeatty-Herrmann 모델\nBeatty-Herrmann 모델은 항목 무응답이 발생하는 과정을 인지적 측면과 응답 경로의 차이에 따라 설명하는 이론적 틀이다. 이 모델은 응답자가 필요한 정보를 얼마나 쉽게 접근할 수 있는지를 기준으로 네 가지 인지 상태를 제시하며, 각 상태에서 응답이 이루어질 가능성과 오류 발생 가능성을 함께 설명한다.\n첫 번째는 가용한 정보 상태이다. 이는 응답자가 질문에 필요한 정보를 쉽게 회상할 수 있는 경우를 의미한다. 이 상태에서는 대부분 정확한 응답이 이루어지며, 응답 오류가 발생할 가능성도 매우 낮다.\n두 번째는 접근 가능한 정보 상태이다. 이 경우, 응답자가 정보를 즉시 기억해내지는 못하지만, 약간의 인지적 노력이나 조사원의 유도에 의해 기억을 떠올릴 수 있다. 이러한 상황에서도 응답은 대체로 이루어지며, 정확성도 비교적 높은 편이지만, 일부 오류가 포함될 수 있다.\n세 번째는 추정 가능한 정보 상태이다. 응답자가 직접적인 기억은 없지만, 유사한 경험이나 논리적 추론을 통해 정보를 생성할 수 있는 경우에 해당한다. 이때 제공되는 응답은 어느 정도 일관성을 가질 수 있지만, 오류가 발생할 가능성이 높으며, 응답자가 스스로 신뢰하지 못해 응답을 포기할 경우 무응답으로 이어질 수 있다.\n마지막으로, 추정 불가능한 정보 상태는 응답자가 해당 질문에 대한 정보를 전혀 기억하지 못하고, 이를 유추할 수 있는 근거조차 없는 경우를 말한다. 이 상태에서는 대부분 응답이 이루어지지 않으며, 항목 무응답으로 직접 연결된다.\n이 모델은 항목 무응답을 단순한 의사결정 결과가 아니라, 응답자의 인지적 정보 처리 과정의 산물로 이해할 수 있도록 도와주며, 질문 설계와 조사 전략 수립에 유용한 시사점을 제공한다.\n\n\n2. 항목 무응답 줄이기 위한 설계적 요소\n\n(1) 응답 과정\n응답 과정은 설문조사에서 응답이 이루어지기까지의 일련의 단계로 구성되며, 일반적으로 접촉 단계, 초기 결정 단계, 그리고 최종 결정 단계의 세 단계로 구분된다. 각 단계는 응답률과 조사 품질에 중요한 영향을 미치며, 단계별로 다양한 요인이 작용한다.\n접촉 단계에서는 응답자와의 접촉이 가능한지를 판단하며, 이는 조사 성공의 첫 번째 조건이 된다. 이 단계에서는 다음과 같은 요소들이 고려된다. 우선, 자료 수집 기간이 길수록 응답자가 설문 요청을 인지하고 응답할 기회를 가질 가능성이 높다. 또한, 면접자에게 과도한 업무량이 배정되면 개별 응답자와 충분히 접촉하기 어려워져 응답률이 낮아질 수 있다. 면접자의 관찰 능력도 중요한데, 가구의 특성을 빠르게 파악하고 응답 가능성을 예측하는 능력은 조사 효율을 높이는 데 기여한다. 아울러, 통화 시도 횟수와 시점 역시 응답자와의 연결 가능성에 영향을 미치므로, 적절한 시간대에 여러 차례 연락하는 전략이 효과적이다.\n초기 결정 단계는 응답자가 설문 참여 여부를 판단하는 시점으로, 다양한 심리적·환경적 요인이 영향을 미친다. 먼저, 사전 통지는 응답자가 조사에 대한 신뢰를 갖고 사전에 준비할 수 있도록 하여 응답률을 높이는 데 도움을 준다. 금전적 보상이나 선물 등의 인센티브는 설문 참여에 대한 동기를 부여하며, 설문이 지나치게 길거나 인지적 부담이 큰 경우에는 오히려 응답률이 낮아질 수 있다. 가구 내 응답자 선택 규칙이 유연할수록 무응답 가능성이 줄어들며, 면접자가 응답자와 신뢰 관계를 형성할 수 있을 경우 응답률이 높아질 가능성이 크다. 조사 주관 기관이 정부나 공신력 있는 기관인 경우에도 설문에 대한 수용도가 높아지는 경향이 있다. 아울러, 면접자가 응답자의 관심사나 상황에 맞게 대화를 조정하는 능력 역시 긍정적인 영향을 미친다.\n최종 결정 단계는 초기 판단 이후에도 응답을 유도하기 위한 추가 조치들이 이루어지는 시점이다. 예를 들어, 응답자의 선호에 따라 조사 방식을 전화에서 대면으로 전환하는 모드 전환 전략이 있으며, 응답을 얻기 어려운 경우 면접자를 교체하는 방법도 있다. 또한, 설문 참여를 거절한 응답자에게 설득 편지를 보내 다시 참여를 유도하거나, 무응답자를 대상으로 별도의 모집단을 구성하여 이중 단계 표본 추출을 실시하는 방식도 활용된다. 마지막으로, 조사 후 분석 단계에서 무응답으로 인한 통계적 편향을 보정하기 위해 가중치를 조정하는 보정 기법이 적용될 수 있다.\n이와 같이 응답 과정은 단일한 선택의 결과가 아니라, 여러 단계와 다양한 요인이 상호작용한 결과이며, 각 단계에서의 전략적 개입은 전체 조사 품질을 높이는 데 중요한 역할을 한다.\n\n\n(2) 항목 무응답 출이기 단계\n설문 대상자와의 접촉 시도 횟수와 시기는 응답률에 중요한 영향을 미친다. 자기기입식 설문과 조사원이 보조하는 설문 모두에서, 설문 요청을 반복적으로 전달할수록 응답자와 성공적으로 접촉할 가능성이 높아진다.\n전화나 대면 조사의 경우, 응답자와의 접촉 가능성이 높은 시간대는 일반적으로 일요일부터 목요일까지의 저녁 시간대와 주말 낮 시간대이다. 반면, 평일 낮 시간에는 대부분의 가구가 부재 중이므로 연락이 어려운 경우가 많다.\n데이터 수집 기간 역시 응답률에 영향을 준다. 수집 기간이 길수록 응답자가 조사 요청을 인지할 가능성이 높아진다. 예를 들어, 미국 인구 센서스는 약 10일간 진행되며 거의 모든 가구와의 접촉에 성공한다. 이는 적절한 조사원 배치만으로도 비교적 짧은 시간 내에 대부분의 초기 접촉이 가능함을 시사한다.\n조사원의 업무량 또한 중요한 요소이다. 조사원에게 할당된 표본 사례당 충분한 시간이 주어져야 응답자와의 접촉과 설득이 가능하다. 예를 들어, 첫 번째 전화 연락 시도에서 접촉이 성공할 확률은 약 50%에 불과하며, 충분한 시간이 확보되지 않거나 과도한 업무량이 주어진 경우 비접촉이나 설득 부족으로 인한 무응답이 증가할 수 있다.\n조사 후원 기관도 응답 협력률에 영향을 미친다. 대부분의 국가에서 정부 기관이 주관하는 조사에 대한 응답률이 대학이나 민간기관보다 높은 경향을 보인다. 특히 조사 후원 기관이 응답자의 소속 집단이나 가치와 관련이 있을 경우 응답률은 더욱 높아진다. 예를 들어, 회원제로 운영되는 단체가 후원하는 조사에서는 소속 응답자들의 참여 의향이 더 높게 나타난다.\n대면 조사는 조사원이 직접 표본 가구를 관찰할 수 있다는 점에서 강점을 가진다. 예를 들어, 마당에 놓인 장난감을 통해 어린이의 존재를 유추하거나, 이웃을 통해 가구 구성에 대한 정보를 얻을 수 있다. 이러한 관찰 정보는 조사 진행과 관리에 유용하게 활용될 수 있으며, 응답자가 설문에 대한 질문을 하는 경우에는 오히려 응답 의향이 있다는 신호로 해석될 수 있다.\n사전 통지도 응답률에 긍정적인 영향을 준다. 응답자에게 우편이나 이메일로 조사 계획을 미리 안내하면 설문 요청의 신뢰도가 높아지고, 실제로 많은 조사에서 응답률이 향상되는 것으로 나타났다. 특히 대학이나 공공기관이 주관하는 경우 그 효과가 더 크게 나타나며, 반대로 시장 조사기관의 경우 사전 통지가 오히려 응답률을 낮추는 결과를 보이기도 한다.\n인센티브는 응답 동기를 높이는 또 다른 요인이다. 현금 보상이 물품 보상보다 더 효과적인 것으로 나타났으며, 설문 완료 이후보다 요청 이전에 인센티브를 제공할 경우 응답률이 더 높게 나타나는 경향이 있다.\n응답자가 느끼는 부담도 응답률에 영향을 미친다. 설문이 너무 길거나 내용이 복잡하면 참여를 꺼리는 경향이 있으며, 실제로 자기기입식 설문지의 페이지 수가 한 장 늘어날 때마다 응답률이 평균 0.4%포인트 감소한다는 연구 결과도 있다.\n가구 내에서 응답자를 선택하는 방식 또한 응답률에 영향을 미친다. 가능한 모든 성인이 응답할 수 있도록 허용하는 방식은 무작위로 성인을 선정하는 방식보다 응답 협력률이 높다. 또한 대리 응답을 허용하는 경우, 직접 응답만 허용하는 방식보다 높은 응답률을 기록하는 경우가 많다.\n특히 전화 조사에서는 인터뷰어의 초기 소개 방식이 중요하다. 억양이나 말하는 속도 등 미묘한 언어적 특징이 응답자의 협조 의사에 영향을 줄 수 있으며, 조사원이 지나치게 정형화된 소개 문구를 읽는 경우 응답 거부율이 높아진다는 연구 결과도 존재한다.\n응답자와 조사자 간의 신뢰 형성을 위한 적절한 매칭도 응답률을 높이는 전략으로 활용될 수 있다. 예를 들어, 혼자 사는 고령 여성 응답자에게는 보다 연령이 높은 여성 조사자를 배정하는 것이 응답 가능성을 높이는 데 도움이 될 수 있다.\n조사 방식의 변경도 응답률 향상에 기여할 수 있다. 예를 들어, 초기에는 비용 효율성이 높은 우편 설문을 사용하고, 이후 무응답자에게는 대면 조사를 적용하는 혼합 설계가 자주 활용된다. 일반적으로 대면 조사는 전화나 우편 방식보다 응답률이 높은 경향이 있다.\n설문 참여를 처음에 거절한 대상자에게 설문 목적과 중요성을 설명하는 설득 편지를 보내는 방법도 있다. 이 편지는 조사원이 다시 방문하여 질문이나 우려 사항에 답변하겠다는 내용을 포함하며, 응답자의 태도 변화와 협조 가능성을 높이기 위한 전략으로 활용된다.\n\n\n(3) 통계적 기법 활용\n무응답 문제를 해결하기 위해 통계적 분석 기법을 활용한 다양한 방법이 개발되어 왔으며, 특히 무응답자에 대해 새로운 접근 방식을 적용하는 시도들이 주목받고 있다.\n먼저, 이중 단계 표본 추출은 무응답자 중 일부를 확률적으로 다시 추출하여 새로운 방식으로 접촉을 시도하고 응답을 유도하는 방법이다. 이 과정에서 얻은 응답 데이터를 활용하면 전체 무응답자의 특성을 추정할 수 있으며, 이는 무응답 편향을 줄이는 데 효과적으로 활용될 수 있다.\n또한, 조사 후 보정은 기존 응답자의 데이터를 바탕으로 무응답자의 특성을 보정하는 방식이다. 예를 들어, 도시 지역에서 응답률이 낮을 경우, 응답한 도시 지역 표본에 더 높은 가중치를 부여하여 전체 결과의 대표성을 확보하고 편향을 줄이는 방식이 여기에 해당한다.\n이와 같은 통계적 기법들은 무응답에 의한 왜곡을 줄이는 데 중요한 역할을 하지만, 여전히 해결되지 않은 여러 연구 과제가 존재한다.\n예를 들어, 응답을 꺼리는 대상자와의 인터뷰가 성공했을 경우, 이들이 제공하는 응답은 다른 응답자보다 측정 오류가 더 클 가능성이 있는지에 대한 의문이 제기된다. 단순히 응답을 확보하는 것이 아니라, 그 응답의 품질 또한 함께 고려해야 한다는 문제의식이다.\n또한, 응답률을 높이기 위한 노력이 항상 무응답 편향 감소로 이어지는지, 또는 특정 조건에서만 효과적인지에 대한 검토도 필요하다. 응답률 자체가 개선되더라도, 응답자 구성의 대표성이 여전히 확보되지 않는다면 무응답 편향은 여전히 존재할 수 있다.\n비접촉 무응답과 거절 무응답을 줄이기 위한 전략 간의 균형도 중요한 과제이다. 예를 들어, 접촉 가능성을 높이기 위해 여러 차례 시도하는 것은 비접촉 무응답을 줄일 수 있지만, 지나치게 빈번한 연락은 오히려 거절 무응답을 증가시킬 위험이 있다. 두 가지 유형의 무응답 간에 어떻게 자원을 배분할지에 대한 전략적 판단이 필요하다.\n마지막으로, 표본 오차와 무응답 오차를 동시에 고려할 때, 제한된 예산 내에서 응답률을 무조건 극대화하지 않아도 되는 조건은 무엇인지에 대한 논의도 중요하다. 예를 들어, 응답률을 약간 희생하더라도 더 넓은 표본을 확보하거나 다른 품질 보정 기법을 적용하는 것이 전체적으로 더 나은 결과를 낳을 수 있다. 이러한 판단은 조사 설계의 목적, 예산, 대상 모집단의 특성 등을 종합적으로 고려하여 이루어져야 한다.\n\n\n\n\nchapter 5. 항목 무응답 대체\n항목 무응답은 조사 대상자가 전체 설문에는 응답했지만, 일부 질문에만 응답하지 않아 특정 항목의 데이터가 결측되는 경우를 의미한다. 이러한 무응답은 소득이나 건강 상태와 같은 민감한 질문에 대한 기피, 질문 내용을 정확히 이해하지 못한 경우, 응답 과정에서의 실수, 또는 조사자의 착오 등 다양한 이유로 발생할 수 있다.\n항목 무응답은 전체 응답률에는 영향을 미치지 않지만, 특정 변수에 대한 응답이 충분하지 않을 경우 해당 변수에 대한 분석이 제한된다. 특히, 항목 무응답이 많아지면 분석 가능한 표본 수가 줄어들고, 결측된 응답이 특정 집단에 집중될 경우 해당 변수와 관련된 분석 결과에 편향이 발생할 수 있다. 이는 결과의 신뢰성을 저하시킬 수 있으며, 모집단을 대표하는 정확한 추정을 어렵게 만든다. 따라서 항목 무응답의 발생 원인을 이해하고, 적절한 보정이나 결측 처리 방법을 적용하는 것이 중요하다.\n\n1. 가중치 조정\n가중치 조정은 단위 무응답으로 인해 발생하는 대표성 문제를 완화하기 위한 방법 중 하나이다. 이 방법은 응답하지 않은 표본의 특성을 고려하여, 응답자에게 부여된 가중치를 조정함으로써 전체 모집단의 분포를 보다 정확하게 반영하고자 한다.\n조정의 핵심 목적은 응답자 표본이 실제 모집단을 대표할 수 있도록 하는 것이다. 예를 들어, 특정 연령대나 지역의 응답률이 낮은 경우, 해당 집단에 속한 응답자에게 더 높은 가중치를 부여함으로써 전체 분석에서 그 집단의 영향력을 보정할 수 있다. 이를 통해 무응답으로 인한 편향을 줄이고, 통계 결과의 신뢰성과 대표성을 높이는 데 기여할 수 있다.\n\n(1) 후보정 가중치(Post-Stratification Weighting)\n후보정 가중치는 모집단의 이미 알려진 특성을 바탕으로 응답자의 가중치를 조정하는 방법이다. 이 방식은 표본과 모집단 간의 불균형을 수정하여, 분석 결과에 포함될 수 있는 편향을 줄이는 데 목적이 있다.\n후보정에서는 성별, 연령, 지역, 교육 수준 등과 같이 모집단의 분포가 사전에 알려진 보조 변수를 활용한다. 응답자 집단이 특정 보조 변수에서 모집단과 다르게 구성되어 있을 경우, 해당 변수의 분포를 기준으로 가중치를 재조정함으로써 전체 표본이 모집단을 보다 잘 대표하도록 보완할 수 있다. 이 과정은 조사 결과의 신뢰성과 정확성을 높이는 데 중요한 역할을 한다.\n\\(w_{i} = \\frac{N_{g}}{n_{g}}\\) 여기서, \\(w_{i}\\)은 응답자 i 의 새로운 가중치, \\(N_{g}\\)은 모집단 내 해당 그룹 \\(g\\)의 크기, \\(n_{g}\\)은 표본 내 해당 그룹 \\(g\\)의 응답자 수\n즉, 후보정 가중치는 각 그룹의 모집단 비율을 반영하여 응답자의 가중치를 조정하는 방식이다. 이를 통해 표본이 모집단의 구조를 보다 정확하게 반영하도록 한다.\n예를 들어, 모집단에서 성별 비율이 남성 60%, 여성 40%인 상황에서 실제 조사에서는 남성과 여성이 각각 50명씩 조사되었다고 가정하자. 이 경우 표본에서는 남성이 과소표집된 상태이며, 모집단의 실제 분포와 불일치가 발생한다. 이를 보정하기 위해 남성 응답자의 가중치를 증가시키고, 여성 응답자의 가중치를 상대적으로 낮춤으로써 분석 결과가 모집단의 구조에 맞도록 조정할 수 있다. 이와 같은 방식은 무응답이나 표본 추출상의 불균형으로 인한 편향을 줄이는 데 유효한 방법이다.\n\\(w_{\\text{남성}} = \\frac{N_{\\text{남성}}}{n_{\\text{남성}}} = \\frac{60}{50} = 1.2\\), \\(w_{\\text{여성}} = \\frac{N_{\\text{여성}}}{n_{\\text{여성}}} = \\frac{40}{50} = 0.8\\)\n즉, 남성 응답의 영향을 증가시키고 여성 응답의 영향을 감소시켜 모집단의 성별 비율을 반영합니다.\n\n\n(2) 무응답 가중치 조정(Nonresponse Weighting Adjustment)\n무응답 가중치 조정은 응답자의 특성을 기준으로 유사한 무응답자 그룹을 식별한 후, 해당 응답자의 가중치를 조정하여 무응답으로 인한 편향을 보정하는 방법이다. 이 기법은 응답자와 무응답자가 유사한 특성(예: 성별, 연령, 지역 등)을 가진 집단 내에 속해 있다고 가정하고, 그 집단(Strata) 내 응답자의 비율을 활용하여 가중치를 조정한다.\n특히 무응답이 특정 집단에 집중되는 경향이 있을 때 이 방법은 효과적으로 작동한다. 예를 들어, 특정 연령대나 지역에서 응답률이 낮은 경우, 그 집단의 응답자에게 더 높은 가중치를 부여함으로써 전체 표본의 대표성을 회복할 수 있다.\n통계적으로는, 응답자가 설문에 응할 확률인 응답 확률 \\(p_i\\) 를 고려하여 각 응답자의 가중치를 \\(1 / p_i\\) 의 형태로 조정할 수 있다. 이러한 방식은 무응답으로 인한 왜곡을 줄이고, 조사 결과의 정확성과 신뢰성을 높이는 데 기여한다.\n\\(w_{i} = \\frac{1}{p_{i}}\\) 여기서, \\(w_{i}\\)은 응답자 \\(i\\)의 가중치, \\(p_{i}\\) 은 응답 확률 (응답자가 해당 조사에 응답할 확률) 또는, 그룹 \\(g\\) 별 응답률을 고려하여 다음과 같이 가중치를 조정할 수 있다.\n\\(w_{i} = w_{i}^{\\text{기존}} \\times \\frac{1}{{\\widehat{p}}_{g}}\\), 여기서, \\({\\widehat{p}}_{g} = \\frac{n_{g}}{m_{g}}\\) (해당 그룹의 응답률), \\(n_{g}\\) 은 해당 그룹 내 응답자 수, \\(m_{g}\\) 은 해당 그룹 내 전체 표본 수이다.\n예를 들어, 특정 연령대(20대)의 응답률이 낮다고 가정해 보겠습니다. 모집단에서 20대는 1,000명이고, 표본에서는 100명을 선정했지만, 그중 50명만 응답했다면 20대의 응답률은 \\({\\widehat{p}}_{20\\text{대}} = \\frac{50}{100} = 0.5\\) 따라서, 응답자의 가중치는 \\(w_{\\text{20대}} = \\frac{1}{0.5} = 2.0\\)이다. 즉, 20대 응답자의 가중치를 2배 증가시켜 모집단의 특성을 반영하도록 보정한다.\n\n\n\n2. 평균, 중앙값, 최빈값 대체(Mean/Median/Mode Imputation)\n\n(1) 평균 또는 중앙값 대체(Mean/Median Imputation)\n평균 대체는 결측값을 해당 변수의 평균 값으로 대체하는 방법이며, 중앙값 대체는 결측값을 해당 변수의 중앙값으로 대체하는 방식이다. 두 방법 모두 결측값을 단일 값으로 채우는 단순 대체 방식에 속하며, 주로 소득, 나이, 키, 체중과 같은 연속형 변수에 적용된다.\n평균 대체는 전체 응답자의 평균을 활용하므로 데이터의 중심 경향을 반영할 수 있지만, 이상값의 영향을 크게 받을 수 있다는 단점이 있다. 반면, 중앙값 대체는 극단값에 덜 민감하므로 데이터의 분포가 비대칭이거나 이상값이 존재하는 경우 보다 안정적인 대체 방법으로 간주된다.\n이러한 대체 방법은 분석에 사용할 수 있는 표본 수를 늘리는 데는 도움이 되지만, 데이터의 변동성을 과소추정하거나 분산을 왜곡할 수 있으므로 해석에 주의가 필요하다.\n\\(X_{\\text{missing}} = \\frac{1}{n}\\overset{n}{\\sum_{i = 1}}X_{i}\\), \\(X_{\\text{missing}} = \\text{Median}(X_{1},X_{2},\\ldots,X_{n})\\)\n\n\n(2) 최빈값 대체(Mode Imputation)\n최빈값 대체는 결측값을 해당 변수에서 가장 자주 나타나는 값, 즉 최빈값으로 대체하는 방법이다. 이 방법은 성별, 직업, 지역과 같은 범주형 변수에서 주로 사용되며, 다음과 같은 방식으로 표현할 수 있다: \\(X_{\\text{missing}} = \\text{Mode}(X_{1},X_{2},\\ldots,X_{n})\\)\n최빈값 대체는 계산이 간단하고 해석이 명확하다는 장점이 있으나, 모든 결측값을 동일한 값으로 대체하므로 데이터의 분포를 왜곡하거나 변이를 과소추정할 수 있다는 단점도 있다.\n보다 신뢰성 있는 대체를 위해, 전체 데이터를 기반으로 대체하는 대신, 층화변수와 내재적 층화변수(즉, 표본 추출에 사용된 층화 기준)를 결합한 세분화된 층 내에서 최빈값을 계산하여 대체하는 방식이 활용될 수 있다. 이렇게 하면 각 응답자 집단의 특성을 보다 잘 반영할 수 있어, 결측값 대체의 정확성과 타당성이 높아진다.\n\n\n\n3. 핫덱(Hot Deck) 또는 콜드덱(Cold Deck) 대체\n핫덱(Hot Deck)과 콜드덱(Cold Deck) 대체 방법은 결측 데이터를 보완하기 위해 사용되는 대표적인 대체 기법이다. 이들은 무응답자의 값을 유사한 응답자나 외부 데이터로부터 가져와 채워 넣는 방식으로, 특히 설문조사나 표본조사에서 자주 활용된다.\n핫덱 대체는 동일한 조사 내에서 결측값이 있는 응답자와 유사한 특성을 가진 응답자의 값을 이용하여 결측값을 채우는 방법이다. 예를 들어, 연령, 성별, 지역 등이 유사한 응답자 중에서 해당 항목의 값을 가져오는 방식이다. 이 방법은 동일한 데이터셋 내에서 정보를 활용하므로 일관성과 응답 환경의 유사성을 유지할 수 있다는 장점이 있다.\n반면, 콜드덱 대체는 외부의 독립적인 데이터나 과거 조사 데이터를 활용하여 결측값을 보완하는 방식이다. 예를 들어, 이전 조사에서 축적된 데이터를 참조하여 현재 결측된 항목을 채우는 방식이다. 이 방법은 현재 조사에서 해당 값이 확보되지 않은 경우에도 활용이 가능하지만, 자료 간 차이로 인해 편차가 생길 수 있다는 점에 유의해야 한다.\n두 방법 모두 무응답 문제를 해결하고 분석의 완성도를 높이는 데 도움이 되며, 선택 시에는 조사 목적, 데이터 특성, 대체 가능한 정보의 적절성 등을 고려해야 한다.\n\n(1) 핫덱 대체 (Hot Deck Imputation)\n핫덱(Hot Deck) 대체는 현재 조사에서 수집된 응답 데이터를 활용하여 결측값을 보완하는 방법이다. 이 방식은 동일한 조사 내에서 무응답자와 유사한 특성을 가진 응답자를 찾아, 해당 응답자의 값을 결측값에 할당함으로써 데이터를 보완한다.\n핫덱 대체는 주로 성별, 연령, 지역 등과 같은 보조 변수를 기준으로 유사한 응답자를 찾는 방식으로 이루어지며, 두 가지 방식으로 수행될 수 있다. 첫째, 무작위 핫덱(random hot deck) 방식은 유사한 집단 내에서 무작위로 한 응답자를 선택하여 그 값을 결측값에 할당한다. 둘째, 층화 핫덱(stratified hot deck) 방식은 사전에 정의된 층화 기준에 따라 동일한 층 내에서 가장 유사한 응답자를 선택하여 대체한다.\n핫덱 대체는 실제 응답 데이터 기반으로 이루어지므로 현실적인 값이 할당될 가능성이 높고, 응답자의 특성을 고려한 대체가 가능하다는 점에서 비교적 신뢰성이 높은 방법으로 간주된다. 다만, 유사한 응답자를 선정하는 기준과 방식에 따라 대체 결과가 달라질 수 있으므로, 적절한 기준 설정이 중요하다.\n핫덱 대체는 결측값을 유사한 응답자의 값으로 대체한다.\n\\[Y_{\\text{결측값},i} = Y_{\\text{유사},j}\\]\n핫덱 대체에서 유사한 응답자를 선택하는 기준은 결측값을 얼마나 정확하게 보완할 수 있는지를 결정짓는 핵심 요소이다. 일반적으로 다음과 같은 세 가지 방법이 활용된다.\n첫째, 계층적 방법(stratified hot deck)은 성별, 연령, 지역 등과 같은 주요 변수를 기준으로 응답자를 그룹화한 뒤, 동일한 그룹 내에서 유사한 응답자의 값을 결측값에 할당하는 방식이다. 이 방법은 미리 정의된 계층 구조를 활용하여 응답자의 특성을 최대한 반영하려는 목적에 적합하다.\n둘째, 무작위 방법(random hot deck)은 응답자의 특성과 상관없이 동일한 모집단 내에서 임의로 응답자의 값을 선택하여 결측값을 대체하는 방식이다. 이 방법은 간단하고 계산이 빠르지만, 유사성 기준이 적용되지 않아 오차가 발생할 가능성이 있다.\n셋째, 최근접 이웃 방법(nearest neighbor hot deck)은 응답자 간 유사도를 수치적으로 계산한 후, 가장 유사한 응답자를 선택하여 해당 값을 대체하는 방식이다. 이 방법은 응답자 특성을 세밀하게 고려할 수 있다는 장점이 있다.\n예를 들어, 설문조사에서 소득 정보가 결측된 응답자 A가 20대 남성이며 대학을 졸업한 경우, 같은 조사 내에서 동일한 특성을 가진 응답자 B가 소득 정보를 제공했다면, A의 결측값을 B의 소득 값으로 대체할 수 있다. 이처럼 핫덱 대체는 현재 조사 내에서 유사한 응답자의 데이터를 활용하여 결측값을 보완한다는 점에서 실용적이며, 현실적인 응답을 반영할 수 있는 유용한 방법이다.\n\n\n(2) 콜드덱 대체 (Cold Deck Imputation)\n콜드덱(Cold Deck) 대체는 현재 조사 내의 데이터를 활용하는 핫덱 대체와 달리, 외부의 독립적인 데이터원을 활용하여 결측값을 보완하는 방법이다. 이때 활용되는 외부 데이터는 이전에 실시된 조사, 행정자료, 공공 데이터, 또는 기존 연구에서 축적된 정보 등이 될 수 있다.\n콜드덱 대체는 과거 조사에서 수집된 자료 중에서 현재 조사 대상자와 유사한 특성을 가진 응답자의 값을 활용하여 결측된 항목을 대체한다. 이 방식은 데이터의 일관성과 안정성이 유지된다면 핫덱 대체보다 더 신뢰도 높은 결과를 제공할 수 있다. 특히 조사 환경이나 문항 구성이 유사할 경우 효과적이다.\n그러나 콜드덱 방식은 외부 데이터가 오래되었거나 현재의 모집단 특성과 차이가 클 경우, 대체의 정확도가 떨어질 수 있다. 시간의 경과로 인한 사회적 변화나 조사 방식의 차이로 인해 과거 데이터가 현재 상황을 적절히 반영하지 못할 위험이 있기 때문이다.\n결론적으로, 콜드덱 대체는 외부 자료의 품질과 최신성, 그리고 현재 조사와의 정합성 여부에 따라 성과가 달라질 수 있으며, 외부 데이터의 적절성을 충분히 평가한 후 활용하는 것이 중요하다.\n\\[Y_{\\text{결측값},i} = Y_{\\text{이전조사},k}\\]\n예를 들어, 2025년에 실시된 소득 조사에서 일부 응답자의 소득 정보가 누락된 경우, 콜드덱 대체 방법을 활용하여 이전 자료에서 해당 결측값을 보완할 수 있다. 이때 2020년 인구조사와 같은 과거 데이터를 참고하여, 동일한 연령, 성별, 지역에 해당하는 집단의 평균 소득 값을 활용하는 방식이다.\n2025년 조사에서 소득 정보가 누락된 응답자 A가 30대 남성이고 특정 지역에 거주하고 있다면, 2020년 인구조사에서 동일한 특성을 가진 집단의 평균 소득이 4,000만 원으로 나타났을 경우, A의 결측된 소득 값은 4,000만 원으로 대체할 수 있다. 이와 같은 방식은 적절한 외부 자료가 존재하고, 그 자료가 현재 조사와 충분히 유사한 구조를 갖고 있을 때 신뢰성 있는 대체 수단이 될 수 있다.\n\n\n\n4. 모델 기반 대체(Model-Based Imputation)\n모델 기반 대체(Model-Based Imputation)는 통계적 또는 기계 학습 모델을 활용하여 결측값을 예측하는 방법이다. 이 방식은 기존 응답 데이터를 바탕으로 변수 간의 패턴을 학습한 뒤, 무응답자의 결측값을 예측하여 채우는 절차로 이루어진다. 단순한 평균이나 최빈값 대체와 달리, 여러 변수 간의 관계를 고려한다는 점에서 보다 정교하고 유연한 접근이 가능하다.\n대표적인 방법으로는 회귀 대체와 다중 대체가 있다. 회귀 대체는 결측값이 있는 변수를 종속 변수로 설정하고, 다른 관측 가능한 변수들을 독립 변수로 사용하여 회귀모형을 적합시킨 후, 예측값으로 결측값을 대체하는 방식이다. 다중 대체(Multiple Imputation)는 단일 예측값이 아닌, 확률적 방법을 통해 여러 개의 대체값을 생성하고, 각각의 대체된 데이터를 분석한 후 그 결과를 종합하여 최종 추정치를 도출하는 방식이다.\n모델 기반 대체는 복잡한 데이터 구조와 변수 간 상호작용을 반영할 수 있다는 장점이 있으나, 모델의 적합성과 가정에 따라 대체 결과가 달라질 수 있으므로 주의 깊은 검토와 모형 진단이 필요하다.\n\n(1) 회귀 대체(Regression Imputation)\n회귀 대체는 응답자의 데이터를 활용하여 회귀 모형을 구성한 뒤, 이를 바탕으로 무응답자의 결측값을 예측하는 방법이다. 이 기법은 변수 간의 통계적 관계를 이용해 결측값을 보완하며, 예측하려는 변수의 성격에 따라 다른 회귀 모형이 사용된다. 예를 들어, 연속형 변수의 경우에는 선형 회귀를, 범주형 변수의 경우에는 로지스틱 회귀를 적용할 수 있다.\n회귀 대체는 단순한 평균이나 최빈값 대체보다 더 많은 정보를 활용하므로 예측력이 높고, 데이터의 구조를 잘 반영할 수 있는 강력한 대체 방법이다. 하지만 회귀 모형의 성능에 따라 대체 결과가 달라질 수 있으며, 특히 모형이 과적합되는 경우에는 실제보다 과도하게 정확한 예측을 제공하는 듯 보일 수 있어 주의가 필요하다. 따라서 회귀 대체를 사용할 때는 적절한 변수 선택, 교차 검증, 잔차 분석 등을 통해 모형의 타당성을 충분히 점검해야 한다.\n선형 회귀 모델(측정형 무응답)\n연속형 변수가 결측된 경우, 회귀식을 통해 예측한다.\n\\(Y = \\beta_{0} + \\beta_{1}X_{1} + \\beta_{2}X_{2} + \\ldots + \\beta_{k}X_{k} + \\epsilon\\), 여기서 \\(Y\\)은 응답자의 값을 사용해 예측할 종속 변수(결측값을 포함하는 변수), \\(X_{1},X_{2},\\ldots,X_{k}\\)은 예측에 사용되는 독립 변수(결측값이 없는 변수들) 회귀 분석을 통해 계수를 추정한 후, 무응답자의 값을 예측값으 로 대체한다.\n\\[Y_{\\text{miss}} = \\widehat{Y} = {\\widehat{\\beta}}_{0} + {\\widehat{\\beta}}_{1}X_{1} + {\\widehat{\\beta}}_{2}X_{2} + \\ldots + {\\widehat{\\beta}}_{k}X_{k}\\]\n예를들어, 설문조사에서 연령, 교육 수준, 직업을 기반으로 소득이 결측된 응답자의 소득을 예측한다고 가정하자. 데이터셋에는 연령, 교육 수준, 직업, 소득 변수가 포함되어 있고 일부 응답자가 소득을 응답하지 않았다면 다음 추정값으로 대체한다. \\(\\widehat{\\text{Income}} = {\\widehat{\\beta}}_{0} + {\\widehat{\\beta}}_{1}\\text{Age} + {\\widehat{\\beta}}_{2}\\text{Education} + {\\widehat{\\beta}}_{3}\\text{Occupation}\\)\n\n\n(2) 다항 로짓회귀모형(범주형 무응답)\n\\[P(Y = j|X) = \\frac{\\exp(X\\beta_{j})}{\\sum_{k = 1}^{J}\\exp(X\\beta_{k})},j = 1,2,\\ldots,J\\]\n\\(P(Y = j|X)\\): 독립 변수 \\(X\\)가 주어졌을 때, 종속 변수가 범주 \\(j\\)를 선택할 확률\n\\(X\\): 예측에 사용되는 독립 변수(결측값이 없는 변수들\n\\(J\\): 가능한 범주의 개수\n\n\n(3) 다중 대체(Multiple Imputation, MI)\n다중 대체(Multiple Imputation)는 결측값을 하나의 예측값으로 대체하는 단일 대체 방법과 달리, 여러 개의 가능한 대체값을 생성하여 결측에 따른 불확실성을 반영하는 방식이다. 단일 대체 방법, 예를 들어 회귀 대체는 하나의 고정된 값을 결측값에 할당하기 때문에 대체값 간의 변동성을 반영하지 못하는 한계가 있다. 반면, 다중 대체는 확률 기반 접근을 통해 서로 다른 대체값을 여러 개 생성하고, 각 대체된 데이터셋에 대해 분석을 수행한 뒤, 그 결과를 통합하여 최종 추정치를 도출한다. 이 과정은 결측으로 인한 통계적 불확실성을 분석 결과에 포함시키는 효과가 있다.\n예를 들어, 개인의 소득(Income), 연령(Age), 교육 수준(Education), 직업(Occupation)을 포함한 설문조사를 수행했을 때 일부 응답자의 소득 정보가 누락되어 결측값이 발생했다고 가정하자. 이 경우, 다중 대체를 적용하면 연령, 교육 수준, 직업 등의 정보를 이용해 소득에 대해 여러 개의 가능한 예측값을 생성할 수 있다. 이후 각 대체값을 포함한 데이터셋으로 동일한 분석을 반복 수행하고, 그 결과를 통합함으로써 보다 신뢰할 수 있고 불확실성이 반영된 추정치를 얻을 수 있다.\n다중 대체는 결측값 처리에서 가장 권장되는 방법 중 하나로, 결측이 분석 결과에 미치는 영향을 최소화하면서도 통계적 일관성을 유지하는 데 효과적인 기법이다. 다중 대체 단계는 다음과 같다.\n1. 대체(Imputation)\n다중 대체는 결측값이 존재하는 데이터를 대상으로 여러 개의 대체값을 생성하는 절차를 포함한다. 이 과정에서는 회귀 분석이나 예측 모델 등 통계적 기법을 활용하여 결측값을 예측하고, 여기에 확률적 오차를 반영함으로써 반복적으로 다양한 대체값을 생성한다.\n예를 들어, 소득 변수에 결측값이 존재하는 경우, 연령, 교육 수준, 직업 등의 변수를 설명 변수로 활용하여 소득을 예측하는 회귀 모형을 구축한다. 이 회귀 모형을 기반으로 확률적 요소를 포함한 예측값을 생성하고, 이를 반복하여 서로 다른 다섯 개의 대체 데이터셋을 만든다. 각 데이터셋은 동일한 결측값에 대해 서로 다른 값을 가지며, 이후 각 데이터셋에 대해 독립적인 분석을 수행한다. 마지막으로 그 결과를 통합함으로써 결측으로 인한 불확실성이 반영된 최종 추정치를 도출할 수 있다.\n이와 같은 방식은 단일 대체에서 발생할 수 있는 과소추정 문제를 보완하고, 보다 신뢰도 높은 분석 결과를 제공하는 데 효과적이다.\n\n\n\n\n\n결측값이 있는 ID 3, 5번의 소득을 5가지 방법으로 대체하여 총 5개의 데이터셋을 생성하여 대체값 추정하였다.\n데이터셋 1: ID 3 → 3,500 / ID 5 → 4,000\n데이터셋 2: ID 3 → 3,800 / ID 5 → 4,200\n데이터셋 3: ID 3 → 3,600 / ID 5 → 4,100\n데이터셋 4: ID 3 → 3,700 / ID 5 → 4,300\n데이터셋 5: ID 3 → 3,900 / ID 5 → 4,500\n2. 분석(Analysis)\n생성된 각 대체 데이터셋을 이용하여 동일한 분석을 수행한다. 각 데이터셋에서 회귀 분석, 평균 추정 등의 통계 분석을 진행한다. 각각의 대체 데이터셋에서 동일한 분석을 수행한다. 예를 들어, 소득과 연령 간의 회귀 분석을 수행하면,\n\n데이터셋 1에서 회귀 계수(β) = 0.25\n데이터셋 2에서 회귀 계수(β) = 0.27\n데이터셋 3에서 회귀 계수(β) = 0.26\n데이터셋 4에서 회귀 계수(β) = 0.28\n데이터셋 5에서 회귀 계수(β) = 0.29\n\n3. 결합(Pooling)\n여러 개의 분석 결과를 결합하여 최종 추정치를 계산한다. 보통 Rubin’s Rules을 사용하여 평균과 표준 오차를 결합한다.\n회귀 계수(β)의 평균 및 표준 오차를 계산: \\(\\overline{\\beta} = \\frac{1}{m}\\overset{m}{\\sum_{j = 1}}\\beta_{j}\\), \\(T = W + \\left( 1 + \\frac{1}{m} \\right)B\\) 여기서, \\(m = 5\\) (대체 데이터셋 개수), \\(W\\) 는 대체 데이터셋 내부의 분산(Within-Imputation Variance), \\(B\\) 는 대체 데이터셋 간의 분산(Between-Imputation Variance)\n최종적으로 회귀 계수(β) = 0.27 ± 0.02로 사용한다.\n\n\n\n5. 최근접 이웃 대체(Nearest Neighbor Imputation)\n응답자 간의 유사성을 바탕으로 결측값을 대체하는 방법은, 결측값을 예측하는 것이 아니라 기존 응답자 중에서 가장 유사한 대상을 찾아 해당 값을 그대로 할당하는 방식이다. 이 방법은 모델 기반 예측이 아닌 실제 데이터 값을 활용하므로, 기존 데이터의 분포를 잘 보존할 수 있다는 장점이 있다.\n무응답자의 특성과 가장 유사한 응답자를 선택하기 위해 거리 기반 알고리즘이 활용되며, 대표적으로 유클리드 거리, 맨해튼 거리, 코사인 유사도 등이 사용된다. 이들 거리 지표는 응답자 간의 특성 차이를 수치적으로 측정하여 가장 가까운 이웃을 찾는 데 사용된다.\n이러한 방식에서 가장 널리 쓰이는 알고리즘은 K-최근접 이웃(K-Nearest Neighbors, KNN) 알고리즘이다. 이 알고리즘은 결측값이 있는 무응답자에 대해, K개의 가장 유사한 응답자를 찾아 그들의 값을 참조하여 대체한다. K가 1인 경우에는 가장 가까운 단일 응답자의 값을 그대로 사용하는 방식이 된다. 이 기법은 연속형 변수와 범주형 변수 모두에 적용이 가능하며, 특히 복수의 예측 변수를 바탕으로 유사도를 정량화할 수 있다는 점에서 유연성이 크다.\n거리계산\n예를 들어, 응답자 i와 j 사이의 유클리드 거리는 다음과 같은 방식으로 계산할 수 있다: \\[d(i,j) = \\sqrt{(X_{i1} - X_{j1})^{2} + (X_{i2} - X_{j2})^{2} + \\ldots + (X_{ik} - X_{jk})^{2}}\\]\n가장 가까운 이웃인 응답자 \\(j^{*}\\) 를 찾고 해당 응답자의 값을 무응답자 \\(i\\) 에게 대체한다. \\(Y_{\\text{결축값},i} = Y_{\\text{유사},j^{*}}\\)\nK-최근접 이웃(K-Nearest Neighbors, KNN) 대체\n최근접 이웃 기반 대체 방법에서는 하나의 이웃만을 사용하는 방식보다, 여러 개의 최근접 이웃(K개의 이웃)을 선택하여 대체값을 산출하는 방식이 보다 안정적인 결과를 제공할 수 있다. 이 접근은 개별 응답자 간의 변동성을 완화하고, 대체값에 대한 신뢰도를 높이는 데 유리하다.\n연속형 변수의 경우, 선택된 K개의 최근접 이웃의 값을 평균하여 결측값을 대체한다. 이 방법은 극단값의 영향을 줄이고, 보다 일반적인 값을 반영할 수 있다는 점에서 효과적이다.\n범주형 변수의 경우에는 K개의 이웃 중 가장 빈번하게 나타나는 값을 선택하여 대체하는데, 이는 최빈값(mode)을 기준으로 결측값을 채우는 방식이다. 이와 같이 최근접 이웃을 복수로 활용하는 K-최근접 이웃(KNN) 대체 방법은 데이터의 유형에 따라 유연하게 적용 가능하며, 단일 이웃 방식보다 일반적으로 더 안정적인 대체 결과를 제공한다.\n\\(Y_{\\text{miss},i} = \\frac{1}{K}\\sum_{j \\in KNN(i)}Y_{j}\\), 여기서 \\(KNN(i)\\)은 무응답자 \\(i\\)와 가장 가까운 \\(K\\)개의 응답자 집합, \\(Y_{j}\\)은 선택된 이웃 응답자의 값이다.\n예를 들어, 설문조사에서 소득(Income) 값이 누락된 응답자 A가 있다고 가정하자.\nA의 특성: 연령: 35세, 교육 수준: 대졸, 직업: 엔지니어\n기존 응답자 중 A와 가장 유사한 가장 가까운 응답자 B, C를 선택 (K=2, KNN방식)\n\n\n\n\n\n\\[{\\widehat{Y}}_{A} = \\frac{4,500 + 4,700}{2} = 4,600\\text{만}\\]\n\n\n6. 기계 학습 기반 대체(Machine Learning Imputation)\n기계 학습 기반 대체는 평균 대체나 핫덱 대체와 같은 전통적인 방식보다 더 정교한 예측 기법을 활용하여 결측값을 보완하는 방법이다. 이 접근은 결측값 예측을 회귀 또는 분류 문제로 간주하고, 머신 러닝 알고리즘을 사용하여 데이터 내의 패턴을 학습한 후 결측값을 예측한다는 점이 특징이다.\n연속형 변수(예: 소득, 키, 체중 등)에 대해서는 회귀(regression) 모델을, 범주형 변수(예: 성별, 직업, 지역 등)에 대해서는 분류(classification) 모델을 사용한다. 대체 과정은 다음과 같이 구성된다:\n\n\n예측 모델 학습: 결측값이 없는 데이터를 이용하여 예측 모델을 학습시킨다.\n결측값 예측: 학습된 모델에 결측이 존재하는 데이터를 입력하여 결측값을 예측한다.\n결측값 보완: 예측된 값을 실제 결측값에 대체함으로써 데이터셋을 보완한다.\n\n\n이러한 방식에 활용되는 대표적인 알고리즘으로는 랜덤 포레스트(Random Forest), K-최근접 이웃(K-Nearest Neighbors, KNN), 그리고 신경망(Neural Networks) 등이 있으며, 각각의 알고리즘은 변수의 특성 및 데이터 구조에 따라 다양한 방식으로 결측값 보완에 활용될 수 있다.\n기계 학습 기반 대체는 데이터의 구조를 충분히 반영하여 보다 정밀한 대체값을 제공할 수 있으며, 특히 다변량 데이터 환경에서 효과적인 결측값 처리 방법으로 평가받는다.\n\n(1) 랜덤 포레스트 기반 대체 (Random Forest Imputation)\n랜덤 포레스트는 여러 개의 결정 트리(Decision Tree)를 앙상블하여 예측하는 모델로, 결측값을 예측할 때 기존 응답자의 데이터를 활용하여 랜덤 포레스트 회귀 또는 분류 모델을 학습한 후, 이를 바탕으로 결측값을 예측한다.\n결측값이 연속형 변수일 경우 → 회귀(Random Forest Regression) 모델을 이용해 예측된 평균값으로 대체 결측값이 범주형 변수일 경우 → 분류(Random Forest Classification) 모델을 이용해 가장 자주 등장한 값(다수결)으로 대체\n이 방식은 다음과 같은 절차로 진행된다:\n\n\n결측값이 없는 데이터를 사용해 랜덤 포레스트 모델을 학습한다.\n결측값이 존재하는 관측치에 대해 예측 수행한다.\n예측된 값을 해당 결측 셀에 대체한다.\n\n\n이 방법은 변수 간 상호작용이나 비선형 관계를 잘 포착할 수 있으며, 기존 데이터의 분포를 유지하면서도 예측 정확도를 높이는 데 효과적이다.\n\\(\\widehat{Y} = \\frac{1}{T}\\overset{T}{\\sum_{t = 1}}f_{t}(X)\\), 여기서, \\(T\\) 랜덤 포레스트에서 사용된 결정 트리의 개수, \\(f_{t}(X)\\)은 각 트리에서의 예측값, \\(\\widehat{Y}\\)은 최종 대체값 (회귀: 평균, 분류: 다수)\n예를 들어 소득 변수에 결측값이 존재하는 경우, 랜덤 포레스트 회귀(Random Forest Regression)를 활용하여 이를 대체할 수 있다. 이 방법은 먼저 소득 값이 결측되지 않은 응답자들의 데이터를 이용해 예측 모델을 학습하는 것으로 시작된다. 이때 소득을 예측하기 위한 독립 변수로는 응답자의 연령, 교육 수준, 직업 등이 사용된다. 학습된 랜덤 포레스트 모델은 다수의 결정 트리(decision tree)로 구성되어 있어, 변수들 간의 복잡한 관계를 효과적으로 포착할 수 있다.\n이후, 소득 값이 결측된 응답자에 대해 해당 응답자의 연령, 교육 수준, 직업 정보를 모델에 입력하면, 랜덤 포레스트는 이를 바탕으로 해당 응답자의 소득을 예측하게 된다. 마지막으로, 이렇게 예측된 값을 원래 결측되어 있던 소득 항목에 대체함으로써 데이터를 보완한다.\n이와 같은 방식은 단순히 평균이나 중앙값으로 대체하는 방법보다 예측 정확도가 높고, 데이터의 구조적 특성을 반영할 수 있다는 장점이 있다. 또한 랜덤 포레스트는 이상값에 강하고 과적합 위험이 낮아, 결측값 보완을 위한 실무적 대안으로 널리 사용된다.\n\n\n(2) K-최근접 이웃(KNN) 기반 대체 (K-Nearest Neighbors Imputation)\nKNN 기반 대체(K-Nearest Neighbors Imputation)는 결측값이 있는 관측치에 대해, 해당 관측치와 유사한 특성을 가진 다른 관측치들을 찾아 그 정보를 이용해 결측값을 보완하는 기계 학습 기반 방법이다. 기본 개념은 전통적인 최근접 이웃 대체 방식과 유사하지만, KNN 알고리즘의 특성을 활용하여 보다 정교하고 자동화된 방식으로 수행된다는 점에서 차별화된다.\n이 방법에서는 먼저 결측값이 없는 관측치를 기준으로, 결측값이 포함된 관측치와의 거리를 계산한다. 거리 측정에는 유클리드 거리, 맨해튼 거리, 마할라노비스 거리 등 다양한 지표가 사용될 수 있으며, 변수 간 스케일 차이를 보정하기 위해 정규화(normalization)를 적용하기도 한다.\n그 후, 계산된 거리값을 기준으로 가장 가까운 K개의 관측치를 선택한다. 이때 K 값은 교차검증(cross-validation) 등을 통해 최적의 값을 자동으로 결정할 수 있으며, 고정된 값이 아닌 데이터 구조에 맞게 유연하게 설정된다.\n결측값이 연속형 변수인 경우에는 K개의 이웃값의 평균이나 중앙값을 사용하여 대체하며, 범주형 변수인 경우에는 가장 자주 등장하는 값(최빈값, mode)을 사용한다.\n예를 들어, 어떤 응답자의 소득 정보가 결측된 상황에서 그 사람의 연령, 직업, 교육 수준 등이 다른 응답자와 유사하다면, 이와 유사한 응답자 K명을 찾아 이들의 소득 평균으로 결측값을 대체한다. 이렇게 하면 개별 응답자의 맥락을 반영한 대체가 가능해져, 보다 정확하고 현실적인 결측값 보완이 가능해진다.\nKNN 기반 대체는 연속형 변수와 범주형 변수 모두에 적용 가능하고, 데이터 분포를 유지하면서도 단순 대체 방식보다 유연하고 정밀한 보완이 가능하다는 점에서 강력한 결측 대체 기법으로 평가된다.\n\\(Y_{\\text{miss},i} = \\frac{1}{K}\\sum_{j \\in KNN(i)}Y_{j}\\), 여기서 \\(KNN(i)\\)은 무응답자 \\(i\\) 와 가장 가까운 \\(k\\) 개의 응답자 집합,\\(Y_{j}\\)은 선택된 이웃 응답자의 값이다.\n예를 들어, 어떤 응답자의 키(height)가 결측된 상황을 가정해 보자. 이때 KNN 기반 대체 방법을 적용하면 다음과 같은 절차로 결측값을 보완할 수 있다.\n먼저, 기존 응답자들의 연령(Age), 체중(Weight), 성별(Gender) 등의 정보를 활용하여 결측된 응답자와의 거리를 계산한다. 이때 거리 계산은 유클리드 거리와 같은 수학적 기준을 사용하며, 변수들의 척도 차이를 보정하기 위해 표준화가 선행될 수 있다.\n그다음, 계산된 거리값을 바탕으로 가장 가까운 K명의 응답자를 선택한다. 이웃 수 K는 사전에 지정하거나 교차검증을 통해 결정할 수 있으며, 일반적으로 3명 또는 5명 등의 값이 많이 사용된다.\n선정된 K명의 응답자 중 키(height) 정보가 결측되지 않은 사람들의 키 값을 평균 내어, 해당 평균값을 결측된 응답자의 키 대신 입력한다. 이를 통해 결측값은 유사한 특성을 가진 집단의 평균을 반영한 값으로 대체되며, 데이터의 구조적 일관성을 유지할 수 있다.\n이러한 방식은 단순 평균 대체보다 개별 응답자의 맥락을 반영할 수 있다는 점에서 통계적 타당성과 해석 가능성을 동시에 확보할 수 있다.\n\n\n(3) 신경망(Neural Network) 기반 대체\n신경망을 활용한 결측값 대체는, 특히 변수 간 관계가 복잡하고 비선형적인 경우에 효과적인 방법이다. 이때 주로 사용되는 구조는 다층 퍼셉트론(MLP, Multi-Layer Perceptron)으로, 입력층(input layer), 하나 이상의 은닉층(hidden layer), 그리고 출력층(output layer)으로 구성된다.\n결측값 대체를 위한 일반적인 절차는 다음과 같다.\n1. 데이터 전처리\n먼저, 결측값이 없는 관측값을 기반으로 입력 변수(예: 연령, 성별, 직업 등)와 타겟 변수(예: 소득, 키, 체중 등)를 분리하여 모델 학습용 데이터를 구성한다. 변수 간 범위 차이가 클 경우에는 정규화(normalization) 또는 표준화(standardization)를 통해 모델 학습을 안정화시킨다.\n2. 신경망 모델 학습\n전처리된 데이터를 이용해 신경망 모델을 학습시킨다. 이 과정에서 모델은 입력 변수와 타겟 변수 간의 비선형 관계를 반복적으로 학습하면서, 결측된 값을 예측할 수 있는 패턴을 습득한다. 역전파(backpropagation) 알고리즘과 옵티마이저(예: Adam, SGD)를 활용해 가중치를 조정하며 학습을 진행한다.\n3. 결측값 예측 및 대체\n학습된 모델을 사용하여 결측이 발생한 관측값의 입력 변수들을 모델에 넣고, 해당 타겟 변수(결측된 값)를 예측한다. 이렇게 생성된 예측값을 원래 데이터의 결측값에 대체하여 보완한다.\n예를 들어, 어떤 응답자의 체중 데이터가 누락된 경우, 신경망은 동일한 조사에서 얻은 연령, 키, 성별 등의 정보를 활용하여 체중을 예측하고, 그 값을 해당 응답자의 결측값에 입력한다.\n이 방식은 단순 대체 방법에 비해 학습 기반의 정교한 예측을 제공하며, 특히 변수 간 관계가 복잡한 대규모 데이터셋에서 우수한 성능을 발휘할 수 있다.\n신경망은 다층 퍼셉트론(MLP)을 기반으로 가중치 \\(w\\)를 최적화하여 결측값을 예측한다.\n\\(Y_{\\text{miss}} = f(WX + b)\\) 여기서, \\(f\\)은 활성화 함수(예: ReLU, Sigmoid), \\(W\\)은 가중치, \\(X\\)은 입력 변수, \\(b\\)는 편향이다.\n예를 들어, 건강 관련 조사에서 일부 응답자의 BMI(체질량지수) 정보가 누락된 경우, 신경망을 이용해 이 결측값을 보완할 수 있다. 이때 먼저 BMI와 밀접하게 관련된 변수들, 예컨대 연령, 체중, 성별, 운동 습관 등의 데이터를 활용하여 신경망 모델을 학습시킨다.\n모델 학습이 완료되면, BMI가 결측된 응답자에 대해 해당 변수들을 입력하여 BMI를 예측한다. 이렇게 예측된 BMI 값은 결측된 위치에 삽입되어 데이터셋을 보완하게 된다. 이 과정은 단순히 평균이나 중앙값을 넣는 방식보다 더 정밀하게 데이터의 패턴을 반영할 수 있다는 장점이 있다.\n\n\n\n7. 재조사 및 보완 조사(Follow-up Survey & Call-back)\n재조사 및 보완 조사(Follow-up Survey & Call-back)는 최초 조사에서 응답하지 않은 대상자에게 다시 연락하여 실제 응답을 확보하는 전략이다. 이 방법은 단순히 통계적 기법으로 결측값을 대체하는 방식과 달리, 응답자에게 직접 접근하여 응답을 얻는다는 점에서 데이터의 정확성과 신뢰도를 크게 향상시킨다.\n이러한 접근은 특히 정부 통계, 보건의료 연구, 선거 여론 조사처럼 정책 결정이나 사회적 영향력이 큰 조사에서 중요하게 활용된다. 보완 조사는 전화, 이메일, 방문 등의 다양한 방식으로 수행되며, 무응답 편향을 줄이고 대표성을 높이는 데 기여한다. 또한, 재조사를 통해 무응답자의 특성과 응답행태를 파악할 수 있어 향후 조사 설계의 개선에도 유용하다.\n후속 조사(Follow-up Survey)\n후속 조사는 초기 조사에서 응답하지 않은 대상자에게 다시 연락하여 응답을 유도하는 방식으로, 무응답률을 낮추고 조사 결과의 신뢰도를 높이는 데 중요한 역할을 한다. 이러한 후속 조사는 다양한 형태로 이루어질 수 있다.\n가장 일반적인 방법 중 하나는 전화 재조사(call-back survey)이다. 이는 초기 연락에서 응답을 얻지 못한 대상자에게 다시 전화를 걸어 설문 참여를 요청하는 방식이다. 전화 재조사는 비교적 빠르게 응답을 확보할 수 있는 장점이 있으며, 응답자의 부담이 적을 경우 응답률이 높아질 가능성이 크다.\n또 다른 방법으로는 이메일이나 문자 메시지를 활용한 독려(contact reminder)가 있다. 설문 링크나 마감일, 간단한 조사 목적을 포함한 메시지를 전송함으로써 대상자에게 설문 참여를 다시 상기시키는 방식이다. 이 방법은 시간과 비용이 적게 들고, 자기기입식 조사와 같이 비접촉 방식의 설문에서 특히 효과적이다.\n보다 적극적인 방식으로는 대면 방문(face-to-face interview)이 있다. 이는 조사원이 직접 대상자의 집을 방문하여 설문에 응하도록 요청하는 방식으로, 인구총조사나 국가 단위의 중요 조사에서 주로 활용된다. 응답자가 조사원과 직접 대화하면서 설문에 참여할 수 있어 응답률을 크게 향상시킬 수 있는 반면, 시간과 비용 부담이 크다는 단점이 있다.\n예를 들어, 선거 여론조사에서 초기 응답률이 낮은 경우, 조사 기관은 응답하지 않은 대상자에게 다시 전화를 걸거나 문자 메시지를 보내 설문 참여를 유도할 수 있다. 이러한 후속 조치를 통해 조사에 대한 응답을 확보하고, 조사 결과의 대표성과 신뢰성을 제고할 수 있다.\n보완 조사(Supplementary Survey)\n기존 조사 방식과는 다른 대체 수단을 활용하여 무응답자에게서 응답을 확보하는 방식은, 조사 대상자의 편의성과 접근성을 고려하여 설계된 전략이다. 이는 조사 모드 변경(mode switch) 또는 혼합 모드 설계(mixed-mode design)로도 불리며, 특정 조사 방식에 대한 응답자의 선호나 제약을 고려해 유연하게 대응할 수 있다는 장점이 있다.\n예를 들어, 전화 설문을 중심으로 한 의료 연구에서 응답률이 낮은 경우, 조사자는 대상자에게 우편 설문지를 보내거나, 온라인 응답 링크를 제공하여 다른 방식으로 응답할 수 있는 기회를 부여할 수 있다. 이는 시간대나 장소, 개인적 선호 등 다양한 이유로 기존 조사 방식에 응답하지 못한 사람들에게 효과적인 대안이 될 수 있다.\n이러한 보완 조사를 통해 확보된 응답 데이터는 원래 조사 데이터와 결합되어 분석되며, 이는 전체 데이터의 완전성을 높이고 무응답 편향을 줄이는 데 기여한다. 단, 조사 방식이 다를 경우 질문 해석이나 응답 방식의 차이로 인해 모드 효과(mode effect)가 발생할 수 있으므로, 결합 시에는 적절한 통계적 조정이 필요할 수 있다.\n\n\n8. 데이터 결합(Data Fusion)\n기존 조사 방식과는 다른 대체 수단을 활용하여 무응답자에게서 응답을 확보하는 방식은, 조사 대상자의 편의성과 접근성을 고려하여 설계된 전략이다. 이는 조사 모드 변경(mode switch) 또는 혼합 모드 설계(mixed-mode design)로도 불리며, 특정 조사 방식에 대한 응답자의 선호나 제약을 고려해 유연하게 대응할 수 있다는 장점이 있다.\n예를 들어, 전화 설문을 중심으로 한 의료 연구에서 응답률이 낮은 경우, 조사자는 대상자에게 우편 설문지를 보내거나, 온라인 응답 링크를 제공하여 다른 방식으로 응답할 수 있는 기회를 부여할 수 있다. 이는 시간대나 장소, 개인적 선호 등 다양한 이유로 기존 조사 방식에 응답하지 못한 사람들에게 효과적인 대안이 될 수 있다.\n이러한 보완 조사를 통해 확보된 응답 데이터는 원래 조사 데이터와 결합되어 분석되며, 이는 전체 데이터의 완전성을 높이고 무응답 편향을 줄이는 데 기여한다. 단, 조사 방식이 다를 경우 질문 해석이나 응답 방식의 차이로 인해 모드 효과(mode effect)가 발생할 수 있으므로, 결합 시에는 적절한 통계적 조정이 필요할 수 있다.\n정확한 키 매칭(Exact Matching)\n유일한 식별자(Unique Identifier)를 활용한 데이터 결합 방식은 가장 정밀하고 신뢰도 높은 방법으로, 주민등록번호, 사업자등록번호, 학생번호 등 각 개인 또는 단위를 고유하게 식별할 수 있는 정보를 기반으로 서로 다른 데이터셋을 연결한다.\n이 방식의 핵심 장점은 데이터 간 연결 정확도가 매우 높다는 점이다. 예를 들어, 인구조사 데이터와 국세청의 소득 신고 자료를 주민등록번호를 기준으로 결합하면, 설문에서 소득 정보를 응답하지 않은 무응답자의 소득 데이터를 행정 기록을 통해 보완할 수 있다. 마찬가지로, 환자의 건강 관련 설문조사 데이터를 건강보험청구자료와 연결하면 병원 방문 이력이나 치료 내역을 확인할 수 있어, 설문 응답 외에 객관적 행태 정보를 확보할 수 있다.\n하지만 이러한 방식은 개인정보 보호 측면에서 큰 주의가 필요하다. 유일 식별자를 직접 사용하는 경우, 개인 식별 가능성이 매우 높아지므로, 법적·윤리적 제한이 뒤따르며, 데이터 마스킹이나 가명 처리 등의 조치가 필요하다. 따라서, 이 방식은 통상적으로 통계청, 건강보험공단 등 공공기관이 보안 체계 하에 한정적으로 운영하거나, 연구 목적에 따라 엄격한 심사를 거친 후에만 사용된다."
  },
  {
    "objectID": "notes/survey/sample_design.html",
    "href": "notes/survey/sample_design.html",
    "title": "조사방법론. 2. 표본설계",
    "section": "",
    "text": "chapter 1. 표본설계 개요\n표본 설계는 설문조사가 모집단의 특성을 신뢰성 있고 효율적으로 반영하며 조사 목적을 충실히 달성하기 위해 필요한 핵심 과정이다.\n설문조사의 주요 목적은 모집단 전체의 특성을 이해하거나 추론하는 것이다. 그러나 모집단의 모든 구성원을 조사하는 것은 현실적으로 어렵기 때문에, 이를 대체할 수 있는 대표 표본을 구성하는 과정이 필수적이다. 표본 설계를 통해 모집단의 다양한 특성을 균형 있게 반영할 수 있으며, 이는 신뢰할 수 있는 통계 추정의 기반이 된다.\n표본이 특정 집단에 치우치거나 무작위성이 확보되지 않는다면 조사 결과는 왜곡될 수 있다. 표본 설계는 모집단의 모든 하위 그룹이 적절히 포함되도록 구성하여 편향을 방지하고, 통계적 균형을 유지하는 데 기여한다.\n또한, 표본 설계는 조사 자원을 효율적으로 사용하는 데 중요한 역할을 한다. 제한된 시간과 예산 안에서 필요한 정확도를 확보하기 위해, 층화 표본이나 군집 표본 등 다양한 표본 추출 방법이 활용된다. 이는 조사 비용을 절감하면서도 충분한 정보를 얻을 수 있도록 돕는다.\n표본이 체계적으로 설계되면 통계적 편향과 오차가 줄어들고, 결과적으로 조사 결과의 신뢰성이 향상된다. 이는 조사 데이터를 기반으로 한 정책 수립이나 연구 분석의 질을 높이는 데 직접적인 영향을 미친다.\n마지막으로, 설문조사는 특정 하위 집단에 대한 통찰을 요구하는 경우가 많다. 표본 설계를 통해 특정 집단을 적절히 포함하거나 과대표함으로써, 해당 집단에 대한 정밀한 분석이 가능해진다. 이를 통해 보다 세분화된 해석과 의사결정이 이루어질 수 있다.\n\n1. 용어\n모집단 population\n조사를 할 때 정보를 얻고자 하는 관심 집단, 즉 관심 대상이 되는 모든 사람들의 모임을 모집단이라고 한다.\n\n목표 모집단은 조사 대상 전체를 의미하며, 조사 시점 기준의 유권자가 이에 해당한다.\n조사 모집단은 실제로 조사가 가능한 모집단으로, 예를 들어 전화번호부 CD에 등재된 유권자가 이에 해당한다.\n모수는 모집단의 관심 특성을 의미하며, 예를 들어 A 후보에 대한 지지율이 이에 해당한다.\n\n표본 sample\n모집단 중 조사를 위하여 추출한 일부를 표본이라고 한다. 추정량은 모수를 추정하기 위하여 표본으로부터 계산된 통계량이며, 이를 통계량(statistic)이라고도 한다. 예를 들어, 표본 1,000명 중 A 후보를 지지하는 사람이 560명이라면, A 후보의 지지율에 대한 추정량은 56%이다.\n표본 프레임 sample frame\n표본을 추출하기 위해 모집단 대상을 식별하고, 컨택(contact)에 필요한 정보를 포함한 목록을 말한다. 일반적으로 식별 아이디, 이름, 주소, 연락처 등으로 구성된다. 예를 들어 전화번호 CD를 활용한 조사에서는 해당 가구의 가구원 정보를 알 수 없기 때문에, 조사원이 “최근 생일이 지난 유권자”를 선정하거나, 현장에서 할당 인원을 조정하며 가구원 중 한 명을 조사 대상으로 선정한다. 표본프레임의 3가지 요건은 다음과 같다.\n\n\n포괄성: 표본프레임은 조사 가능한 대상을 모두 포함하고 있어야 한다.\n추출 확률의 동일성: 모집단의 각 구성원이 표본으로 추출될 확률이 동일해야 한다.\n효율성: 사전에 조사되기를 바라는 사람들로만 표본프레임을 구성하는 것은 불가능하지만, 가능한 한 조사 목적에 부합하는 대상이 추출되도록 효율성을 고려한다.\n\n\n표본추출: 표본추출은 조사 대상인 표본을 모집단으로부터 확률적으로 선택하는 과정이며, 조사 목적을 최대한 달성할 수 있도록 설계되어야 한다.\n표본 크기: 표본 크기는 조사의 신뢰수준, 표본추출 방법, 허용오차(예: 변동계수) 등을 고려하여 결정한다.\n조사단위\n\n표본추출단위: 표본을 추출하는 기본 단위. 예를 들어 전화여론조사의 경우 ’가구’가 해당된다.\n\n\n조사단위: 실제로 응답하는 사람. 대부분의 경우 표본추출단위와 조사단위는 일치하지만, 다른 경우도 존재한다. (상이한 경우) 전화여론조사에서 가구를 추출한 뒤, 그 안의 가구원 중 한 명이 응답하는 경우 (동일한 경우) 인터넷 쇼핑몰 이용 고객 실태조사에서 고객 자체가 추출단위이자 조사단위인 경우\n\n\n\n2. 표본과 추정\n모든 설문조사 표본이 확률적 방법으로 선택되는 것은 아니다. 많은 설문조사는 즉흥적으로 표본을 구성하거나, 조사 목적에 부합하도록 표본을 선택한다. 예를 들어, 쇼핑몰에 방문하는 사람들에게 설문 응답을 요청하고, 일정 수의 인터뷰가 완료될 때까지 조사를 계속 진행하는 경우가 있다. 이러한 즉흥적 또는 편의 표본추출 방법에는 공통된 약점이 존재하는데, 바로 모집단의 특성을 설명하기 위한 이론적 기반이 부족하다는 점이다. 이에 반해, 확률 표본추출은 단일 표본으로부터 모집단에 대해 통계적으로 유의미한 진술을 할 수 있는 기반을 제공하며, 일정한 신뢰 수준에서 추론이 가능하다는 장점이 있다.\n외식비용 가구 설문조사의 프레임 모집단은 전화 서비스를 이용하는 모든 가구의 성인을 포함한다. 외식비의 전체 분포를 관찰하거나 완벽히 알 수는 없다. 설문조사는 이러한 분포를 알아내기 위해 수행된다. 모집단 개별 개체는 \\(Y\\)로 표시하고 모집단 평균은 \\(\\overline{Y}\\), 모집단 분산 \\(S^{2}\\)으로 표현하고 설문조사의 주목적은 \\(\\overline{Y}\\)를 추정하는 것이다.\n각 설문조사는 여러 가지 발생 가능한 확률 표본 설계 중 하나의 실현으로 간주될 수 있다. 표본 조사 결과는 소문자를 사용하여 표현한다. 표본 관측값 \\(y\\) 값은 모집단 분포에 대한 평균과 분산을 가지고 있다. 표본평균은 \\(\\overline{y}\\)로 불리며 이를 이용하여 \\(\\overline{Y}\\)를 추정한다. \\(y_{i}\\) 관측값의 표본분산은 \\(s^{2}\\)으로 표시된다.\n표본평균의 표집 분포 분산을 추정하기 위해 하나의 표본 실현에서 계산을 사용하여 \\(V(\\overline{y})\\)를 추정한다. 표본평균의 표집 분포 분산의 또 다른 용어는 ”표본평균의 표본분산”이며 그 제곱근은 ”평균의 표준오차”라고 한다.\n주어진 표본 조사(하나의 실현)에서 프레임 모집단 평균을 추정하고자 할 때, 표준오차를 사용하여 주어진 추정치에 대한 신뢰 구간을 설정한다. ”신뢰구간”은 모든 표본 실현에서 계산된 평균이 전체 프레임 모집단 평균으로부터 일정한 거리 내에 있을 신뢰 수준을 설명한다(포괄성, 비응답 및 기타 설문 조사 오류를 무시한 경우).\n예를 들어, 소비자 조사에서 표본 외식비 평균을 \\(\\overline{y} = 42\\)(천원)로 추정했다고 가정하자. 이 추정치의 표준오차를 2(천원)으로 추정 했다면, 95% 신뢰구간은 (42 - 2*2, 42 + 2*2) = (38, 46)가 된다.\n표본통계의 표준오차는 동일한 설계에서 가능한 표본 실현들 간의 통계적 분포 또는 변동성을 측정하는 지표이다. 표준오차는 \\(se(\\overline{y}) = \\sqrt{v(\\overline{y})}\\) 이며, 표본분산의 제곱근으로 계산된다.\n\n\n\n\n\n\n\n\n\n구분\n모집단 분포\n표본 분포\n표본평균 분포\n\n\n\n\n현황\n실현된 분포\n모름\n모름\n\n\n크기\n\\[i = 1,2,...,n\\]\n\\[i = 1,2,...,N\\]\n\\[s = 1,2,...,S\\]\n\n\n개별 원소\n\\[y_{i}\\]\n\\[Y_{i}\\]\n\\[{\\overline{y}}_{s}\\]\n\n\n평균\n\\[\\overline{y}\\]\n\\[\\overline{Y}\\]\n\\[E({\\overline{y}}_{s})\\]\n\n\n분포 분산\n\\[s^{2}\\]\n\\[S^{2}\\]\n\\[V(\\overline{y})\\]\n\n\n표준오차\n\\[s\\]\n\\[S\\]\n\\[se(\\overline{y})\\]\n\n\n\n샘플링으로 인한 오류의 정도는 설계의 네 가지 기본 원칙에 의해 결정된다.\n\n선택된 표본의 크기.\n다른 모집단 요소가 표본에 선택될 가능성.\n개별 요소가 독립적으로 또는 그룹(요소 또는 클러스터 샘플로 불리는) 내에서 선택되었는지 여부\n표본이 샘플 내 주요 하위 모집단(층화)의 대표성을 제어하도록 설계되었는지 여부\n\n\n\n3. 표본설계 절차\n\n(1) 조사 목표 모집단 정의\n조사의 출발점은 명확한 조사 목표를 설정하는 것이다. 이는 어떤 주제에 대해 누구를 대상으로 정보를 수집할 것인지를 규정하는 단계이다. 예를 들어, 고객 만족도를 평가하거나 특정 지역의 유권자 여론을 파악하고자 할 때, 각각의 조사 목적에 따라 관심 대상이 되는 모집단이 달라진다. 따라서 조사 목표에 부합하는 모집단을 명확히 정의하는 것이 설계 과정의 핵심이다.\n\n\n(2). 모집단 정의 및 분석\n조사 목표가 설정되면, 그에 따라 모집단의 범위를 명확히 정의해야 한다. 모집단이란 조사 대상이 되는 전체 집단으로, 예를 들어 특정 연령대의 고객이나 특정 지역의 거주자 등이 이에 해당한다. 모집단을 정의한 후에는 그 특성을 구체적으로 파악하는 작업이 필요하다. 모집단의 크기, 인구 구성, 지리적 분포와 같은 정보는 표본 설계의 기초가 되며, 이러한 분석을 통해 조사 대상의 대표성을 확보할 수 있다. 모집단에 대한 충분한 분석은 조사 과정 전반의 신뢰성을 높이며, 궁극적으로 정확하고 의미 있는 조사 결과를 도출하는 데 필수적인 단계이다.\n\n\n(3). 표본 프레임 정의\n표본을 추출하기 위해서는 먼저 모집단을 대표할 수 있는 표본 프레임을 정의해야 한다. 표본 프레임은 모집단의 구성원을 식별하고 접근할 수 있도록 구성된 명단이나 데이터베이스로, 예를 들어 고객 리스트, 유권자 명부, 전화번호 디렉터리 등이 이에 해당한다.\n표본 프레임이 조사 목적에 부합하려면 그 정확성과 적절성을 면밀히 평가해야 한다. 프레임이 모집단 전체를 충분히 포괄하고 있는지, 동일한 대상이 중복 포함되어 있지는 않은지, 정보가 최신 상태로 유지되고 있는지 등을 점검하는 과정이 필요하다. 이러한 검토를 통해 불포함 오류나 중복 오류와 같은 표본 추출상의 문제를 최소화할 수 있다.\n\n\n(4). 표본 설계 유형 선택\n표본 설계는 조사 목표와 모집단의 특성에 따라 가장 적합한 추출 방법을 선택하는 과정이다. 이는 조사 결과의 신뢰성과 대표성을 확보하기 위한 핵심 단계로, 단순 무작위추출, 층화추출, 집락추출, 다단계추출 등 다양한 방법 중에서 조사 상황에 맞는 방식을 선택하게 된다. 적절한 표본 설계 유형을 선택하기 위해서는 모집단의 이질성 여부, 표본 프레임의 구조, 예산 및 시간 제약 등 여러 요소를 종합적으로 고려해야 한다.\n\n\n(5). 표본 크기 결정\n표본의 크기는 조사 결과의 정밀도와 신뢰도를 확보하는 데 직접적인 영향을 미치므로 신중하게 결정되어야 한다. 표본크기는 조사 목적, 허용 오차, 신뢰 수준, 모집단의 변동성, 그리고 예산 및 시간 제약 등을 종합적으로 고려하여 산정한다.\n\n\n(6). 표본 추출\n표본 추출은 사전에 설계된 표본 설계 유형에 따라 모집단에서 실제 표본을 선택하는 과정이다. 이 과정에서는 주관적 판단이 개입되지 않도록 무작위성을 철저히 확보해야 하며, 이는 추후 통계적 추론의 타당성을 담보하는 핵심 요건이다.\n\n\n(7). 조사 및 데이터 수집\n표본으로 선정된 대상에게 설문조사, 전화 인터뷰 등 다양한 방법을 통해 자료를 수집한다. 높은 응답률을 확보하기 위해 조사 전 사전 안내를 실시하고, 비응답자에 대해서는 추후 연락을 계획하는 등 응답 유도를 위한 전략이 병행된다.\n\n\n(8). 자료 분석 및 결과 검토\n수수집된 데이터를 분석하여 모집단의 특성을 추정한다. 분석 과정에서는 표본 설계에 따른 가중치를 적용하여 편향을 보정하고, 설계 효과(design effect)를 산출함으로써 표본 설계가 추정 결과에 미친 영향을 평가한다.\n\n\n(9). 결과 보고\n분석 결과는 조사 목적에 맞게 해석하여 보고되며, 조사 결과와 함께 사용된 표본 설계 방법, 표본 추출 방식, 표본 크기, 조사 한계점 등을 함께 제시해야 한다. 이를 통해 결과의 신뢰성, 정밀도, 해석 가능성을 독자가 판단할 수 있도록 한다.\n\n\n\n\nchapter 2. 표본설계 방법\n\n1. 단순임의추출방법 SRS Simple Random Sampling\n\n(1) 정의 및 절차\n\n\n\n\n\n단순임의 표본추출은 모집단에 포함된 모든 요소가 동일한 확률로 선택될 수 있도록 하는 표본 추출 방법이다. 즉, 크기 n의 모든 가능한 표본이 동일한 확률로 선택될 수 있도록 설계된다. 이 방법은 대표성과 무작위성을 보장하는 가장 기본적인 확률 표본 추출 방식으로, 표본 추출의 이론적 기준점이 된다.\n단순임의 표본추출의 절차는 다음과 같다.\n\n\n표본 프레임에 포함된 N개의 모든 원소에 일련번호를 부여한다.\n난수 생성기를 활용하여 중복되지 않는 n개의 난수를 생성한다.\n해당 난수에 해당하는 원소를 표본으로 선택한다.\n\n\n이 과정은 모집단의 각 구성원이 표본에 포함될 동등한 기회를 가지도록 하며, 편향 없는 표본 구성을 가능하게 한다.\n\n\n(2) 추정\n단순 무작위 표본에서 평균 및 표본분산 계산\n\\(\\overline{y} = \\frac{1}{n}\\overset{n}{\\sum_{j = 1}}y_{i}\\), \\(v(\\overline{y}) = \\frac{(1 - f)}{n}s^{2}\\), 여기서 \\(f = n/N\\) 표본 비율, \\(s^{2}\\)은 표본분산이다. \\((1 - f)\\)은 유한모집단보정계수(Finite Population Correction, FPC)이라 하고 \\(N\\)이 충분히 크면 1에 근사하여 \\(v(\\overline{y}) \\approx \\frac{s^{2}}{n}\\) 이다.\n\\(v(\\overline{y})\\)는 표본평균의 샘플링 분산에 대한 표본 기반 추정치에 불과하다는 점을 주목할 필요가 있다. 여기에는 실제 모집단 값 \\(V(\\overline{y})\\)이 존재하며 \\(v(\\overline{y})\\)는 \\(V(\\overline{y})\\)의 불편 추정치로 간주된다. 여론조사와 같이 응답자의 선호 비율을 묻는 경우 표본평균(표본비율)의 표본분산은 다음과 같다. \\(v(p) = \\frac{(1 - f)}{n - 1}p(1 - p)\\). 표본분산 \\(v(p)\\)는 \\(fpc\\)(유한모집단보정계수), 표본크기 \\(n\\), 모집단 비율 \\(p\\) 값 자체에만 의존하므로 \\(y_{i}\\) 값을 알 필요 없이 계산 가능하다.\n표본크기 결정 시 모평균 추정의 경우 모집단 분산 \\(S^{2}\\)에 대한 정보가 있어야 하지만 모비율 추정 시에는 보수적으로 \\(p = 0.5\\)을 사용하여 결정한다.\n모집단 평균 95% 신뢰구간\n\\(\\overline{y} + z_{0.975}se(\\overline{y})\\), 여기서 \\(se(\\overline{y}) = \\frac{s}{\\sqrt{n}}\\)\n모비율 평균 95% 신뢰구간\n\\(\\widehat{p} + z_{0.975}se(\\overline{y})\\), 여기서 \\(se(\\widehat{p}) = \\frac{p(1 - p)}{\\sqrt{n}}\\)\n표본크기 허용오차 margin of error 표본오차: \\(e\\)\n【표본크기 : 유한 모집단】\n모평균 추정: \\(n = \\frac{N \\cdot z^{2} \\cdot S^{2}}{(N - 1) \\cdot e^{2} + z^{2} \\cdot S^{2}}\\)\n모비율 추정: \\(n = \\frac{N \\cdot z^{2} \\cdot p \\cdot (1 - p)}{(N - 1) \\cdot e^{2} + z^{2} \\cdot p \\cdot (1 - p)}\\)\n【표본크기 : 무한 모집단】\n모평균 추정: \\(n = \\frac{z^{2} \\cdot S^{2}}{e^{2}}\\)\n모비율 추정: \\(n_{0} = \\frac{z^{2} \\cdot p \\cdot (1 - p)}{e^{2}}\\) (유한 \\(n = \\frac{n_{0}}{1 + \\frac{n_{0} - 1}{N}}\\))\n\n\n\n2. 군집추출방법 cluster sampling\n\n\n\n\n\n단순임의표본추출과 군집표본추출의 비교\n단순임의표본추출은 모집단의 모든 구성 요소 중에서 각각이 동일한 확률로 선택될 수 있도록 표본을 추출하는 방법이다. 이 방식은 통계적으로 가장 이상적인 방법 중 하나지만, 실제 조사에서는 표본 프레임의 구성과 개별 요소에 접근하는 데 드는 시간과 비용이 상당히 크다는 한계가 있다.\n이러한 실용적 제약을 해결하기 위해 사용되는 방법이 군집표본추출이다. 군집표본추출은 모집단을 사전에 정의된 군집(집단) 단위로 나누고, 이 군집들 중 일부를 무작위로 선택한 뒤, 선택된 군집 내의 모든 요소를 조사하는 방식이다. 이는 전체 프레임을 구성하지 않고도 비교적 적은 비용으로 표본을 수집할 수 있다는 장점이 있다.\n군집표본추출의 절차와 사례\n예를 들어, 표본 프레임이 총 60가구로 구성되어 있고, 그 중 30가구는 O로, 나머지 30가구는 +로 표시되어 있다고 하자. 전체 가구 중 20가구를 조사 대상으로 선택해야 하는 경우, 단순임의추출을 사용할 경우 60가구 중 무작위로 20가구를 선택하게 된다.\n반면 군집표본추출을 적용하면, 예컨대 10가구씩 구성된 6개의 단지를 군집으로 보고, 이 중 2개의 단지를 무작위로 선택한 후, 선택된 단지의 모든 가구(총 20가구)를 조사하게 된다.\n군집표본추출의 장점과 주의점\n군집표본추출은 조사 단위를 군집화함으로써 조사에 소요되는 시간과 비용을 크게 줄일 수 있다. 예를 들어 단지 단위로 이동하거나 접촉하는 것이 개별 가구를 조사하는 것보다 훨씬 효율적일 수 있다.\n그러나 이 방법에는 중요한 단점도 존재한다. 만약 무작위로 선택된 두 개의 단지(예: 5번과 6번 단지)에 O로 표시된 가구가 거의 없는 경우, 전체 표본에서 O 가구의 비율이 실제 모집단에서의 비율과 현저히 달라질 수 있다. 이로 인해 표본의 대표성이 훼손되고, 왜곡된 추정 결과를 초래할 수 있다.\n\n(1) 군집표본 추출 절차\n군집표본추출은 모집단을 일정 기준에 따라 여러 개의 소집단, 즉 군집으로 나눈 뒤, 일부 군집만을 무작위로 선택하고 그 안의 구성원들을 표본으로 조사하는 방법이다. 모집단을 그룹으로 나눈다는 점에서는 층화추출과 유사하지만, 군집 간에는 응답의 차이가 없다고 가정한다는 점에서 차이가 있다. 따라서 한 군집이 무작위로 선택되면, 그 안에 포함된 구성원만을 대상으로 표본을 추출하게 된다.\n군집표본추출의 절차는 다음과 같다.\n\n\n모집단을 인구학적 특성이나 지리적 위치 등을 기준으로 군집으로 나눈다.\n난수를 이용하여 군집을 무작위로 선택한다.\n선택된 군집에 속한 모든 응답자를 표본으로 포함시킨다. 단, 군집의 크기가 표본 크기보다 클 경우에는 군집 내부에서 다시 단순임의추출 방법으로 일부 응답자만 선택한다.\n\n\n군집추출은 조사 비용과 시간을 줄이는 데 효과적이지만, 군집 간의 동질성이 확보되지 않을 경우 표본의 대표성이 떨어질 수 있으며, 그에 따라 추정의 정확도도 낮아질 수 있다. 따라서 군집을 설정할 때에는 군집 간 차이를 최소화하고, 군집 내의 다양성을 충분히 확보하는 것이 중요하다.\n가구조사 표본추출 사례\n가구조사에서는 규모 비례 확률 방법을 사용하여 전국을 200개 지역으로 층화하고, 이후 일련의 계통추출 과정을 통해 가구 내 응답자를 선택한다. 표본추출은 다음과 같은 네 단계로 이루어진다.\n첫째, 전국을 12개 층으로 구분한다. 6개 광역도시는 서울, 부산, 대구, 인천, 대전, 광주이며, 8개 도는 경기, 강원, 충청남북도, 경상남북도, 전라남북도로 구분된다. 도 지역은 다시 시, 읍, 면으로 세분화한다.\n둘째, 6개 도시와 각 도의 시·읍·면을 모집단으로 배열한 후, 각 지역 내 동(또는 면의 경우 리)을 계통추출 방식으로 선택한다. 이 단계에서 선택된 동 또는 리는 1차 표본 지역(primary sampling location)으로 정의된다. 표본의 전체 크기가 1,500일 경우, 약 200개의 1차 표본 지역이 확보된다.\n셋째, 실질적인 최종 표본 지역(actual final sampling location)인 반 또는 부락이 선택될 때까지 계통추출을 반복한다. 반은 대체로 약 20가구, 부락은 20~80가구로 구성된다.\n넷째, 조사원은 선정된 표본 지역을 직접 방문하여 주민 명부를 바탕으로 8가구를 임의로 선정한다. 각 가구에서 응답자는 18세 이상인 사람 중 생일이 가장 빠른 사람으로 지정하며, 최초 방문 시 해당 응답자를 만나지 못한 경우에는 재방문하여 조사를 진행한다.\n이 사례는 다단계 층화 계통추출의 전형적인 구조를 보여주며, 실제 조사의 대표성과 실현 가능성을 동시에 고려한 표본설계의 예라 할 수 있다. 필요하시면 이 내용을 도식화하거나 표본설계 흐름도로도 제공해드릴 수 있습니다.\n\n\n(2) 표본평균 추정\n평균은 SRS와 동일하게 계산되지만, 각 단지 내의 모든 소득을 합한 후, 각 단지의 합계를 더하고 최종적으로 표본크기로 나누는 방식으로 이루진다.\n\\(\\overline{y} = \\frac{\\sum_{\\alpha = 1}^{a}\\sum_{\\beta = 1}^{B}y_{\\alpha\\beta}}{aB}\\), 여기서 \\(a\\)는 단지 수, \\(\\beta\\)는 가구 수이다.\n평균의 표본분산\n무작위화는 단지에만 적용되며, 단지가 표본 단위이다. 어떤 단지가 선택되는지에 따라 \\(\\overline{y}\\)의 값이 달라진다. 어떤 면에서는 모든 것이 SRS와 동일하게 유지되지만, 군집을 표본 내 요소로 간주한다는 점이 다르다.\n\\(v(\\overline{y}) = \\left( \\frac{1 - f}{a} \\right)s_{a}^{2}\\), 여기서 \\(s_{a}^{2}\\)는 \\(a\\)개의 단지에 걸친 평균 소득의 변동성을 나타낸다. 즉, \\(s_{a}^{2} = \\left( \\frac{1}{a - 1} \\right)\\overset{a}{\\sum_{\\alpha = 1}}\\left( \\overline{y}\\alpha - \\overline{y} \\right)^{2}\\), 여기서 \\(\\overline{y}\\alpha\\)는 \\(\\alpha\\)번째 단지의 평균 소득이다. 군집표본의 경우, 요소 분산 \\(s^{2}\\)대신 ”군집 간 분산”을 사용한다.\n\n\n(3) 설계효과 design effect\n단순임의추출과 비교했을 때 실제 사용된 표본설계로 인해 표본분산이 얼마나 증가했는지를 나타내는 지표를 설계효과(design effect)라고 한다. 설계효과는 설문조사에서 군집화 효과, 층화 효과, 또는 복합 표본설계가 추정 결과에 미치는 영향을 정량적으로 표현하는 데 사용된다.\n설계효과: \\(d^{2} = \\frac{v(\\overline{y})}{v_{\\text{SRS}}(\\overline{y})}\\)\n\\(v(\\overline{y})\\): 군집표본에서의 표본 분산\n\\(v_{\\text{SRS}}(\\overline{y})\\): 동일한 표본 크기에서 SRS 표본분산\n설계효과와 군집 내 동질성\n설계효과는 군집 표본 추출이 단순임의추출에 비해 표본분산에 미치는 영향을 측정하는 지표로, 군집 내 요소들의 동질성과 밀접한 관련이 있다. 일반적으로 군집 표본은 요소 단위의 표본에 비해 표본분산이 더 크게 나타나는 경향이 있다. 이는 군집 간 평균값의 차이가 존재한다는 것을 의미하며, 동시에 각 군집 내에서는 요소들이 서로 유사한 특성을 가질 가능성이 높다는 것을 나타낸다.\n예를 들어, 단지 간 평균 소득에 차이가 있다는 것은 각 단지 내 가구들의 소득이 서로 유사하다는, 즉 군집 내 동질성이 높다는 의미로 해석될 수 있다. 이러한 경우, 한 군집에서 여러 요소를 표본으로 포함시켜도 얻어지는 정보는 중복될 가능성이 높다.\n이러한 맥락에서 다음과 같은 질문이 제기된다. “같은 군집에서 한 요소를 추가로 표본에 포함시킬 때, 모집단에 대한 어떤 새로운 정보를 얻을 수 있는가?” 극단적인 예로, 교실의 모든 학생이 동일한 시험 점수를 가지고 있다고 가정하면, 한 학생의 점수를 알게 된 순간 나머지 학생들의 점수는 별다른 추가 정보를 제공하지 못하게 된다. 이 경우, 각 교실에서 단 한 명만 조사해도 전체 분포를 파악할 수 있으므로, 조사 비용을 크게 절감할 수 있게 된다.\n군집 내 동질성이 높을수록 설계효과는 커지며, 이는 추정의 정확도를 낮출 수 있다. 따라서 군집 표본을 설계할 때는 군집 간 이질성을 확보하고 군집 내 동질성을 최소화하려는 노력이 필요하다.\n설계효과 측정\n군집 내 요소 값의 상관성 intraclass correlation을 사용하는 것이다. 이는 군집 간을 평균으로 한 상관계수로, 군집 내 변수 값들이 군집 외부의 값들과 비교하여 서로 얼마나 상관되어 있는지를 측정한다. 군집 내 동질성 rate of homogeneity은 \\(roh\\)로 나타내며, 이는 거의 항상 0보다 큰 양수이다.또한, 이 \\(\\rho\\)를 설계효과와 연결할 수 있다. 이는 크기 n인 SRS에서 크기 n인 군집 표본으로 전환할 때 표본 분산 변화의 요약값을 제공한다.\n설계효과는 \\(d^{2} = 1 + (b - 1)roh\\), 여기서 \\(b\\)는 각 군집(예: 단지)에서 표본으로 추출된 요소의 크기를 나타낸다. 즉, 표본분산의 증가는 단지 소득에서 관찰된 가구 간 동질성의 정도와 각 단지에서 추출된 표본크기 \\(b\\)에 실제로 의존한다. \\(roh\\)는 변수의 유형에 따라 달라지며, 군집 내 동질성의 비율이 높은 변수는 평균에 대해 더 큰 설계효과를 가진다. 일반적으로 이 표는 사회경제적 변수에서 높은 roh 값을 보이며, 태도나 여성의 출산 경험과 같은 변수에서는 낮은 roh 값을 보인다. 군집에서 추출된 표본 크기가 클수록 설계효과도 커진다. 각 군집에서 1개만 선택하거나, \\(roh = 0\\)이면 설계효과는 1로 SRS 분산과 동일하다.\n\\(roh\\) 추정 및 활용\n추정: \\(roh = \\frac{(d^{2} - 1)}{(b - 1)}\\)\n동일하거나 유사한 주제에 대해, 거의 동일한 모집단에서 사전 조사 결과로부터 \\(roh_{old}\\)값을 계산했다고 가정한다. 새로운 설계에서 표본 분산을 추정하려면, 먼저 새로운 설계 효과를 계산해야 한다.\n\\(d_{\\text{new}}^{2} = 1 + (b_{\\text{new}} - 1)roh_{\\text{old}}\\), 여기서 \\(b_{\\text{new}}\\)는 새로운 설계에서 군집당 표본 요소 수이다. 그런 다음, 이 새로운 설계 효과를 사용하여 새 표본의 평균에 대한 SRS 분산 추정치에 곱한다.\n\\(v(\\overline{y}) = \\left( \\frac{1 - f}{n} \\right)s^{2},\\) 여기서 \\(s^{2}\\)는 사전 조사에서 추정되며, \\(n\\)은 새로운 설계에 의해 결정된다.\n설계효과 \\(d^{2}\\)의 또 다른 해석\n\\(d^{2}\\)는 군집 표본을 추출함으로써 정밀도가 손실된 정도를 나타낸다. 표본 크기는 200명이고 설계효과 \\(d^{2} = 3.13\\)라 하자. 실제로 동일한 분산을 가지는 SRS 표본 크기는 \\(n_{\\text{eff}} = \\frac{200}{3.13} \\approx 64\\)로 200명이 아니다. ”효과적 표본 크기”는 실제 설계로 달성된 것과 동일한 표본 분산을 산출하는 SRS 표본 크기이다.\n선택된 군집 내 하위 표본 추출\n선택된 군집 내 표본 크기(예: 단지당 선택된 가구 수)를 줄이면 평균 소득의 표본 분산에 대한 군집 효과를 감소시킬 수 있다. 이는 군집화가 결과의 정밀도에 미치는 해로운 영향을 완화하려는 타협점이다. 군집 표본을 더 많은 군집에 분산시키는 것은 전체 표본 요소 수를 유지하면서도 일반적으로 총 비용을 증가시킨다.\n\n\n\n3. 층화추출방법 stratified sampling\n\n\n\n\n\n확률 표본 설계는 모집단의 하위 그룹들이 표본 내에 적절히 대표되도록 보장하는 방식으로 개선될 수 있다. 이러한 기능 중 하나가 층화(stratification)이다.\n층화는 표본 프레임에 포함된 모집단 요소들이 사전에 정의된 기준에 따라 상호 배타적인 그룹, 즉 층(strata)으로 구분될 수 있는 정보를 가지고 있다는 전제에 기반한다. 각 요소는 오직 하나의 층에만 속할 수 있으며, 이처럼 나뉜 층은 서로 겹치지 않는 범주로 구성된다.\n층화표본추출에서는 각 층에서 표본을 독립적으로 선택한다. 이때 모든 층에서 동일한 표본추출 절차(예: 단순임의추출)를 사용할 수도 있고, 층의 특성에 따라 서로 다른 추출 방법(예: 어떤 층에서는 단순임의추출, 다른 층에서는 군집추출)을 적용할 수도 있다.\n층화는 특히 모집단 내에 중요한 이질적 특성이 존재할 경우 유용하며, 각 하위 집단의 특성을 보다 정확하게 추정할 수 있도록 도와준다. 또한 전체 표본의 분산을 줄이는 데에도 기여할 수 있다.\n\n(1) 층화표본 추출 절차\n층화추출과 군집추출은 모두 모집단을 그룹으로 나누는 방식이지만, 그 목적과 활용 방식에서 중요한 차이가 있다.\n층화추출은 모집단을 사전에 정의된 기준에 따라 여러 개의 층으로 구분하고, 각 층에서 독립적으로 표본을 추출하는 방법이다. 이때 각 층은 내부적으로는 비교적 동질적이고, 층 간에는 이질적인 특성을 가지도록 구성된다. 이러한 방식은 모집단 내 다양한 하위 집단이 표본에 반드시 포함되도록 보장하므로, 조사 결과의 대표성과 정밀도를 높이는 데 효과적이다. 특히 성별, 연령대, 지역 등과 같이 사전에 알려진 중요한 구분 기준이 있는 경우 유용하다.\n반면, 군집추출은 모집단을 물리적 또는 행정적 단위에 따라 군집으로 나눈 후, 이들 군집 중 일부만을 무작위로 선택하여 조사하는 방식이다. 선택된 군집 내에서는 모든 구성원을 조사하거나, 추가적인 표본추출 절차를 통해 일부만을 조사할 수 있다. 군집 간에는 큰 차이가 없다고 가정하며, 각 군집은 내부적으로 다양한 특성을 가진 이질적인 집단으로 구성되는 것이 일반적이다. 군집추출은 시간과 비용을 절감하는 데 유리하지만, 군집 간 이질성이 크고 군집 내 동질성이 높을 경우 표본 오차가 증가할 수 있다는 단점이 있다.\n요약하자면, 층화추출은 하위 집단의 대표성을 보장하고 정밀도를 높이기 위해 사용되며, 군집추출은 접근성과 비용 효율성을 고려할 때 활용되는 방법이다. 두 방법은 모두 확률 표본 설계의 일종이지만, 조사 목적과 상황에 따라 적절히 선택되어야 한다.\n\n\n(2) 층화표본 추출 절차\n층화표본추출은 모집단을 인구학적 특성에 따라 서로 다른 집단으로 구분한 뒤, 각 집단에서 일정 수의 표본을 선택하여 전체 표본을 구성하는 방식이다. 여기서 각 집단은 ’층’이라고 하며, 층화의 목적은 모집단 내 서로 다른 응답 성향을 가진 하위 집단이 표본에 적절히 대표되도록 보장하는 데 있다.\n일반적으로 집단 간에는 응답의 차이가 존재하며, 각 집단 내의 개체는 모집단의 특성을 반영하는 정보를 고르게 가지고 있다고 본다. 따라서 각 층에서 고르게 표본을 추출함으로써, 전체 표본의 대표성과 추정의 정확도를 높일 수 있다.\n층화표본추출은 다음과 같은 절차에 따라 진행된다.\n첫째, 모집단을 인구학적 특성에 따라 분류한다. 성별, 연령, 학년, 직업, 거주 지역 등과 같이 응답 성향에 영향을 미칠 수 있는 변수들을 기준으로 그룹화하며, 이를 통해 모집단을 이질적인 하위 집단으로 나눈다.\n둘째, 각 층에서 몇 개의 표본을 추출할 것인지를 결정한다. 이는 각 층의 크기를 기준으로 하여 비례적으로 할당하거나, 조사 목적에 따라 비례하지 않게 배정할 수도 있다. 표본 수를 층의 크기에 비례하여 결정하는 방식을 확률비례추출이라 한다.\n셋째, 각 층에서 배정된 표본을 실제로 추출한다. 이때 표본추출 방법은 단순임의추출(SRS)이나 계통추출(Systematic Sampling)과 같은 무작위 방법을 사용할 수 있다.\n이와 같이 층화표본추출은 표본의 구조적 대표성을 높이는 데 효과적인 방법이며, 특히 모집단 내 이질성이 예상될 때 유용하게 활용된다.\n\n\n(3) 표본평균 추정\n비례 할당은 각 층에서 동일한 선택 확률을 사용하여 표본을 선택하는 것과 동일하며, 이는 동등한 확률 선택 방법이다. 즉, \\(f_{h} = n_{h}/N_{h}\\)로 층 \\(h\\) 의 표본 추출 비율이다. 층 \\(h\\)의 \\(n_{h}\\)크기의 표본을 선택할 수 있으며, 이때 표본에서 해당 층의 요소 비율은 모집단에서 해당 층의 요소 비율 \\(\\frac{N_{h}}{N}\\)과 동일하다. 여기서 \\(N_{h}\\)는 층 \\(h\\)에 있는 모집단 요소의 수를 의미하며, \\(\\frac{N_{h}}{N}\\)은 각 층의 모집단 비율이다.\n전체 모집단에 대한 추정치를 얻으려면 각 층의 결과에 모집단 비율 \\(W_{h}\\)를 가중치로 사용하는 것이다. 예를 들어, 모집단 평균을 추정하고자 하고, 각 층에 대한 평균 \\({\\overline{y}}_{h}\\)를 계산했다고 하자. 모집단 평균에 대한 층화된 추정치는 \\({\\overline{y}}_{st}\\)로 불리며, 여기서 st는 ”층화”를 나타낸다.\n\\({\\overline{y}}_{st} = \\overset{H}{\\sum_{h = 1}}W_{h}{\\overline{y}}_{h}\\), 여기서 \\({\\overline{y}}_{st}\\)는 층 평균의 가중 합이다.\n\n\n(4) \\({\\overline{y}}_{st}\\)의 표본 분산\n\\(v({\\overline{y}}_{h}) = \\left( \\frac{1 - f_{h}}{n_{h}} \\right)s_{h}^{2}\\), 여기서 \\(f_{h} = n_{h}/N_{h}\\)는 층 h의 표본 추출률이다. \\(s_{h}^{2}\\)는 층 \\(h\\)의 요소 분산으로, 층 내 요소 평균 \\({\\overline{y}}_{h}\\)를 기준으로 다음과 같이 계산된다. \\(s_{h}^{2} = \\left( \\frac{1}{n_{h} - 1} \\right)\\overset{n_{h}}{\\sum_{i = 1}}(y_{hi} - {\\overline{y}}_{h})^{2}\\).\n따라서, 층화된 임의 표본 추출에서는 SRS에서처럼 하나의 요소 분산만 계산하는 것이 아니라, 각 층에 대해 별도의 분산을 계산해야 한다.\n\\(v(\\overline{y}st) = \\overset{H}{\\sum_{h = 1}}W_{h}^{2}\\left( \\frac{1 - f_{h}}{n_{h}} \\right)s_{h}^{2}\\), 여기서 \\(W_{h}\\)는 각 층의 모집단 비율로, 각 층의 SRS 분산에 대해 모집단 비율의 제곱을 가중치로 사용한다.\n\n\n(5) 설계효과\n\\[d^{2} = \\frac{v(\\overline{y}st)}{v\\text{SRS}(\\overline{y})} = \\frac{\\sum_{h = 1}^{H}W_{h}^{2}\\left( \\frac{1 - f_{h}}{n_{h}} \\right)s_{h}^{2}}{\\left( \\frac{1 - f}{n} \\right)s^{2}}\\]\n이 설계효과는 1보다 작거나, 1과 같거나, 심지어 1보다 클 수도 있다. 설계효과의 크기는 각 층에서 선택된 표본 크기, 즉 층화 내 표본 할당 방식에 크게 의존한다.\n비율의 추정 절차는 평균에 대한 추정 절차와 유사하며, 실제로 동일한 공식을 사용할 수 있다. 그러나 비율의 추정은 종종 다음과 같은 비율의 형태로 표현된다.\n\\(p_{st} = \\overset{H}{\\sum_{h = 1}}W_{h}p_{h}\\), \\(v(p_{st}) = \\overset{H}{\\sum_{h = 1}}W_{h}^{2}\\left( \\frac{1 - f_{h}}{n_{h} - 1} \\right)p_{h}(1 - p_{h})\\)\n모평균 추정치 \\({\\overline{y}}_{st} = \\overset{H}{\\sum_{h = 1}}W_{h}{\\overline{y}}_{h} = \\overset{H}{\\sum_{h = 1}}\\left( \\frac{N_{h}}{N} \\right){\\overline{y}}_{h}\\)을 대수적 방법으로 재표현 하면 \\(\\overline{y}st = \\frac{\\sum_{h = 1}^{H}\\sum_{i = 1}^{n_{h}}w_{hi}y_{hi}}{\\sum_{h = 1}^{H}\\sum_{i = 1}^{n_{h}}w_{hi}}\\), 여기서 \\(w_{hi}\\)는 데이터 세트의 가중치 변수로, 층 \\(h\\)에 있는 요소 \\(i\\)의 \\(w_{hi} = \\frac{N_{h}}{n_{h}}\\)이다. 즉, 가중 평균은 가중 총합을 가중치의 합으로 나눈 값이다.\n\\({\\overline{y}}_{st}\\)의 표본 분산은 가장 간단하게 층 전체의 분산에 대한 가중 합으로 표현될 수 있다. 각 층에서 단순 임의 표본 추출(SRS)을 사용했다면, 다음과 같이 계산된다.\n\\[v({\\overline{y}}_{st}) = \\overset{H}{\\sum_{h = 1}}W_{h}^{2}(\\text{variance of}h\\text{-th stratum mean})\\]\n\\(v({\\overline{y}}_{st}) = W_{1}^{2}\\left( \\frac{1 - f_{1}}{n_{1}} \\right)s_{1}^{2} + W_{2}^{2}\\left( \\frac{1 - f_{2}}{n_{2}} \\right)s_{2}^{2} + W_{3}^{2}\\left( \\frac{1 - f_{3}}{n_{3}} \\right)s_{3}^{2} + \\cdots\\), 여기서 \\(W_{h}\\)는 층 \\(h\\)의 모집단 비율, \\(f_{h} = n_{h}/N_{h}\\)는 층 \\(h\\)의 표본 추출률, \\(s_{h}^{2}\\)는 층 \\(h\\)의 분산이다. 즉, 분산의 추정은 층별로 계산된 후, 층별 결과를 결합하여 이루어진다.\n\n\n(6) 층화 추출의 설계효과가 \\(d^{2} &lt; 1\\) (1보다 작은 경우)\n설계효과 \\(d^{2} = \\frac{v(\\overline{y}st)}{v\\text{SRS}(\\overline{y})} = \\frac{\\sum_{h = 1}^{H}W_{h}^{2}\\left( \\frac{1 - f_{h}}{n_{h}} \\right)s_{h}^{2}}{\\left( \\frac{1 - f}{n} \\right)s^{2}}\\)이므로 만약 \\(\\overset{H}{\\sum_{h = 1}}W_{h}^{2}s_{h}^{2}\\)가 \\(s^{2}\\)보다 작아지면 설계효과는 1보다 작아진다.\n1. 층 간 변동이 큰 경우\n층화 추출의 가장 큰 이점은 **층 간의 이질성(층 간 변동)**을 활용하여 전체 모집단의 변동을 줄이는 데 있다. 층화 추출은 동일한 크기의 단순 임의 추출(SRS)보다 표본이 더 모집단을 잘 대표하도록 설계된다. 각 층 내부는 상대적으로 동질적이지만, 층 간 차이가 클 경우, 층화는 표본 평균의 변동성을 줄여 설계효과가 1보다 작아지게 된다.\n예를 들어, 수입 수준, 교육 수준, 지역별 생활비 등에서 층 간 큰 차이가 있는 경우 설계효과가 1보다 작아질 가능성이 높다.\n2. 적절한 층화 및 비례 할당\n층화는 모집단을 적절히 나누고, 각 층에서 비례적으로 표본을 추출할 때 설계효과가 줄어들 가능성이 크다.\n\\(f_{h} = \\frac{n_{h}}{N_{h}},W_{h} = \\frac{N_{h}}{N}\\) 비례 할당을 통해 각 층이 모집단을 더 잘 대표하도록 표본을 추출하면, 분산 감소 효과가 더 크게 나타난다.\n3. 층 내 내부 변동이 작은 경우\n각 층 내부의 요소들이 상대적으로 동질적일수록(층 내 변동이 작을수록), 해당 층의 분산 s_h^2가 감소하여 전체 표본 분산이 줄어든다.\n층 내 요소들이 비슷한 특성을 가지고 있다면, 적은 표본으로도 각 층을 충분히 대표할 수 있다.\n4. 효율적인 표본 배분 (Neyman Allocation)\nNeyman Allocation과 같은 최적 배분 방식을 사용하면 각 층의 변동성에 비례하여 표본을 배분할 수 있다. \\(n_{h} \\propto N_{h} \\cdot s_{h}\\)\n각 층의 크기와 분산을 고려하여 표본을 배분하면, 층화 추출의 효율성이 높아지고 설계효과가 줄어든다.\n\n\n(7) 층에 대한 비례하지 않은 할당\n층화표본추출에서 가장 일반적인 표본 할당 방식은 각 층의 크기에 비례하여 표본을 배분하는 비례할당이다. 그러나 비례할당 외에도 단순임의표본추출에 비해 더 작은 표본 분산을 유도할 수 있는 다양한 할당 방법이 존재한다.\n표본설계에서 어떤 할당 방식이 사용되느냐에 따라 추정값의 정확도가 달라질 수 있으며, 특정 할당 방법은 가능한 모든 할당 방식 중에서 표본 평균의 분산을 최소화하는 특징을 가진다. 이러한 최적의 할당 방식은 폴란드의 통계학자 예지 네이만(Jerzy Neyman)에 의해 제안되었으며, 그의 이름을 따서 네이만 할당(Neyman allocation)이라 불린다.\n네이만 할당은 각 층의 표준편차와 크기를 함께 고려하여 표본을 배분하는 방식으로, 층 간 변동성이 클수록 더 많은 표본을 배정하게 된다. 이를 통해 표본의 효율성을 극대화하고, 동일한 표본 크기 하에서 보다 정밀한 추정을 가능하게 한다.\n층화 표본을 위한 네이만 할당은 여러 비례하지 않은 할당 방법 중 하나이다. 하지만 이 방법은 동일한 크기의 표본을 사용할 때 가장 작은 표본 분산을 가지는 특징을 가진다. 네이만 할당을 사용하려면 각 층의 모집단 비율 \\(W_{h}\\)뿐만 아니라, 표준편차 \\(S_{h} = \\sqrt{S_{h}^{2}},\\)도 알고 있어야 한다.\n표본크기\n각 층에 대해 \\(W_{h}S_{h}\\)의 곱을 계산하고, 이를 모든 층에서 합산한다. 그러면 네이만 할당 방식에서 각 층의 표본 크기는 다음과 같이 주어진다. \\(n_{h} = n\\frac{W_{h}S_{h}}{\\sum_{h = 1}^{H}W_{h}S_{h}}\\). 즉, 표본 크기는 \\(W_{h}\\)에 비례하는 것이 아니라 \\(W_{h}S_{h}\\)에 비례하도록 할당된다. 따라서 층의 크기가 크면, 비례 할당과 마찬가지로 더 많은 표본을 할당한다. 하지만, 층 내 요소들의 변동성이 클 경우에도 더 많은 표본을 할당하게 된다. 즉, 층 내 변동성이 높을수록 표본 크기를 증가시키게 된다.\n\\(S_{h} = \\sqrt{S_{h}^{2}}\\) 값이 클수록 해당 층에 더 많은 표본을 할당해야 한다. 다시 말해, 층 내 요소들의 분산이 높은 경우, 해당 층에서 더 큰 표본을 추출하여 다른 층보다 상대적으로 더 안정적인 표본 통계를 얻을 수 있도록 한다.\n네이만 할당 관련 코멘트\n네이만 할당은 단순 임의 표본 추출(SRS)보다, 심지어 비례 할당보다도 정밀도를 크게 향상시킬 수 있다. 그러나 이 방법에는 몇 가지 단점이 있다.\n첫째, 네이만 할당은 반드시 비율 추정에 적합하지 않다. 비율 데이터를 다룰 때 층 간 비율 차이가 크게 나야 하지만, 그러한 차이를 가지는 변수를 찾기가 어려울 수도 있다.\n둘째, 네이만 할당은 한 번에 하나의 변수에 대해서만 최적화된다. 만약 조사에서 여러 개의 변수를 수집하고 있다면, 한 변수에 대한 네이만 할당이 다른 변수에 대한 할당과 다를 수 있다. 그리고 주된 관심 변수에 대해 최적화된 네이만 할당이 다른 변수들에는 적절하지 않을 가능성이 있다. 이로 인해 일부 변수에서는 오히려 설계효과가 1보다 커지는 경우가 발생할 수 있다.\n층 내 분산에 대한 충분한 정보 없이 비례하지 않은 할당을 적용하는 것은 위험하다. 이는 전체 표본의 표준 오차 증가로 이어질 수 있다. 예를 들어, 단순한 할당 방식으로 모든 층에 동일한 표본 크기를 배정하는 경우를 생각해 보자. 층 크기가 다름에도 불구하고 동일한 표본 크기를 배정하면 표본 오차가 증가할 수 있다.\n\n\n\n4. 계통 표본 추출 systematic selection\n\n\n\n\n\n\n(1) 계통 추출 절차\n표본을 추출하기 위해 먼저 모집단의 크기와 원하는 표본의 크기를 정한다. 그런 다음, 모집단 크기를 표본 크기로 나눈 값을 추출 간격 k로 계산한다. 이때 \\(k = \\frac{\\text{모집단 크기}}{\\text{표본 크기}}\\) 이며, k는 정수일 수도 있고 아닐 수도 있다. k가 정수가 아닌 경우에는 소수점을 포함한 값을 사용한 뒤, 최종적으로 추출 시 정수 부분만을 활용한다.\n초기 시작점은 1부터 k 사이의 숫자 중 무작위로 하나를 선택하여 결정하며, 이 값을 시작으로 이후 매 k번째 위치에 있는 요소를 순차적으로 표본에 포함시킨다. 이러한 방식은 간단하지만, 모집단이 일정한 방식으로 배열되어 있거나 정렬된 상태일 경우, 특정 패턴에 따라 다양한 모집단 특성을 균등하게 반영할 수 있다.\n이러한 체계적 표본추출은 표면적으로는 단순한 방식이지만, 적절한 조건 하에서는 층화표본추출처럼 모집단의 하위 구조를 고르게 대표할 수 있는 장점을 지닌다. 특히, 모집단 요소들이 조사 변수와 관련 있는 기준에 따라 정렬되어 있는 경우에는 단순임의추출보다 더 높은 정밀도를 확보할 수 있다.\n\n\n(2) 계통추출 관련 코멘트\n체계적 표본추출은 단순임의추출(SRS)이나 층화임의추출에 비해 절차가 간단하고 적용이 용이한 방법이다. 일정한 간격을 기준으로 모집단에서 표본을 선택하는 방식으로, 모집단 전체에 대한 리스트가 준비되어 있다면 빠르고 효율적으로 표본을 구성할 수 있다.\n첫째, 체계적 표본추출에서 사용되는 추출 간격 k는 모집단 크기 N을 원하는 표본 크기 n으로 나누어 계산된다. 이때 k가 항상 정수가 되지 않을 수 있는데, 이러한 경우에는 소수점을 포함한 값을 계산한 뒤 소수점 이하는 버리고 정수 부분만을 사용하여 추출 간격을 설정한다.\n둘째, 체계적 표본추출은 정렬된 리스트를 기반으로 수행될 경우 묵시적 층화 효과를 제공할 수 있다. 이를 ’묵시적 층화 표본추출(implicit stratified sampling)’이라고 하며, 실제 층화를 하지 않았더라도 비례할당이 적용된 층화표본추출과 유사한 결과를 낳을 수 있다. 특히, 정렬 기준이 조사 변수와 상관관계를 가질 경우 표본의 정밀도는 단순임의추출보다 현저히 향상될 수 있다.\n셋째, 이러한 정렬 방식의 대표적인 예로는 지리적 정렬을 들 수 있다. 예를 들어, 특정 지역 내 기업들을 대상으로 평균 종업원 수를 추정하고자 할 때, 기업을 남동쪽에서 북서쪽으로 이동하는 순서로 정렬하면, 대도시 지역과 농촌 지역의 기업들이 자연스럽게 구분되어 배치된다. 일반적으로 대도시 기업은 종업원 수가 많고, 농촌 기업은 적은 경향이 있으며, 유사한 규모의 기업들이 지리적으로 모여 있는 경우도 많다. 이러한 리스트에서 체계적 표본을 추출하면 자동으로 규모별 층화와 유사한 효과를 얻게 되어, 단순임의추출보다 높은 정밀도의 추정을 가능하게 한다.\n이처럼 체계적 표본추출은 간단한 절차에도 불구하고, 적절한 정렬이 이루어진 경우 상당한 효율성을 확보할 수 있는 표본설계 방법이다.\n\n\n\n\nchapter 3. 표본설계 방법 가중치, 추정분산\n\n(1) 단순임의추출\n단순임의추출은 모집단 내 모든 요소가 동일한 확률로 선택되는 가장 기본적인 확률 표본추출 방법이다. 이 방식에서는 각 요소가 표본에 포함될 기회가 동등하게 주어지며, 통계학에서 널리 사용되는 이론적 기준이 된다.\n단순임의추출은 절차가 명확하고 이해하기 쉬우며, 통계적 추론의 기초를 이루는 중요한 방법이다. 그러나 모집단의 이질성이 크거나 하위 집단 간에 중요한 차이가 존재하는 경우에는 층화추출이나 군집추출과 같은 다른 표본설계 방식보다 효율성이 떨어질 수 있다. 특히 동일한 표본 크기 하에서 추정의 정확도나 분산 측면에서 상대적으로 불리할 수 있다.\n이 방식에서는 모든 표본 요소의 가중치가 동일하게 적용된다. 각 표본 요소의 가중치는 \\(w_{i} = \\frac{N}{n}\\) 으로 계산되며, 여기서 N은 모집단의 크기, n은 표본의 크기를 의미한다. 이는 각 표본이 모집단에서 차지하는 비율이 같다는 것을 의미하며, 단순임의추출의 대표성과 균형성을 수학적으로 뒷받침한다.\n표본분산 (추정 오차)\n\\(v(\\overline{y}) = \\left( \\frac{1 - f}{n} \\right)s^{2}\\), \\(s^{2}\\) 은 모집단의 분산, \\(f = \\frac{n}{N}\\)은 표본비율 (유한 모집단 보정) \\(n\\) 증가 시 표본분산 감소한다.\n\n\n(2) 계통추출\n계통추출은 모집단을 일정한 순서로 정렬한 후, 고정된 간격마다 표본을 선택하는 확률 표본추출 방법이다. 추출 간격 k는 모집단 크기 N을 표본 크기 n으로 나누어 계산하며, \\(k = \\frac{N}{n}\\) 으로 정의된다. 먼저 1부터 k까지의 숫자 중 무작위로 하나를 선택하여 시작점을 정한 뒤, 그 이후로 매 k번째 요소를 표본으로 포함시킨다.\n계통추출의 장점은 단순임의추출보다 적용이 간편하고 조사 준비 시간이 짧다는 점이다. 특히 모집단이 정렬되어 있을 때, 그 정렬 기준이 조사 변수와 상관관계를 가질 경우 표본의 대표성과 추정의 정밀도가 높아질 수 있다. 예를 들어, 지리적 순서, 알파벳 순서, 시간 순서 등으로 정렬된 목록에서 계통추출을 수행하면 묵시적 층화 효과를 얻을 수 있다.\n계통추출에서도 각 표본 요소는 동일한 확률로 선택되므로, 가중치는 단순임의추출과 동일하게 적용된다. 각 요소의 가중치는 \\(w_i = \\frac{N}{n}\\) 이며, 이는 전체 모집단을 표본으로 환산할 때의 비율을 나타낸다.\n표본분산 (추정 오차)\n\\(v({\\overline{y}}_{sys}) \\approx v({\\overline{y}}_{SRS})\\)\n\n모집단이 주기성을 가지면 편향 발생 가능이 있다.\n모집단이 정렬된 상태라면 층화표본추출과 유사한 효과를 가진다.\n\n\n\n(3) 군집추출\n군집표본추출은 모집단을 여러 개의 군집으로 나눈 뒤, 이들 중 일부 군집을 무작위로 선택하여 표본으로 삼는 방법이다. 선택된 군집에 포함된 모든 개체를 조사 대상으로 포함시킨다는 점에서, 개별 요소가 아닌 집단 단위로 표본을 구성하는 특징이 있다.\n이 방식은 조사 대상이 지리적으로 넓게 분포해 있거나, 개별 단위에 직접 접근하는 데 시간이 많이 소요되는 경우에 유리하다. 군집 단위로 접근하고 조사함으로써 조사 비용과 시간이 크게 절감될 수 있다. 그러나 군집 내 개체들이 서로 유사한 특성을 가질 가능성이 높기 때문에, 군집 간의 이질성이 충분히 확보되지 않으면 표본 오차가 커질 수 있다는 단점이 있다.\n가중치는 단순임의추출과 유사하게 계산되며, 선택된 군집 내의 각 표본 요소는 동일한 가중치를 갖는다. 이때 가중치는 \\(w_{c} = \\frac{N}{n}\\) 으로 표현되며, 여기서 N은 모집단 전체의 개체 수, n은 조사에 포함된 전체 표본 개체 수를 의미한다.\n표본분산 (추정 오차)\n\\(v({\\overline{y}}_{cl}) = \\left( \\frac{1 - f}{a} \\right)s_{a}^{2}\\), 여기서 \\(a\\)는 선택된 군집 수, \\(s_{a}^{2}\\)은 군집 평균 간 분산,\n\n군집 내 동질성이 클수록(높은 \\(\\rho\\)) 표본 오차가 증가한다.\n군집 내 이질성이 크고, 군집 간 동질성이 높을수록 효과적이다.\n\n\n\n(4) 층화추출\n층화표본추출은 모집단을 서로 겹치지 않는 여러 개의 층(stratum)으로 구분한 뒤, 각 층에서 단순임의추출(SRS)을 독립적으로 수행하는 방식이다. 각 층은 내부적으로는 동질적이고, 층 간에는 이질적인 특성을 갖도록 구성된다. 이 방법은 모집단 내 특정 특성이 층마다 다르게 나타나는 경우, 보다 정밀하고 대표성 있는 추정을 가능하게 한다.\n층화표본추출의 가장 큰 장점은 표본 분산을 줄이고 추정의 정확도를 높일 수 있다는 점이다. 특히 조사 대상의 응답 성향이나 특성이 층별로 뚜렷하게 구분될 경우, 단순임의추출보다 훨씬 효율적인 결과를 기대할 수 있다.\n이 방법에서 가중치는 층별로 계산되며, 각 층 h에 속한 표본 요소의 가중치는 다음과 같다. \\(w_h = \\frac{N_h}{n_h}\\) , 여기서 \\(N_h\\) 는 층 h의 모집단 크기, n_h는 해당 층에서 추출된 표본의 크기를 의미한다. 이는 층별 대표성 보정을 위해 각 표본의 기여도를 조정하는 역할을 한다.\n표본분산 (추정 오차)\n\\(v(\\overline{y}st) = \\overset{H}{\\sum_{h = 1}}W_{h}^{2}\\left( \\frac{1 - f_{h}}{n_{h}} \\right)s_{h}^{2}\\), 여기서 \\(s_{h}^{2}\\)은 층 \\(h\\)의 분산, \\(W_{h} = \\frac{N_{h}}{N}\\)은 층의 모집단 비율, \\(f_{h} = \\frac{n_{h}}{N_{h}}\\)은 층의 표본비율\n\n층 간 변동이 크고, 층 내 변동이 작을수록 효율적이다."
  },
  {
    "objectID": "notes/survey/index.html",
    "href": "notes/survey/index.html",
    "title": "조사방법론",
    "section": "",
    "text": "조사방법론은 자료 수집의 전 과정을 체계적으로 다루는 통계 방법론이다.\n조사 설계, 표본추출, 측정, 자료 정제와 보정까지의 흐름을 이해하는 것은\n신뢰할 수 있는 통계 분석의 출발점이 된다.\n왼쪽 메뉴에서 주제를 선택하시오."
  },
  {
    "objectID": "notes/survey/data_process.html",
    "href": "notes/survey/data_process.html",
    "title": "조사방법론. 5. 데이터 처리",
    "section": "",
    "text": "chapter 1. 데이터 처리 개요\n설문조사 데이터 처리 단계는 전체 분석의 신뢰성과 타당성을 확보하기 위한 일련의 핵심 절차로, 수집된 원자료(raw data)를 정제하고 분석 가능한 형식으로 전환하는 과정이다. 이 단계는 단순히 데이터를 저장하거나 정리하는 수준을 넘어, 조사 설계의 오류를 최소화하고 결과 해석의 타당성을 높이는 데 목적이 있다.\n우선, 데이터 수집은 설문 응답을 체계적으로 취합하는 초기 단계로, 온라인 조사, 전화 인터뷰, 대면 면접, 우편 설문 등 다양한 방법을 통해 이루어진다. 이때 수집된 응답은 원자료 형태로 존재하며, 즉각적인 분석보다는 사전 정제와 구조화가 필요하다. 그다음 단계인 텍스트 데이터 코딩은 자유응답형(예: 기타 의견 서술) 응답을 분류 기준에 따라 범주화하고, 이를 수치형 코드로 전환하여 정형 데이터와 통합할 수 있도록 한다. 이 과정은 주관적 해석이 개입될 수 있으므로 사전 정의된 코딩 기준이 필수적이다.\n데이터 입력은 응답 정보를 전산화된 데이터베이스로 옮기는 단계로, 자동화 시스템을 사용하더라도 이중 입력 검증, 일관성 점검 등을 통해 정확성을 확보해야 한다. 이어서 편집 및 오류 확인 단계에서는 응답 값 간 논리적 불일치, 이상치, 무응답 값 등 오류 가능성을 점검하고, 필요한 경우 수정하거나 주석을 붙인다. 이 과정은 설문 설계상의 문제를 사후적으로 보완하는 역할도 수행한다.\n결측값 대체는 응답 누락으로 인해 생기는 데이터 불완전성을 해소하기 위한 과정으로, 단순 평균 대체, 회귀 추정, 다중 대체(Multiple Imputation) 등의 통계적 기법이 사용된다. 결측값을 어떻게 다루느냐에 따라 분석 결과의 신뢰도와 일반화 가능성이 달라질 수 있으므로, 대체 방법의 선택은 신중해야 한다.\n가중치 생성은 조사 표본이 실제 모집단을 얼마나 잘 대표하는지를 보정하기 위한 단계로, 층화 변수(예: 성별, 연령, 지역) 기반의 보정 가중치, 비응답 보정, 사후 층화(post-stratification) 등이 사용된다. 이는 특히 비확률 표본이거나 응답률이 낮은 조사에서 필수적인 절차이다.\n마지막으로 분산 추정은 분석 결과의 불확실성을 측정하는 단계로, 표준 오차, 신뢰구간 등을 산출한다. 복잡 표본 설계(예: 층화, 군집)가 사용된 경우, 단순 무작위 표본 추출을 가정한 일반 분석 방법이 적합하지 않기 때문에 설계 효과(design effect)를 고려한 분산 추정 기법을 적용해야 한다. 이로써 표본 추정치의 통계적 정확도를 보다 정교하게 평가할 수 있다.\n전체적으로 이 과정은 단순한 데이터 정리의 수준을 넘어서, 분석의 정합성과 재현 가능성을 높이기 위한 필수 절차로 기능하며, 설문조사의 신뢰성과 타당성을 실질적으로 결정짓는 핵심 단계라 할 수 있다.\n\n1. 데이터 코딩\n설문 조사에서 코딩은 응답 데이터를 정형화하여 분석 가능한 형태로 전환하는 과정으로, 전체 데이터 품질과 해석의 정확성을 좌우하는 핵심 단계다. 폐쇄형 질문의 경우, 미리 정의된 선택지에 따라 각 응답을 숫자로 부호화하는 것이 일반적이며, 이는 비교적 간단하고 자동화가 용이하다. 예를 들어, “귀하의 현재 고용 상태는 무엇입니까?”라는 질문에 대해 ‘정규직’, ‘비정규직’, ‘실업’, ‘학생’ 등의 응답 항목이 있을 경우, 각각을 1, 2, 3, 4 등으로 코드화할 수 있다.\n반면, 개방형 질문은 응답자가 자신의 생각을 자유롭게 서술하는 형태로, 훨씬 더 복잡한 코딩 절차를 요구한다. 동일한 의미를 전달하더라도 표현 방식이 제각각이기 때문에, 일관된 분류 체계를 적용하기 위해서는 사전에 코딩 기준표(codebook)를 개발하고, 훈련된 코더가 이를 토대로 수작업 혹은 반자동화 방식으로 응답을 분류해야 한다. 예를 들어, “귀하가 생각하는 현재 경제의 가장 큰 문제는 무엇입니까?”라는 질문에 대해 ‘물가 상승’, ‘인플레이션’, ’가격이 너무 비쌈’이라는 응답이 들어왔다면, 이들을 하나의 범주로 묶어 같은 코드값을 부여해야 한다.\n이 과정에서 주관적 판단이 개입될 수 있으므로, 동일한 응답에 대해 다른 코더가 동일한 코드를 부여하도록 하는 코딩 일치도(inter-coder reliability) 점검이 필요하다. 또한 산업, 직업, 질병 분류 등과 같이 공공 표준 분류체계(KSIC, ISCO 등)를 사용하는 경우에는 더 정교한 매핑 작업이 요구된다.\n결과적으로, 코딩은 단순한 기술적 작업이 아니라 설문 응답의 의미를 체계적으로 해석하고 분석 가능하게 만드는 중요한 지적 과정이다. 특히 개방형 응답이나 복잡한 다중응답 항목에서는 코딩의 품질이 조사 전체의 신뢰성과 타당성에 직결되므로, 코딩 기준의 명확성, 코더의 훈련, 품질 점검 절차가 필수적이다.\n\n예비 분석: 수집된 응답을 검토하여 공통 패턴을 파악한다. 응답 내용을 기반으로 주요 범주를 설정한다.\n코드북 개발: 응답을 분류할 코드 체계를 설계한다. 각 코드에 대한 명확한 정의를 포함한다. 예를 들어, 산림 활용 관련 개방형 질문이 있다면, 1 = 목재 생산, 2 = 레크리에이션(산림욕, 캠핑 등), 3 = 환경 보호, 4 = 연구 및 교육, 5 = 기타\n응답 매핑: 개별 응답을 코드북에 따라 적절한 코드로 변환한다. 응답이 여러 개의 범주에 해당할 경우, 우선순위 규칙을 적용하거나 다중 코드를 허용할 수 있다.\n신뢰성 검토: 여러 명의 분석자가 같은 응답을 동일한 코드로 부여하는지 확인한다. 코딩 일관성 검증을 위해 상호 신뢰도 테스트를 수행한다.\n\n\n(1) 코딩의 본질\n코딩은 단순히 텍스트를 숫자로 바꾸는 기계적 과정이 아니라, 조사 응답 내용을 분석이 가능한 형태로 요약하고 구조화하는 핵심 작업이다. 개별 응답을 의미 있는 범주로 분류함으로써, 방대한 데이터를 정리하고 비교 가능한 자료로 전환할 수 있다. 즉, 코딩은 원자료를 해석 가능하고 통계 처리에 적합한 형태로 가공하는 과정이며, 응답자의 다양한 의견을 정제된 분석 단위로 추출해내는 역할을 한다.\n효과적인 코딩 시스템을 구축하기 위해서는 몇 가지 필수 요소가 갖추어져야 한다. 첫째, 모든 코드는 통계 분석 과정에서 고유하게 식별될 수 있도록 독립된 숫자 값으로 설정되어야 하며, 이는 데이터 처리의 정확성과 효율성에 직결된다. 둘째, 각 코드에는 응답 내용을 명확하게 설명해주는 텍스트 라벨이 함께 제공되어야 해, 코드가 어떤 의미를 담고 있는지를 쉽게 파악할 수 있도록 해야 한다. 셋째, 코딩 구조는 포괄적이어야 하며, 응답자의 어떤 답변도 최소한 하나의 코드에 포함될 수 있어야 한다. 넷째, 각 코드는 서로 겹치지 않도록 상호 배타적으로 구성되어야 하며, 동일한 응답이 중복해서 분류되는 일이 없어야 한다. 마지막으로, 코드 범주는 분석의 목적과 샘플 크기에 맞추어 실용적인 수준으로 유지되어야 하며, 지나치게 세분화된 코드는 분석의 복잡성을 높일 수 있다.\n이러한 코딩의 원칙은 산림 관련 조사를 포함한 다양한 주제의 조사에 적용될 수 있다. 예를 들어, 응답자가 “건강을 위해 산에 자주 간다”고 응답한 경우, 이 내용을 ‘산림 복지 이용’이라는 범주로 정의하고 ‘2’라는 숫자 코드를 부여할 수 있다. 마찬가지로 “목재를 직접 채취해서 사용한다”는 응답은 ‘1=목재 생산’ 코드에 포함시킬 수 있다. 이처럼 명확한 코딩 구조는 응답자의 서술형 답변을 체계적으로 정리하고, 정량적 분석의 기반을 다지는 데 필수적인 도구로 작동한다.\n코드 구조의 지속적인 검토와 개선\n코딩 과정에서는 사전에 정의된 코드 체계를 기반으로 응답을 분류하지만, 실제 조사 현장에서 수집되는 응답은 예외적이거나 복합적인 경우가 많아 기존 코드 구조에 완벽히 부합하지 않을 수 있다. 특히 개방형 문항이나 새로운 사회 현상을 반영한 응답에서는, 기존의 분류 기준으로는 설명이 어려운 경우가 발생한다. 이러한 상황을 해결하기 위해 초기 응답 데이터를 활용하여 코드 체계의 타당성을 사전에 검토하고, 필요 시 코딩 카테고리를 수정하거나 보완하는 절차가 필요하다.\n예를 들어, 기존의 산림 이용 목적을 ‘목재 생산’, ‘관광’, ‘사유림 보존’ 등으로 분류하던 코드 체계는 ’탄소중립을 위한 산림 보전’과 같은 새로운 응답 내용을 충분히 설명하지 못할 수 있다. 이런 경우 기존 코드를 억지로 적용하기보다는, 새로운 분류를 도입하거나 세부 카테고리를 추가하여 코드 구조를 유연하게 재편성하는 것이 바람직하다.\n이처럼 코딩 구조는 한 번 설정하고 끝나는 것이 아니라, 응답 자료의 다양성과 변화하는 조사 환경에 맞춰 지속적으로 검토하고 개선해야 한다. 이를 통해 코딩의 정확성과 응답 분류의 적절성을 높일 수 있으며, 조사 결과의 해석도 보다 정밀해진다. 따라서 코딩 체계는 고정된 틀이라기보다, 실질적인 데이터와 맥락에 맞춰 유기적으로 진화해야 하는 작업으로 이해되어야 한다.\n모든 응답을 포괄할 수 있는 코딩 체계를 설계하는 것은 데이터의 완전성과 분석의 정확성을 확보하는 데 필수적이다. 조사 과정에서는 응답자가 질문에 명확하게 답변하는 경우만 있는 것이 아니라, 무응답, 해당 없음, 거부 응답 등 다양한 예외 상황이 발생한다. 이러한 모든 경우를 체계적으로 반영할 수 있도록 코딩 구조는 사전에 신중하게 설계되어야 한다.\n첫 번째 전략은 응답이 없는 경우를 위한 코드 설정이다. 응답자가 질문에 아무런 답변을 하지 않았을 때, 이를 단순한 누락으로 처리하면 분석 과정에서 혼란이 발생할 수 있다. 따라서 이 경우에는 “미확인”이라는 의미의 별도 코드를 부여하여, 해당 응답이 의도적으로 빠졌음을 명확히 표시한다. 예를 들어, 단일 숫자 체계에서는 9, 두 자리 숫자 체계에서는 99를 사용할 수 있다.\n두 번째로는 특정 질문이 응답자에게 적용되지 않는 경우이다. 이는 설문 흐름상 논리적으로 해당 질문이 불필요한 경우인데, 이를 구분 없이 단순 결측으로 처리하면 응답자의 특성을 왜곡할 수 있다. 따라서 “적용 불가”라는 의미의 고유 코드(예: 0 또는 00)를 사용하여, 분석자가 이를 명확히 식별하고 분석 대상에서 제외하거나 별도로 처리할 수 있도록 해야 한다.\n세 번째는 기타 특수 응답의 처리이다. 예를 들어, “답변을 거부함”, “잘 모르겠다”와 같은 응답은 설문 문항에 대한 반응이긴 하지만, 내용적으로 유효한 데이터로 보기 어려운 경우가 많다. 이 역시 별도의 코드를 부여하여 정규 응답과 구분하는 것이 바람직하다. 이를 통해 분석자는 이러한 특수 응답을 전체 응답률 계산에서 제외하거나, 추가 분석 시 변수로 활용할 수 있다.\n이러한 보조 응답 코드들을 미리 정의하고, 설문지 설계 시점부터 적용하는 것은 조사 자료의 품질을 높이고, 추후 분석 시 오류나 혼동을 줄이는 데 중요한 역할을 한다. 특히 대규모 표본조사나 반복조사에서는 이러한 표준화된 코딩 체계가 일관성과 비교 가능성을 보장하는 데 필수적이다.\n결론 및 시사점 코딩은 단순히 응답을 숫자로 변환하는 기술적 절차를 넘어, 조사 목적에 맞는 응답 내용을 요약하고 구조화하는 해석적 과정이다. 이를 위해 코드의 수와 범주 분류 체계는 분석 목표에 부합하도록 신중하게 설계되어야 하며, 이 과정에서 상호 배타성과 포괄성이라는 원칙을 동시에 충족해야 한다. 즉, 하나의 응답이 두 개 이상의 코드에 중복되지 않도록 하면서도, 어떤 응답도 누락되지 않도록 체계를 갖추는 것이 중요하다.\n연구자가 설정한 가설이나 분석 목적에 따라 코드 체계는 유연하게 설계되어야 하며, 통계적으로 의미 있는 비교와 해석이 가능하도록 조정되어야 한다. 특히 새로운 데이터가 기존 코드 체계와 맞지 않을 경우, 해당 구조가 여전히 유효한지 재검토하고 필요한 경우 수정하는 과정이 필수적이다. 이를 통해 변화하는 응답 경향과 사회적 맥락에 맞춰 설문의 분석력을 유지할 수 있다.\n또한, “응답 없음”, “미확인”, “적용 불가” 등과 같은 예외 응답들을 위한 별도 코드 체계를 마련하는 것은 데이터 누락이나 분석 왜곡을 방지하는 데 큰 역할을 한다. 이러한 부가 응답 코드는 설문 응답의 전체 구조를 온전하게 반영할 수 있게 해주며, 분석 시 결과의 신뢰성을 높여준다.\n결론적으로, 코딩 체계의 설계와 유지보수는 단순한 데이터 정리 작업이 아니라, 조사 데이터의 질을 결정짓는 핵심적인 과정으로 이해되어야 하며, 그 중요성은 설문 설계 초기 단계부터 반영되어야 한다.\n\n\n(2) 필드 코딩\n필드 코딩의 개념\n필드 코딩은 설문조사 현장에서 개방형 질문의 장점과 폐쇄형 질문의 효율성을 결합한 방식으로, 조사원이 응답자의 자유로운 진술을 들은 직후, 해당 응답을 사전에 정의된 코드 범주 중 하나로 분류하는 실시간 코딩 기법이다. 응답자는 자신의 언어로 서술형 답변을 제공하되, 조사원은 이미 준비된 코딩 체계에 따라 즉시 해당 응답을 구조화된 데이터로 전환한다.\n이러한 방식은 사후에 별도로 개방형 응답을 수집하고 코딩하는 번거로움을 줄이고, 조사 현장에서 코딩 기준이 일관되게 적용될 수 있다는 장점을 지닌다. 특히 대규모 조사를 수행하거나 반복적인 질문이 많은 경우, 시간과 인력의 효율성을 크게 향상시킨다. 단, 조사원의 코딩 판단이 결과에 직접 영향을 미치기 때문에, 사전 교육과 명확한 코드북 제공이 필수적이며, 조사 중 혼란이 발생하지 않도록 지속적인 모니터링과 품질 점검이 필요하다.\n필드 코딩의 절차\n\n응답자에게 개방형 질문을 제시 → 자유롭게 서술형 응답을 하도록 유도한다.\n조사원이 즉시 응답을 해석 → 해당 응답이 포함될 수 있는 코드 범주를 검토한다.\n사전 정의된 코드 리스트와 비교하여 적절한 코드 선택 → 가장 적합한 카테고리에 배정한다.\n코딩을 완료하고 데이터 저장 → 이후 데이터 분석에서 활용 가능하도록 정리한다.\n\n필드 코딩 장단점\n\n\n\n\n\n\n\n\n구분\n항목\n설명\n\n\n장점\n즉각적인 코딩 가능\n응답을 수집하면서 동시에 코딩을 수행하여 사후 코딩 과정이 불필요\n\n\n질문에 대한 깊이 있는 탐색 가능\n인터뷰어가 응답 내용을 추가로 탐색하여 보다 정확한 분류 가능\n\n\n응답자의 자유도 보장\n완전한 폐쇄형 질문과 달리, 응답자의 입장에서 보다 자연스러운 답변 제공 가능\n\n\n단점\n인터뷰어의 해석에 따른 오류 발생 가능\n같은 응답이라도 조서자마다 코드 부여 방식이 다를 수 있음\n\n\n실시간 처리의 부담\n조사원이 실시간으로 응답을 분석하고 코드화해야 하므로 부담이 클 수 있음\n\n\n복잡한 코드 체계의 경우 어려움\n코드의 수가 많거나 복잡하면 인터뷰어가 즉시 적절한 범주를 선택하기 어려울 수 있음\n\n\n\n필드 코딩과 사후 코딩 비교\n\n\n\n\n\n\n\n\n구분\n필드 코딩\n오피스 코딩\n\n\n코딩 시점\n실시간 코딩 (인터뷰 진행 중)\n조사 후 코딩 (설문 응답을 저장한 후)\n\n\n책임자\n조사원이 직접 코드화\n전문 코더가 응답을 해석하여 코드화\n\n\n장점\n즉각적인 데이터 정리 가능, 추가 탐색 가능\n보다 체계적이고 일관성 있는 코딩 가능\n\n\n단점\n인터뷰어의 주관 개입 가능, 실시간 처리 부담\n시간이 많이 소요됨, 사후 코드 검토 필요\n\n\n\n\n\n(3) 코딩 품질 지표\n코딩 과정에서 발생할 수 있는 개념적 오류나 실행의 비일관성은 설문조사 데이터의 신뢰성과 타당성에 중대한 영향을 미친다. 개념적 오류는 질문의 의도와 코딩 범주 간의 불일치에서 비롯되며, 응답자의 진술을 잘못 해석하거나 본래의 의미와 다르게 분류하는 경우 발생한다. 예를 들어, 응답자가 “산림을 가족과 함께 쉬러 간다”고 했을 때, 이를 단순히 ’여가활동’으로만 코딩하면 ’가족 중심 활동’이라는 사회문화적 맥락이 누락될 수 있다.\n또한 실행의 비일관성은 동일한 응답이 조사원마다 서로 다른 코드로 분류되는 경우로, 코딩 지침이 모호하거나 조사원 교육이 충분하지 않을 때 자주 발생한다. 이는 데이터의 비교 가능성을 떨어뜨리고 분석 결과의 왜곡을 초래할 수 있다. 따라서 일관된 코딩 기준을 마련하고, 정기적인 코딩 검토 및 품질 관리를 통해 이러한 오류를 최소화하는 것이 중요하다.\n코딩 구조의 취약점\n코딩 구조가 취약하게 설계될 경우, 의미적으로 서로 다른 응답이 동일한 코드 범주에 포함되어 분석 결과에 왜곡을 초래할 수 있다. 이는 단순한 분류상의 실수가 아니라, 체계적인 오분류로 이어져 분석의 신뢰성을 떨어뜨리는 요인이 된다. 특히 응답 항목 간의 차이가 미묘하거나 사회문화적 맥락이 개입되는 경우, 이러한 위험은 더욱 커진다.\n예를 들어, 학력을 기준으로 소득을 분석할 때 “고등학교 졸업”이라는 항목에 정규 고등학교 졸업자와 검정고시 합격자가 동일하게 포함되면, 두 집단 간 경험과 사회적 배경의 차이를 간과하게 된다. 더 나아가, ’3년 전 졸업’과 ’최근 검정고시 합격’은 시간적, 교육적 맥락에서도 차이가 있으므로 별도의 코딩 처리가 필요하다. 따라서 코딩 구조는 분석 목적과 변수의 의미를 충분히 반영하도록 정교하게 설계되어야 하며, 애매하거나 복합적인 응답에 대해 세분화된 코드 범주를 마련하는 것이 체계적인 오류를 줄이는 핵심이다.\n코더 변량 coder variance\n코더 변량(coder variance)은 동일한 응답에 대해 서로 다른 코더들이 다르게 코딩함으로써 발생하는 데이터의 변동성을 의미한다. 이는 주관적인 판단이 개입되는 개방형 질문이나 복잡한 분류 작업에서 자주 나타나며, 조사 결과의 일관성과 신뢰도를 저하시킬 수 있다.\n코더 변량은 조사 데이터의 전체 오차 중 ’코딩 단계’에서 발생하는 오차를 구성하며, 설문 응답 그 자체의 내용이나 응답자의 특성이 아닌 코더의 해석이나 경험, 숙련도에 따라 달라질 수 있다. 예를 들어, 동일한 “기타 의견” 응답을 어떤 코더는 ’환경 보호’로, 다른 코더는 ’산림 복지’로 분류할 경우, 해당 항목에 대한 통계적 추정치에 차이가 발생하게 된다.\n따라서 코더 변량을 줄이기 위해서는 명확한 코딩 지침서 제공, 사전 훈련, 사후 일관성 검사 등의 조치가 필요하며, 가능한 경우 자동화된 코딩 시스템이나 복수 코더 간의 교차 검토 절차를 도입하는 것이 바람직하다. 이처럼 코더 변량은 조사 설계의 품질 관리와 데이터 정합성을 확보하는 데 반드시 고려되어야 하는 요소이다.\n코더마다 같은 코딩 구조를 다르게 사용할 가능성이 있음\n코딩 과정에서 각 코더가 동일한 코딩 구조를 다르게 해석하고 사용하는 경우가 발생할 수 있다. 이는 코더마다 특정 코드 범주를 선택하는 경향성이 다르기 때문이며, 조사 데이터의 일관성과 정확성에 영향을 미치는 주요 요인 중 하나다.\n예를 들어, 어떤 코더는 애매한 응답에 대해 “기타”나 “명확히 지정되지 않음”과 같은 잔여 코드를 자주 사용하는 반면, 다른 코더는 더 구체적인 범주에 억지로라도 할당하려는 경향을 보일 수 있다. 또한, 동일한 응답 내용에 대해 해석 방식이 달라 각기 다른 코드가 부여되는 경우도 발생한다. 이러한 차이는 코더의 경험, 훈련 수준, 또는 주관적 판단 기준에서 기인할 수 있다.\n결과적으로, 이러한 비일관성은 동일한 응답이 상황에 따라 다르게 처리되어 통계 분석의 왜곡을 초래할 수 있으므로, 코더 간의 해석 차이를 줄이기 위한 명확한 지침과 코딩 기준의 일관된 적용이 필수적이다.\n코더별 코딩 편차를 측정하는 방법\n코더별 코딩 편차는 조사 데이터의 품질을 정량적으로 평가하기 위해 중요한 분석 대상이다. 이를 측정하는 대표적인 방법 중 하나는 내집단 상관계수(interclass correlation coefficient, ICC)를 활용하여 코더 간의 일관성을 분석하는 것이다. 이 계수는 동일한 응답에 대해 여러 코더가 얼마나 유사한 코드를 부여했는지를 수치로 표현하며, 값이 1에 가까울수록 높은 일치도를 의미한다.\n실제 사례로, 영국에서 수행된 한 연구에서는 코더 간 ICC 평균값이 0.001로 보고되었다. 이는 매우 낮은 값으로, 코더 간 일치도가 크지 않다는 것을 보여준다. 비록 이 수치는 일반적으로 조사원(interviewer) 효과보다 작은 수준이지만, 여전히 조사 통계의 변동성과 신뢰도에 영향을 줄 수 있다.\n따라서, 코딩 과정에서 코더 간 편차를 최소화하고, 통계 결과의 왜곡을 방지하기 위해서는 코더 훈련 강화, 코드북의 명확화, 정기적인 모니터링과 평가가 필요하다.\n코더 변량이 조사 통계에 미치는 영향\n코더 변량은 코딩 과정에서 각 코더가 동일한 응답을 해석하고 분류하는 방식의 차이로 인해 발생하는 변동성으로, 이로 인해 조사 통계의 정확도와 신뢰도에 영향을 미칠 수 있다. 특히 개방형 질문이나 복잡한 분류 기준이 있는 항목의 경우, 코더 간 해석 차이는 미묘한 코딩 편차를 초래할 수 있으며, 이러한 편차가 누적되면 전체 통계 추정치에 영향을 줄 수 있다.\n예를 들어, 특정 직업을 “전문직”으로 분류할지 “기술직”으로 분류할지를 놓고 코더마다 판단이 다르면, 직업군별 분포나 관련 변수와의 연관성 분석에서 통계적으로 유의미한 차이가 나타날 수 있다. 비록 개별 코딩의 차이는 작을지라도, 표본 규모가 크고 코더 수가 많을수록 이 편차가 전체 조사 결과의 분산을 증가시키고, 결과적으로 통계적 해석의 불확실성을 높일 수 있다.\n코더 변량이 조사 통계의 분산에 미치는 영향을 나타내는 공식: \\(Deff = 1 + \\rho_{c}(m - 1)(1 - r)\\), 여기서 \\(\\rho_{c}\\)은 코더의 내집단 상관계수, \\(m\\)은 개별 코더가 코딩한 평균 사례 수, \\(r\\)은 특정 코드의 신뢰도이다.\n코더 변량이 조사 결과에 미치는 영향\n코더 변량이 조사 결과에 미치는 영향은 작지만 무시할 수 없는 수준이다. 비록 동일한 코드 구조를 사용하더라도, 코더는 개인의 판단과 해석 방식에 따라 동일한 응답을 다르게 분류할 수 있다. 이러한 차이는 특히 자유응답형(open-ended) 문항이나 복잡한 범주 체계를 사용하는 경우 두드러지며, 결과적으로 조사 추정치의 일관성과 신뢰성을 저해할 수 있다.\n훈련된 코더는 일반적으로 코딩 결정에서 더 높은 일관성을 보이며, 조사원에 비해 상대적으로 작은 변동성을 유발한다. 그러나 한 명의 코더가 처리하는 응답 수는 대개 매우 많기 때문에, 적은 수준의 코더 변량도 전체 표본 추정치에 영향을 줄 수 있다. 이는 표준 오차의 증가로 이어져 결과 해석 시 통계적 불확실성을 높이는 요인이 된다. 따라서 코딩 매뉴얼의 명확화, 반복 교육, 이중 코딩(double coding)을 통한 검증 과정이 중요하다.\n\n\n\n2. 수치 데이터를 파일에 입력하기\n데이터 입력의 개념\n데이터 입력, 또는 데이터 캡처는 설문조사 과정에서 수집된 정보를 전자적 형태로 전환하는 단계로, 분석을 위한 기초 작업 중 하나이다. 이 과정은 수집된 응답을 컴퓨터 파일에 체계적으로 입력하여 이후 통계 처리와 해석이 가능하도록 준비하는 것을 의미한다. 데이터 입력 방식은 설문조사의 수행 방식에 따라 다양하게 결정된다.\n예를 들어, 컴퓨터 지원 면접 조사(CAPI, Computer-Assisted Personal Interviewing)의 경우, 조사원이나 응답자가 설문 소프트웨어를 통해 직접 데이터를 입력함으로써 별도의 입력 과정 없이 응답과 동시에 데이터가 수집된다. 전화 응답 시스템에서는 터치톤 입력 또는 음성 인식을 통해 응답자가 직접 입력한 정보가 자동으로 기록되며, 이는 인터뷰어 없이도 데이터 입력이 이루어지는 자동화 방식이다.\n반면 종이 설문지를 사용하는 전통적인 조사에서는 데이터 입력자가 일일이 수기로 응답 내용을 전산 시스템에 입력해야 한다. 이 과정은 시간이 많이 소요되며 오류 가능성이 존재하기 때문에, 최근에는 마크 문자 인식(Mark Recognition) 또는 광학 문자 인식(OCR, Optical Character Recognition) 기술을 활용하여 입력 속도를 높이고 정확성을 개선하려는 노력이 증가하고 있다.\n결국, 데이터 입력은 수집된 응답을 디지털 자산으로 전환하는 핵심 과정이며, 입력 방식의 선택은 조사 방식, 예산, 시간, 데이터 정확성 요구 수준 등에 따라 결정된다.\n인간 데이터 입력 방식의 한계\n인간이 직접 수행하는 데이터 입력 방식은 설문조사에서 전통적으로 사용되어 왔으나, 여러 가지 한계를 가지고 있다. 가장 큰 문제는 높은 인건비와 시간 소요이다. 사람이 응답지를 일일이 검토하고 숫자화하는 과정은 정확도를 확보하기 위해 반복 검증이 요구되며, 이는 전반적인 조사 비용을 크게 증가시키는 요인이 된다.\n보통 이러한 방식에서는 데이터 정확성을 확보하기 위해 100% 재입력(Double Data Entry) 또는 2인 검증 등의 절차가 수행된다. 하지만 이러한 노력에도 불구하고 사람의 실수로 인한 오류 가능성은 완전히 제거되지 않는다. 특히 대규모 조사에서는 수만 건의 응답을 처리해야 하므로, 누락, 중복, 오타 등 다양한 입력 오류가 발생할 수 있다.\n이러한 한계를 극복하고자 최근에는 컴퓨터 지원 데이터 수집(CAPI, CAWI 등) 방식이 선호되고 있다. 이는 입력 작업을 자동화함으로써 입력 오류를 줄이고, 동시에 비용과 시간을 절감하는 효과를 기대할 수 있다. 결과적으로, 설문조사의 효율성과 정확성을 동시에 확보하기 위해 사람의 수작업보다 디지털 기반 입력 방식이 점차 주류로 자리잡고 있는 추세다.\n\n\n3. 데이터 편집 editing\n데이터 편집의 개념\n데이터 편집은 설문조사에서 수집된 응답 자료가 통계 분석에 적합하도록 정제하는 초기 단계로, 데이터의 정확성과 논리적 일관성을 확보하는 데 필수적인 과정이다. 수집된 데이터는 종종 응답 누락, 상호 모순된 답변, 비현실적인 값 등 다양한 오류를 포함할 수 있기 때문에, 분석 전에 반드시 점검과 수정이 필요하다.\n이 과정은 인터뷰어가 현장에서 수기로 응답을 확인하거나, 조사 감독자가 응답지를 검토하며 오류를 찾아내는 방식으로 진행될 수 있다. 또한, 데이터 입력 과정에서 입력 담당자가 입력값을 확인하고, 전문 연구자가 조사 설계와 비교하여 논리적 타당성을 점검하는 작업도 포함된다. 최근에는 자동화된 컴퓨터 소프트웨어를 활용한 논리적 규칙 기반 검증 시스템을 통해 일관성 없는 응답이나 허용 범위를 벗어난 값을 자동으로 탐지하고 수정하는 방식이 널리 사용되고 있다.\n편집의 목적은 단순히 오류를 제거하는 데 그치지 않고, 원래 설계된 질문 의도에 부합하는 응답 체계를 회복하는 데 있다. 이를 통해 분석 결과의 신뢰성을 높이고, 조사 데이터의 품질을 한층 더 향상시킬 수 있다.\n편집의 주요 내용\n편집의 주요 내용은 다음과 같은 세 가지 요소로 요약할 수 있다.\n첫째, 편집은 조사원이나 응답자가 기록한 원자료를 점검하고 보정함으로써 데이터 품질을 향상시키는 데 중점을 둔다. 응답자가 실수로 잘못 기입했거나 조사원이 부정확하게 기록한 항목을 수정하여, 수집된 데이터가 현실을 보다 정확하게 반영하도록 한다.\n둘째, 코딩과 결측값 대체도 편집의 일환으로 포함된다. 특히 개방형 응답을 범주화하여 수치 코드로 변환하는 코딩 작업과, 누락된 값을 적절한 통계기법으로 보완하는 대체 작업은 편집 과정 중 핵심적인 절차에 해당한다.\n셋째, 편집은 무엇보다도 데이터가 논리적 정합성을 유지하도록 정리하는 데 목적이 있다. 예를 들어, 연령이 5세인데 직업이 있다고 응답한 경우처럼 명백히 논리적으로 모순된 자료를 탐지하고 수정함으로써, 전체 분석 결과의 타당성과 신뢰도를 확보할 수 있다. 이러한 논리적 일관성은 단일 항목 내에서도 중요하지만, 항목 간 관계에서도 일관되게 유지되어야 한다.\n데이터 편집 주요 유형\n\n\n\n\n\n\n\n\n편집 유형\n설명\n예시\n\n\n범위 편집\n데이터 값이 허용된 범위 내에 있는지 확인\n연령 값이 1개월 이상, 120년 이하인지 검사\n\n\n비율 편집\n특정 값 간 비율이 논리적으로 맞는지 확인\n농장에서 생산된 우유 갤런 수와 젖소의 수 비율이 적절한지 확인\n\n\n이력 데이터 비교\n이전 조사 데이터와 비교하여 일관성을 점검\n1차 조사와 2차 조사에서 가구원 수가 유사한지 확인\n\n\n균형 편집\n여러 변수의 합이 일정해야 하는 경우 확인\n집, 직장, 기타 장소에서 보낸 시간의 합이 100%인지 검사\n\n\n최대·최소 값 확인\n비정상적으로 큰 값이나 작은 값이 존재하는지 검사\n극단적인 소득 값이 존재하는지 확인\n\n\n일관성 편집\n논리적 관계가 성립하는지 점검\n12세 미만 응답자의 혼인 상태는 '미혼'이어야 함\n\n\n\nCAPI와 편집\n컴퓨터 지원 면접조사(CAPI: Computer-Assisted Personal Interviewing)의 도입은 편집 과정을 설문 응답 단계에서 실시간으로 수행할 수 있게 해 주었다. 이러한 방식은 설문 응답이 완료되기 전부터 오류 탐지 및 수정 작업을 자동화함으로써, 전체 데이터 품질을 크게 향상시킬 수 있다는 장점이 있다.\n예를 들어, 응답자가 나이를 “5세”로 입력하고 직업을 “회사원”으로 응답했을 경우, CAPI 시스템은 논리적 오류를 탐지하고 즉시 경고 메시지를 띄워 인터뷰어나 응답자가 해당 항목을 다시 확인하도록 유도할 수 있다. 이처럼 실시간 편집은 응답 도중 오류를 줄이고, 후속 데이터 정제 부담을 줄이는 데 효과적이다.\nCAPI에서 사용하는 오류 점검 방식은 일반적으로 두 가지로 구분된다. 첫째는 강제 체크(hard edit check)로, 명백한 오류가 발생했을 때 설문 진행이 중단되며 반드시 수정이 이루어져야 한다. 둘째는 소프트 체크(soft edit check)로, 값이 비정상적으로 보이긴 하지만 이론적으로 허용 가능한 범위 내에 있을 경우 경고 메시지만 제공하고 응답을 유지할 수 있도록 한다.\n그러나 이러한 시스템이 항상 응답자의 실제 상황을 완벽하게 반영하는 것은 아니다. 응답자가 비표준적인 특수 사례에 속하거나, 반복적으로 유사한 방식으로 응답을 고집하는 경우, 인터뷰어가 시스템의 논리에 따라 응답을 수정하도록 유도하다 보면 실제 상황과 불일치가 발생할 수 있다. 따라서 실시간 편집 기능은 설문 설계자의 전문적 판단과 병행되어야 하며, 시스템 오류 경고에 과도하게 의존하지 않는 균형이 필요하다.\n데이터 편집 발전\n데이터 편집의 방식은 전통적인 수작업 중심에서 점점 더 규칙 기반(rule-based) 및 컴퓨터 지원 방식으로 진화하고 있다. 이러한 전환은 데이터 품질을 보다 체계적이고 일관되게 관리할 수 있도록 하며, 전체 조사 비용을 절감하는 데도 기여하고 있다.\n자동화된 편집 시스템은 설문 응답이 데이터베이스에 입력되는 즉시 미리 정의된 논리 규칙에 따라 오류를 탐지하고 수정 가능성을 평가한다. 예를 들어, 응답자의 연령과 직업이 논리적으로 맞지 않는 경우, 시스템은 사전에 설정된 규칙에 따라 경고를 띄우거나 자동으로 수정 여부를 제안할 수 있다. 이러한 규칙 기반 편집은 복잡한 변수 간 논리 관계를 빠르게 탐지하고, 반복적인 오류를 자동 처리함으로써 사후 편집의 필요성을 크게 줄인다.\n특히 최근에는 데이터 수집 단계에서 편집 절차를 선제적으로 통합하는 경향이 두드러지고 있다. 이는 실시간 오류 수정 기능(CAPI, CAWI 등)과 연계되어, 조사 중에 잘못된 응답이 입력되는 것을 방지하거나 수정하도록 유도하는 방식이다. 이처럼 편집이 데이터 수집과 동시에 이루어지면, 조사 이후의 대규모 오류 수정 작업이 불필요해질 수 있다.\n더 나아가, 최신 편집 시스템은 점차 전문가 지식과 인공지능(AI) 기반 알고리즘을 활용하는 방향으로 발전하고 있다. 예컨대, 과거 편집 사례에서 학습한 패턴을 기반으로 오류 가능성을 자동 예측하거나, 비정상적 응답에 대해 맥락 기반 추천을 제시하는 기능이 개발되고 있다. 이는 전문가 개입 없이도 더 정교하고 신뢰할 수 있는 편집이 가능하게 하며, 편집 작업의 일관성과 효율성을 한층 높일 수 있다.\n\n\n\nchapter 2. 가중치 산정\n조사 표본에서 가중치는 통계적 추정의 정확성을 높이고, 모집단을 보다 대표할 수 있도록 설계상의 왜곡과 응답 편향을 보정하는 데 사용된다. 특히 현실의 많은 표본조사에서는 단순무작위추출(SRS)을 적용하기 어렵고, 실제로는 층화(stratification), 집락(clustering), 비확률(nonresponse) 등의 복잡한 설계가 포함되기 때문에, 이에 대한 보정이 필수적이다.\n예를 들어, 특정 연령대나 지역 집단이 과도하게 추출되었거나, 응답률이 낮은 집단이 있을 경우, 해당 집단의 실제 비중을 반영하기 위해 가중치를 조정해야 한다. 또한, 조사 응답자가 표본설계 당시 예상했던 분포와 다르게 분포되었을 경우에도 보정이 필요하다. 이때 가중치는 개별 응답자의 수치를 모집단 전체에 합리적으로 확장하는 기능을 하며, ’확장 계수(expansion factor)’로 불리기도 한다.\n가중치는 단일 방식으로 계산되지 않으며, 조사 설계의 복잡도와 조사 목적에 따라 다양한 조정 절차가 적용된다. 예를 들어, 층화 및 군집 표본 설계에 따른 기본 가중치(base weight), 비응답 조정을 위한 보정 가중치(nonresponse adjustment weight), 외부 모집단 분포와의 정합성을 맞추기 위한 보정 계수(post-stratification or calibration weight) 등이 순차적으로 적용될 수 있다. 이렇게 다단계로 조정된 최종 가중치는 분석 단계에서 반드시 반영되어야 하며, 그렇지 않으면 모집단을 왜곡하는 결과를 초래할 수 있다.\n이와 같은 가중치 조정의 이론적 토대와 실무 적용에 대해서는 Kalton(1981)의 표본조사 기법에 대한 논의와 Bethlehem(2002)의 응답 편향 보정 연구가 대표적인 참고 문헌으로 자주 인용된다. 두 연구는 표본조사에서의 가중치 조정이 단순한 비율 조정이 아니라, 정교한 통계 모델링과 설계 기반 추정 절차의 일부임을 강조한다.\n\n1. 단계 비율 조정 가중치\n다단계 표본 설계에서는 모집단 전체를 한 번에 추출하지 않고, 여러 단계를 거쳐 표본을 선택한다. 가장 일반적인 예는 1단계에서 지역 단위의 기본 표본 단위(PSU)를 선택하고, 그 안에서 가구 또는 개인을 다시 추출하는 방식이다. 이때 PSU는 모집단의 특정 속성(예: 인구 수, 가구 수 등)을 나타내는 크기 척도에 비례하여 추출된다. 즉, 규모가 큰 PSU일수록 선택될 확률이 높다.\n이러한 방식으로 표본을 추출하면, 각 단위의 선택 확률이 서로 다르게 되므로, 이를 보정하기 위한 가중치가 필요하다. 이 가중치는 보통 “확률의 역수”로 정의되며, 어떤 단위가 선택될 확률이 작을수록 더 큰 가중치를 갖는다. 이를 통해 각 단위가 모집단 전체를 대표할 수 있도록 보정한다.\n또한, 모집단의 실제 크기가 아닌 대체 가능한 보조 지표(예: 인구센서스, 행정자료 등)를 통해 PSU를 선정하는 경우가 많기 때문에, 가중치 산정 시 이 지표에 대한 신뢰성과 대표성을 충분히 고려해야 한다. 이처럼 단계 비율 조정 가중치는 복잡 설계에서 모집단 전체를 대표하는 데 필수적인 절차이며, 분석 단계에서 반드시 반영되어야 한다.\n1단계 비율 조정 가중치 개념\n1단계 비율 조정 가중치는 다단계 표본 설계에서 균등 확률 표본(EPSM: Equal Probability Sample)을 실현하기 위해 사용되는 보정 방식이다. 이 방식의 핵심은, 최종적으로 추출된 표본이 각 층의 모집단 크기에 비례하도록 가중치를 부여하는 것이다.\n예를 들어, 어떤 층이 전체 모집단에서 차지하는 비율이 0.5%라면, 이 층에서는 전체 표본의 약 0.5%가 추출되도록 설계되어야 한다. 그러나 실제로는 조사비용이나 가용 인력, 지역 간 접근성 등의 이유로 층별 표본 수가 균형 있게 배분되지 않을 수 있다. 이런 경우, 1단계에서 선택된 기본 표본 단위(PSU, 예: 시군구)에 대해 가중치를 조정하여 층별 대표성을 확보하게 된다.\n이때 사용되는 가중치는 각 층 내 PSU가 선택될 확률의 역수를 기반으로 하며, 모집단 내 각 층의 상대적 규모를 고려하여 조정된다. 이를 통해 결과적으로, 조사 결과가 전체 모집단의 구조를 더 정확하게 반영할 수 있게 된다. 이러한 비율 조정은 모집단 크기에 대한 외부 보조 정보(예: 인구총조사, 등록 자료 등)를 바탕으로 수행되며, EPSM 설계 원칙에 따라 각 층의 비중이 전체 추정치에 제대로 반영되도록 한다. \\[\\text{Estimated Stratum Population Total} = \\frac{\\text{Population Total in Selected PSU}}{\\text{Probability of Selecting PSU}}\\]\n즉, 1단계 비율 조정 가중치의 핵심은 층 전체의 모집단 규모를 반영하여, 실제로 선택된 PSU(Primary Sampling Unit, 예: 시군구)의 모집단 총계를 기반으로 층 내 대표성을 확보하는 것이다.\n이는 선택된 PSU가 그 층의 전체를 제대로 대표하지 못하는 경우라도, 해당 PSU의 모집단 규모를 활용하여 가중치를 조정함으로써 층 전체의 특성을 보다 정확히 추정할 수 있도록 한다. 다시 말해, 어떤 특정 PSU가 선택되었든지 간에 그 층의 다른 PSU가 선택되었을 때와 유사한 결과가 도출되도록 설계하는 것이다.\n이러한 가중 조정 방식은 층 내 비선택 PSU들이 포함하는 모집단의 정보까지도 간접적으로 반영하는 역할을 하며, 복잡한 표본 설계에서도 추정의 일관성과 신뢰성을 유지하는 데 중요한 기법이다.\n실질적인 가중치 적용\n실질적인 가중치 적용 단계에서는, 1단계에서 선택된 PSU(Primary Sampling Unit, 예: 시군구 등)에 포함된 모든 조사 대상자(응답자)에게 동일한 1단계 비율 조정 가중치가 부여된다. 이때 생성되는 새로운 가중치 변수는 \\(W_{i1}\\) 로 표시되며, 여기서 첨자 1은 이 가중치가 여러 단계 중 첫 번째 조정 단계에서 부여된 값임을 의미하고, 대문자 W는 이 가중치가 표본 데이터가 아닌 프레임 모집단(설계 당시 모집단)의 구조를 반영한 것임을 나타낸다.\n즉, \\(W_{i1}\\) 은 선택된 PSU의 모집단 규모와 표본 추출 비율에 따라 계산된 값이며, 해당 PSU 내 모든 응답자에게 동일하게 적용된다. 이 과정은 PSU가 포함된 층 전체의 대표성을 확보하는 데 필수적인 절차로, 이후 단계에서 추가 가중치 조정이 이루어질 경우, 이 \\(W_{i1}\\) 이 기초 가중치로서 출발점이 된다.\n\n\n2. 차등 선택 확률 가중치\n예를 들어, 성폭력 피해 경험 조사를 수행할 때 전체 모집단이 1,000명이고 이 중 남성이 600명, 여성이 400명이라고 하자. 이 경우, 전체 인구 비율을 반영한다면 표본도 남성 120명, 여성 80명으로 구성되어야 모집단의 비율과 일치한다. 그러나 연구자는 여성의 피해 경험을 보다 정밀하게 분석하고자 여성 응답자를 120명, 남성을 80명으로 선정했다면, 이는 여성을 과대표본(over-sampling) 한 셈이다.\n이처럼 각 계층이 모집단 내 실제 비율과 다르게 추출된 경우, 분석 결과는 전체 모집단을 제대로 반영하지 못할 수 있다. 이러한 대표성 왜곡을 보정하기 위해 차등 선택 확률 가중치를 부여한다. 이 가중치는 모집단 내 각 집단의 비율과 표본에서의 비율을 비교하여 계산된다. 예를 들어, 남성은 모집단의 60%를 차지하지만 표본에서는 40%에 불과하므로, 이 차이를 보정하기 위해 남성 응답자에게는 더 높은 가중치가 부여된다. 반대로 여성은 과대표집 되었으므로 상대적으로 낮은 가중치를 받게 된다.\n결과적으로, 차등 선택 확률 가중치는 표본 설계의 불균형을 보정하여, 표본으로부터 계산한 통계량이 실제 모집단의 특성을 보다 정확하게 반영할 수 있도록 돕는다.\n비례 배분(EPSEM, Equal Probability Selection Method)\n표본이 모집단의 주요 특성 분포를 그대로 반영하도록 설계된 추출 방식이다. 예를 들어, 모집단의 성비가 남성 60%, 여성 40%인 경우, 200명의 표본을 추출할 때 이를 그대로 반영하여 남성 120명, 여성 80명으로 구성하는 방식이다. 이때 각 성별이 동일한 확률로 선택되며, 이는 모집단의 구성 비율에 비례한다는 점에서 “비례 배분”이라고 부른다.\n이러한 방식에서는 모든 단위가 동일한 선택 확률을 가지므로, 결과적으로 표본 설계 자체가 모집단을 잘 대표하게 된다. 따라서 분석 과정에서 추가적인 가중치를 적용할 필요가 없다. 모든 표본 단위의 가중치는 동일하며, 예를 들어 1,000명의 모집단에서 200명을 추출했다면 각 표본의 가중치는 \\(1/(1/5) = 5\\)가 된다. 남녀 모두 동일한 5의 가중치를 가지므로 통계 추정에 있어 왜곡이 발생하지 않는다.\n즉, 비례 배분 방식은 설계 자체가 균형을 유지하므로 별도의 보정 없이도 신뢰할 수 있는 분석 결과를 제공하는 장점이 있다. 하지만, 소수 집단이나 관심 집단의 표본 수가 너무 적어 분석이 어려울 경우에는 비례 배분보다 과표집(over-sampling) 방식이 필요하며, 이때는 앞서 설명한 차등 선택 확률 가중치가 적용되어야 한다.\n차등 배분(Disproportionate Allocation)\n차등 배분(disproportionate allocation)은 표본 추출 시 특정 집단의 표본 수를 인위적으로 조정하는 방식으로, 특히 드문 특성을 조사할 때 통계적 정밀도를 확보하기 위해 사용된다. 예를 들어, 성폭력 피해율처럼 일반적으로 여성에게 더 많이 나타나는 현상을 분석할 때, 여성 응답자의 수를 늘려 보다 정확한 추정치를 확보하려는 경우다.\n모집단이 남성 600명, 여성 400명으로 구성되어 있다고 가정하고, 전체 표본을 200명으로 설정할 때, 단순히 모집단 비율에 따라 추출한다면 남성 120명, 여성 80명을 선택하게 된다. 이는 비례 배분(equal allocation)이며, 따로 가중치 조정이 필요하지 않다.\n그러나 여성 피해자의 수를 늘려 보다 정밀한 분석을 수행하기 위해, 남성과 여성을 각각 100명씩 조사하는 차등 배분을 실시한다고 가정하자. 이 경우, 모집단 대비 표본 비율은 다음과 같다: • 남성: 600명 중 100명 → 표본 비율 1/6 → 가중치 6 • 여성: 400명 중 100명 → 표본 비율 1/4 → 가중치 4\n즉, 각 응답자의 응답은 그가 속한 집단의 모집단 대비 표본 비율의 역수만큼 가중치를 부여받는다.\n조사 결과, 다음과 같은 피해율이 관찰되었다고 가정하자: • 남성: 10% (100명 중 10명) • 여성: 30% (100명 중 30명)\n이때, 가중치를 적용한 전체 모집단의 추정 피해율은 다음과 같이 계산된다.\n\\({\\overline{Y}}_{w} = \\frac{\\sum w_{i}Y_{i}}{\\sum w_{i}}\\), \\(w_{i}\\) 성별 총 가중치, 남성 (100x6), 여성 (100x4)\n\\[\\text{추정 피해율} = \\frac{(10\\% \\times 600) + (30\\% \\times 400)}{600 + 400} = 18\\%\\]\n즉, 단순 표본 평균(20%)이 아닌, 모집단 구조를 반영한 추정값은 18%가 된다. 이처럼 차등 배분은 분석의 정확도를 높이는 동시에, 대표성 유지를 위해 가중치 조정이 필수적이라는 점을 잘 보여준다.\n가중치 조정이 필요한 이유\n차등 배분을 활용한 조사에서는 특정 집단의 표본 수를 인위적으로 늘려 분석의 정밀도를 확보할 수 있지만, 그만큼 가중치 조정이 반드시 필요하다. 예를 들어 성폭력 피해율처럼 여성에게서 더 빈번히 나타나는 현상을 조사할 경우, 여성 응답자의 수를 늘리는 것은 더 많은 사례 확보와 정밀한 분석이라는 측면에서 바람직하다.\n하지만 이렇게 여성 표본이 과대표본(over-represented) 상태가 되면, 그 자체로는 모집단의 실제 구조를 왜곡하게 된다. 이 경우 여성의 피해율이 과장되어 전체 피해율이 실제보다 높게 추정될 수 있다. 따라서, 남성과 여성의 실제 모집단 비율에 맞춰 가중치를 적용함으로써, 편향된 표본 구성을 보정할 수 있다.\n가중치는 표본에서 각 집단이 차지하는 비중을 조정하는 역할을 하며, 이를 통해 모집단 전체를 대표할 수 있는 평균값(가중 평균)을 산출할 수 있다. 예를 들어 남성 가중치를 6, 여성 가중치를 4로 설정하면, 실제 인구 구성(남성 600명, 여성 400명)에 비례한 결과를 계산할 수 있게 된다.\n이처럼 성폭력 피해 조사에서 차등 선택 확률 가중치는 두 가지 측면에서 반드시 필요하다.\n\n분석 정밀도 확보: 여성 응답자 수를 늘려 피해율의 신뢰도를 높일 수 있음\n대표성 회복: 가중치를 적용하여 실제 인구 구조를 반영한 추정치를 산출\n\n결과적으로, 가중치 조정은 조사 결과의 타당성과 일반화 가능성을 동시에 확보하기 위한 필수적인 절차다.\n\n\n3. 단위 무응답 조정을 위한 가중치\n\n(1) 무응답의 문제와 표본 구성\n성폭력 피해 조사는 주제의 민감성으로 인해 응답률이 낮을 수 있으며, 특히 성별에 따라 응답률이 상이하게 나타나는 경향이 있다. 위의 예시에서와 같이 모집단은 남성 600명(60%), 여성 400명(40%)으로 구성되어 있고, 이 구조를 반영한 비례 배분으로 표본 200명을 선정하여 남성 120명, 여성 80명이 포함되었다고 가정하자.\n그러나 실제 조사에 응답한 인원은 성별 응답률에 따라 다음과 같이 달라진다.\n\n남성 응답률: 90% → 120명 중 108명 응답\n여성 응답률: 75% → 80명 중 60명 응답\n\n결과적으로 실제 응답자 중 성별 분포는 다음과 같다: - 전체 응답자 수: 168명 - 남성 비율: 108 / 168 ≈ 64.3% - 여성 비율: 60 / 168 ≈ 35.7%\n이렇게 되면 여성은 원래 모집단에서 40%를 차지했음에도 불구하고, 실제 응답자 중에서는 35.7%로 과소 대표되는 문제가 발생한다. 만약 이를 보정하지 않으면, 여성 집단의 특성이 전체 피해율 추정에 충분히 반영되지 않아 편향된 결과가 도출될 수 있다.\n\n\n(2) 무응답 조정 가중치 계산\n각 집단의 응답률의 역수를 사용하여 가중치를 설정한다.\n\\[W_{male} = \\frac{1}{\\text{응답률}_{male}} = \\frac{1}{0.9} = 1.11\\]\n\\[W_{female} = \\frac{1}{\\text{응답률}_{female}} = \\frac{1}{0.75} = 1.33\\]\n\n\n(3) 가중 평균 적용\n조사 응답 결과에 따르면, 남성 응답자 108명 중 약 10%인 10.8명이 성폭력 피해를 경험하였고, 여성 응답자 60명 중 30%에 해당하는 18명이 피해 경험이 있는 것으로 나타났다. 그러나 이 응답자 구성은 실제 모집단의 성비를 반영하지 않으며, 특히 여성의 응답률이 낮아 원래보다 적은 수가 응답에 포함되었기 때문에, 피해율이 과소 추정될 가능성이 있다.\n이러한 문제를 해결하기 위해 무응답 조정 가중치를 적용한다. 남성의 응답률이 90%이므로 가중치는 1 / 0.9 = 1.11, 여성은 응답률이 75%이므로 가중치는 1 / 0.75 = 1.33으로 설정한다. 이에 따라 가중치를 적용한 전체 피해자 수는 다음과 같이 계산된다.\n\n남성 피해자 가중치 총합: 10.8 = 11.988\n여성 피해자 가중치 총합: 18 = 23.94\n전체 피해자 가중치 총합: 11.988 + 23.94 = 35.928\n\n한편, 가중치를 반영한 응답자 수는 다음과 같다. • 남성 응답자 총 가중치: 108 = 119.88 • 여성 응답자 총 가중치: 60 = 79.8 • 전체 응답자 가중치 합계: 119.88 + 79.8 = 199.68\n따라서 무응답 조정 가중치를 적용한 피해율은 다음과 같이 계산된다.\n\\(\\text{추정 피해율} = \\frac{(10\\% \\times 119.88) + (30\\% \\times 79.8)}{199.68} = \\frac{11.988 + 23.94}{199.68} \\approx 0.1799 = 17.99\\%\\)\n이는 단순 응답자의 피해율 평균인 (10% + 30%) / 2 = 20%보다 낮은 수치로, 모집단 성비와 응답률 차이를 반영해 보다 현실적인 피해율을 제시한다.\n\n\n\n4. 사후 가중치\n\n(1) 사후 층화 가중치 개념\n사후 층화 가중치(post-stratification weight)는 조사 과정에서 마지막으로 수행되는 가중치 조정 절차로, 조사 표본이 실제 모집단 분포와 최대한 일치하도록 조정하는 데 목적이 있다. 특히 성별, 연령대, 지역 등 인구 통계학적 특성이 외부의 신뢰할 수 있는 자료(예: 국가 통계청, 인구총조사)와 비교해 불일치할 경우, 그 차이를 보정하기 위해 사용된다.\n예를 들어, 성폭력 피해 실태조사를 수행한 후, 비례 배분, 차등 선택 확률 가중치, 무응답 조정 가중치 등을 단계적으로 적용하여 조사 표본을 보정했다고 하자. 그 결과, 최종적으로 남성과 여성의 가중치 총합이 각각 전체의 50%로 나타났다고 가정할 수 있다. 그런데 외부의 신뢰할 수 있는 모집단 자료에 따르면, 실제 모집단은 여성 52%, 남성 48%로 구성되어 있다면, 이 가중치 분포는 모집단과 일치하지 않는다.\n이처럼 기존의 모든 가중치 조정을 적용한 뒤에도 여전히 조사 표본의 가중 분포가 모집단과 불일치할 수 있다. 이 경우, 사후 층화(post-stratification)를 통해 응답자의 가중치를 다시 조정하여, 성별 비율이 48:52로 되도록 재보정한다.\n사후 층화는 응답자 특성에 따라 사후적으로 층(stratum)을 구분하고, 각 층별로 모집단 분포와 표본 분포를 비교하여 비례에 따라 가중치를 조정하는 방식이다. 예를 들어, 남성 응답자에게는 가중치를 0.96배, 여성 응답자에게는 1.04배 곱하여 가중치 총합이 48:52가 되도록 한다.\n\n사후 층화는 모집단 외부 정보를 기준으로 최종적으로 가중치를 조정하는 방식이다.\n기존 가중치 적용만으로는 실제 모집단 분포와 완전히 일치하지 않을 수 있으며, 그 차이를 보정하기 위해 필수적이다.\n대표성을 더욱 높이고자 하는 실증연구나 공공통계 생산에서 정확하고 현실적인 추정값을 제공하는 데 중요한 역할을 한다.\n\n\n\n(2) 사후 층화 가중치 적용 방법\n\n현재 조사 표본의 성별 가중치 비율 확인 조사 결과에서 남성과 여성의 총 가중치 합이 각각 50%씩 동일한 상태라고 가정한다. 이는 표본 내 가중치 분포가 남녀 각각 600:400으로 구성되어 있을 때 비례 가중치를 적용한 결과일 수 있다.\n모집단의 실제 성비 확인 및 비교 외부 인구통계 자료에 따르면, 모집단은 남성 48%, 여성 52%로 구성되어 있다. 이와 비교하면, 조사 데이터에서 남성이 과대표본되었고 여성이 과소표본되었음을 알 수 있다.\n가중치 조정 비율 계산 표본과 모집단 간의 차이를 반영하기 위해 다음과 같은 비율로 가중치를 조정한다.\n\n\n남성 가중치 감소: \\(\\frac{0.48}{0.50} = 0.96\\)\n여성 가중치 증가: \\(\\frac{0.52}{0.50} = 1.04\\)\n\n이는 남성의 가중치를 4% 줄이고, 여성의 가중치를 4% 증가시키는 조정이다.\n\n조정된 가중치를 적용한 총 가중치 합 계산\n\n\n남성 총 가중치 조정 후 합: \\(0.96 \\times 600 = 576.0\\)\n여성 총 가중치 조정 후 합: \\(1.04 \\times 400 = 416.0\\)\n\n\n가중 평균을 통한 성폭력 피해율 추정\n\n남성과 여성의 피해율이 각각 10%, 30%일 경우, 사후 층화 가중치를 반영한 최종 성폭력 피해율은 다음과 같이 계산된다. \\[\\text{추정 피해율} = \\frac{(10\\% \\times 576.0) + (30\\% \\times 416.0)}{576.0 + 416.0} = 18.39\\%\\]\n\n\n\n\nchapter 3. 결측치 대체\n조사에서 항목 무응답(item nonresponse)은 응답자가 전체 설문에는 참여했지만 특정 문항에는 답변하지 않는 현상을 의미한다. 이는 응답자가 질문을 민감하게 느껴 의도적으로 답변을 거부하거나, 설문 진행 과정에서 실수로 특정 문항을 건너뛰는 등의 이유로 발생한다. 예를 들어, 성폭력 피해 조사에서 응답자는 ‘피해 경험 유무’에는 답했지만, ‘가해자와의 관계’나 ‘자신의 소득 수준’ 등 민감한 항목에는 응답하지 않는 경우가 이에 해당한다.\n이처럼 항목 무응답이 누적되면 해당 변수에 대한 분석이 어려워지고, 전체 조사 결과의 신뢰도와 대표성에도 영향을 미칠 수 있다. 특히 특정 집단에서 무응답이 체계적으로 발생할 경우, 표본이 왜곡되어 모집단의 특성을 제대로 반영하지 못할 위험이 있다. 이를 방지하고 분석 가능성을 높이기 위해 결측값 대체(Imputation) 기법이 활용된다.\n결측값 대체는 단순히 빈칸을 임의의 값으로 채우는 것이 아니라, 응답자의 특성이나 다른 문항과의 관계를 고려하여 가장 적절한 값을 통계적으로 추정하는 과정이다. 예컨대, 소득 항목이 누락된 경우 유사한 성별, 연령, 직업을 가진 응답자의 평균 소득을 활용하거나, 회귀모형을 이용해 예측값을 산출하여 채워 넣는 방식이 있다. 보다 정교한 방법으로는 여러 번의 예측을 수행해 불확실성을 반영하는 다중 대체(Multiple Imputation)가 있으며, 최근에는 조사 데이터 분석에서 점차 널리 사용되고 있다.\n결론적으로 항목 무응답은 조사 데이터 품질을 저하시킬 수 있는 중요한 문제이며, 이를 보완하기 위한 체계적이고 신뢰성 있는 대체 기법의 적용은 필수적인 분석 준비 절차라 할 수 있다.\n\n1. 결측값을 처리하는 방법\n결측값을 무시하는 방법\n결측값을 무시하는 방법은 조사 데이터에서 특정 항목에 응답하지 않은 사례를 통째로 제외하고 나머지 완전한 응답만을 가지고 분석을 수행하는 방식이다. 이는 완전 사례 분석(complete case analysis) 혹은 사례 삭제(casewise deletion)라고 불린다.\n예를 들어, 성폭력 피해 경험에 관한 조사를 실시하고, 응답자들의 나이, 성별, 소득 수준을 바탕으로 피해율을 분석한다고 가정해보자. 이때, 응답자가 소득 수준에 대해 응답하지 않았다면, 해당 항목만 생략하지 않고 전체 응답을 분석에서 제거하게 된다. 즉, 분석에 필요한 모든 변수를 완전히 응답한 사례만을 선택하여 분석을 수행한다.\n이 방법의 장점은 분석이 매우 단순하고 직관적이라는 데 있다. 별도의 통계적 가정이나 복잡한 계산 없이, 응답이 완전한 자료만을 사용해 분석할 수 있으므로 절차가 간단하고 구현이 용이하다. 또한, 추가적인 대체값 생성이나 추정 과정이 없기 때문에 분석 결과 해석이 비교적 명확하다.\n그러나 단점도 뚜렷하다. 첫째, 결측값이 있는 응답을 통째로 제거하므로 분석에 사용 가능한 표본 크기가 줄어들게 되며, 이로 인해 통계적 검정의 정확도와 신뢰도가 저하될 수 있다. 둘째, 더 큰 문제는 결측이 무작위로 발생하지 않을 경우, 즉 특정 응답자 집단에서 결측이 집중된다면, 표본의 대표성이 왜곡될 수 있다. 예컨대, 고소득층이나 심각한 피해 경험을 가진 응답자가 소득을 밝히지 않는 경향이 있다면, 이들을 제거하는 과정에서 해당 집단이 과소표집되어 분석 결과가 편향될 위험이 있다.\n따라서 결측값을 무시하는 방식은 가장 단순한 처리 방법이지만, 결측의 발생 원인과 패턴을 충분히 고려한 후 적용해야 하며, 무작위 결측(missing completely at random, MCAR) 조건이 충족되지 않을 경우에는 대체 방법의 도입이 바람직하다.\n결측값을 보완하는 방법\n결측값을 보완하는 방법은 분석 대상 데이터에서 누락된 값을 합리적인 방식으로 예측하거나 추정하여 채우는 절차를 말하며, 이를 일반적으로 결측값 대체(imputation)라고 한다. 이 방법은 단순히 결측 사례를 제거하는 것보다 분석의 신뢰성과 표본의 대표성을 유지하는 데 유리하다.\n결측값을 무시하고 분석할 경우 표본 수가 줄어들어 분석의 정밀도가 떨어지며, 특정 집단이 과소대표될 수 있다. 반면, 결측값을 대체하면 기존의 정보는 유지하면서도 누락된 부분을 보완할 수 있기 때문에 분석 결과의 왜곡을 줄이고 더 나은 추정을 가능하게 만든다.\n예를 들어, 성폭력 피해 경험에 관한 설문에서 일부 응답자가 소득 수준을 누락했다면, 같은 연령대, 성별, 지역 등의 응답 정보를 바탕으로 해당 소득 값을 추정해 채워 넣을 수 있다. 이렇게 하면 해당 응답자의 다른 정보는 그대로 유지되므로 분석 대상에서 제외되지 않으며, 보다 풍부하고 정확한 분석이 가능해진다.\n\n(3) 결측값 대체 방법\n\n\n\n\n\n\n\n\n대체 방법\n장점\n단점\n\n\n완전 사례 분석\n단순하고 직관적이며, 추가적인 가정 없이 분석 가능\n데이터 손실 발생 가능, 모집단 대표성이 감소할 위험\n\n\n평균 대체\n계산이 간단하고 빠르며, 데이터 손실 없음\n분산 감소로 인해 데이터 변동성이 왜곡될 가능성 있음\n\n\n확률적 대체\n변동성을 유지하여 데이터 왜곡을 방지\n무작위성이 도입되어 결과 변동성이 증가할 수 있음\n\n\n회귀 대체\n다른 변수와의 관계를 고려하여 현실적인 값 대체 가능\n모델이 잘못 설정되면 왜곡된 값이 대체될 위험 있음\n\n\n핫덱 대체\n실제 응답자의 데이터를 활용하여 자연스러운 대체 가능\n적절한 유사 기준을 설정하는 것이 중요, 표본 크기 작으면 부적절한 대체 발생 가능\n\n\n다중 대체\n불확실성을 반영하여 보다 신뢰성 높은 결과 제공\n계산량이 많고 통계적 해석이 다소 복잡할 수 있음\n\n\n\n\n\n\n2. 대체방법\n\n(1) 평균 대체 mean imputation\n평균 대체(mean imputation)는 결측값을 처리하는 가장 단순한 방식 중 하나로, 누락된 값을 해당 변수의 평균값으로 채워 넣는 방법이다. 예를 들어, 성폭력 피해 조사에서 응답자의 가족 소득 정보를 수집했는데 일부 응답자가 해당 질문에 답하지 않은 경우, 그 결측된 소득 값을 다른 응답자들의 평균 소득으로 대체하는 식이다. 이 방식은 계산이 간단하고, 결측값이 있다고 해서 해당 응답을 분석에서 제외하지 않아도 되므로 표본 수를 유지할 수 있다는 장점이 있다.\n하지만 평균 대체는 몇 가지 중요한 한계를 갖는다. 모든 결측값이 동일한 평균값으로 채워지기 때문에, 원래 데이터가 가지고 있던 자연스러운 변동성이 줄어들게 된다. 결과적으로 변수 간 상관관계나 회귀 분석 결과 등이 왜곡될 수 있으며, 데이터의 분산도 인위적으로 낮아질 수 있다. 또한 평균 대체는 결측이 발생한 원인이나 맥락을 전혀 고려하지 않기 때문에, 결측값이 체계적인 이유로 발생한 경우에는 적절한 대처 방식이 되지 못한다. 따라서 평균 대체는 응답률이 높고 결측이 무작위로 발생한 경우에만 제한적으로 사용하는 것이 바람직하며, 보다 정교한 대체 기법과 병행하여 활용하는 것이 권장된다.\n\n\n(2) 확률적 대체 stochastic imputation\n확률적 대체(stochastic imputation)는 평균 대체가 가지는 변동성 손실 문제를 보완하기 위해 고안된 방식으로, 평균값을 중심으로 하되 무작위성을 부여하여 보다 현실적인 데이터를 생성한다. 이 방법에서는 결측값을 단순히 전체 평균으로 채우는 것이 아니라, 해당 변수의 분포—일반적으로 정규분포—를 가정하고, 평균과 표준편차를 기준으로 난수를 생성해 그 값을 결측값에 채운다. 이를 통해 원래 데이터의 분산과 패턴을 보다 충실히 보존할 수 있다.\n예를 들어, 응답자 100명이 보고한 월소득의 평균이 500만원, 표준편차가 100만원인 상황을 가정해보자. 이때, 5명의 응답자가 소득 항목에 응답하지 않아 결측이 발생하였다면, 평균 대체 방식에서는 이 5명을 모두 500만원으로 채우게 된다. 하지만 확률적 대체 방식은 평균값 500만원을 중심으로 하는 정규분포 N(500, 100^2) 에서 임의의 값을 5개 추출하여 각 응답자의 결측값에 입력하게 된다. 이렇게 하면 소득 데이터의 자연스러운 분산이 유지되고, 분석 시 통계량이 왜곡될 위험도 줄어든다.\n다만 이 방법은 무작위 요소를 포함하기 때문에, 대체할 때마다 결과가 달라질 수 있다는 점에서 분석의 재현성과 일관성에 주의가 필요하다. 또한, 정규분포와 같은 통계적 가정이 적절하지 않다면 오히려 왜곡을 유발할 수도 있다. 따라서 확률적 대체는 결측의 성격과 변수의 분포를 충분히 고려한 후 신중하게 적용해야 한다.\n\n\n(3) 회귀 대체 regression imputation\n회귀 대체(Regression Imputation)는 결측값을 보완하는 보다 정교한 방법으로, 해당 변수와 관련 있는 다른 변수들을 활용하여 결측값을 예측하는 방식이다. 이 방법은 단순히 평균이나 확률적 수치로 채우는 것보다, 데이터 내 변수 간의 관계를 반영하여 보다 현실성 있는 추정값을 산출할 수 있다는 장점이 있다.\n예를 들어, 응답자의 소득 정보에 결측이 있는 경우, 같은 응답자가 응답한 나이(Age), 교육 수준(Education Level), 직업(Job Type) 등의 정보를 바탕으로 회귀모형을 설정할 수 있다. 이 회귀모형은 소득이 응답된 응답자들의 데이터를 기반으로 회귀계수를 추정하고, 해당 계수를 이용해 소득이 누락된 응답자들의 소득을 예측한다. 회귀식은 다음과 같다. \\[y_{i(r)} = \\alpha + \\beta_{1}\\text{Age}_{i(r)} + + \\beta_{2}\\text{Edu}_{i(r)} + \\beta_{3}\\text{Job}_{i(r)} + e_{i(r)}\\]\n여기서 \\(\\hat{y}{i(r)}\\) 는 소득이 결측된 i번째 응답자의 예측 소득이고, \\(\\alpha, \\beta_1, \\beta_2, \\beta_3\\) 는 회귀계수이며, \\(e{i(r)}\\) 는 오차항이다.\n이 방법의 장점은, 단순 대체보다 훨씬 더 응답자 개인의 특성을 반영한 맞춤형 대체가 가능하다는 점이다. 예컨대, 고학력자이면서 특정 직종에 종사하는 중년의 응답자는 실제로도 소득이 높을 가능성이 있기 때문에, 단순 평균이 아닌 회귀 예측을 통한 값이 더 설득력 있다.\n하지만 단점도 존재한다. 회귀모형이 잘못 설정되면 편향된 추정치가 생성될 수 있으며, 대체된 값들이 모형의 구조를 따르기 때문에 실제 데이터보다 변동성이 작고 지나치게 규칙적인 값을 보일 수 있다. 이는 전체 데이터 분석에서 과도한 일관성을 유발하여 통계적 검정이나 분산 추정에 오류를 가져올 수 있다.\n따라서 회귀 대체는 변수 간 관계가 명확하고 모형이 잘 적합될 때 강력한 결측 보정 방법이지만, 그만큼 모형 설정과 변수 선택에 대한 신중한 검토가 필수적이다.\n\n\n(4) 핫덱 대체 Hot-Deck imputation\n핫덱 대체(Hot-Deck Imputation)는 결측값을 비슷한 특성을 가진 응답자의 실제 데이터로 대체하는 방법이다. 이는 결측값이 있는 항목과 유사한 특성을 가진 다른 응답자의 값을 찾아 채워 넣는 방식으로, 조사 데이터에서 유사한 패턴을 유지하면서도 데이터의 신뢰도를 높일 수 있는 장점이 있다.\n예를 들어, 성폭력 피해 조사에서 피해자의 나이, 성별, 지역 등이 유사한 다른 응답자의 데이터를 참조하여 결측값을 채운다면 보다 현실적인 대체가 가능하며, 데이터의 변동성을 유지하면서도 모집단을 반영할 수 있다. 핫덱 대체는 통계적 방법뿐만 아니라 설문 조사 및 행정 데이터 분석에서도 널리 사용되는 결측값 처리 기법이다.\n이 방법의 장점은 평균 대체나 회귀 대체보다 현실적인 값이 채워질 가능성이 크다는 점이다. 그러나 적절한 유사 기준을 설정하는 것이 중요하며, 표본 크기가 작을 경우에는 적절한 대체값을 찾기 어려울 수 있다는 단점도 존재한다.\n핫덱 Hot-Deck\n핫덱(Hot-Deck) 대체는 동일한 조사 데이터셋 내에서 결측값이 있는 응답자와 유사한 특성을 가진 다른 응답자의 값을 가져와 결측값을 대체하는 방식이다. 이 방법은 동일한 조건을 가진 응답자가 충분히 많을 때 특히 효과적이며, 현실적인 값을 유지하면서도 데이터의 구조와 분포를 보존할 수 있다는 장점이 있다.\n핫덱 대체는 성별, 연령, 지역, 교육 수준 등 주요 변수들이 일치하거나 유사한 응답자를 기준으로 결측값을 채워 넣기 때문에, 평균 대체나 회귀 대체보다 더 실제적인 데이터 보완이 가능하다. 또한 외부 자료 없이도 자체 데이터만으로 대체가 가능하므로, 조사 대상자가 많은 대규모 조사에서 활용도가 높다.\n콜드덱 Cold-Deck\n콜드덱(Cold-Deck) 대체는 과거의 다른 데이터셋이나 외부 자료에서 값을 가져와 결측값을 대체하는 방식이다. 이 방법은 현재 조사에서 유사한 특성을 가진 응답자가 부족하거나, 결측값을 직접적으로 대체할 수 있는 정보가 내부 데이터에 없는 경우에 활용된다.\n콜드덱 방식은 이미 수집된 자료를 바탕으로 하므로 대체 기준이 명확할 수 있고, 반복되는 조사에서 일관성을 유지하는 데 유리하다. 예를 들어, 전년도에 수행된 동일한 주제의 조사에서 유사한 응답자의 값을 가져와 결측값을 보완하는 경우가 이에 해당한다.\n그러나 이 방법은 사용하는 데이터가 과거 자료이거나 시차가 있는 외부 자료이기 때문에, 현재 상황과 불일치할 가능성이 있으며, 시대적 변화나 환경의 차이가 반영되지 않을 위험이 있다. 따라서 콜드덱 대체는 데이터의 맥락을 충분히 고려한 신중한 적용이 필요하다.\n핫덱 대체 과정\n핫덱(Hot-Deck) 대체는 동일한 데이터셋 내에서 유사한 특성을 가진 응답자의 값을 활용하여 결측값을 보완하는 방식이다. 이 과정은 몇 가지 단계를 거쳐 체계적으로 이루어진다.\n첫째, 데이터 정렬 및 유사 집단 형성 단계에서는 응답자들의 연령, 성별, 교육 수준, 지역 등의 변수에 따라 그룹화를 수행한다. 예컨대, ’20~29세 여성, 서울 거주’와 같이 세부 집단을 정의하여 응답자 간 유사성을 확보한다.\n둘째, 결측값이 있는 응답자 식별 단계에서는 특정 변수에서 값이 누락된 사례들을 찾아낸다. 예를 들어, ‘소득 수준’ 항목에서 결측된 응답자를 선별한다.\n셋째, 적절한 응답 값 선택 단계에서는 동일한 그룹 내에서 결측 항목을 갖지 않은 응답자의 데이터를 참조하여, 해당 결측값을 채운다. 이때 일반적으로 가장 유사한 응답자의 값을 사용하지만, 유사한 응답자가 여럿일 경우 무작위로 선택하여 대체할 수도 있다.\n넷째, 결측값 대체 수행 단계에서는 선택된 응답자의 값을 결측값이 있는 변수에 그대로 적용하여 데이터를 완성한다.\n핫덱 대체는 내부 데이터 기반으로 결측을 보완하므로 현실성이 높고, 데이터의 분포나 구조를 잘 보존할 수 있다는 장점이 있다. 다만, 적절한 유사 기준을 설정하고, 충분한 표본이 확보되어야 효과적인 적용이 가능하다.\n\n\n(5) 다중 대체 multiple imputation\n다중 대체(Multiple Imputation)는 하나의 결측값에 대해 하나의 추정값만을 사용하는 기존의 단일 대체 방식과 달리, 여러 개의 대체값을 생성하여 각각의 데이터셋을 구성하고 독립적으로 분석한 후, 이들의 결과를 종합하는 방법이다. 이 방식은 결측값에 내재된 불확실성을 반영하고자 하는 목적에서 출발한다.\n예를 들어, 성폭력 피해 조사에서 소득 정보를 응답하지 않은 사례가 있을 때, 이 결측값에 대해 평균 대체, 회귀 대체, 핫덱 대체 등 다양한 방식으로 3~5개의 서로 다른 대체값을 생성하여 각각의 대체값을 포함하는 데이터 세트를 만든다. 각 데이터 세트를 별도로 분석한 뒤, 최종 분석 결과는 이들 결과의 평균이나 조합을 통해 도출된다. 이렇게 하면 대체 과정에서 발생할 수 있는 편향과 추정의 불확실성을 함께 고려할 수 있다.\n다중 대체의 장점은 단일 대체보다 더 신뢰할 수 있는 통계 추정치를 제공하며, 결측값으로 인한 분석의 왜곡을 줄일 수 있다는 것이다. 특히 표준오차와 같은 추정의 정확도에 대한 정보를 함께 제공할 수 있어, 불확실성을 정량화할 수 있는 장점이 있다.\n반면, 다중 대체는 계산량이 많고 구현이 복잡하다는 단점이 있다. 특히 대체 방법의 설정, 반복 횟수, 결과의 통합 방식 등에 대한 통계적 이해와 숙련된 기술이 필요하다. 그럼에도 불구하고 최근에는 통계 소프트웨어에서 다중 대체 기능을 쉽게 사용할 수 있게 되어, 복잡한 조사나 사회과학 데이터 분석에서 점점 널리 활용되고 있다.\n\n\n\n\nchapter 4. 복합표본 분산 추정\n조사 데이터는 실제 수집 과정에서 단순한 무작위 추출 방식이 아닌, 층화(stratification), 군집(clustering), 다단계 추출(multi-stage sampling), 가중치(weighting), 결측값 대체(imputation) 등의 다양한 절차가 적용되기 때문에 복잡한 구조를 지닌다. 이러한 복합 설계는 표본 추출 단위 간의 독립성을 약화시키고, 집단 내 응답자들 간의 유사성을 증가시켜 표본 분산에 영향을 준다.\n예를 들어, 동일한 지역에서 다수의 가구를 선택하는 클러스터 표본의 경우, 해당 지역 내 응답자들은 비슷한 사회경제적 특성을 가질 가능성이 높아 서로 상관관계가 형성된다. 이로 인해 실제 분산은 단순 확률 표본보다 작거나 클 수 있으며, 만약 이를 무시하고 일반적인 분산 추정 방식을 사용하면 표준 오차가 부정확하게 추정될 위험이 있다.\n따라서, 통계 분석에서는 단순한 표본 설계 가정이 아닌, 조사에서 실제 사용된 복합 표본 설계 정보를 반영한 분산 추정 방법(예: Taylor 선형화, 재표집 기법, Balanced Repeated Replication 등)을 적용해야 하며, 이를 통해 보다 정확한 신뢰구간과 유의성 검정이 가능해진다. 이 과정은 특히 정책 결정이나 민감한 사회 이슈를 다루는 조사에서 필수적인 절차라 할 수 있다.\n\n1. 테일러 급수 근사법(Taylor Series Approximation)\n테일러 급수 근사법(Taylor Series Linearization)은 비선형 통계량—예를 들어 비율, 평균 대비 비율, 오즈비(odds ratio)와 같은 복잡한 함수형 통계량—의 분산을 추정할 때 널리 사용되는 대표적인 방법이다. 이러한 통계량은 단순한 합산이나 평균 연산으로는 분산을 직접 계산할 수 없기 때문에, 함수 형태를 일차 선형화하여 근사하는 방식이 필요하다.\n테일러 급수 근사법 핵심 개념\n테일러 급수 근사법은 분석 대상 통계량을 주어진 모수(parameter)에 대한 함수로 보고, 이 함수를 일차 테일러 급수로 전개하여 근사한 후 분산을 추정하는 기법이다. 다시 말해, 복잡한 비선형 함수를 선형 함수로 근사하여 분산 추정이 가능한 형태로 바꾸는 것이다.\n테일러 급수 근사를 이용한 가중 평균의 분산\n조사 데이터에서 가중 평균(\\({\\overline{Y}}_{w})\\)을 사용하여 분산을 추정하는 방법은 다음과 같다.\n\\[\\overline{Y}w = \\frac{\\sum{i = 1}^{n}w_{i}y_{i}}{\\sum_{i = 1}^{n}w_{i}}\\]\n테일러 급수 근사법을 사용하면, 해당 가중 평균의 분산은 다음과 같이 표현된다.\n\\[\\frac{1}{(\\sum w_{i})^{2}}\\left\\lbrack Var(\\sum w_{i}y_{i}) + {\\overline{Y}}_{w}^{2}Var(\\sum w_{i}) - 2{\\overline{Y}}_{w}Cov(\\sum w_{i}y_{i},\\sum w_{i}) \\right\\rbrack\\]\n이는 단순한 분산 계산보다 복잡하지만, 복합 표본 설계에서의 정확한 분산 추정을 위해 필수적인 접근법이다.\n테일러 급수 근사법 특징\n테일러 급수 근사법은 현재 복합 표본 설계 데이터를 분석할 때 가장 널리 사용되는 분산 추정 방법 중 하나다. 여러 통계 소프트웨어 패키지—예: SAS, Stata, SUDAAN—에서 기본 옵션으로 제공되며, 그만큼 신뢰성과 적용성이 높다.\n이 방법은 비율, 평균, 회귀 계수와 같은 다양한 통계량에 적용 가능하며, 특히 비선형 함수 형태를 선형 근사로 바꾸어 처리할 수 있기 때문에 활용 범위가 넓다. 무엇보다도, 단순 무작위 표본이 아닌 층화, 군집화, 가중치 부여 등 복합 표본 설계의 구조적 특성을 반영할 수 있어 표본 설계로 인한 분산 과소 추정을 방지하고 정확한 표준오차 계산을 가능하게 한다.\n따라서, 조사 통계 분석에서 테일러 근사는 복잡한 설계의 현실을 반영한 정밀한 분산 추정 기법으로 자리잡고 있다.\n\n\n2. 균형 반복 복제법(Balanced Repeated Replication, BRR)\n균형 반복 복제법(BRR)은 복합 표본 설계에서 분산을 추정하기 위해 널리 사용되는 방법 중 하나로, 전체 표본을 여러 개의 하위 표본(Replicates)으로 나누어, 각 하위 표본에 대해 통계량을 계산한 뒤 이들의 변동성을 이용해 모집단 통계량의 분산을 추정한다.\n가장 기본적인 방식은 전체 표본을 두 개의 하위 표본으로 나누는 작업을 반복하면서 여러 개의 균형 잡힌 복제(Replicate)를 생성하는 것이다. 이러한 반복 분할은 표본 설계의 층화 구조를 고려하여 체계적으로 수행되며, 각 복제본에 대해 통계량을 계산하고 그 결과의 분산을 기반으로 전체 분산을 추정한다.\nBRR의 핵심은 다음과 같다.\n\n각 복제본은 전체 표본의 하위 집합이며, 균형 잡힌 방식으로 구성된다.\n전체 표본의 통계량과 복제 통계량 간의 차이를 통해 분산을 계산한다.\n보통 Fay’s BRR처럼 가중치를 부드럽게 조정하는 변형 방식도 존재한다.\n\n이 방법은 특히 2차 단위 선택이 없는 이단계 층화 설계에서 유용하며, 여러 통계 소프트웨어(SAS, Stata, SUDAAN 등)에서 BRR 지원 기능이 제공된다.\n각 표본에서 평균(\\({\\overline{Y}}_{r}\\))을 계산하고, 그 평균의 변동성을 기반으로 전체 표본의 분산을 추정한다.\n\\[\\overline{Y} = \\frac{1}{c}\\overset{c}{\\sum_{r = 1}}{\\overline{Y}}_{r}\\]\n분산은 다음과 같이 계산된다.\n\\[Var(\\overline{Y}) = \\frac{1}{c(c - 1)}\\overset{c}{\\sum_{r = 1}}({\\overline{Y}}_{r} - \\overline{Y})^{2}\\]\n균형 반복 복제법(BRR)의 특징\n균형 반복 복제법(BRR)은 복합 표본 설계에서 분산을 추정하기 위해 널리 사용되는 방법 중 하나이다. 이 기법은 전체 표본을 여러 개의 하위 표본(복제본, replicates)으로 나누어 반복적으로 통계량을 계산하고, 이들 간의 변동성을 통해 전체 통계량의 분산을 추정하는 방식이다. 특히, 층화된 복합 표본 설계에서 매우 효과적으로 활용될 수 있으며, 2단계 이상의 다단계 표본 설계에도 적절하게 적용된다.\nBRR의 가장 큰 장점은 복잡한 조사 설계의 구조를 반영하면서도 실제 표본의 변동성을 잘 반영할 수 있다는 점이다. 이는 단순한 이론적 공식보다 더 현실적인 분산 추정을 가능하게 한다. 그러나 BRR을 적용하려면 각 층(stratum)마다 두 개의 1차 표본 단위(PSU)가 존재해야 하며, 복제본을 만들기 위해 층별로 적절히 균형을 맞추는 것이 필요하다. 표본 설계가 지나치게 복잡하거나 PSU 수가 부족한 경우에는 이러한 균형을 유지하기 어려워, BRR의 적용이 제한될 수 있다. 이처럼 BRR은 강력한 분산 추정 도구이지만, 설계와 구현에 있어 정교한 구조 설정이 요구된다.\n\n\n3. 잭나이프 반복 복제법(Jackknife Repeated Replication, JRR)\n잭나이프 반복 복제법(Jackknife Repeated Replication, JRR)은 복합 표본 설계에서 분산을 추정하는 데 널리 사용되는 기법으로, 데이터에서 하나의 표본 또는 클러스터를 순차적으로 제거하면서 반복적으로 통계량을 계산하는 방식이다. 이 기법의 핵심은 표본 전체를 사용하는 것이 아니라, 매 반복마다 하나의 단위를 제거한 후 통계량(예: 평균)을 계산하고, 이렇게 얻은 여러 개의 통계량을 바탕으로 그 변동성을 측정함으로써 전체 표본의 분산을 추정하는 데 있다.\n잭나이프 방법은 계산이 비교적 단순하다는 장점을 가지며, 특히 비선형 통계량(예: 비율, 오즈비 등)의 분산 추정에도 안정적인 결과를 제공할 수 있다. 반복 복제 방식으로 인해 복잡한 분포를 따르는 표본에서도 적용이 가능하며, 다양한 통계 소프트웨어에서 지원된다.\n다만, 잭나이프 방법은 표본 수가 충분히 많을 때 그 효과가 극대화되며, 표본 크기가 작을 경우 반복 횟수가 제한되어 분산 추정의 정확도와 신뢰도가 낮아질 수 있다는 한계도 존재한다. 그럼에도 불구하고, 단순함과 안정성 덕분에 복잡한 조사 설계에서도 유용하게 사용되는 방법 중 하나이다.\n\n\n4. 방법 비교\n복합 표본 설계에서 분산을 추정하는 주요 방법들은 각각의 장점과 제한점을 가지고 있으며, 적용 환경에 따라 적절한 방법을 선택하는 것이 중요하다.\n테일러 급수 근사법은 가장 널리 사용되는 방식으로, 비선형 통계량(비율, 회귀 계수 등)을 근사화하여 분산을 추정한다. 이 방법은 계산이 비교적 효율적이고, 대부분의 통계 소프트웨어(SAS, Stata, SUDAAN 등)에서 기본값으로 제공되므로 실무 활용도가 높다.\n균형 반복 복제법(BRR)은 층화된 복합 표본 설계에서 특히 높은 정확도를 보인다. 전체 표본을 반복적으로 절반씩 나누어 복제 통계량을 계산함으로써 분산을 추정하며, 구조가 일정하고 표본 크기가 충분할 때 신뢰도가 높다. 하지만 설계 구조가 복잡하거나 층화 기준이 불균형할 경우 적용에 제한이 있을 수 있다.\n잭나이프 반복 복제법(JRR)은 표본이나 클러스터를 하나씩 제거하여 복제 통계량을 계산하는 방식으로, 계산 구조가 단순하다는 장점이 있다. 특히 비선형 통계량에도 안정적으로 적용 가능하나, 복잡한 다단계 표본 설계에서는 반복 횟수 부족이나 표본 구조의 제약으로 인해 한계를 보일 수 있다.\n따라서, 테일러 급수 근사법은 일반적인 기본값으로 활용되고, BRR은 층화 구조가 명확할 때, 잭나이프는 구조가 단순하거나 반복 제거 방식이 적합할 때 선택적으로 사용된다.\n\n\n\nchapter 5. 조사 데이터 문서화 및 메타 데이터\n조사 데이터는 단 한 번의 분석을 위해 수집되는 것이 아니다. 데이터 수집 이후에도 다양한 연구자가 수년에 걸쳐 반복적으로 재분석하고, 새로운 분석 목적에 맞추어 지속적으로 활용된다. 이러한 재사용 가능성을 높이기 위해서는 조사 데이터를 체계적으로 문서화하고, 그 의미와 구조를 명확하게 설명하는 정보가 반드시 필요하다.\n이때 활용되는 핵심 정보가 바로 메타데이터(metadata)이다. 메타데이터는 데이터 자체가 아니라 데이터를 설명하는 데이터로, 변수의 정의, 측정 단위, 응답 범주, 결측 처리 방식, 코딩 기준 등 연구자가 데이터 분석 전에 반드시 이해해야 할 기본 속성을 포함한다. 메타데이터는 연구자 간 일관된 해석을 가능하게 하고, 데이터의 맥락을 제공함으로써 정확한 분석과 비교 연구를 지원한다.\n한편, 설문조사 과정에서 자동으로 수집되는 보조적 데이터인 파라데이터(paradata)도 함께 중요한 역할을 한다. 이는 설문 응답 자체가 아니라 응답이 생성되는 과정을 설명하는 데이터로, 예를 들어 문항별 응답 시간, 응답 수정 여부, 조사 기기의 종류, 응답자의 이동 경로, 면접자의 질문 방식 등이 해당된다. 파라데이터는 설문 문항의 난이도나 응답자의 이해 수준을 파악하거나, 성의 없는 응답이나 비정상적인 응답 패턴을 식별하는 데 유용하다.\n따라서 조사 데이터의 품질과 활용도를 높이기 위해서는 메타데이터와 파라데이터를 함께 수집하고 체계적으로 관리하는 것이 필수적이다. 이 두 정보는 단순한 부속 자료를 넘어, 조사 데이터의 해석과 재사용 가능성을 결정짓는 핵심 도구로 기능한다.\n\n1. 메타 데이터\n메타데이터란 조사 데이터에 대한 정보(”데이터에 대한 데이터”)를 의미하며 연구자가 데이터를 이해하고 활용할 수 있도록 제공하는 모든 정보를 포함한다.\n메타데이터의 주요 목적은 조사 데이터의 속성을 명확하게 설명하여 연구자가 데이터를 효과적으로 활용할 수 있도록 하는 것이다.\n특정 연구자가 아닌, 전 세계 누구나 데이터의 의미를 쉽게 이해할 수 있도록 표준화된 문서화를 제공하는 것이 핵심이다.\n메타데이터의 주요 유형\n\n\n\n\n\n\n\n메타데이터 유형\n설명\n\n\n정의적 메타데이터\n조사 대상 모집단, 표본 설계, 질문 문구, 코딩 용어 등을 설명\n\n\n절차적 메타데이터\n조사원 교육 절차, 표본 선정 방법, 데이터 수집 과정 등 조사 프로토콜을 설명\n\n\n운영적 메타데이터\n결측 데이터 비율, 데이터 수정 실패율, 평균 조사 시간, 조사원이 완료한 케이스 수 등 조사 품질 평가 정보 포함\n\n\n시스템 메타데이터\n데이터 파일 형식, 파일 위치, 데이터 검색 및 호출 방법, 변수 정의 등을 설명\n\n\n\n메타데이터 예시\n\n\n\n\n\n\n\n항목\n설명\n\n\n변수명\nAR21 (피해 보고 항목)\n\n\n질문 문구\n당신의 재산이 손상되었거나 파괴된 적이 있습니까?'\n\n\n데이터 위치\n컬럼 140, 너비 1\n\n\n결측값 처리\n-9 (무응답), -0 (모름)\n\n\n데이터 수준\n가구 데이터 또는 개인 데이터\n\n\n추가 메타데이터\n표시(X)하여 해당하는 모든 항목을 선택하세요.'\n\n\n\n메타데이터 설계의 중요성\n메타데이터의 발전은 조사 방법론 연구자에게 새로운 기회와 도전 과제를 동시에 제공한다.\n조사 품질 평가 및 사용자 확장: 조사 품질을 측정하는 재조사 연구, 응답 분산 추정 등의 정보가 쉽게 제공될 수 있다. 이를 통해, 조사 데이터의 신뢰성을 높이고, 다양한 연구자들이 데이터를 활용할 수 있도록 지원한다.\n설문지 개발 과정에서의 연계: 특정 문항에 대해 과거 연구에서의 활용 방식, 문항 재설계 사례, 응답자 행동 데이터(Behavior Coding) 등과 연계하여 문항 개발이 가능하다.\n연구자의 데이터 활용 지원: 연구자가 분석을 수행하기 전, 특정 변수가 과거 연구에서 어떻게 활용되었는지를 쉽게 확인할 수 있도록 설계할 수 있다. 이를 통해 조사 데이터의 신뢰성을 유지하면서도, 보다 효율적인 데이터 분석을 지원할 수 있다.\n메타데이터는 단순한 부가 정보가 아니라, 조사 데이터를 보다 효과적으로 활용할 수 있도록 지원하는 핵심 요소이다. 특히, 대규모 조사 데이터에서는 메타데이터의 체계적인 관리와 전자 문서화가 필수적이다.\n메타데이터는 조사 데이터의 활용도를 높이고, 연구자가 데이터를 쉽게 이해할 수 있도록 돕는 중요한 역할을 한다.\n전통적인 코드북에서 벗어나, 전자 문서화와 웹 기반 시스템이 발전하면서 보다 효율적인 데이터 관리가 가능해지고 있다.\n조사 설계 단계에서부터 메타데이터의 체계적인 구성을 고려하는 것이 중요하며, 이를 통해 조사 품질을 높이고 연구자들이 보다 쉽게 데이터를 활용할 수 있도록 해야 한다.\n결국, 좋은 조사 데이터는 메타데이터가 잘 정리되어 있어야 한다. 조사 데이터를 체계적으로 문서화하고, 연구자들이 손쉽게 접근할 수 있도록 메타데이터를 설계하는 것이 향후 조사 연구의 핵심 과제가 될 것이다.\n\n\n2. 파라 데이터\n조사 데이터 문서화 체계에서 파라데이터(paradata)는 설문 응답이라는 결과물이 생성되는 과정을 실시간으로 추적하고 기록한 행태적(behavioral) 및 기술적(technical) 정보의 집합이다. 이는 응답자가 어떤 답변을 했는지에 대한 내용적 정보가 아니라, 그 답변을 어떻게 도출했는지에 대한 과정 정보로서, 설문 조사 품질을 정밀하게 평가하고 설계 오류를 사전에 진단할 수 있는 중요한 도구로 기능한다.\n파라데이터의 구체적 예\n웹 기반 자기기입식 설문(CAWI)에서는 다음과 같은 항목이 대표적인 파라데이터로 수집된다.\n\n각 문항에 소요된 응답 시간\n응답자가 뒤로 가기(back) 버튼을 눌러 응답을 수정한 횟수\n문항 미응답 또는 건너뛰기 횟수\n키보드 입력의 정정 횟수 (backspace, delete 키 입력 빈도)\n응답 순서 변경 패턴\n디바이스 정보(PC, 모바일, 태블릿 등)\n브라우저 및 운영체제 정보\n응답자의 위치 정보(IP 또는 GPS 기반) 등\n\n면접조사(CAPI, CATI 등)에서는 다음과 같은 파라데이터가 활용된다.\n\n면접자가 질문을 읽는 데 걸린 시간\n응답자가 주저하거나 이해하지 못하는 구간의 음성/행동 코딩\n면접 환경(예: 소음, 제3자 존재 여부)\n질문 순서를 건너뛴 경우나 보완 질문 삽입 여부\n면접 시작 및 종료 시각, 소요 시간\n면접 중 발생한 기술적 문제 로그 등\n\n파라데이터의 활용 목적\n\n응답 품질 진단: 지나치게 짧은 응답 시간이나 동일 패턴 반복(예: 한쪽 극단값만 선택)은 무성의한 응답자의 특징일 수 있다. 문항별 응답시간 분포가 비정상적으로 길 경우, 문항 이해가 어려웠음을 시사한다.\n설문 설계 개선: 반복적으로 건너뛰는 문항이나 수정 빈도가 높은 문항은 설계의 문제(문항 길이, 복잡성, 용어 선택 등)를 내포할 가능성이 있다. 파라데이터를 분석함으로써 특정 문항이 응답자의 인지 부담을 초래하는지 파악할 수 있다.\n면접자 모니터링 및 교육: 면접자의 응답 시간 패턴, 질문 생략 여부 등을 통해 조사 수행의 일관성과 품질을 평가할 수 있다. 특정 면접자가 다른 면접자에 비해 비정상적인 면접 패턴을 보일 경우, 교육 강화나 재훈련이 필요할 수 있다.\n데이터 후처리 및 가중치 조정: 응답 완결성, 무응답 패턴, 설문 중단률 등을 기준으로 분석에 포함할 응답자 범위를 결정하거나, 가중치를 부여하는 데 활용된다.\n\n파라데이터의 의의\n파라데이터는 단순히 “보조적 로그 정보”를 넘어, 응답자가 설문에 어떻게 접근했는지, 설문 시스템이 어떻게 작동했는지, 그리고 데이터 생성의 맥락이 무엇이었는지를 입증하는 정량적 기록이다. 따라서 현대 조사 연구에서 파라데이터는 데이터 품질 관리의 핵심 요소로 간주되며, 점차 정밀한 설문 설계를 위한 기반 자료로 적극 활용되고 있다. 특히 빅데이터 분석, 응답자 분류, AI 기반 설문 인터페이스 개발 등에서도 파라데이터의 활용 범위는 계속 확장되고 있다.\n결론적으로, 파라데이터는 응답자의 행동적 궤적을 가시화하는 데이터이며, 이는 설문조사의 타당성과 신뢰도를 평가하고 향후 조사 설계를 개선하는 데 없어서는 안 될 핵심 자원이라 할 수 있다.\n\n\n3. 메타데이터 vs 파라데이터 비교\n\n\n\n\n\n\n\n\n구분\n메타데이터 (Metadata)\n파라데이터 (Paradata)\n\n\n\n\n정의\n조사 데이터에 대한 기술적 정보 (데이터에 대한 데이터)\n조사 과정 중 생성되는 부차적 정보 (조사 절차의 부산물)\n\n\n주요 내용\n변수명, 질문 문항, 응답 코드, 데이터 유형 등\n응답 시간, 응답 순서, 설문 경로, 중단 여부, 장치 종류 등\n\n\n사용 목적\n데이터의 해석 및 분석 지원, 일관성 유지\n응답 품질 평가, 비표본오차 분석, 조사 설계 개선\n\n\n생성 시점\n설문 설계 및 자료 구축 단계에서 생성\n조사 수행(수집) 과정 중 자동적으로 생성\n\n\n관리 주체\n조사 설계자, 데이터 관리자\n조사 시스템, 조사 소프트웨어, 응답 로그 등 자동 기록 시스템\n\n\n활용 예시\n분석 시 변수 라벨 확인, 코드북 제작, 설문 설계 문서 활용\n응답 시간 분석을 통한 신뢰도 판단, 반복 응답 여부 확인\n\n\n형식 예시\nQ1 = 성별 (1: 남자, 2: 여자), Q2 = 연령 (숫자형)\nQ1 응답 시간: 15초, 설문 중 3번 중단, 모바일 응답\n\n\n\n【참고】 메타데이터는 주로 분석 전 준비 정보, 파라데이터는 분석 및 품질평가 보조 정보로 구분된다."
  },
  {
    "objectID": "notes/mda/mda_discriminant.html",
    "href": "notes/mda/mda_discriminant.html",
    "title": "다변량분석 4.판별분석 (전통적방법)",
    "section": "",
    "text": "Chapter 1. 판별분석 개요\n\n1. 다변량 분석 비교\n판별분석(Discriminant Analysis)은 군집분석(Clustering Analysis)과 함께 개체 유도 기법(Individual-directed Techniques) 에 속한다. 이들은 개체별로 측정된 여러 특성(변수) 값을 이용하여 개체 간의 유사성 또는 집단 소속을 판별하는 방법이다.\n판별분석은 사전에 집단이 알려져 있는 경우, 각 집단의 특성을 반영하는 판별함수를 추정하여 새로운 개체가 어느 집단에 속하는지를 예측하는 분석 방법이다. 반면 군집분석은 집단의 구분이 알려져 있지 않은 상태에서 개체 간 유사성을 기준으로 자연스러운 집단(Cluster) 을 형성하는 탐색적 분석 방법이다.\n이에 대응되는 또 다른 축의 분석 범주는 변수 유도 기법(Variable-directed Techniques) 이다. 이 기법은 \\(p\\)개의 원변수 간 상관관계(공분산 행렬 또는 상관계수 행렬)를 이용하여 변수 구조를 축약하거나 잠재 요인을 도출하는 방법이다. 변수 유도 기법에는 다음과 같은 두 가지 대표적 방법이 있다.\n주성분분석(Principal Component Analysis, PCA)은 원변수들의 선형결합을 통해 새로운 주성분 변수를 도출하며, 전체 분산의 약 80% 이상을 설명하는 몇 개(보통 2~4개)의 주성분으로 차원을 축소한다.\n요인분석(Factor Analysis)은 변수들 간의 상관구조를 분석하여 이들을 서로 상호배타적인 잠재 요인(Factor)으로 묶는 방법이다. 각 요인은 변수들 간의 공통된 변동(공통분산, Communality)을 설명한다.\n\n\n2. 군집분석과 판별분석 차이\n판별분석은 사전에 집단이 이미 정의되어 있을 때, 개체의 여러 특성값(변수)을 이용하여 집단 간 차이를 설명하고 새로운 개체가 어느 집단에 속할지를 예측하는 지도학습(supervised learning) 기법이다. 즉, 집단 정보가 주어진 상태에서 개체를 구분하는 판별함수를 추정하는 것이 목적이다.\n반면 군집분석은 집단이 알려져 있지 않은 상태에서, 개체 간의 유사성(Similarity) 또는 거리(Distance) 를 기준으로 자연스럽게 형성되는 집단을 찾아내는 비지도학습(unsupervised learning) 기법이다. 즉, 집단 정보를 모르는 상태에서 개체들을 유사한 속성을 지닌 그룹으로 묶는 과정이다.\nH대학교 비즈니스통계학과 졸업생을 대상으로, 취업 여부에 영향을 미칠 것으로 예상되는 다음 7개 변수를 측정했다고 하자. 평점 (학업성취도), 비만도 (= 키/몸무게: 외모척도), 영어 성적, 자격증 개수, 원서 지원 회수, 가족 총 연소득 (재정능력), 친구 수 (사교력)이다.\n【판별분석】 졸업생 중 6개월 이내 취업한 졸업생 30명과 취업하지 못한 졸업생 20명을 무작위 추출하였다(취업률 60%). 이들의 7개 변수값을 이용하여 ’취업 여부(2집단)’를 구분하는 판별함수를 추정한다.\n\\(D = a + b_{1}X_{1} + b_{2}X_{2} + \\cdots + b_{7}X_{7}\\), 여기서 D의 값이 기준치보다 크면 ”취업 가능”, 작으면 ”미취업”으로 분류된다. 새로운 졸업예정자에게 7개 변수를 측정하면, 위의 판별식을 통해취업 가능성을 확률적으로 예측할 수 있다. → 즉, 판별분석은 집단 정보(취업 여부)가 사전에 주어진 상태에서 개체를 분류하는 모형이다.\n【군분석】 이번에는 같은 학과 졸업예정자 전체를 대상으로 위의 7개 변수를 측정했다고 하자. 이때 취업 여부는 아직 알 수 없다. 따라서 각 학생 간의 유사성을 유클리드 거리로 계산한 뒤, 유사성이 높은 학생끼리 묶어 군집(Cluster) 을 형성한다.\n군집분석의 핵심 단계는 개체 간 유사성 계산 → 유사성에 따라 집단을 형성하는 기준 설정 (계층적 또는 비계층적 방법) → 군집의 수 결정 → 각 군집의 해석 및 명명 순이다. 이렇게 형성된 군집은 예를 들어 다음과 같이 해석할 수 있다.\n군집 1: 높은 평점과 영어성적, 자격증 보유 — “취업 유망형”  군집 2: 낮은 영어성적과 지원회수 — “취업 소극형”  군집 3: 높은 사교력과 재정능력 — “취업 네트워크형”  → 즉, 군집분석은 집단 정보가 없는 상태에서 개체 간 유사성을 바탕으로 자연스러운 집단 구조를 탐색하는 방법이다.\n\n\n\n\n\n\n\n\n구분\n판별분석\n군집분석\n\n\n집단 정보\n사전에 알려져 있음 (지도학습)\n알려져 있지 않음 (비지도학습)\n\n\n목적\n집단을 구분하는 판별식 도출\n개체 간 유사성에 따른 자연 집단 형성\n\n\n주요 도구\n판별함수, 판별점수\n거리행렬, 유사도, 덴드로그램\n\n\n적용 예\n취업 여부 예측, 고객 이탈 예측\n고객 세분화, 시장 세분화\n\n\n분석 결과\n예측 모델(선형·2차 판별식)\n군집의 수와 각 군집의 특성\n\n\n대표 방법\nFisher 선형판별분석, 로지스틱 판별\nK-평균, 계층적 군집(Hierarchical)\n\n\n\n판별분석은 자료 수집 단계에서 이미 그룹이 구분되어 있는 경우, 이들 집단을 가장 잘 구분하는 판별규칙을 도출하여, 새로운 개체가 어느 집단에 속할지를 예측하는 방법이다.\n\n\n\n\n\n군집분석은 개체의 그룹 정보가 사전에 주어지지 않은 상태에서 출발한다. 즉, 처음에는 모든 개체가 구분 표시가 없이 동일한 상태이며, 분석을 통해 유사성이 가까운 개체들끼리 묶이면서 군집이 형성된다.\n\n\n\n\n\n\n\n3. 판별규칙\n판별규칙은 모집단에 속한 개체들을 가능한 한 정확하게 구분하기 위한 판별함수의 형태로 설정된다. 즉, 두 개 이상의 모집단이 있을 때, 각 모집단의 특성을 반영하는 변수들의 선형결합 또는 비선형결합을 통해 새로운 개체가 어느 모집단에 속하는지를 오분류 확률이 최소가 되도록 분류하는 규칙을 찾는 것이다.\n아래 그림은 두 모집단 A와 B가 두 개의 판별변수 (y, z)로 표현된 예이다. 점선으로 표시된 직선은 두 집단을 구분하는 선형 판별규칙이다. 직선 판별선 아래쪽의 히스토그램은 각 집단의 판별점수 분포를 나타내며, 빗금친 부분이 오분류 확률을 의미한다.\n(B 모집단의 왼쪽 빗금 부분): 실제로는 B집단에 속하지만, 판별규칙에 의해 A집단으로 잘못 분류된 비율.  (A 모집단의 오른쪽 빗금 부분): 실제로는 A집단에 속하지만, 판별규칙에 의해 B집단으로 잘못 분류된 비율.\n이 두 오분류 확률의 합이 전체 분류 오류율이며, 판별규칙은 이 오류율을 최소화하도록 설정된다.\n\n\n\n\n\n다음 그림은 두 가지 형태의 판별경계를 보여준다. 왼쪽은 직선 형태의 판별규칙, 오른쪽은 다항식 형태의 판별규칙이다. 산점도를 보면, 다항식 형태의 곡선 판별선이 두 집단을 더 잘 구분하여 오분류가 적어 보이지만, 실제 분석에서는 이차 함수 형태까지만 안정적으로 추정이 가능하다. 그 이상의 고차식 판별규칙은 과적합(overfitting) 문제가 발생하거나 모수 추정이 불안정해지는 경우가 많기 때문이다.\n따라서 실제 분석에서는 단순히 시각적으로 구분이 잘 되는 곡선을 선택하기보다, 오분류율의 검정 결과와 예측 정확도를 함께 고려하여 최적의 판별규칙 형태(선형 또는 이차)를 선택한다.\n\n\n\n\n\n\n\n4. 오분류 misclassification ratio\n\n(1) confusion matrix 혼동행렬\n마취과 의사가 마취 양을 결정할 때 (나이, 혈압, 맥박, 몸무게)가 영향을 준다고 하여 4개 변수로 마취안전여부를 판단하는 판별규칙을 만들었다고 하자.  양성 positive ={마취 안전한 하다고 판단}  음성 negative ={마취 불안전하다고 판단}\n\n\n\n\n\n\n\n\n실제 / 예측\n안전(Safe, positive)\n불안전(Unsafe, negative)\n\n\n안전(Safe)\nTrue Positive (TP) – 실제로 안전하며, 안전하다고 정확히 분류\nFalse Negative (FN) – 실제로 안전하지만 불안전하다고 잘못 분류\n\n\n불안전(Unsafe)\nFalse Positive (FP) – 실제로 불안전하지만 안전하다고 잘못 분류\nTrue Negative (TN) – 실제로 불안전하며, 불안전하다고 정확히 분류\n\n\n\n\n\n\n\n\n\n\n\n지표\n수식\n해석\n\n\n정확도 (Accuracy)\n\\[\\frac{TP + TN}{TP + TN + FP + FN}\\]\n전체 중 정확히 분류된 비율\n\n\n민감도 (Sensitivity, Recall)\n\\[\\frac{TP}{TP + FN}\\]\n실제 안전 환자 중 ’안전’으로 올바르게 판별한 비율\n\n\n특이도 (Specificity)\n\\[\\frac{TN}{TN + FP}\\]\n실제 불안전 환자 중 ’불안전’으로 올바르게 판별한 비율\n\n\n양성예측도 (Positive Predictive Value, PPV)\n\\[\\frac{TP}{TP + FP}\\]\n안전하다고 분류된 환자 중 실제로 안전한 비율\n\n\n음성예측도 (Negative Predictive Value, NPV)\n\\[\\frac{TN}{TN + FN}\\]\n불안전하다고 분류된 환자 중 실제로 불안전한 비율\n\n\n오분류율 (Error rate)\n\\[\\frac{FP + FN}{TP + TN + FP + FN}\\]\n잘못 분류된 비율\n\n\nF1 점수 (F1-score)\n\\[\\frac{2TP}{2TP + FP + FN}\\]\nPrecision과 Recall의 조화평균 — 불균형 데이터에서 유용\n\n\nAUC (Area Under ROC Curve)\n–\n다양한 임계값에서 모델의 전반적 판별능력\n\n\nBalanced Accuracy\n\\[\\frac{\\text{Sensitivity} + \\text{Specificity}}{2}\\]\n불균형 클래스일 때 정확도 보정지표\n\n\n\n\n\n(2) Reciever Operating Curve\n아래 그림은 진단 모형의 성능을 평가하기 위해 x축을 (1 − 특이도), y축을 민감도로 하여 나타낸 이차원 곡선(ROC Curve) 이다. 이 그래프는 양성과 음성을 가장 정확하게 구분하기 위한 기준값(cut-off value) 을 결정하는 데 활용되며, 의학 및 생명과학 분야에서 진단검사의 정확도를 비교하는 표준적인 도구로 사용된다.\n\n\n\n\n\n축의 의미\nx축 (1 − Specificity) : 실제 음성 환자를 양성으로 잘못 분류할 확률(위양성율, False Positive Rate)\ny축 (Sensitivity) : 실제 양성 환자를 양성으로 올바르게 분류할 확률(True Positive Rate)\n원점 (0,0)은 민감도 0, 특이도 1, 즉 ”모두 음성으로 판단하는 완전한 보수적 진단”을 의미한다. 반대로 (1,0)은 민감도 1, 특이도 0으로, ”모두 양성으로 판단하는 진단”을 뜻한다. (1,1)은 민감도와 특이도 모두 1인 이상적인 완벽 진단을 의미한다.\nAUC (Area Under the Curve)\nROC 곡선 아래의 면적을 AUC(Area Under the Curve) 라고 하며,\n이 값은 진단 모형의 전반적인 구분 능력을 정량적으로 평가하는 지표이다.\nAUC = 1.0 → 완벽한 판별 (perfect discrimination)\nAUC = 0.5 → 무작위(random) 판별, 즉 45° 대각선 수준\nAUC &lt; 0.5 → 무작위보다 못한 진단\n따라서, AUC는 0.5보다 커야 의미 있는 진단방법이며, AUC 값이 가장 큰 모형이 가장 적절한 진단방법으로 간주된다. 그림에서 곡선이 다른 곡선보다 AUC가 넓다면, 그 진단방법이 우수하다.\n기준값(Cut-off value)의 결정\nROC 곡선에서 양성과 음성을 구분하는 기준값(c) 을 선택하는 가장 일반적인 방법은 Youden의 J 통계량(Youden’s Index) 을 활용하는 것이다.\n\\[J = \\text{Sensitivity} + \\text{Specificity} - 1\\]\n이 값이 가장 큰 지점이 곧 최적 기준값(c*) 에 해당하며, 그 지점이 바로 그림 속 원으로 표시된 c이다. 즉, Youden J가 최대가 되는 점에서 민감도와 특이도의 균형이 가장 잘 맞아 양성과 음성을 가장 효율적으로 구분할 수 있다.\n요약하면, ROC 곡선은 진단검사의 판별력을 시각적으로 표현한 그래프이며, AUC는 진단력의 크기를, Youden J는 최적 기준값을 결정하기 위한 통계량으로 사용된다.\n\n\n\n5. 오분류 추정방법\nRe-substitution 규칙\n수집된 데이터로부터 얻은 판별식을 원 데이터에 적용하여 개체를 분류하여 오분류 비율을 구하는 것으로 정분류 비율이 높게 추정될(overestimate) 가능성이 있어 거의 사용하지 않는다. 그러나 대부분의 소프트웨어는 이를 사용한다. 빅데이터에서는 train(학습), test(감증) 데이터로 나누어 판별규칙 검증한다.\n테스트 데이터 이용\n데이터를 양분하여 한 개체 그룹으로부터 판별식을 유도하고, 이 판별식을 사용하여 다른 그룹의 개체를 분류하여 오분류 비율을 추정한다. 표본 자료의 1/2만 사용하여 판별식을 구하므로 모집단 분류에 적합한 판별식을 얻을 가능성이 낮고 데이터를 많이 수집해야 한다는 단점으로 인하여 이 방법 역시 사용 빈도가 낮았지만 빅데이터에서는 가능하다. 가장 많이 사용.\nCross-validation 추정법\nLachenbrush(1968)가 제안한 방법으로 가장 널리 사용된다. 첫 번째 개체 하나를 제외하고 판별식을 구하여 그 개체를 분류하고, 첫 번째 개체를 다시 넣고 두 번째 개체를 제외하고 판별식을 구한 후 두 번째 개체를 분류하고…… 이렇게 하여 오분류 비율을 추정한다. 이 방법을 Jackknife 방법이라고도 한다.\n\n\n6. 분석 사례 데이터\n모집단 그룹 2개: titanic 데이터\n타이타닉호(Titanic) 는 1912년 4월 10일 영국 사우샘프턴(Southampton)을 출발해 뉴욕으로 향하던 초호화 여객선이었습니다.\n1912년 4월 15일 새벽, 북대서양에서 빙산과 충돌해 침몰하면서 약 2,224명 중 1,500명 이상이 사망한 비극적인 사고로 기록되었다.\n당시 타이타닉은 ”절대 침몰하지 않는 배(The unsinkable ship)” 로 알려졌지만, 부족한 구명보트, 계급(객실등급)에 따른 대피 우선순위, 성별·연령에 따른 생존 차이 등 사회적 요인이 생존율에 크게 작용하였다. 이 사건은 단순한 해상 사고를 넘어, 사회 계층, 성별, 나이, 경제력이 생존 확률에 어떤 영향을 주었는지를 보여주는 대표적 사회통계 데이터로 평가되고 있다.\n\n\n\n\n\n\n\n\n변수명\n설명\n변수 유형\n\n\nsurvived\n생존 여부 (0 = 사망, 1 = 생존)\n종속변수(Binary)\n\n\npclass\n객실 등급 (1 = 1등급, 2 = 2등급, 3 = 3등급)\n범주형(서열형)\n\n\nsex\n성별 (male, female)\n범주형(명목형)\n\n\nage\n나이\n연속형\n\n\nsibsp\n함께 탄 형제/배우자 (siblings/spouses aboard)\n이산형\n\n\nparch\n함께 탄 부모/자녀 수 (parents/children aboard)\n이산형\n\n\nfare\n운임 요금 (단위: 파운드)\n연속형\n\n\nembarked\n승선항 (C=쉐르부르, Q=퀸즈타운, S=사우샘프턴)\n범주형\n\n\nname\n승객 이름 (칭호 포함)\n문자형\n\n\nticket\n승선권 번호\n문자형\n\n\ncabin\n객실 번호\n문자형\n\n\nboat\n구명보트 번호\n문자형\n\n\nbody\n시신 인식번호\n연속형\n\n\nhome.dest\n거주지(목적지)\n문자형\n\n\n\n모집단 그룹 3개: Penguins 데이터\nPalmer Penguins 데이터는 Kristen M. Gorman 연구팀이 2007–2009년 사이 남극 Palmer Station 주변 세 섬(Biscoe, Dream, Torgersen)에서 수행한 장기 생태 모니터링 프로그램의 실측 자료를 기반으로 한다. 본 자료는 이후 교육과 연구 활용을 위해 palmerpenguins 패키지 형태로 정리·공개되었다.\n\n\n\n\n\n개체의 형태적 특징을 나타내는 대표적 변수들은 bill_length_mm(부리 길이), bill_depth_mm(부리 깊이), flipper_length_mm(지느러미 길이), body_mass_g(몸무게) 등이 있다. 부리 길이와 깊이는 먹이 습성과 종별 형태적 차이를 반영하며, 지느러미 길이와 몸무게는 개체의 크기나 생태적 적응성을 파악하는 데 유용하다. 또한 sex 변수는 개체의 성별을 나타내며, male 또는 female로 기록된다. 일부 종에서는 암수 간 신체 치수가 일정 부분 차이를 보이므로, 성별은 형태 분석이나 생존 전략 연구에서 중요한 보조 변수 역할을 한다.\n\n\n\n\n\n\n\n\n\n변수명\n유형\n단위\n설명\n\n\nspecies\n범주형\n–\n펭귄의 종(Adélie, Chinstrap, Gentoo)\n\n\nisland\n범주형\n–\n개체가 관찰된 섬(Biscoe, Dream, Torgersen)\n\n\nbill_length_mm\n연속형\nmm\n부리의 길이. 종 간 형태적 차이 분석에 활용됨\n\n\nbill_depth_mm\n연속형\nmm\n부리의 깊이. 먹이 습성 및 종 분류에 영향\n\n\nflipper_length_mm\n연속형\nmm\n지느러미 길이. 종별 크기 및 생태적 차이를 반영\n\n\nbody_mass_g\n연속형\ng\n몸무게. 생리적 상태 및 종 차이를 파악하는 데 사용\n\n\nsex\n범주형\n–\n펭귄의 성별(male / female)\n\n\nyear\n정수형\n년도\n개체가 측정된 연도(2007, 2008, 2009)\n\n\n\n\n\n\nChapter 2. 모집단이 2개 집단인 경우 판별분석\n\n1. 정의\n데이터 벡터, 모집단 분포\n판별 측정변수 벡터 : \\(\\mathbf{x} = \\left( \\begin{array}{r}\nx_{1} \\\\\nx_{2} \\\\\n\\vdots \\\\\nx_{p}\n\\end{array} \\right) \\sim N_{p}(\\mathbf{\\mu},\\mathbf{\\Sigma})\\)\n모집단 1: \\(\\pi_{1} \\sim f_{1}(\\mathbf{x}) \\sim MN(\\mathbf{\\mu}_{1},\\mathbf{\\Sigma}_{1})\\),\n모집단 2: \\(\\pi_{2} \\sim f_{2}(\\mathbf{x}) \\sim MN(\\mathbf{\\mu}_{2},\\mathbf{\\Sigma}_{2})\\)\n각 모집단으로부터 \\((n_{1},n_{2})\\) 크기의 표본 개체를 추출하여 판별 측정변수의 관측값을 조사했다고 하자. 각 표본의 평균벡터를 \\(\\overline{\\mathbf{x}} = ({\\overline{x}}_{1},{\\overline{x}}_{2})\\), 공분산행렬을 \\((S_{1}^{2},S_{2}^{2})\\)이라 하자.\n기호 notation\n사전확률 Prior Probability: \\(P(\\pi_{i}) = \\pi_{i},\\pi_{1} + \\pi_{2} = 1\\), 여기서 \\(\\pi_{i}\\)는 모집단 i에 속할 확률 (사전정보)\n조건부 분류확률 Conditional Probability of Classification: \\(P(X \\in R_{i} \\mid \\pi_{j})\\), 모집단 j에 속한 개체가 판별식에 의해 모집단 i로 분류될 확률, \\(i = j\\) → 정분류이고 \\(i \\neq j\\) → 오분류이다.\n분류비용 ost of Misclassification: \\(c(j \\mid i)\\), 모집단 i에 속한 개체를 모집단 j 로 판별했을 때 발생하는 비용으로 \\(i = j\\)이면 \\(c(j \\mid i) = 0\\) 정분류 → 비용 없음이고, \\(i \\neq j\\) 이면 \\(c(j \\mid i) &gt; 0\\) 오분류 비용이 존재한다.\n전체 오분류확률 Total Probability of Misclassification: \\(P(\\text{error}) = P(\\pi_{1})P(X \\in R_{2} \\mid \\pi_{1}) + P(\\pi_{2})P(X \\in R_{1} \\mid \\pi_{2})\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n기호\n의미\n비고\n\n\n\\[\\pi_{i}\\]\n모집단 i의 사전확률\n\\[\\pi_{1} + \\pi_{2} = 1\\]\n\n\n\\[R_{i}\\]\n모집단 i로 분류되는 영역\n판별경계에 의해 결정\n\n\n\\[P(X \\in R_{i} \\mid \\pi_{j})\\]\n실제 j집단이 i로 분류될 확률\n정분류/오분류 확률\n\n\n\\[c(j \\mid i)\\]\ni를 j로 분류할 때의 비용\ni=j이면 0\n\n\n\\[P(\\text{error})\\]\n전체 오분류 확률\n두 오분류항의 합\n\n\n\n\n\n2. 판별규칙\n오분류 기대비용 ECM 최소화\n분류 기대비용 최소화는 베이즈 접근의 핵심 개념으로, 오분류 확률뿐 아니라 오분류로 인한 손실까지 고려한 ”경제적 판별기준”을 제공한다.\n이로부터 우도비 검정 형태의 판별식이 유도되며, Fisher의 선형판별식은 그 특수한 형태(등비용·등분산의 경우)에 해당한다.\n두 모집단을 각각 \\(\\pi_{1},\\pi_{2}\\)라 하고, 사전확률을 \\(p_{1} = P(\\pi_{1}),p_{2} = P(\\pi_{2})\\)라 하자. 또한, 모집단 1의 개체를 잘못 2로 분류할 때의 비용을 \\(c(2|1)\\), 모집단 2의 개체를 잘못 1로 분류할 때의 비용을 \\(c(1|2)\\) 라고 하자. 이때 전체 기대비용은 다음과 같이 표현된다.\n\\(ECM = c(2|1)P(2|1)p_{1} + c(1|2)P(1|2)p_{2}\\), 여기서\n\\(P(2|1)\\)은 실제 모집단 1에 속한 개체를 2로 잘못 분류할 확률, \\(P(1|2)\\)은 실제 모집단 2에 속한 개체를 1로 잘못 분류할 확률을 의미한다.\n이 기대비용을 최소화하기 위한 최적의 판별규칙은 다음의 베이즈 의사결정 규칙에 의해 결정된다.\n\\(R_{1}:\\frac{f_{1}(x)}{f_{2}(x)} \\geq \\frac{p_{2}}{p_{1}} \\cdot \\frac{c(1|2)}{c(2|1)}\\), \\(R_{2}:\\frac{f_{2}(x)}{f_{1}(x)} \\geq \\frac{p_{1}}{p_{2}} \\cdot \\frac{c(2|1)}{c(1|2)}\\), 여기서 \\(f_{i}(x)\\)은 모집단 i의 확률밀도함수, \\(\\frac{f_{1}(x)}{f_{2}(x)}\\)은 우도비, \\(\\frac{p_{2}}{p_{1}}\\)은 사전확률비, 그리고 \\(\\frac{c(1|2)}{c(2|1)}\\)은 비용비를 의미한다. 따라서,\n새로운 관측치 x에 대하여 \\(\\frac{f_{1}(x)}{f_{2}(x)}\\)가 오른쪽 항보다 크면 모집단 1로,\n작으면 모집단 2로 분류한다.\n우도함수 likelihood ratio (다변량 정규분포)\n모집단이 다변량정규분포(\\(\\mathbf{x} \\mid \\pi_{i} \\sim N_{p}(\\mathbf{\\mu}_{i},\\mathbf{\\Sigma}_{i}),i = 1,2\\))를 따른다면, 새로운 개체 \\(\\mathbf{x}_{0}\\)에 대하여 \\(L(\\mathbf{x}_{0};\\mathbf{\\mu}_{1},\\mathbf{\\Sigma}_{1}) \\geq L(\\mathbf{x}_{0};\\mathbf{\\mu}_{2},\\mathbf{\\Sigma}_{2})\\)이 성립하면 모집단 1로 그렇지 않으면 모집단 2로 판별한다.\n즉, 우도비 판별은 베이즈 의사결정이론의 핵심 원리로, 두 모집단의 확률밀도함수를 비교하여 가장 ”가능성이 높은” 모집단으로 새로운 개체를 분류하는 방법이다.\nFisher 선형 linear 판별\n다변량 정규분포의 두 모집단이 동일한 공분산(\\(\\mathbf{\\Sigma}_{1} = \\mathbf{\\Sigma}_{2} = \\mathbf{\\Sigma}\\))을 갖는다면 다음과 같이 통합분산에 의해 추정된다.\n통합 공분산 : \\(\\widehat{\\mathbf{\\Sigma}} = \\frac{(n_{1} - 1)\\mathbf{S}_{1} + (n_{2} - 1)\\mathbf{S}_{2}}{n_{1} + n_{2} - 2}\\)\n그리고 우도함수식은 다음의 판별규칙으로 간편화 된다.\n\\((\\mathbf{\\mu}_{1} - \\mathbf{\\mu}_{2})'{\\widehat{\\mathbf{\\Sigma}}}^{- 1}\\mathbf{x}_{0}\\frac{1}{2}(\\mathbf{\\mu}_{1} - \\mathbf{\\mu}_{2})'{\\widehat{\\mathbf{\\Sigma}}}^{- 1}(\\mathbf{\\mu}_{1} + \\mathbf{\\mu}_{2}) \\geq c\\), 여기서 \\(c = \\ln\\left\\lbrack \\frac{c(1|2)p_{2}}{c(2|1)p_{1}} \\right\\rbrack\\)는 사전확률(\\(p_{1},p_{2}\\))과 오분류비용(\\(c(1|2),c(2|1)\\))을 반영한 판별경계이다.\n만약 두 모집단의 사전확률이 동일하고 오분류 비용도 동일하다면 \\(p_{1} = p_{2},c(1|2) = c(2|1)\\) 이므로 \\(c = 0\\)이 된다.\n따라서 판별규칙은 다음과 같이 단순화된다.\n\\[(\\mathbf{\\mu}_{1} - \\mathbf{\\mu}_{2})'{\\widehat{\\mathbf{\\Sigma}}}^{- 1}\\left( \\mathbf{x}_{0} - \\frac{1}{2}(\\mathbf{\\mu}_{1} + \\mathbf{\\mu}_{2}) \\right) \\geq 0\\]\n요약하자면 Fisher의 선형판별함수는 두 집단 간 평균의 차이를 공분산행렬로 표준화하여 개체의 위치를 선형결합 형태로 비교하는 판별식이다. 동일 공분산 가정 하에서 판별경계는 선형이 되며, 이는 오분류율을 최소화하는 베이즈 판별규칙의 특수한 경우로 해석된다.\n\n\n\n\n\n두 모집단의 이분산 경우 Fisher는 Quadratic 판별규칙\n앞에서 다룬 Fisher의 선형판별식은 두 모집단의 공분산행렬이 동일하다는 가정하에서만 유효하였다. 그러나 실제 데이터에서는 모집단별로 변동성이 서로 다른 경우가 많으며, 이때는 공분산이 서로 다름을 인정하고 다음과 같이 일반화한다.\n\\(\\mathbf{x} \\mid \\pi_{i} \\sim N_{p}(\\mathbf{\\mu}_{i},\\mathbf{\\Sigma}_{i}),i = 1,2\\). 즉, 각 모집단은 서로 다른 평균벡터와 공분산행렬을 갖는 다변량 정규분포를 따른다고 가정한다.\n각 모집단의 확률밀도함수는 다음과 같다.\n\\[f_{i}(\\mathbf{x}) = \\frac{1}{(2\\pi)^{\\frac{p}{2}}|\\mathbf{\\Sigma}_{i}|^{\\frac{1}{2}}}\\exp\\left\\{ - \\frac{1}{2}(\\mathbf{x} - \\mathbf{\\mu}_{i})'\\mathbf{\\Sigma}_{i}^{- 1}(\\mathbf{x} - \\mathbf{\\mu}_{i}) \\right\\},i = 1,2\\]\n베이즈 의사결정 규칙에 따라 \\(\\pi_{i}f_{i}(\\mathbf{x}_{0})\\)중 더 큰 값을 갖는 집단으로 분류한다. 즉, \\(\\text{모집단 1로 분류:}\\pi_{1}f_{1}(\\mathbf{x}_{0}) \\geq \\pi_{2}f_{2}(\\mathbf{x}_{0})\\)\n우도함수를 로그변환하여 정리하면, 판별함수는 다음과 같이 이차(quadratic) 형태로 표현된다.\n\\[g_{i}(\\mathbf{x}) = - \\frac{1}{2}\\ln|\\mathbf{\\Sigma}_{i}| - \\frac{1}{2}(\\mathbf{x} - \\mathbf{\\mu}_{i})'\\mathbf{\\Sigma}_{i}^{- 1}(\\mathbf{x} - \\mathbf{\\mu}_{i}) + \\ln p_{i}\\]\n\\[\\text{집단 1로 분류:}g_{1}(\\mathbf{x}_{0}) &gt; g_{2}(\\mathbf{x}_{0}),\\text{집단 2로 분류:}g_{2}(\\mathbf{x}_{0}) &gt; g_{1}(\\mathbf{x}_{0})\\]\n이때 두 함수 \\(g_{i}(\\mathbf{x})\\)는 \\(\\mathbf{x}\\)의 제곱항을 포함하므로 결정경계는 이차곡면이 된다.\nDA는 LDA를 일반화한 형태로, 집단 간 공분산이 서로 다를 때 적용하는 판별기법이다. LDA의 선형경계가 하나의 평면이라면, QDA는 공분산 차이에 따라 곡면 형태의 결정경계를 형성한다. 표본수가 충분히 많고 각 집단의 분산 구조가 뚜렷이 다를 때 QDA는 LDA보다 더 정교한 분류 성능을 보인다.\n\n\n\n\n\n\n\n3. Fisher 판별분석\nFisher의 선형판별모형은 각 판별변수가 다변량 정규분포를 따른다는 전제하에 평균과 공분산 구조를 이용하여 판별축을 구하므로, 범주형 변수(예: 성별, 직업, 지역 등)를 직접 포함하는 것은 여러 통계적 한계를 가진다. 범주형 변수는 본질적으로 이산(discrete) 자료이기 때문에 평균과 분산의 개념이 연속형 변수와 다르며, 예를 들어 성별 변수(Sex = {0, 1})처럼 두 값만 존재하는 자료를 정규분포로 근사하는 것은 분포적 가정을 위배한다.\n또한 이러한 0/1 변수는 다른 연속형 변수와의 공분산이 매우 작거나 특정 집단에서 분산이 0이 되어 공분산행렬이 비가역(singular) 형태로 나타나며, 이는 \\(\\mathbf{\\Sigma}^{- 1}\\)계산 단계에서 수치적 불안정성을 초래한다. 더 나아가 LDA는 평균 간 거리와 군집 내 분산의 비율을 통해 최적의 판별축을 찾는 기법인데, 범주형 변수의 경우 ’거리’ 개념이 정의되지 않기 때문에 남성(0)과 여성(1)의 차이를 단순히 1이라는 수치로 간주하면 실제 의미 없는 기호적 차이를 동일한 수량적 거리로 취급하는 왜곡이 발생한다.\n사례 데이터 불러오기\n\n\n\n\n\n\n\n\n\n변수명\n설명\n변수유형\n활용 형태\n\n\nSurvived\n성별 (male/female)\n범주형\n모집단 그룹\n\n\nAge\n나이\n연속형\n판별변수\n\n\nFare\n운임 요금\n연속형\n판별변수\n\n\nSibSp\n함께 탑승한 형제/배우자 수\n연속형(이산형)\n판별변수\n\n\nParch\n함께 탑승한 부모/자녀 수\n연속형(이산형)\n판별변수\n\n\n\n# tatanic data\nimport pandas as pd\nurl = \"https://by-sekwon.github.io/api/titanic.xlsx\"\ndf = pd.read_excel(url)\n# 결측치 제거\ndf[['age','survived','fare']].dropna(inplace=True)\ndf.info()\nData columns (total 14 columns):\n0 survived 1309 non-null int64  1 name 1309 non-null object  2 sex 1309 non-null object  3 age 1046 non-null float64  4 sibsp 1309 non-null int64  5 parch 1309 non-null int64  6 pclass 1309 non-null int64  7 ticket 1309 non-null object  8 fare 1308 non-null float64  9 cabin 295 non-null object  10 embarked 1307 non-null object  11 boat 486 non-null object  12 body 121 non-null float64  13 home.dest 745 non-null object\n\n(1) 사전분석\n모집단 그룹별 판별변수 분포\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# 변수 선택\ncols = ['age', 'sibsp', 'parch', 'fare', 'survived']\ndf_plot = df[cols].dropna()\n\n# 산점도 행렬 (집단별 색상)\nsns.pairplot(\n    data=df_plot,\n    vars=['age', 'sibsp', 'parch', 'fare'],\n    hue='survived',\n    diag_kind='kde',\n    palette='Set1',\n    plot_kws={'alpha': 0.7, 's': 40, 'edgecolor': 'k'}\n)\n\nplt.suptitle(\"Scatter Matrix by Survival\", y=1.02, fontsize=13)\nplt.show()\n사망자(0) 집단의 평균 나이는 약 30.5세, 생존자(1) 집단의 평균 나이는 약 28.9세로, 생존자의 연령이 약간 더 낮게 나타났다. 표준편차는 두 집단 모두 약 14~15로 비슷하여, 연령 분포의 폭에는 큰 차이가 없다. 이는 전반적으로 어린 승객이 생존할 가능성이 약간 높았음을 시사한다.\nfare(운임)는 두 집단 간의 차이가 뚜렷합니다. 사망자의 평균 운임은 약 23.4, 생존자의 평균 운임은 약 49.4로, 생존자의 운임이 두 배 이상 높게 나타났다. 표준편차 또한 생존자 집단에서 훨씬 크기 때문에(68.6 vs 34.1), 생존자 중에는 고운임 승객이 많았고 분산도 컸음을 알 수 있다. 이는 상위 선실을 이용한 승객일수록 생존 가능성이 높았다는 잘 알려진 Titanic의 구조 상황과 일치한다.\nparch(부모·자녀 수)와 sibsp(형제자매·배우자 수)는 두 집단에서 모두 평균이 0.3~0.5 수준으로 비슷하다. 다만, 생존자의 평균이 약간 더 높게 나타나 가족 동반 승객의 생존 확률이 다소 높았을 가능성을 보여준다. 그러나 표준편차가 작고 분포가 편중되어 있어, 이 두 변수의 판별력은 fare나 age에 비해 상대적으로 약할 것으로 예상된다.\n전체적으로 보면, 생존자와 사망자 간에 가장 큰 차이를 보이는 변수는 fare, 다음은 age, 그리고 sibsp와 parch는 차이가 미미하다. 따라서 피셔의 판별분석에서 fare가 판별함수의 주된 기여변수로 작용할 가능성이 높다.\n\n\n\n\n\n모집단 그룹별 판별변수 평균, 표준편차\n# survived별 평균, 표준편차 계산\ndf.pivot_table(\n    index='survived',\n    values=['age', 'sibsp', 'parch', 'fare'],\n    aggfunc=['mean', 'std'],\n    margins=True)\n사망자 집단의 평균 나이는 30.5세로, 생존자 집단의 평균 나이 28.9세보다 약간 높다. 표준편차는 각각 13.9와 15.1로 비슷한 수준이다. 이는 두 집단 간의 연령 분포가 거의 겹치지만, 상대적으로 젊은 승객의 생존 비율이 다소 높았음을 의미한다.\n운임(fare)의 차이는 훨씬 뚜렷하다. 사망자 집단의 평균 운임은 23.4, 생존자 집단은 49.4로 약 두 배 이상 차이가 난다. 표준편차 또한 생존자 집단이 훨씬 크다(68.6 &gt; 34.1). 이는 운임이 높을수록, 즉 상위 객실 승객일수록 생존 가능성이 높았음을 보여준다. Titanic 사고의 사회계층적 구조가 통계적으로 확인되는 대목이다.\n부모·자녀 수(parch)의 평균은 사망자 0.33, 생존자 0.48로 소폭의 차이를 보인다. 가족을 동반한 승객이 다소 높은 생존 확률을 가졌던 것으로 해석되지만, 표준편차가 작고 대부분의 값이 0에 몰려 있어 판별력은 크지 않다. 형제·배우자 수(sibsp) 역시 두 집단 간 차이가 거의 없으며(0.52 vs 0.46), 변수의 변동성 또한 크지 않다.\n요약하면, 네 변수 가운데 생존 여부를 가장 강하게 구분하는 변수는 fare(운임) 이다. age는 약한 수준의 보조 변수로 작용하며, parch와 sibsp는 판별 기여도가 거의 없는 변수로 보인다. 결국 Titanic의 생존 확률은 나이보다는 사회적 지위(운임 수준) 에 따라 뚜렷한 차이를 보였다고 해석된다.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsurvived\nmean\nstd\n\n\nage\nfare\nparch\nsibsp\nage\nfare\nparch\nsibsp\n\n\n0\n30.55\n23.35\n0.33\n0.52\n13.92\n34.15\n0.91\n1.21\n\n\n1\n28.92\n49.36\n0.48\n0.46\n15.06\n68.65\n0.78\n0.69\n\n\nAll\n29.85\n36.69\n0.42\n0.50\n14.39\n55.73\n0.84\n0.91\n\n\n\n\n\n\n(2) 등분산 검정\n귀무가설: 각 집단의 공분산행렬이 동일하다 (등분산)\n대립가설: 적어도 하나의 집단은 공분산행렬이 다르다\nimport numpy as np\nimport pandas as pd\nfrom scipy.stats import chi2\n\n# 분석 변수 선택\nX = df[['age', 'sibsp', 'parch', 'fare']].dropna()\ny = df.loc[X.index, 'survived']\n\n# 각 집단별 데이터 분리\ngroups = [X[y == g] for g in np.unique(y)]\n\n# 표본 수, 변수 수\nn_groups = len(groups)\nn_vars = X.shape[1]\n\n# 각 그룹의 공분산행렬\ncov_mats = [np.cov(g, rowvar=False) for g in groups]\n\n# 전체 풀드(결합) 공분산행렬\nN = [len(g) for g in groups]\npooled_cov = sum([(n - 1) * S for n, S in zip(N, cov_mats)]) / (sum(N) - n_groups)\n\n# Box's M 통계량 계산 함수\ndef box_m_test(cov_mats, N):\n    g = len(cov_mats)\n    p = cov_mats[0].shape[0]\n    pooled = sum([(n - 1) * S for n, S in zip(N, cov_mats)]) / (sum(N) - g)\n    \n    logdet_pooled = np.log(np.linalg.det(pooled))\n    logdet_groups = sum([(n - 1) * np.log(np.linalg.det(S)) for n, S in zip(N, cov_mats)])\n    M = (sum(N) - g) * logdet_pooled - logdet_groups\n    \n    # Correction factor (approximation)\n    C = ((2*p**2 + 3*p - 1) / (6*(p + 1)*(g - 1))) * (sum([1/(n - 1) for n in N]) - 1/(sum(N) - g))\n    M_corrected = (1 - C) * M\n    df = (g - 1) * p * (p + 1) / 2\n    p_value = 1 - chi2.cdf(M_corrected, df)\n    \n    return M_corrected, df, p_value\n\n# 실행\nM, df_bm, pval = box_m_test(cov_mats, N)\nprint(f\"Box’s M = {M:.3f}, df = {df_bm:.1f}, p-value = {pval:.4f}\")\nBox’s M = 389.687, df = 10.0, p-value = 0.0000\n즉, 유의확률이 &lt;0.001이므로 공분산 행렬의 등분산 가정이 성립하지 않으며 QDA가 적합하다.\n\n\n(3) 판별모형 도출 및 혼동행렬 산정\n피셔 LDA\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\nimport pandas as pd\n\n# 분석 데이터\nX = df[['age', 'sibsp', 'parch', 'fare']].dropna()\ny = df.loc[X.index, 'survived']\n\n# LDA 모델 적합\nlda = LDA()\nlda_fit = lda.fit(X, y)\n\n# 예측 및 판별점수\ny_pred = lda.predict(X)\nlda_scores = lda.decision_function(X)\n\n# 판별계수(선형식의 가중치)\ncoef = pd.Series(lda.coef_[0], index=X.columns)\nintercept = lda.intercept_[0]\n\nprint(\"=== LDA 선형판별식 ===\")\nprint(f\"D = {intercept:.4f} + \" + \" + \".join([f\"{c:.4f}*{v}\" for v, c in coef.items()]))\n\nprint(\"\\n=== 변수별 판별계수 ===\")\nprint(coef)\nprint(\"\\n절편:\", intercept)\nprint(\"\\n훈련집합 정확도:\", lda.score(X, y))\n=== LDA 선형판별식 ===  D = -0.2155 + -0.0179*age + -0.2633*sibsp + 0.2086*parch + 0.0107*fare  === 변수별 판별계수 ===  age -0.017923  sibsp -0.263302  parch 0.208623  fare 0.010663  절편: -0.21554471170015432\n훈련집합 정확도: 0.6555023923444976\nfrom sklearn.metrics import confusion_matrix, classification_report\n\n# 혼동행렬 계산\ncm = confusion_matrix(y, y_pred)\nprint(\"=== Confusion Matrix ===\")\nprint(cm)\n\n# 상세 분류 지표\nprint(\"\\n=== Classification Report ===\")\nprint(classification_report(y, y_pred, target_names=[\"Dead (0)\", \"Survived (1)\"]))\n=== Confusion Matrix ===  [[576 42]  [318 109]]  === Classification Report ===  precision recall f1-score support  Dead (0) 0.64 0.93 0.76 618  Survived (1) 0.72 0.26 0.38 427  accuracy 0.66 1045  macro avg 0.68 0.59 0.57 1045  weighted avg 0.68 0.66 0.60 1045\n피셔 QDA\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis as QDA\nfrom sklearn.metrics import confusion_matrix, classification_report\n\n# 분석 데이터\nX = df[['age', 'sibsp', 'parch', 'fare']].dropna()\ny = df.loc[X.index, 'survived']\n\n# QDA 모델 적합\nqda = QDA()\nqda_fit = qda.fit(X, y)\n\n# 예측\ny_pred_qda = qda.predict(X)\n\n# 혼동행렬\ncm_qda = confusion_matrix(y, y_pred_qda)\nprint(\"=== QDA Confusion Matrix ===\")\nprint(cm_qda)\n\n# 분류 정확도 및 지표\nprint(\"\\n=== QDA Classification Report ===\")\nprint(classification_report(y, y_pred_qda, target_names=[\"Dead (0)\", \"Survived (1)\"]))\n\n# 정확도\nprint(\"훈련집합 정확도:\", qda.score(X, y))\n=== QDA Confusion Matrix ===  [[576 42]  [303 124]]  === QDA Classification Report ===  precision recall f1-score support  Dead (0) 0.66 0.93 0.77 618  Survived (1) 0.75 0.29 0.42 427  accuracy 0.67 1045  macro avg 0.70 0.61 0.59 1045  weighted avg 0.69 0.67 0.63 1045  훈련집합 정확도: 0.6698564593301436\n최종 판별모형 선택\n\n\n\n\n\n\n\n\n구분\nLDA\nQDA\n\n\n공분산 가정\n동일(Σ₁=Σ₂)\n서로 다름(Σ₁≠Σ₂)\n\n\nBox’s M p-value\n등분산 가정 위배)\n적용 가능\n\n\n전체 정확도\n65.60%\n67.00%\n\n\n생존자 재현율\n26%\n29%\n\n\n경계 형태\n선형\n이차(비선형)\n\n\n\nTitanic 데이터에서 집단 간 공분산이 다르며, QDA가 LDA보다 약간 더 높은 정확도(0.67 vs 0.66)와 생존자 예측력을 보였다. 따라서 최종 판별모형은 QDA로 선정하는 것이 타당하다.\n\n\n(4) 판별결과 활용(1) 새로운 개체 판별\nage 30세, 동반 배우자 형제 수 1명, 부모 자식 동반자 수 2명, 요금 100$인 승객의 생존율? LDA 방법 61.5%, QDA 방법 68.5%로 생존한다고 판단할 수 있다.\nimport pandas as pd\n\n#새로운 개체 정의\nnew_passenger = pd.DataFrame({\n    \"age\":   [30],\n    \"sibsp\": [1],\n    \"parch\": [2],\n    \"fare\":  [100]})\n\n# 1) LDA로 판별\nlda_class = lda.predict(new_passenger)\nlda_prob  = lda.predict_proba(new_passenger)\n\nprint(\"LDA 예측 클래스:\", lda_class[0])     # 0 또는 1\nprint(\"LDA posterior 확률:\", lda_prob)      # 각 집단에 속할 사후확률\n\n\n# 2) QDA로 판별\nqda_class = qda.predict(new_passenger)\nqda_prob  = qda.predict_proba(new_passenger)\n\nprint(\"QDA 예측 클래스:\", qda_class[0])\nprint(\"QDA posterior 확률:\", qda_prob)\nLDA 예측 클래스: 1  LDA posterior 확률: [[0.38532454 0.61467546]]  QDA 예측 클래스: 1  QDA posterior 확률: [[0.31523609 0.68476391]]\n\n\n(5) 판별결과 활용(2) 주성분 분삭 활용 판별 시각화\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\n\n# 1. PCA 준비\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X_scaled)\n\n# 2. 시각화용 데이터프레임\ndf_vis = pd.DataFrame({\n    \"PC1\": X_pca[:, 0],\n    \"PC2\": X_pca[:, 1],\n    \"true\": y.values,\n    \"pred\": y_pred_qda\n}, index=X.index)\n\n# 3. 영어 버전 그룹 레이블\ndef make_group_label(row):\n    if row[\"true\"] == 0 and row[\"pred\"] == 0:\n        return \"true: died  → predicted: died\"\n    elif row[\"true\"] == 0 and row[\"pred\"] == 1:\n        return \"true: died  → predicted: survived (misclassified)\"\n    elif row[\"true\"] == 1 and row[\"pred\"] == 0:\n        return \"true: survived → predicted: died (misclassified)\"\n    else:\n        return \"true: survived → predicted: survived\"\n\ndf_vis[\"group\"] = df_vis.apply(make_group_label, axis=1)\n\n# 4. 색 / 마커 설정\ncolor_map = {\n    \"true: died  → predicted: died\": \"tab:blue\",\n    \"true: died  → predicted: survived (misclassified)\": \"tab:orange\",\n    \"true: survived → predicted: died (misclassified)\": \"tab:red\",\n    \"true: survived → predicted: survived\": \"tab:green\"\n}\n\nmarker_map = {\n    \"true: died  → predicted: died\": \"o\",\n    \"true: died  → predicted: survived (misclassified)\": \"s\",\n    \"true: survived → predicted: died (misclassified)\": \"X\",\n    \"true: survived → predicted: survived\": \"D\"\n}\n\n# 5. 시각화\nplt.figure(figsize=(10, 7))\n\nfor g in df_vis[\"group\"].unique():\n    sub = df_vis[df_vis[\"group\"] == g]\n    plt.scatter(\n        sub[\"PC1\"], sub[\"PC2\"],\n        label=g,\n        c=color_map[g],\n        marker=marker_map[g],\n        alpha=0.7,\n        edgecolor=\"k\",\n        s=60\n    )\n\n# 6. 축 설명력 표시\npc1_var = pca.explained_variance_ratio_[0] * 100\npc2_var = pca.explained_variance_ratio_[1] * 100\n\nplt.xlabel(f\"PC1 ({pc1_var:.1f}% variance explained)\")\nplt.ylabel(f\"PC2 ({pc2_var:.1f}% variance explained)\")\nplt.title(\"PCA Visualization of QDA Classification Results\")\n\nplt.legend(loc=\"best\", fontsize=9)\nplt.axhline(0, color=\"lightgray\", linewidth=0.8)\nplt.axvline(0, color=\"lightgray\", linewidth=0.8)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n주성분 이름 부여\n# PCA loadings (주성분 부하)\nloadings = pd.DataFrame(\n    pca.components_.T,      # (변수 × PC)\n    index=X.columns,        # ['age','sibsp','parch','fare']\n    columns=['PC1', 'PC2']\n)\n\nprint(\"=== PCA Loadings ===\")\nprint(loadings.round(3))\n=== PCA Loadings ===  PC1 PC2  age -0.349 0.677  sibsp 0.636 -0.080  parch 0.625 0.127  fare 0.288 0.720\nPC1은 가족 구성 관련 변수(sibsp, parch)에 강한 양(+) 부하를 갖고 있으며 요금(fare)도 약하게 양의 방향을 가진다. 반면, 나이(age)는 음(–)의 방향으로 나타난다. 가족 규모가 클수록 PC1 점수는 증가한다. 요금이 높은 승객도 상대적으로 PC1이 증가하는 경향을 보인다. 반대로 나이가 많을수록 PC1 점수는 감소한다.\n따라서 PC1은 ”가족 규모 및 경제적 여건을 반영하는 축(Family-size / Fare axis)” 또는 ”젊고 가족이 많은 승객 vs 나이가 많고 가족 동반이 적은 승객”을 구분하는 축으로 해석할 수 있다.\nPC2는 요금(fare)과 나이(age)에 의해 거의 전적으로 결정되는 축이다. 요금이 높을수록 PC2 점수가 증가한다. 나이가 많을수록 PC2 점수도 증가한다 sibsp, parch는 거의 영향이 없다. 따라서 PC2는 ”요금–나이 축(Fare–Age axis)“, 또는 경제적 지위(요금)와 연령 특성을 반영하는 축으로 명명하는 것이 적절하다.\n\n\n4. 로지스틱 판별분석\n\n(1) 개념\n이분형 분류(binary classification) 문제에서 모집단이 \\(\\text{모집단 1} = \\{\\text{성공}\\}\\), \\(\\text{모집단 2} = \\{\\text{실패}\\}\\)로 정의되어 있을 때, 로지스틱 판별분석은 하나 이상의 설명변수(판별변수)를 이용하여 주어진 관측값이 성공 집단 혹은 실패 집단에 속할 확률을 추정하고, 그 확률을 기준으로 집단을 분류하는 방법이다.\n로지스틱 판별분석은 LDA이나 QDA와는 달리 모집단의 분포(정규성, 공분산 구조 등)에 대한 가정을 필요로 하지 않으며, 종속변수가 범주형일 때 널리 사용되는 일반화선형모형의 대표적 예이다.\n모형의 구조\n설명변수 벡터를 x, 성공일 확률을 \\(p(x) = P(\\text{성공} \\mid x)\\)라고 할 때, 로지스틱 회귀모형은 다음과 같은 로짓 함수에 의해 정의된다.\n\\[\\log\\frac{p(x)}{1 - p(x)} = \\beta_{0} + \\beta_{1}x_{1} + \\cdots + \\beta_{k}x_{k}\\]\n여기서 왼쪽의 로짓값은 성공에 대한 상대적 위험도를 선형식으로 표현한 것이며, 모수 \\(\\beta\\)는 최대우도법을 이용하여 추정된다. 추정된 모형으로부터 성공확률은 \\(\\widehat{p}(x) = \\frac{e^{{\\widehat{\\beta}}_{0} + {\\widehat{\\beta}}_{1}x_{1} + \\cdots + {\\widehat{\\beta}}_{k}x_{k}}}{1 + e^{{\\widehat{\\beta}}_{0} + {\\widehat{\\beta}}_{1}x_{1} + \\cdots + {\\widehat{\\beta}}_{k}x_{k}}}\\)의 형태로 계산된다.\n일반적인 분류 규칙\n로지스틱 판별분석에서 가장 널리 사용되는 기본 분류 규칙은 다음과 같다.\n\\(\\widehat{p}(x) &gt; 0.5\\)이면 성공 집단으로 분류\n\\(\\widehat{p}(x) &lt; 0.5\\)이면 실패 집단으로 분류\n즉, 추정된 사후확률이 0.5보다 크면 성공 집단, 그렇지 않으면 실패 집단으로 판단하는 규칙이다. 이 기준값(0.5)은 두 집단의 사전확률이 동일하고, 성공과 실패의 오분류 비용이 동일하다고 가정할 때 Bayes 규칙에서 자연스럽게 도출된다.\nROC 분석을 통한 최적 기준값 선정\n실무에서는 0.5라는 기준값이 항상 최적인 것은 아니다. 두 집단의 사전확률이 크게 다르거나, 성공을 실패로 잘못 분류하는 비용이 실패를 성공으로 분류하는 비용보다 클 경우에는 더 적절한 기준값을 선택해야 한다. 이를 위해 흔히 사용하는 방법이 ROC(Receiver Operating Characteristic) 분석이다.\n\n\n\n(2) ROC 곡선\n이진 분류모형의 성능을 평가할 때, 모델이 산출한 사후확률을 어떤 기준값(cutoff)으로 나누느냐에 따라 민감도와 특이도가 달라지게 된다. ROC(Receiver Operating Characteristic) 커브는 이러한 기준값의 변화에 따른 분류 성능의 변화를 종합적으로 나타낸 곡선이다.\n가로축은 False Positive Rate(FPR)=(’정답을 얼마나 잘 맞추는가’)=민감도, 세로축은 True Positive Rate(TPR)=(’실패를 성공으로 잘못 분류한 비율’)=(1-특이도)를 나타내며, 기준값을 0에서 1까지 연속적으로 변화시키면서 계산된 점들을 연결하여 ROC 곡선을 얻는다.\n\\(\\text{TPR} = \\frac{\\text{True Positive}}{\\text{True Positive + False Negative}}\\), \\(\\text{FPR} = \\frac{\\text{False Positive}}{\\text{False Positive + True Negative}}\\)\n기준값 변화에 따른 곡선의 의미\n모형은 사후확률을 이용하여 성공/실패를 예측하는데, 이때 기준값을 낮추면 ’성공’으로 분류하는 개체가 늘어나므로 TPR과 FPR이 동시에 증가한다. 반대로 기준값을 높이면 두 지표 모두 감소한다.\nROC 커브는 이러한 trade-off 관계를 시각적으로 보여주어, 특정 기준값에서 모델이 어떤 수준의 민감도와 특이도를 가지는지 파악할 수 있게 해준다.\nROC 커브의 위치와 모델의 성능\nROC 곡선이 왼쪽 위 모서리에 가까울수록 성능이 우수한 모델을 의미한다. 이 영역에 가까운 곡선은 동일한 FPR에서 더 높은 TPR을 달성하기 때문이다.\n다음 그림에서 서로 다른 세 가지 모델(NetChop, TAP+ProteaSMM-i, ProteaSMM-i)의 ROC 곡선이 함께 제시되어 있는데, 각 모델이 기준값 변화에 따라 서로 다른 형태의 trade-off를 보임을 알 수 있다. 곡선이 전체적으로 더 위쪽에 위치한 모델일수록 분류 성능이 상대적으로 우수하다. NetChop 모델은 FPR=0.25가 최적 cut-off이다.\n\n\n\n\n\n대각선의 점선은 아무 정보도 사용하지 않는 완전 무작위 분류(random guess)의 ROC 선이며, 실제 ROC 곡선이 이 점선보다 위에 존재해야 모델로서 의미가 있다.\nAUC(Area Under the Curve)의 해석\nROC 커브 아래 면적(AUC 값)은 모델 전체 성능을 하나의 숫자로 요약한 지표이다. \\(0.5 \\leq \\text{AUC} \\leq 1\\)\nAUC = 0.5 → 무작위 분류기  AUC = 1.0 → 완벽한 분류기\nAUC 값이 높다는 것은 임의로 뽑은 성공 사례가 임의의 실패 사례보다 더 높은 사후확률을 가질 확률이 높다는 의미이며, 모델의 분별력이 좋음을 나타낸다.\n실제 활용: 최적 기준값 선택\nROC 분석의 중요한 목적 중 하나는 성공/실패를 구분하는 최적의 기준값을 정하는 것이다. 일반적으로 다음 기준을 활용한다.\n오분류율을 최소화하는 기준값\n\n민감도+특이도가 최대가 되는 기준값(Youden Index 최대)\n특정 FPR 또는 TPR을 만족시키는 기준값\n\n이처럼 ROC 분석은 로지스틱 판별분석에서 모델의 분류 성능을 평가할 뿐 아니라, 실무에서 적절한 cutoff를 설정하기 위한 핵심 도구로 활용된다.\n\n\n(3) YOUDEN 지표\n이진 분류 모형에서 기준값(cutoff)을 어떻게 설정하느냐에 따라 민감도와 특이도가 달라지게 된다. ROC 분석의 목적 중 하나는 이러한 기준값을 조정하여 모형의 분류 성능이 가장 균형 있게 나타나는 지점을 찾는 데 있다. Youden Index는 이 과정에서 널리 사용되는 지표로, 특정 기준값에서 모델의 전반적 분별력을 하나의 수치로 요약해준다.\n정의\nYouden Index J는 민감도와 특이도의 합이 최대가 되는 지점을 찾기 위해 사용되며, 다음과 같이 정의된다. \\(J = \\text{Sensitivity} + \\text{Specificity} - 1\\), 또는 \\(J = \\text{TPR} - \\text{FPR}\\)의 형태로도 표현된다.\n값의 범위와 의미\n\\(0 \\leq J \\leq 1\\): J = 0 → 모델이 무작위 분류기와 동일함(분별력 없음)\nJ = 1 → 완벽한 분류기(민감도=1, 특이도=1) 값이 클수록 특정 기준값에서 모델이 성공과 실패를 더 잘 구분하고 있음을 의미한다.\nYouden Index는 민감도와 특이도 사이의 균형을 평가하므로, 어느 한쪽만 높아지는 왜곡된 cutoff 대신 양쪽 성능을 모두 고려한 최적의 기준값을 제공한다.\n최적 기준값 찾기\nROC 분석에서 각 cutoff c에 대해 J(c) 값을 계산한 뒤, 다음 기준을 만족하는 cutoff를 선택한다. \\(c^{*} = \\arg\\max_{c}J(c)\\) 즉, Youden Index를 최대화하는 기준값을 ’최적 기준값’으로 선택한다.\n이 기준값은 오분류율 최소화, 민감도와 특이도 균형 확보, 의료·공학·사회과학 등 다양한 분야에서 실무적으로 적용 가능 하다는 점에서 널리 사용된다.\n해석적 장점\n민감도와 특이도를 동시에 고려 → 한쪽이 높고 다른 한쪽이 낮은 cutoff는 적절하지 않음을 수치적으로 판단할 수 있다. 간단한 계산으로 모델의 가장 균형 잡힌 cutoff 제시 → 0.5라는 임의 기준에 의존하지 않는다.\n모델 간 비교 가능\n특정 cutoff에서 Youden Index가 더 큰 모델이 더 우수한 분류력을 가진다.\nAUC와 보완적 관계\nAUC는 전체적인 성능을 평가하지만, Youden Index는 특정 cutoff에서의 최적 성능을 찾는 데 적합하다.\n실제 활용의 예\n로지스틱 판별분석이나 의료 진단 모형에서 사후확률을 얻은 후, 민감도·특이도 표를 작성하고 cutoff별 Youden Index를 계산한 뒤 J가 최대가 되는 cutoff를 선택하여 ”성공/실패”, ”질병/정상”, ”위험군/비위험군” 등을 보다 합리적으로 분류할 수 있다.\n\n(4) 피셔 판별방법과 비교\nFisher의 선형판별분석은 두 집단 분류를 위하여 집단 간 분산 대비 집단 내 분산을 최대화하는 방향으로 선형판별축을 찾는 방법으로, 통계학적으로 매우 우수한 고전적 판별기법이다. 그러나 Fisher 방법은 판별점수를 기준으로 두 집단을 구분할 뿐이며, 추정된 판별함수의 값 자체가 확률적 의미를 가지지 않는다는 한계를 갖는다.\n이에 비해 로지스틱 판별분석은 일반화선형모형(GLM)의 구조를 기반으로 하여 성공에 대한 사후확률을 직접적으로 추정할 수 있다는 점에서 여러 가지 해석적·실무적 장점을 제공한다. 다음에서는 Fisher 방법에 비해 로지스틱 판별분석이 갖는 주요 장점은 다음과 같다.\n회귀계수의 부호와 크기 활용\n로지스틱 판별분석에서 각 설명변수의 회귀계수 \\(\\beta_{j}\\)는 \\(\\log\\frac{p}{1 - p}\\)(성공에 대한 log-odds)의 변화에 미치는 직접적인 효과를 나타낸다. 따라서 계수의 부호는 해당 변수가 성공 확률을 증가시키는지 감소시키는지를 명확하게 파악할 수 있으며, 계수의 크기는 log-odds 변화의 정도를 정량적으로 평가할 수 있게 해준다.\n이는 Fisher 방법에서 제공되지 않는 해석적 장점으로, 변수의 영향력 해석이 중요한 응용 분야(사회과학, 의학, 경영학 등)에서 특히 유용하다.\n성공 집단에 속할 사후확률(0~1 사이 연속값)을 추정할 수 있음\n로지스틱 판별분석은 각 개체에 대하여 성공 집단에 속할 확률을 \\(\\widehat{p}(x) = P(\\text{성공} \\mid x)\\) 형태로 0~1 사이의 연속적 확률값으로 산출한다. 이 사후확률은 단순히 집단을 배정하는 용도뿐 아니라, 개체 간 성공 가능성의 상대적 크기 비교, 특정 집단 내에서 위험도나 우선순위 매김(ranking), 의사결정 과정에서 확률 기반 기준 설정, 불확실성(uncertainty) 수준 평가 등 다양한 분석 작업에 활용될 수 있다.\n반면, Fisher 방법은 판별점수의 크기가 집단 간 차이를 반영하긴 하지만, 그 값이 ”성공확률”이라는 명확한 해석을 제공하지는 못한다.\nROC 분석을 통한 최적 기준값(cutoff) 설정이 가능함\n기본적인 분류 규칙인 \\(\\widehat{p}(x) &gt; 0.5\\)는 두 집단의 사전확률과 오분류 비용이 동일할 때 가장 합리적이다. 그러나 실제 분석에서는 이 조건이 충족되지 않는 경우가 많다. 로지스틱 판별분석은 사후확률을 추정하기 때문에 ROC 분석을 통해 다양한 기준값에 대해 민감도, 특이도, 오분류률, Youden 지표 등을 계량적으로 평가할 수 있다. 이를 통해 오분류를 최소화하거나, 또는 문제의 목적에 가장 부합하는 기준값을 선택할 수 있다.\nFisher 방법은 판별함수 점수를 기준으로 cut-off를 정할 수 있으나, 그 점수의 해석적 기반이 확률이 아니기 때문에 ROC 기반 기준 선정이 로지스틱 분석만큼 자연스럽지 않다.\n분포 가정이 완화되어 있어 적용 범위가 넓음\nFisher LDA는 두 집단이 공분산 행렬을 공유하는 다변량 정규분포라는 가정을 전제로 하지만, 로지스틱 판별분석은 이러한 분포적 가정을 필요로 하지 않는다. 이는 다음과 같은 장점으로 이어진다. 설명변수가 정규분포를 따르지 않아도 활용 가능, 이상치(outlier)가 있어도 비교적 안정적, 범주형 변수 또는 연속형 변수가 혼합된 경우에도 자연스럽게 적용할 수 있다. 따라서 실제 비정규적 데이터가 많은 사회·행정·의료·경영 분야에서는 로지스틱 판별분석이 더 적합한 경우가 많다.\n해석 및 의사결정에서의 활용도가 높음\n로지스틱 판별분석의 출력은 단순히 ”성공/실패”의 이분 결정보다 더 많은 정보를 제공한다. 예를 들어, 성공 확률을 이용한 위험군 분류(risk stratification), 확률 기반 의사결정 지원 시스템(DSS) 구축, 마케팅·금융 분야에서의 고객군 세분화와 스코어링 모델, 의료 분야에서의 중증도 예측 및 진단 모델링 등 다양한 실무적 의사결정으로 확장 가능하다.\n이에 반해 Fisher 방법은 상대적으로 단순한 판별값 제공에 그치기 때문에, 확률적 의사결정에 직접적으로 활용하기 어렵다.\n모형 확장성이 우수함\n로지스틱 판별분석은 GLM 계열이므로, 다항 로지스틱(multi-class) 확장, 정규화 로지스틱(L1/L2, elastic net), 교호작용 및 비선형항 추가, 랜덤효과를 포함한 계층적 로지스틱(HGLM) 등 다양한 확장 모델과 자연스럽게 연결된다.\n반면, Fisher 방법은 판별축 1개를 찾는 고전적 형태로 구조적 확장이 제한적이다.\n\n\n(5) 타이타닉 예제\n# tatanic data\nimport pandas as pd\nurl = \"https://by-sekwon.github.io/api/titanic.xlsx\"\ndf = pd.read_excel(url)\n\n# 결측치 제거\ndf.dropna(subset=['age','survived','fare'], inplace=True)\n\n# 로지스틱 판별분석 (Logistic Regression)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import (\n    confusion_matrix,\n    classification_report,\n    roc_curve,\n    roc_auc_score\n)\n\n# 1. 분석 데이터 준비 (LDA, QDA와 동일한 변수 사용)\nX = df[['age', 'sibsp', 'parch', 'fare']].dropna()\ny = df.loc[X.index, 'survived']\n\n# 2. 로지스틱 회귀 적합 (sklearn)\nlogit = LogisticRegression(max_iter=1000)\nlogit.fit(X, y)\n\n# 3. 예측값(클래스) 및 사후확률(생존 확률: P(Y=1|X))\ny_pred  = logit.predict(X)\ny_proba = logit.predict_proba(X)[:, 1]   # 클래스 1(생존)일 확률\naccuracy = logit.score(X, y)\n\nprint(\"=== Logistic Discriminant Analysis (Logistic Regression, sklearn) ===\")\nprint(f\"훈련집합 정확도: {accuracy:.4f}\\n\")\n\n# 4. 혼동행렬 및 분류지표(기준값 0.5 사용)\ncm = confusion_matrix(y, y_pred)\nprint(\"\\n=== Confusion Matrix (cutoff = 0.5) ===\")\nprint(cm)\n\nprint(\"\\n=== Classification Report (cutoff = 0.5) ===\")\nprint(classification_report(y, y_pred,\n                            target_names=[\"Dead (0)\", \"Survived (1)\"]))\n\n# 5. ROC 곡선 및 AUC\nfpr, tpr, thresholds = roc_curve(y, y_proba)\nauc = roc_auc_score(y, y_proba)\nprint(f\"\\nAUC (ROC 곡선 아래 면적) = {auc:.4f}\")\n\nplt.figure(figsize=(6, 6))\nplt.plot(fpr, tpr, label=f\"ROC curve (AUC = {auc:.3f})\")\nplt.plot([0, 1], [0, 1], linestyle=\"--\", label=\"No-discrimination line\")\nplt.xlabel(\"1 - Specificity (FPR)\")\nplt.ylabel(\"Sensitivity (TPR)\")\nplt.title(\"ROC Curve - Logistic Discriminant Analysis\")\nplt.legend()\nplt.grid(True, linestyle=\":\")\nplt.tight_layout()\nplt.show()\n\n# 6. Youden index를 이용한 최적 기준값 예시 (강의용)\nJ = tpr - fpr\nidx_best = np.argmax(J)\nbest_threshold = thresholds[idx_best]\nprint(f\"\\nYouden index를 최대화하는 기준값 = {best_threshold:.4f}\")\nprint(f\"해당점에서 (TPR, FPR) = ({tpr[idx_best]:.3f}, {fpr[idx_best]:.3f})\")\n\n# 7. statsmodels를 이용한 회귀계수 p-값 및 오즈비(odds ratio) 추정\nimport statsmodels.api as sm\nimport numpy as np\n\n# 상수항 추가\nX_sm = sm.add_constant(X)   # 열: const, age, sibsp, parch, fare\n\nlogit_sm = sm.Logit(y, X_sm)\nresult = logit_sm.fit()\n\nprint(\"\\n=== statsmodels Logit 결과 요약 (회귀계수, 표준오차, z, p-값 등) ===\")\nprint(result.summary())\n\n# 오즈비 및 95% 신뢰구간 + p-값 정리\nparams = result.params           # 회귀계수 β\nconf   = result.conf_int()       # β 기준 95% CI\n\nor_table = pd.DataFrame({\n    \"OR\":   np.exp(params),      # 오즈비 e^β\n    \"2.5%\": np.exp(conf[0]),     # 오즈비 95% CI 하한\n    \"97.5%\":np.exp(conf[1]),     # 오즈비 95% CI 상한\n    \"p-value\": result.pvalues    # 계수 검정 p-값\n})\n\nprint(\"\\n=== Odds Ratio (오즈비) 및 95% CI, p-값 ===\")\nprint(or_table.round(4))\n로지스틱 판별모형은 Dead 집단을 비교적 정확하게 분류하지만, Survived 집단에 대해서는 재현율이 0.31로 매우 낮아 생존자를 충분히 식별하지 못하는 한계를 보인다. 이는 cutoff=0.5가 두 집단 간 민감도–특이도의 균형을 맞추기에는 적절하지 않음을 시사하며, ROC 분석을 활용한 cutoff 조정이나 클래스 불균형 보정이 필요함을 의미한다.\n=== Logistic Discriminant Analysis (Logistic Regression, sklearn) ===  훈련집합 정확도: 0.6708\n=== Confusion Matrix (cutoff = 0.5) ===  [[568 50]  [294 133]]\n=== Classification Report (cutoff = 0.5) ===  precision recall f1-score support  Dead (0) 0.66 0.92 0.77 618  Survived (1) 0.73 0.31 0.44 427  accuracy 0.67 1045  macro avg 0.69 0.62 0.60 1045  weighted avg 0.69 0.67 0.63 1045\nROC 분석 결과, Youden Index를 최대화하는 기준값은 0.3966으로 나타났다. 이 기준값에서 민감도(TPR)는 0.541, FPR은 0.209이며, 이는 기존 기준값인 0.5보다 생존 집단(Survived)을 보다 효과적으로 식별하면서도, Dead 집단의 오분류 증가를 최소화하는 지점이다. 따라서 cutoff=0.3966은 민감도와 특이도의 균형을 고려한 최적의 분류 기준으로 해석할 수 있다.\nAUC (ROC 곡선 아래 면적) = 0.6961\n\n\n\n\n\nYouden index를 최대화하는 기준값 = 0.3966\n해당점에서 (TPR, FPR) = (0.541, 0.209)\n\n\n\n\n\nTitanic 데이터에 대한 로지스틱 회귀 결과, 나이와 형제·배우자 수는 생존 확률을 낮추는 반면, 운임요금(fare)과 부모·자녀 동반 수는 생존 확률을 증가시키는 요인으로 나타났다. 특히 fare와 age는 강한 통계적 유의성을 보이며, 사회경제적 지위와 생물학적 취약성이 생존 여부에 중요한 역할을 했음을 확인할 수 있다.\nage (나이) coef = -0.0203 (p &lt; 0.001)\n나이가 1살 증가할 때 생존할 log-odds가 감소한다. OR = 0.9799, 즉 나이가 한 살 많아질수록 생존 확률은 약 2% 감소. → 나이가 많을수록 생존 가능성이 감소하는 경향이 뚜렷함.\nsibsp (함께 탑승한 형제/배우자 수) coef = -0.2955 (p &lt; 0.001)\n동반한 가족 수가 많을수록 생존에 불리한 영향을 준다. OR = 0.7442, 즉 sibsp가 1명 늘어날 때 생존할 오즈는 약 26% 감소. → 많은 가족과 함께 탑승한 승객은 생존 가능성이 감소하는 패턴을 보임.\nparch (부모/자녀 동반 수): coef = 0.1786 (p ≈ 0.038)\n부모·자녀 수가 많은 경우 생존할 log-odds가 증가하는 방향. OR = 1.1955, 즉 동반한 부모·자녀가 1명 늘어날 때 생존 확률이 약 20% 증가. → 가까운 직계 가족과 함께 탑승한 경우 생존 가능성이 약간 높아지는 경향.\nfare (지불한 운임요금): coef = 0.0143 (p &lt; 0.001)\n요금이 높을수록 생존 확률이 증가. OR = 1.0144, 즉 요금이 1단위 증가할 때 생존 오즈가 약 1.4% 증가. → 높은 요금은 일반적으로 상위 객실 등급을 의미하므로, 구조 상황에서 우위가 있었던 것으로 해석할 수 있다.\n# 최적 cutoff 적용\ncutoff = 0.3966   # 또는 cutoff = 0.39\n\ny_pred_039 = (y_proba &gt;= cutoff).astype(int)\n\n# 혼동행렬\ncm_039 = confusion_matrix(y, y_pred_039)\nprint(\"=== Confusion Matrix (cutoff = 0.39) ===\")\nprint(cm_039)\n\n# 정확도\nacc_039 = (y_pred_039 == y).mean()\nprint(f\"\\n정확도 (cutoff = 0.39): {acc_039:.4f}\")\n\n# 민감도(TPR), 특이도(TNR)\nTN, FP, FN, TP = cm_039.ravel()\nTPR = TP / (TP + FN)\nTNR = TN / (TN + FP)\n\nprint(f\"민감도 (TPR): {TPR:.4f}\")\nprint(f\"특이도 (TNR): {TNR:.4f}\")\n전체 정확도는 68.8%로 cutoff=0.5일 때의 정확도 67.1%보다 약간 상승하였다. 이는 cutoff 조정을 통해 두 집단 간 균형이 개선되었음을 의미한다.\n민감도는 53.9%로, cutoff=0.5일 때의 민감도 31%보다 크게 향상되었다. → 생존자를 생존자로 올바르게 판별하는 능력이 크게 증가한 것이다. FN(생존→사망 오분류)이 크게 감소했음을 알 수 있다. 특이도는 79.1%, 즉 실제 Dead 중 약 79%를 정확히 Dead로 분류했다.\ncutoff를 낮추면 일반적으로 특이도가 감소하지만, 여기서는 Dead 집단 분류 능력이 크게 손상되지 않으면서 Survived 분류 성능만 증가한 모습이다.\n=== Confusion Matrix (cutoff = 0.39) ===  [[489 129]  [197 230]]  정확도 (cutoff = 0.39): 0.6880  민감도 (TPR): 0.5386  특이도 (TNR): 0.7913\n설명변수 성별 추가 하였을 경우\n# --- 1) 성별 변수 생성 ---\ndf['female'] = (df['sex'] == 'female').astype(int)\n\n# --- 2) 분석변수 선택 ---\nX = df[['age', 'sibsp', 'parch', 'fare', 'female']].dropna()\ny = df.loc[X.index, 'survived']\n\n# =====================\n# sklearn Logistic\n# =====================\nlogit = LogisticRegression(max_iter=2000)\nlogit.fit(X, y)\n\nprint(\"훈련 정확도:\", logit.score(X, y))\n\n# =====================\n# statsmodels Logistic (p-value, OR 포함)\n# =====================\nX_sm = sm.add_constant(X)\nlogit_sm = sm.Logit(y, X_sm)\nresult = logit_sm.fit()\n\nprint(result.summary())\n\n# 오즈비 계산\nparams = result.params\nconf = result.conf_int()\n\nor_table = pd.DataFrame({\n    \"OR\": np.exp(params),\n    \"2.5%\": np.exp(conf[0]),\n    \"97.5%\": np.exp(conf[1]),\n    \"p-value\": result.pvalues\n})\n\nprint(\"\\n=== Odds Ratio (오즈비) + 95% CI + p-value ===\")\nprint(or_table.round(4))\n성별(female) 변수를 추가한 로지스틱 판별모형에서 훈련집합 정확도는 약 77.4%로 나타났다. 이는 모형이 전체 관측치의 약 4분의 3을 올바르게 분류했음을 의미하며, 성별을 포함하지 않았을 때의 정확도(약 67%)에 비해 유의미하게 향상된 성능을 보여준다.\n훈련 정확도의 증가폭은 성별 변수가 Titanic 생존 여부를 설명하는 강력한 예측 요인이라는 점을 다시 한 번 확인해 준다. 실제로, 여성 승객의 생존 오즈가 남성 대비 약 11.7배에 달하는 것으로 나타났기 때문에, 성별의 추가는 생존·사망을 구분하는 결정경계의 품질을 크게 향상시킨다.\n또한 정확도 77% 수준은 Titanic 데이터의 특성을 고려하면 비교적 높은 수치로, 생존자의 특성(나이, 동반 가족 수, 요금, 성별)을 반영한 분류 규칙이 데이터의 패턴을 안정적으로 포착하고 있음을 의미한다.\n성별(female)의 효과: coef = +2.4583 (p &lt; 0.001)\nOR = 11.6851 (95% CI: 8.4833 ~ 16.0954) 여성의 생존할 odds는 남성 대비 약 11.7배 높다. 이는 Titanic 생존 자료에서 가장 잘 알려진 특징인 ”여성 우선(women and children first)” 규칙을 반영한 강력한 효과이다. female은 생존 여부를 결정하는 가장 강력한 단일 설명변수이다.\n훈련 정확도: 0.7741626794258373\n\n\n\n\n\n# 새로운 개체 정의\nnew_passenger = pd.DataFrame({\n    \"age\": [30],\n    \"sibsp\": [1],\n    \"parch\": [2],\n    \"fare\": [100],\n    \"female\": [1]   # 여성=1, 남성=0\n})\n\n# 사후확률 계산\nproba_sklearn = logit.predict_proba(new_passenger)[:, 1]\nprint(\"사후확률(생존=1):\", proba_sklearn[0])\n사후확률(생존=1): 0.8283876734447347\n\n\n\n\nChapter 3. 모집단이 3개 이상인 경우 판별분석\n다집단 분류 문제에서 관측값의 개체를 어떤 모집단에 분류할 것인지는 분류 규칙에 의해 결정된다. 전통적 통계학에서 가장 기본이 되는 규칙은 Bayes 규칙이며, 이를 다른 관점으로 표현한 MAP 규칙, 그리고 실제 계산을 위해 판별함수 형태로 제시한 ECM 규칙이 순차적으로 연결되어 있다. LDA와 QDA는 이러한 Bayes 규칙을 특정 확률모형(정규분포)에 적용하여 얻어진 판별 방법이다.\n\n1. 판별규칙\n\n\n(1) Bayes 규칙(Bayes Classification Rule)\nBayes 규칙은 분류 문제에서 오분류 확률을 최소화하는 최적의 분류 규칙이다. 모집단이 G개 존재하고, 각 모집단의 사전확률을 \\(\\pi_{g}\\), 조건부밀도함수를 \\(f_{g}(x)\\)라 할 때, Bayes 규칙은 \\(\\widehat{g}(x) = \\arg\\max_{g}\\pi_{g}f_{g}(x)\\) 으로 정의된다.\n즉, 주어진 관측값 \\(x\\)가 각 모집단에서 나왔을 가능성을 평가하여, 그 값이 가장 큰 모집단을 선택한다. Bayes 규칙은 모든 통계적 분류기의 이론적 기준이 되며, 이후 나오는 MAP 규칙, ECM 규칙, LDA·QDA 모두 이 원리에서 출발한다.\n\n\n(2) MAP 규칙(Maximum A Posteriori Rule)\nMAP 규칙은 Bayes 규칙을 사후확률(posterior probability) 관점에서 표현한 것이다. Bayes 정리에 의해 \\(P(G = g \\mid x) = \\frac{\\pi_{g}f_{g}(x)}{\\sum_{h = 1}^{G}\\pi_{h}f_{h}(x)}\\)가 정의되므로, MAP 규칙은 \\(\\widehat{g}(x) = \\arg\\max_{g}P(G = g \\mid x)\\)이 된다.\n분모는 모든 집단에 대해 동일하므로 MAP 규칙은 Bayes 규칙과 완전히 동일한 선택을 한다. 다만, 사전확률과 조건부몰도 기반의 Bayes 규칙을 사후확률이 가장 큰 집단이라는 직관적인 형태로 다시 표현한 것이 MAP 규칙이다.\n\n\n(3) ECM 규칙(Extended Classification Maximum Rule)\nECM 규칙은 Bayes/MAP 규칙을 실제 계산이 가능한 판별함수 형태로 바꾼 것이다. 각 집단별로 로그-우도 기반의 판별함수를 \\(\\delta_{g}(x) = \\ln\\left( \\pi_{g}f_{g}(x) \\right)\\)와 같이 정의하면, 분류 기준은 매우 간단히 \\(\\widehat{g}(x) = \\arg\\max_{g}\\delta_{g}(x)\\) 이 된다. 이 규칙은 LDA와 QDA를 포함한 대부분의 판별분석에서 공통적으로 사용되는 실질적 분류 규칙이며, 다집단(3개 이상) 상황에서도 가장 자연스럽게 적용할 수 있는 분류 기준이다.\n\n\n(4) LDA와 QDA: Bayes 규칙을 정규모형에 적용한 판별방법\nLDA와 QDA는 Bayes 규칙에 특정한 확률모형을 적용하여 도출된 판별 방법이다.\nLDA: 정규분포 + 공통 공분산 가정\n모집단별 자료가 다변량 정규분포를 따르고, 모든 모집단이 같은 공분산행렬를 공유한다고 가정하면, \\(X \\mid G = g \\sim N(\\mu_{g},\\Sigma)\\) Bayes 규칙을 적용하여 만드는 판별함수는 선형식이 된다.\n\\(\\delta_{g}(x) = x^{\\top}\\Sigma^{- 1}\\mu_{g} - \\frac{1}{2}\\mu_{g}^{\\top}\\Sigma^{- 1}\\mu_{g} + \\ln\\pi_{g}\\). 그리고 LDA의 분류 기준은 ECM 규칙과 동일하다. \\(\\widehat{g}(x) = \\arg\\max_{g}\\delta_{g}(x)\\) 이로부터 LDA의 결정경계는 선형이 된다.\nQDA: 정규분포 + 모집단별 공분산행렬 허용\n만약 모집단별 공분산행렬이 서로 다를 수 있다면, \\(X \\mid G = g \\sim N(\\mu_{g},\\Sigma_{g})\\) Bayes 규칙에서 도출되는 판별함수는 이차식이 되고, \\(\\delta_{g}(x) = - \\frac{1}{2}\\ln|\\Sigma_{g}| - \\frac{1}{2}(x - \\mu_{g})^{\\top}\\Sigma_{g}^{- 1}(x - \\mu_{g}) + \\ln\\pi_{g}\\)\nQDA의 분류도 ECM 규칙을 그대로 따른다. \\(\\widehat{g}(x) = \\arg\\max_{g}\\delta_{g}(x)\\). 따라서 QDA의 결정경계는 곡선을 형성한다.\n\n\n2. 다항 로지스틱 판별모형\n이항 로지스틱 회귀는 종속변수가 두 범주(예: 성공/실패)일 때 적용되는 모형이지만, 모집단이 3개 이상인 경우에는 다항 로지스틱 회귀를 사용하며, 하나의 기준집단을 설정한 뒤 나머지 K-1개의 집단에 대해 log-odds 방정식을 구성한다. 이때 절편의 수는 집단 수보다 하나 적으며(절편 = K-1), 각 집단별 회귀계수는 기준집단과의 상대적 비교를 의미한다. 또한 종속변수가 순서형인 경우(예: 리커트 척도, 학점 등)에는 다항 로지스틱보다 순서형 로지스틱 모형이 해석과 추론에 더 적합하다.\n모집단 기호와 모형 구조\n모집단이 \\(G = \\{ 1,2,\\ldots,K\\}\\)의 형태로 K개 존재한다고 하자. 다항 로지스틱 모형은 이 중 하나의 집단을 기준(reference) 집단으로 정하여, 나머지 K-1개의 집단에 대해 기준 집단에 대한 log-odds를 모델링한다. 즉, 기준집단을 g=K로 선택하면 다음과 같은 형태가 된다.\n\\[\\log\\frac{P(G = g \\mid x)}{P(G = K \\mid x)} = \\beta_{0g} + \\beta_{1g}x_{1} + \\cdots + \\beta_{pg}x_{p},g = 1,2,\\ldots,K - 1\\]\n집단이 \\(G = \\{ 1,2,3\\}\\)의 3개라고 하면, 집단 3을 기준집단으로 두었을 때 다항 로지스틱 모형은 다음 두 식으로 구성된다.\n\\[\\log\\frac{P(G = 1 \\mid x)}{P(G = 3 \\mid x)} = \\beta_{01} + \\beta_{11}x_{1} + \\cdots + \\beta_{p1}x_{p}\\]\n\\[\\log\\frac{P(G = 2 \\mid x)}{P(G = 3 \\mid x)} = \\beta_{02} + \\beta_{12}x_{1} + \\cdots + \\beta_{p2}x_{p}\\]\n이 두 개의 logit 식을 통해 각 집단의 사후확률을 다음과 같이 계산한다.\n\\[P(G = 1 \\mid x) = \\frac{\\exp(\\eta_{1})}{1 + \\exp(\\eta_{1}) + \\exp(\\eta_{2})}\\]\n\\[P(G = 2 \\mid x) = \\frac{\\exp(\\eta_{2})}{1 + \\exp(\\eta_{1}) + \\exp(\\eta_{2})}\\]\n\\[P(G = 3 \\mid x) = \\frac{1}{1 + \\exp(\\eta_{1}) + \\exp(\\eta_{2})}\\]\n여기서 \\(\\eta_{1} = \\beta_{01} + \\beta_{11}x_{1} + \\cdots + \\beta_{p1}x_{p}\\), \\(\\eta_{2} = \\beta_{02} + \\beta_{12}x_{1} + \\cdots + \\beta_{p2}x_{p}\\).\n각 집단에 대한 사후확률이 \\(P(G = k \\mid x),k = 1,2,3\\)로 계산되었을 때, 다항 로지스틱 모형이 사용하는 분류 규칙은 다음과 같다.\n\\(\\widehat{g}(x) = \\arg\\max_{g \\in \\{ 1,2,3\\}}P(G = g \\mid x)\\). 즉, 사후확률이 가장 큰 집단이 그 개체의 예측 집단이 된다.\n절편(intercept)이 K–1개 존재하는 이유\n이항 로지스틱에서는 절편이 하나이지만, 다항 로지스틱에서는 각 집단 g마다 독립된 방정식을 갖기 때문에 절편도 K−1개 필요하다. 따라서 문장 ”절편이 하나가 아니라 (집단 수 – 1)로 모집단 개수보다 1개 적음”은 다음과 같이 표현할 수 있다.\n”다항 로지스틱 회귀에서는 기준집단을 제외한 K-1개의 집단마다 하나씩 절편이 존재하므로, 절편의 수는 모집단 개수보다 하나 적다.”\n집단변수의 특성과 해석\n명목형(Nominal): 순서가 없는 범주: Adélie / Chinstrap / Gentoo, 혈액형 A/B/O/AB → 다항 로지스틱(multinomial logistic) 사용\n순서형(Ordinal): 자연스러운 순서가 있는 범주, 5점 리커트 척도, A/B/C/D/F 학점, 고객 만족도 등급(상–중–하) → 순서형 로지스틱 모형(ordinal logistic regression)을 사용하면 해석이 훨씬 쉬워진다.\n예를 들어, 점수나 등급이 증가할수록 어떤 경향이 커지는지를 자연스럽게 해석할 수 있고, logit이 누적비(cumulative odds)로 정의되기 때문에 해석이 직관적이다.\n순서형 종속변수는 K개의 범주를 가진 경우, 이를 (K–1)개의 누적 확률 비교, 즉 \\(Y \\leq kvs.Y &gt; k\\)의 이항 로지스틱 방정식으로 분해할 수 있다. 따라서 순서형 로지스틱 회귀는 구조적으로 (K–1)개의 이항 로지스틱을 동시에 적합한 모형이며, 절편만 범주마다 달라지고 회귀계수는 동일하게 유지된다. 이는 순서 정보를 가장 효율적으로 활용하는 방법이다.\n\n\n3. 사례분석\n\n(1) 피셔 판별분석\n등분산 검정\nimport numpy as np\nimport pandas as pd\nfrom scipy.stats import chi2\n\n# 분석 변수 선택\nX = penguins[['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm', 'body_mass_g']].dropna()\ny = penguins.loc[X.index, 'species']\n\n# 각 집단별 데이터 분리\ngroups = [X[y == g] for g in np.unique(y)]\n\n# 표본 수, 변수 수\nn_groups = len(groups)\nn_vars = X.shape[1]\n\n# 각 그룹의 공분산행렬\ncov_mats = [np.cov(g, rowvar=False) for g in groups]\n\n# 전체 풀드(결합) 공분산행렬\nN = [len(g) for g in groups]\npooled_cov = sum([(n - 1) * S for n, S in zip(N, cov_mats)]) / (sum(N) - n_groups)\n\n# Box's M 통계량 계산 함수\ndef box_m_test(cov_mats, N):\n    g = len(cov_mats)\n    p = cov_mats[0].shape[0]\n    pooled = sum([(n - 1) * S for n, S in zip(N, cov_mats)]) / (sum(N) - g)\n\n    logdet_pooled = np.log(np.linalg.det(pooled))\n    logdet_groups = sum([(n - 1) * np.log(np.linalg.det(S)) for n, S in zip(N, cov_mats)])\n    M = (sum(N) - g) * logdet_pooled - logdet_groups\n\n    # Correction factor (approximation)\n    C = ((2*p**2 + 3*p - 1) / (6*(p + 1)*(g - 1))) * (sum([1/(n - 1) for n in N]) - 1/(sum(N) - g))\n    M_corrected = (1 - C) * M\n    df = (g - 1) * p * (p + 1) / 2\n    p_value = 1 - chi2.cdf(M_corrected, df)\n\n    return M_corrected, df, p_value\n\n# 실행\nM, df_bm, pval = box_m_test(cov_mats, N)\nprint(f\"Box’s M = {M:.3f}, df = {df_bm:.1f}, p-value = {pval:.4f}\")\nBox’s M = 74.731, df = 20.0, p-value = 0.0000\nBox’s M 검정에서 유의확률 p-value가 0.000으로 나타나, 세 집단(Adelie, Chinstrap, Gentoo) 간 공분산행렬이 동일하다는 등분산 가정이 기각되었다. 따라서 공분산을 각 집단별로 다르게 추정하는 QDA를 적용하여 판별함수가 추정되었다.\n피셔 QDA 방법\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis as QDA\nfrom sklearn.metrics import confusion_matrix, classification_report\n\n# 분석 변수 선택\nX = penguins[['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm', 'body_mass_g']].dropna()\ny = penguins.loc[X.index, 'species']\n\n# QDA 모델 적합\nqda = QDA()\nqda_fit = qda.fit(X, y)\n\n# 예측\ny_pred_qda = qda.predict(X)\n\n# 혼동행렬\ncm_qda = confusion_matrix(y, y_pred_qda)\nprint(\"=== QDA Confusion Matrix ===\")\nprint(cm_qda)\n\n# 분류 정확도 및 지표\nprint(\"\\n=== QDA Classification Report ===\")\n# ★ target_names를 3개 종 이름으로 맞추거나, 그냥 생략해도 됩니다.\nprint(classification_report(y, y_pred_qda, target_names=qda.classes_))\n# 또는: print(classification_report(y, y_pred_qda))\n\n# 정확도\nprint(\"훈련집합 정확도:\", qda.score(X, y))\n=== QDA Confusion Matrix ===  [[144 2 0]  [ 2 66 0]  [ 0 0 119]]\n=== QDA Classification Report ===  precision recall f1-score support  Adelie 0.99 0.99 0.99 146  Chinstrap 0.97 0.97 0.97 68  Gentoo 1.00 1.00 1.00 119  accuracy 0.99 333  macro avg 0.99 0.99 0.99 333  weighted avg 0.99 0.99 0.99 333\n훈련집합 정확도: 0.987987987987988\nQDA 분석 결과, 전체 333개 관측치에 대해 훈련집합 정확도는 약 98.8%로 매우 높은 수준의 분류 성능을 보였다. 혼동행렬을 살펴보면 대부분의 개체가 자신의 실제 종에 정확하게 분류되었으며, 오분류는 Adelie 2개, Chinstrap 2개에 불과했다. Gentoo는 단 한 개의 오분류도 발생하지 않았다.\n정확도 지표에서도 동일한 패턴이 확인된다. Adelie는 precision과 recall이 모두 0.99로 나타나 거의 완벽하게 구분되었고, Chinstrap 역시 precision·recall 모두 0.97로 매우 높은 판별력을 보였다. Gentoo는 precision과 recall이 모두 1.00으로, 주어진 변수(부리길이·부리깊이·날개길이·체중)를 통해 다른 종과 완벽히 구분되었다.\n이러한 결과는 펭귄 세 종이 신체 치수에서 뚜렷한 차이를 보이며, 집단 간 공분산 구조 역시 서로 다르다는 점과 잘 부합한다. 특히 날개길이와 체중에서 Gentoo가 크게 분리되는 경향이 있어 QDA의 비선형 판별경계가 효과적으로 작동한 것으로 해석된다. 전체적으로 QDA는 펭귄 데이터에서 매우 높은 구분 능력을 보였으며, 변수 조합만으로도 세 종을 거의 완벽하게 분류할 수 있음을 시사한다.\n새로운 개체 판별\n# QDA 사후확률 예측 (경고 없음)\nimport pandas as pd\n\n# 예시: 새로운 펭귄 개체 입력 (DataFrame 형태로)\nnew_penguin_qda = pd.DataFrame({\n    'bill_length_mm': [45.0],\n    'bill_depth_mm': [17.0],\n    'flipper_length_mm': [200.0],\n    'body_mass_g': [4500.0]\n})\n\n# 예측된 집단\npred_class_qda = qda.predict(new_penguin_qda)\n\n# 사후확률\npred_proba_qda = qda.predict_proba(new_penguin_qda)\n\nprint(\"=== QDA 예측 결과 ===\")\nprint(\"예측된 종:\", pred_class_qda[0])\nprint(\"각 종에 대한 사후확률:\")\nfor cls, p in zip(qda.classes_, pred_proba_qda[0]):\n    print(f\"  {cls}: {p:.3f}\")\n=== QDA 예측 결과 ===  예측된 종: Adelie  각 종에 대한 사후확률:  Adelie: 0.717  Chinstrap: 0.283  Gentoo: 0.000\n\n\n(2) 다항 로짓 판별분석\n# 다항 로지스틱 판별분석 (Multinomial Logistic Discriminant Analysis)\nimport numpy as np\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n\n# 분석 변수 선택\nX = penguins[['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm', 'body_mass_g']].dropna()\ny = penguins.loc[X.index, 'species']\n\n# 다항 로지스틱 회귀(판별모형) 적합\nlogit_clf = LogisticRegression(\n    solver='lbfgs',\n    max_iter=2000                # 수렴 경고 방지\n)\n\nlogit_clf.fit(X, y)\n\n# 전체 데이터에 대해 예측\ny_pred = logit_clf.predict(X)\n\n# 사후확률\ny_proba = logit_clf.predict_proba(X)\n\n# 결과 출력\nprint(\"=== Multinomial Logistic Discriminant Analysis (Full Data) ===\")\nprint(f\"전체데이터 정확도: {accuracy_score(y, y_pred):.4f}\\n\")\n\nprint(\"=== Confusion Matrix ===\")\nprint(confusion_matrix(y, y_pred), \"\\n\")\n\nprint(\"=== Classification Report ===\")\nprint(classification_report(y, y_pred, target_names=logit_clf.classes_))\n=== Multinomial Logistic Discriminant Analysis (Full Data) ===  전체데이터 정확도: 1.0000\n=== Confusion Matrix ===  [[146 0 0]  [ 0 68 0]  [ 0 0 119]]\n=== Classification Report ===  precision recall f1-score support  Adelie 1.00 1.00 1.00 146  Chinstrap 1.00 1.00 1.00 68  Gentoo 1.00 1.00 1.00 119  accuracy 1.00 333  macro avg 1.00 1.00 1.00 333  weighted avg 1.00 1.00 1.00 333\nimport pandas as pd\nimport numpy as np\n\n# 클래스 순서\nclasses = logit_clf.classes_\nfeatures = X.columns\n\n# 계수(beta) 행렬 (K-1개 식)\ncoef = logit_clf.coef_\nintercept = logit_clf.intercept_\n\nprint(\"=== Multinomial Logistic Regression Estimated Coefficients ===\")\n\nfor i, cls in enumerate(classes):\n    print(f\"\\n[참조 대비: {cls} 로짓함수]\")\n    \n    coef_df = pd.DataFrame({\n        'Feature': features,\n        'Beta': coef[i],\n        'Odds Ratio': np.exp(coef[i])\n    })\n    \n    print(coef_df.to_string(index=False))\n    print(f\"Intercept (절편): {intercept[i]:.4f}\")\n=== Multinomial Logistic Regression Estimated Coefficients ===  [참조 대비: Adelie 로짓함수]  Feature Beta Odds Ratio  bill_length_mm -1.145597 0.318034  bill_depth_mm 1.334007 3.796226  flipper_length_mm 0.017936 1.018098  body_mass_g 0.001935 1.001937  Intercept (절편): 16.9885\n[참조 대비: Chinstrap 로짓함수]  Feature Beta Odds Ratio  bill_length_mm 1.251207 3.494557  bill_depth_mm -0.105142 0.900197  flipper_length_mm -0.063269 0.938691  body_mass_g -0.006714 0.993308  Intercept (절편): -13.9010\n[참조 대비: Gentoo 로짓함수]  Feature Beta Odds Ratio  bill_length_mm -0.105609 0.899776  bill_depth_mm -1.228865 0.292624  flipper_length_mm 0.045332 1.046375  body_mass_g 0.004779 1.004791  Intercept (절편): -3.0875\nAdelie는 깊고 짧은 부리를 가진 경우 강하게 판별된다. Chinstrap은 긴 부리가 가장 중요한 구분 요인이다. Gentoo는 얕은 부리 + 큰 체구로 다른 두 종과 구분된다. 즉, 다항 로지스틱 판별분석은 신체 치수의 비율·크기 조합을 로짓함수 형태로 모델링하여 각 종의 형태적 특징을 정량적·확률적으로 구분하는 역할을 수행한다.\nAdelie 대비 다른 종과의 구별을 설명하는 로그오즈식에서, 부리길이 계수는 -1.15(Beta), 오즈비 0.32로 나타났다.이는 부리길이가 길어질수록 Adelie일 가능성이 급격하게 감소함을 의미하며, 같은 길이 증가에 대해 오즈가 약 68% 감소한다. 즉, 긴 부리는 Adelie보다 Chinstrap 또는 Gentoo에서 더 자주 나타나는 특징이다.\n부리깊이는 1.33, 오즈비 3.80으로 매우 큰 양(+)의 효과가 나타났다.동일한 조건에서 부리깊이가 깊어질수록 Adelie로 분류될 가능성이 3.8배 증가함을 의미한다. 이 변수는 Adelie의 독립적인 신체적 특성을 가장 잘 반영하는 판별요인이다.\n날개길이와 체중은 계수 크기가 매우 작고 오즈비는 1.01 수준으로, Adelie 구분에서는 상대적으로 영향력이 낮다. 정리: Adelie는 ’짧고 깊은 부리’라는 조합이 결정적으로 구분되는 특징이며, 체중·날개길이는 판별력이 상대적으로 작다.\nChinstrap 대비 다른 종과의 차이를 설명하는 식에서는 다음 특징이 나타난다. 부리길이 계수 1.25, 오즈비 3.49 부리길이가 길수록 Chinstrap일 가능성이 매우 커지며, 길이 1mm 증가에 대해 오즈가 약 3.5배 증가한다. 이는 Chinstrap이 세 종 중 가장 긴 부리를 갖는 경향과 잘 부합한다.\n부리깊이는 -0.105(오즈비 0.90)로 약하게 음(-)의 영향을 갖는다.깊은 부리는 Chinstrap보다는 Adelie에서 더 많이 나타난다는 의미이다. 날개길이(-0.063), 체중(-0.0067)의 영향은 작지만 음(-)의 부호이므로, 날개가 길고 체중이 무거울수록 Chinstrap일 가능성은 소폭 감소한다.\n정리: Chinstrap 구분에는 부리길이가 가장 핵심적인 판별 변수이며, 부리깊이가 깊을수록 Adelie로, 날개가 길수록 Gentoo로 분류되는 경향이 강화된다.\nGentoo는 신체 크기가 큰 종이므로, 결과도 이를 반영한다. 부리길이는 계수 -0.106(오즈비 0.90)**으로 작은 음(-) 효과를 보이지만 큰 판별력은 아니다.\n부리깊이 계수 -1.23, 오즈비 0.29는 큰 음(-)의 영향을 나타낸다.즉, 부리가 깊어질수록 Gentoo일 가능성이 71% 감소한다. Gentoo는 세 종 중 가장 얕은 부리를 가지고 있으며, 부리깊이는 Gentoo를 구분하는 데 매우 중요한 음의 지표이다.\n날개길이는 +0.045(오즈비 1.046)으로 약한 양(+)의 영향이 있지만, Gentoo는 실제로 날개길이가 월등히 길어 QDA·LDA에서는 강력한 분리 요인이 되므로, 다항로짓에서도 같은 방향성을 보인다.\n체중 계수 0.0048(오즈비 1.0048)도 양(+)이지만 크기는 매우 작다.하지만 Gentoo의 체중이 세 종 중 가장 무겁다는 점을 반영한 결과이다.\n정리: Gentoo는 ’얕은 부리 + 큰 체구(긴 날개, 높은 체중)’라는 조합으로 판별되며, 특히 부리깊이가 Gentoo를 배제하는 가장 강력한 변수이다.\n# 새 펭귄 데이터 입력 (DataFrame 형태로)\nnew_penguin = pd.DataFrame({\n    'bill_length_mm': [45.0],\n    'bill_depth_mm': [17.0],\n    'flipper_length_mm': [200.0],\n    'body_mass_g': [4500.0]\n})\n\n# 경고 없이 예측\npred_class = logit_clf.predict(new_penguin)\npred_proba = logit_clf.predict_proba(new_penguin)\n\nprint(\"예측된 종:\", pred_class[0])\nprint(\"각 종에 대한 사후확률:\")\nfor cls, p in zip(logit_clf.classes_, pred_proba[0]):\n    print(f\"  {cls}: {p:.3f}\")\n예측된 종: Gentoo  각 종에 대한 사후확률:  Adelie: 0.190  Chinstrap: 0.013  Gentoo: 0.796"
  },
  {
    "objectID": "notes/mda/mda_factor.html",
    "href": "notes/mda/mda_factor.html",
    "title": "다변량분석 3. 요인분석",
    "section": "",
    "text": "Chapter 1. 요인분석 개념\n요인분석은 인간의 지능이 단일한 능력인지, 혹은 여러 하위 능력으로 구성되어 있는지를 탐구하려는 심리학적 연구에서 출발한 분석 방법이다.\n초기의 심리학자들은 지능검사 점수들 사이에 일정한 상관이 존재한다는 사실에 주목하였다. 예를 들어 수리능력, 언어능력, 공간능력 등 서로 다른 과목의 점수가 비슷하게 높거나 낮게 나타나는 현상은, 이들 능력의 배후에 공통된 요인이 존재함을 암시한다.\n이러한 가정 아래, 요인분석(factor analysis)은 여러 관측변수들 간의 상관구조를 분석하여 그 배후에 숨겨진 잠재적 요인(latent factors)을 찾아내는 통계적 방법이다.\n즉, 관측된 데이터가 몇 개의 기본적인 차원이나 요인으로 설명될 수 있다는 전제를 두고, 그 요인들이 서로 어떤 의미를 가지는지를 탐색한다.\n결국 요인분석은 인간의 지능뿐 아니라 태도, 성격, 소비행동, 사회적 가치 등 다양한 복합적 현상 속에서 ’표면의 다양성 뒤에 숨어 있는 공통 구조’를 밝히는 도구라 할 수 있다.\n\n1. 역사\n\n(1) Charles Spearman, 1904: 일반 지능 요인(G 요인) 이론\n찰스 스피어만(Charles Spearman, 1904)은 요인분석의 출발점을 마련한 인물이다.\n그는 여러 지능 검사 점수들 사이에 높은 상관관계가 존재한다는 사실을 발견하고, 이러한 공통된 변동성이 하나의 근본적인 능력, 즉 일반 지능 요인(g 요인)에 의해 설명될 수 있다고 주장하였다\n스피어만은 인간의 지능이 단일한 능력으로만 구성되어 있지 않으며, 각 과목이나 과제에는 그 과제 특유의 능력도 함께 작용한다고 보았다. 이에 따라 그는 2요인 이론(two-factor theory)을 제안하였다.\n즉, 각 개인의 관측된 성적(또는 변수 값)은 모든 검사에 공통적으로 작용하는 일반 요인(g)과, 각 검사에 고유하게 작용하는 특수 요인(s)의 결합으로 설명된다고 하였다.\n이 이론은 심리학에서 ’지능의 구조’를 설명하는 최초의 통계적 접근으로, 데이터에 내재된 공통 구조(common structure)를 수학적으로 찾아내려는 시도의 출발점이 되었다.\n결국 스피어만의 연구는 단순한 지능 연구를 넘어, 복잡한 현상을 몇 개의 잠재 요인으로 요약하여 해석할 수 있다는 통계적 사고의 기반을 마련한 셈이다.\n\n\n(2) Louis Thurstone, 1930s: - 다중 요인 이론\n루이스 서스톤(Louis Thurstone, 1930s)은 스피어만의 단일 g 요인 이론에 도전하며, 다중 요인 이론(Multiple Factor Theory)을 제안한 인물이다. 그는 인간의 지능이 하나의 일반 요인으로만 설명될 수 없으며, 여러 개의 독립적인 능력으로 구성되어 있다고 주장하였다.\n서스톤은 다양한 지능 검사를 실시한 뒤, 그 점수들 사이의 상관구조를 분석하여 몇 가지 기본적인 능력 요인을 추출하였다. 그는 이 과정을 통해 언어 능력, 공간적 시각화, 수리 능력, 추론 능력, 기억력) 등 서로 구별되는 기본 정신 능력이 존재함을 제시하였다.\n그의 접근은 단일 요인을 중심으로 한 스피어만의 시각과 달리, 여러 요인이 동시에 작용하여 인간의 복잡한 지적 행동을 설명할 수 있다는 점을 강조하였다.\n또한 서스톤은 이러한 다요인적 구조를 검증하기 위해 탐색적 요인분석(Exploratory Factor Analysis, EFA)의 기초적인 절차를 체계화하였다. 그 결과, 요인분석은 심리학을 넘어 교육, 사회과학, 경영, 마케팅 등 다양한 분야에서 데이터 속 잠재 구조를 탐색하는 통계적 도구로 발전하게 되었다.\n\n\n(3) 확증적 요인분석(CFA)의 발전 - 1960년대 이후\n1960년대 이후에는 요인분석이 탐색적 접근에서 한 단계 더 발전하여, 확증적 요인분석(Confirmatory Factor Analysis, CFA)이라는 새로운 패러다임이 등장하였다.\nCFA는 단순히 데이터 속에서 요인을 찾아내는 것이 아니라, 이론적으로 설정된 요인 구조가 실제 자료에 부합하는지를 검증하는 절차이다.\n즉, 연구자가 사전에 요인 수, 각 변수와 요인의 관계, 요인 간 상관 구조 등을 명시적으로 가정하고, 그 가정이 데이터에 의해 지지되는지를 통계적으로 검토한다.\n이러한 접근은 기존의 탐색적 요인분석(EFA)이 ”데이터로부터 구조를 찾아내는 방법”이었다면, 확증적 요인분석은 ”이론이 데이터에 적합한지를 검증하는 방법”이라는 점에서 본질적인 차이를 가진다.\nCFA의 발전은 심리학, 교육학, 사회학, 마케팅 연구 등 다양한 분야에서 측정모형(measurement model) 검증을 가능하게 하였고, 더 나아가 구조방정식모형(Structural Equation Modeling, SEM)의 핵심 구성 요소로 자리잡게 되었다.\n결국 CFA는 ”요인의 존재를 밝히는 기술”에서 ”이론을 경험적으로 검증하는 과학적 절차”로 요인분석을 진화시킨 전환점이라 할 수 있다.\n\n\n(4) 구조방정식 모형(SEM)과의 통합\n1980년대 이후 요인분석은 구조방정식 모형(Structural Equation Modeling, SEM)으로 확장·통합되면서 한층 정교한 분석 체계를 갖추게 되었다.\nSEM은 요인분석의 측정모형과 회귀분석의 구조모형을 결합하여, 잠재 변수와 관측 변수 간의 관계를 동시에 추정할 수 있는 통합적 통계 방법이다.\n이 접근을 통해 연구자는 단순히 ”요인이 존재하는가?“를 넘어서, ”요인들이 서로 어떤 인과적 관계를 맺고 있는가?”까지 검증할 수 있게 되었다. 즉, SEM은 측정의 타당성과 이론적 구조의 인과적 경로를 동시에 분석하는 틀을 제공한다.\n이 방법은 심리학, 교육학, 사회학, 경영학, 마케팅 등 인간 행동과 인식의 복합적 구조를 다루는 사회과학 분야에서 폭넓게 활용되고 있다. 결국 구조방정식 모형의 등장은 요인분석을 단순한 탐색 도구에서 이론 검증을 위한 과학적 모델링 체계로 발전시키는 결정적 전환점이 되었다.\n\n\n\n2. 스피어만 연구결과\n\n(1) 스피어만의 2요인 이론(two factor theory)\n스피어만의 2요인 이론은 인간의 지적 수행을 설명하는 최초의 체계적 요인모형으로, 모든 인지적 과제가 공통 요인과 특수 요인에 의해 결정된다고 보는 접근이다.\n일반 요인(g 요인)\n스피어만은 g 요인을 모든 인지적 활동에 공통적으로 작용하는 일반 지능으로 정의하였다. 이는 문제 해결, 논리적 추론, 정보 처리 등 다양한 과제 수행의 기반이 되는 보편적 정신 능력이다.\n서로 다른 형태의 지능 검사―예를 들어 언어, 수리, 공간 과제―에서 상관이 관찰되는 이유는, 이 모든 과제들이 일정 부분 동일한 일반 지능(g)의 영향을 받기 때문이라 보았다.\n특수 요인(s 요인)\n그러나 스피어만은 모든 지적 활동이 완전히 동일한 능력에 의해 설명된다고는 보지 않았다. 각 과제에는 그 과제만의 특수한 능력, 즉 특수 요인(s)이 존재하며, 이는 특정 영역에만 작용한다.\n예를 들어 수학 문제 해결에는 일반 지능(g)이 기여하지만, 동시에 수학적 사고나 계산 능력과 같은 수학 고유의 s 요인이 별도로 작용한다.\n이와 같이 스피어만의 2요인 이론은 모든 인지적 수행이 공통된 일반 능력(g)과 과제 특유의 특수 능력(s)의 결합으로 이루어진다고 설명하였다. 이 모델은 이후의 요인분석과 다중지능 이론 발전의 기초가 되었으며, 인간 지능의 구조를 이해하는 데 중요한 이정표가 되었다.\n\n\n(2) 상관계수 행렬과 부하값\n\n\n\n\n\n부하값 0.80 0.75 0.65 0.85 0.60\n\n\n(3) 결과 해석\n요인부하값(factor loading)은 각 변수가 요인과 얼마나 강하게 관련되어 있는지를 나타내는 지표로, 값이 높을수록 해당 변수가 그 요인의 특성을 잘 반영한다는 뜻이다.\n수리 능력의 g 요인 부하값이 0.80이라면, 수리 능력 점수의 분산 중 약 80%가 일반 지능 요인(g)에 의해 설명된다는 의미이다. 즉, 수리 능력 검사는 개인의 전반적인 지적 능력 수준을 비교적 잘 반영하는 과제라 할 수 있다.\n추론 능력이 g 요인에 대해 0.85로 가장 높은 부하값을 가진다면, 이는 추론 과제가 일반 지능을 가장 직접적으로 측정하고 있음을 의미한다. 다시 말해, 논리적 사고나 문제 해결 능력은 g 요인과의 관련성이 특히 강한 인지 영역이다.\n반면, 지각 속도의 부하값이 0.60이라면 g 요인과의 연관성이 상대적으로 낮음을 뜻한다. 이는 지각 속도 과제의 수행이 일반 지능보다는 과제 특유의 처리 속도, 주의 집중, 시각적 변별 능력 등과 같은 특수 요인(s 요인)의 영향을 더 많이 받기 때문으로 해석된다.\n따라서 요인부하값을 통해 각 인지 능력이 얼마나 공통 요인(g)에 의해 설명되는지, 그리고 어느 정도가 특수 요인(s)에 기인하는지를 정량적으로 파악할 수 있다.\n이러한 해석은 요인분석이 단순한 상관관계 탐색을 넘어, 인간의 인지 구조를 이해하는 데 기여하는 핵심적인 근거가 된다.\n\n\n\n3. 주요 용어\n공통요인(Common Factors)은 여러 관측변수가 함께 공유하는 잠재적 원인으로, 변수들 간의 상관관계를 설명하는 근본적 구조이다. 즉, 각 변수의 총 변동성 가운데 여러 변수에 공통적으로 작용하는 부분을 의미하며, 인간의 지능, 태도, 성격 등 복합적 특성의 공통된 차원을 찾아내는 데 활용된다.\n특수요인(Specific Factors)은 각 변수에 고유하게 작용하는 요인으로, 다른 변수들과는 공유되지 않는다. 이는 각 변수가 가지는 독립적인 특성이나 측정상의 특수한 영향을 반영하며, 공통요인으로 설명되지 않는 잔여 변동성을 나타낸다.\n요인 부하(Factor Loadings)는 각 요인이 관측변수에 미치는 영향의 크기이자, 변수와 요인 간의 상관계수로 해석된다. 요인 부하값이 높을수록 해당 변수가 그 요인의 특성을 잘 반영하며, 값이 낮을수록 요인과의 관련성이 약함을 의미한다. 따라서 요인 부하는 각 변수의 의미를 해석하고 요인의 성격을 규정하는 핵심 단서가 된다.\n요인 회전은 추출된 요인들의 해석을 단순하고 명확하게 하기 위한 절차이다. 초기 요인 추출 후의 결과는 해석이 모호할 수 있으므로, 요인축을 회전시켜 변수들이 특정 요인에 더 뚜렷하게 부하되도록 만든다. 회전 방식에는 요인 간 상관을 허용하지 않는 직교 회전(Varimax)과, 요인들 간의 상관을 허용하는 사교 회전(Oblimin) 등이 있으며, 이를 통해 연구자는 각 요인의 의미를 보다 명확하게 해석할 수 있다.\n\n\n4. 요인분석 종류\n탐색적 요인분석(Exploratory Factor Analysis, EFA)은 데이터에 내재된 요인 구조를 사전 가정 없이 탐색하는 통계적 방법이다. 연구자가 요인의 수나 변수와 요인 간의 관계에 대해 미리 알지 못하는 상태에서, 관측된 변수들 간의 상관관계를 분석하여 잠재 요인(latent factors)을 찾아내는 데 목적이 있다.\n즉, 데이터로부터 요인의 개수와 구조를 ’발견’하는 과정이라 할 수 있다. EFA는 새로운 개념이나 척도를 개발할 때, 또는 복잡한 현상의 기저 구조를 파악하고자 할 때 주로 사용된다.\n확증적 요인분석(Confirmatory Factor Analysis, CFA)은 이와 달리, 연구자가 이론적으로 설정한 요인 구조가 실제 데이터에 부합하는지를 검정하는 절차이다. EFA가 요인을 찾아내는 과정이라면, CFA는 이미 설정된 모형이 데이터를 잘 설명하는지 확인하는 과정이다. 따라서 CFA에서는 요인 수, 변수와 요인의 연결 관계, 요인 간 상관 구조 등을 사전에 명시해야 하며, 모형의 적합도 지수(fit indices)를 통해 이론적 모형의 타당성을 평가한다.\n탐색적 요인분석과 확증적 요인분석은 모두 여러 관측변수에 내재된 잠재 요인을 밝히려는 동일한 목표를 가지고 있지만, 접근 방식과 사용 목적에서 본질적인 차이를 가진다.\nEFA는 말 그대로 탐색적 성격을 갖는다. 연구자가 요인의 개수나 변수와 요인의 관계에 대해 명확한 사전 가정을 두지 않은 상태에서, 데이터 자체에 내재된 상관구조를 분석하여 잠재 요인의 수와 구성을 찾아내는 데 목적이 있다.\n즉, 데이터로부터 자연스럽게 드러나는 패턴을 통해 ”어떤 요인이 존재하는가”를 발견하는 과정이다. 이 과정에서 요인의 개수 결정, 요인 회전(rotation), 요인 부하 해석 등은 연구자의 판단과 통계적 기준(예: 고유값, 스크리 플롯, 누적 설명력)에 의존한다. 따라서 EFA는 이론의 발견을 위한 초기 단계의 분석이라 할 수 있다.\n반면, CFA는 확증적 성격을 지닌다. EFA로 탐색된 요인 구조나 기존 이론에 기반하여, 연구자가 미리 설정한 요인 모형이 실제 데이터에 적합한지를 검증한다. 즉, ”이 변수들은 특정 요인에 속한다”, ”이 요인들은 서로 어떤 관계를 가진다”라는 가정이 이미 존재하며, CFA는 그 가정이 데이터에 의해 얼마나 지지되는가를 평가하는 절차이다.\n이를 위해 요인 부하의 경로를 사전에 명시하고, 변수 간의 상관 구조를 제약하며, 모형 적합도 지수(예: CFI, TLI, RMSEA, SRMR 등)를 통해 전체 모형의 타당성을 판단한다. CFA는 단순히 요인을 찾는 데 그치지 않고, 이론적 모형의 검증과 측정의 타당성 평가에 초점을 둔다.\n요약하자면, EFA는 ”데이터가 어떤 구조를 갖고 있는가?“를 묻는 과정이고, CFA는 ”가정한 구조가 데이터에 부합하는가?”를 검증하는 과정이다. 이 두 방법은 상호보완적인 관계를 이루며, 일반적으로 연구는 EFA를 통해 요인 구조의 윤곽을 파악한 후, CFA를 이용하여 그 구조가 이론적으로나 경험적으로 타당한지를 검증하는 순서로 진행된다.\n이러한 연속적 접근은 단순히 변수들 간의 상관을 기술하는 수준을 넘어, 데이터 속 잠재적 개념을 수학적으로 모델링하고, 이론적 타당성을 검증하는 구조방정식 모형(SEM)의 기초로 이어지게 된다. 따라서 EFA와 CFA는 요인분석의 두 축이자,”탐색에서 검증으로” 이어지는 사회과학 연구의 논리적 흐름을 완성하는 핵심적인 분석 틀이라 할 수 있다.\n\n\n\nChapter 2. 탐색적 요인분석\n\n1. 탐색적 요인분석 절차\n탐색적 요인분석은 관찰된 여러 변수들 간의 상관관계를 분석하여, 그 배후에 존재하는 잠재 요인(latent factors)을 찾아내는 통계적 절차이다. 즉, 다수의 변수들이 실제로는 몇 개의 공통된 차원 또는 개념에 의해 설명될 수 있다는 가정에서 출발한다.\n탐색적 요인분석의 기본 목적은 데이터 속에서 변수 간의 상관구조를 탐색하고, 그 구조를 가장 잘 요약하는 잠재 요인을 식별하는 것이다. 이 과정을 통해 변수들이 어떤 요인에 의해 함께 움직이는지를 파악할 수 있으며, 복잡한 데이터의 차원을 축소하고 의미 있는 해석 단위를 만들어 낼 수 있다.\n예를 들어, 한 개인의 학업 능력을 측정하는 여러 과목 점수(수학, 과학, 언어, 사회 등)가 서로 높은 상관을 보인다면, 이들 과목은 ”학업 성취도”라는 공통 요인에 의해 설명될 수 있다. 탐색적 요인분석은 바로 이러한 공통 구조를 찾아내어 변수들을 체계적으로 묶는 방법이다.\n따라서 EFA는 이론이 명확히 정립되지 않았거나, 새로운 영역의 데이터를 다룰 때 특히 유용하다. 데이터로부터 요인 수를 추정하고, 각 변수들이 어떤 요인에 강하게 연관되어 있는지를 밝힘으로써 연구자는 변수의 집단적 의미를 탐색하고, 후속 이론 구축의 기초 자료를 얻을 수 있다.\n결국 탐색적 요인분석은 복잡한 현상 속에서 데이터의 숨은 질서를 발견하는 과정이며, 이를 통해 연구자는 변수 간의 관계를 단순화하고, 데이터에 내재된 패턴을 이론적으로 해석할 수 있게 된다.\n\n(1) 데이터 수집\n요인분석을 수행할 때는 충분한 표본 크기를 확보하는 것이 중요하다. 일반적으로 변수 한 개당 최소 10개 이상의 표본이 권장되며, 표본 수가 많을수록 분석 결과의 안정성과 신뢰도가 높아진다.\n요인분석에 포함되는 변수들은 동일한 개념 또는 구성개념을 측정해야 한다. 따라서 구조화된 설문조사에서 하나의 개념을 여러 하위 문항으로 측정한 리커트 척도 자료가 요인분석에 가장 적합하다. 예를 들어, ”고객 서비스”에 관한 설문 문항들—응답 속도, 직원의 친절도, 문제 해결 능력 등—은 공통적으로 서비스 품질이라는 잠재 요인을 반영할 수 있다.\n\n\n(2) 요인분석 모형\np개의 확률변수 데이터 \\({\\underset{¯}{x}}' = (x_{1},x_{2},...,x_{p})\\)의 평균 벡터 \\(\\underset{¯}{\\mu}\\), 공분산행렬을 \\(\\Sigma\\)라 하면, \\({\\underset{¯}{x}}_{p} = L_{p \\times m}{\\underset{¯}{f}}_{m} + {\\underset{¯}{\\eta}}_{p}\\)이다.\n\\(f_{1},f_{2},...,f_{m}\\): 공통 common 요인, \\(\\underset{¯}{f} \\sim (\\underset{¯}{0},I)\\) 공통 요인들의 평균은 0이고 분산은 1이며, 공통 요인간 상관 관계는 없다.\n\\(L_{p \\times m}\\): 부하행렬(loading matrix)로, 각 관측변수가 요인에 의해 얼마나 영향을 받는지를 나타내는 계수를 포함한다. 즉, \\(L_{ij}\\)는 i-번째 변수와 j-번째 요인 간의 관계 강도를 의미한다.\n\\(\\eta_{1},\\eta_{2},...,\\eta_{p}\\): 특수 specific 요인, \\(\\underset{¯}{\\eta} \\sim (\\underset{¯}{0},\\Psi)\\) 특수 요인(specific factors)으로, 각 변수에 고유하게 작용하는 변동 요인을 나타낸디.\n\\(\\underset{¯}{f}\\) 와 \\(\\underset{¯}{\\eta}\\)는 서로 독립이다.\n\n\n(3) 전제 조건 확인\n요인분석을 실시하기 전에, 변수들 간의 상관관계가 충분히 높아 공통 요인을 추출할 수 있는 구조인지 확인해야 한다. 즉, 변수들이 서로 일정한 관계망을 형성해야 요인분석이 의미 있는 결과를 낼 수 있다.\n이를 위해 다음의 적합성 검정 절차를 수행한다.\n\n\n1. KMO(Kaiser-Meyer-Olkin) 검정\nKMO 지수는 변수들 간의 상관관계가 요인분석에 적합한지를 평가하는 지표이다. 값이 0과 1 사이에서 산출되며, 일반적으로 0.7 이상이면 요인분석에 적합하다고 본다. 0.8 이상이면 매우 양호한 수준으로 판단된다.\n\n\n2. Bartlett의 구형성 검정(Bartlett’s Test of Sphericity)\n이 검정은 변수들 간의 상관관계가 전반적으로 유의한지를 평가한다. 귀무가설은 ”상관행렬이 단위행렬과 같다”이며, 검정 결과가 통계적으로 유의(p &lt; 0.05)할 경우 변수들 간 상관이 충분하므로 요인분석을 수행할 수 있다.\n\n\n3. 상관행렬(Correlation Matrix) 분석\n변수들 간의 상관행렬을 생성하여 상관 패턴을 검토한다. 특정 변수들이 다른 변수들과 전반적으로 높은 상관을 가진다면, 이 변수들은 공통된 잠재 요인을 공유하고 있을 가능성이 높다. 반대로 상관이 매우 낮거나 불규칙한 변수는 요인구조에 잘 부합하지 않을 수 있다.\n\n\n(4) 요인 추출 방법 선택\n\n\n1. 주축 요인법 (Principal Axis Factoring)\n주축 요인법은 변수들의 상관행렬 \\(R_{p \\times p}\\)로부터 고유값과 고유벡터를 구하여 요인을 추출하는 방법이다. 고유값을 \\(\\lambda_{1} \\geq \\lambda_{2} \\geq \\ldots \\geq \\lambda_{p}\\), 고유벡터를 \\((e_{1},e_{2},\\ldots,e_{p})\\)라 하면, 상관행렬은 다음과 같이 분해된다.\n\\[R_{p \\times p} = \\begin{bmatrix}\n\\sqrt{\\lambda_{1}}e_{1} & \\sqrt{\\lambda_{2}}e_{2} & \\ldots & \\sqrt{\\lambda_{p}}e_{p}\n\\end{bmatrix}\\left\\lbrack \\begin{array}{r}\n\\sqrt{\\lambda_{1}}e_{1}' \\\\\n\\sqrt{\\lambda_{2}}e_{2}' \\\\\n\\vdots \\\\\n\\sqrt{\\lambda_{p}}e_{p}'\n\\end{array} \\right\\rbrack = LL'\\]\n이때 L은 부하행렬로, 각 변수와 요인 간의 관계 강도를 나타낸다. 상관행렬로부터 부하행렬을 계산하고, 이를 이용하여 주성분 점수 \\(y = Lx\\)를 구한 뒤, 요인 점수는 다음과 같이 계산된다. \\(f_{k} = \\frac{y_{k}}{\\sqrt{\\lambda_{k}}}\\)\n\n\n2. Bartlett 방법\nBartlett 방법은 관측치 간의 상관이 없다는 가정하에 요인 점수를 추정하는 방식이다. 이 방법은 요인구조가 명확하고, 변수 간 독립성이 어느 정도 보장될 때 적합하다. 요인 점수를 추정할 때 오차를 최소화하는 가중치를 부여하여, 각 요인의 상대적 영향력을 추정한다.\n\n\n3. 회귀법 (Thompson Method)\nThompson 방법은 요인 점수를 선형회귀 원리에 따라 추정하는 방식이다. 관측변수 x에 대하여, 다음과 같이 표준화된 벡터 \\(z = (x - \\mu)/\\sigma\\)를 정의한다.\n이때 z와 f의 결합분포를 다음과 같이 놓는다.\n\\(\\left\\lbrack \\begin{array}{r}\nz \\\\\nf\n\\end{array} \\right\\rbrack \\sim N\\left( \\left\\lbrack \\begin{array}{r}\n0 \\\\\n0\n\\end{array} \\right\\rbrack,\\begin{bmatrix}\nP & L \\\\\nL' & I\n\\end{bmatrix} \\right)\\), 조건부 기댓값을 이용하면 요인 점수의 추정치는 다음과 같이 주어진다. \\(E(f \\mid z) = L'P^{- 1}z\\) 또는 상관행렬 R을 이용하면, \\(f_{r} = L'R^{- 1}z_{r}\\) 로 표현된다. 이 방식은 요인과 관측변수 간의 선형 관계를 바탕으로 요인 점수를 회귀적으로 추정하므로, 계산이 간편하고 실제 응용에서 널리 사용된다.\n요약하면, 주축 요인법은 상관행렬의 고유값 분해를 기반으로 요인을 추출하는 방법이며, Bartlett 방법과 회귀법은 각각의 가정 하에서 요인 점수를 산출하는 절차이다. 이들 방법의 선택은 연구 목적, 데이터 특성, 그리고 요인구조의 명확성에 따라 달라진다.\n\n\n(5) 추출할 요인의 수 결정\n요인분석에서 중요한 단계는 몇 개의 요인을 추출할 것인가를 결정하는 것이다. 요인의 수가 너무 적으면 데이터의 복잡한 구조를 충분히 설명하지 못하고, 너무 많으면 불필요한 요인이 포함되어 해석이 어렵게 된다. 따라서 적절한 요인 수를 선택하기 위해 여러 통계적 기준이 사용된다.\n\n\n1. 고유값(Eigenvalue) 기준\n요인분석에서는 상관행렬 R의 고유값을 이용하여 각 요인이 설명하는 분산의 크기를 평가한다. 고유값이 1보다 크다는 것은, 해당 요인이 하나의 변수 이상을 설명할 만큼의 정보량을 갖고 있다는 의미이다. 따라서 일반적으로 고유값이 1 이상인 요인만을 추출하는 것이 기준이다. 이 기준은 Kaiser 기준이라 불리며, 가장 널리 사용되는 요인 수 결정 방법이다.\n\n\n2. 총 분산 설명율(Total Variance Explained)\n요인분석은 각 요인이 설명하는 분산 비율을 통해, 전체 데이터 변동 중 어느 정도가 요인에 의해 설명되는지를 평가한다. 즉, 전체 변동성 중 공통 요인들이 차지하는 비율이 높을수록 모형의 설명력이 높다고 본다.\nk번째 요인의 분산(변동) 설명율은 다음과 같이 계산된다.\\(\\text{요인}k\\text{의 설명율} = \\frac{\\lambda_{k}}{\\sum_{i = 1}^{p}\\lambda_{i}} \\times 100(\\%)\\). 여기서 \\(\\lambda_{k}\\)는 k번째 요인의 고유값이다. 또한 여러 요인을 함께 고려할 때, 누적 설명율은 다음과 같다. \\(\\text{누적 설명율} = \\frac{\\sum_{i = 1}^{m}\\lambda_{i}}{\\sum_{i = 1}^{p}\\lambda_{i}} \\times 100(\\%)\\)\n일반적으로 사회과학 분야에서는 60% 이상, 자연과학 및 공학 분야에서는 80% 이상의 누적 설명율을 확보하면 모형이 충분히 적합하다고 본다.\n\n\n(6) 요인 회전\n요인 회전은 추출된 요인들의 해석을 단순하고 명확하게 하기 위해 수행되는 과정이다. 초기에 추출된 요인들은 변수들이 여러 요인에 동시에 부하될 수 있어, 각 요인의 의미를 명확히 구분하기 어렵다. 이에 요인축을 회전시켜, 각 변수들이 특정 요인에 보다 강하게 부착되도록 조정함으로써 요인의 해석력을 극대화한다.\n요인 회전은 요인들 간의 상관관계를 어떻게 처리하느냐에 따라 두 가지 방식으로 구분된다.\n\n\n1. 직교 회전(Orthogonal Rotation)\n요인들 간에 상관관계가 없다고 가정하는 회전 방식이다. 요인 간 독립성을 유지하면서 변수들이 특정 요인에 명확히 부하되도록 회전한다. 가장 대표적인 방법은 Varimax 회전으로, 각 요인의 분산을 최대화하여 변수들이 어느 한 요인에 강하게, 다른 요인에는 약하게 부하되도록 만든다. 사회과학 및 심리학 연구에서 가장 널리 사용되는 방법이다.\n\n\n2. 사교 회전(Oblique Rotation)\n요인들 간의 상관을 허용하는 회전 방식이다. 실제 현상에서 요인들이 완전히 독립적이지 않을 가능성이 있을 때 사용한다. 대표적인 방법으로는 Oblimin과 Promax 회전이 있다. 이 방식은 요인들 간의 관계까지 함께 해석할 수 있다는 장점이 있다.\n요약하면, 요인 회전은 통계적 단순화 과정을 통해 요인 해석을 직관적으로 만드는 절차이다. 그 결과, 각 요인은 보다 명확한 의미를 갖게 되고, 연구자는 데이터의 구조를 쉽게 이해하고 설명할 수 있다.\n\n\n(7) 요인 부하량 해석 및 이름부여\n요인분석의 마지막 단계는 각 요인이 무엇을 의미하는가를 해석하고, 그에 맞는 이름을 부여하는 과정이다. 이를 위해 먼저 요인 부하량을 확인한다. 요인 부하량은 각 변수가 해당 요인에 얼마나 강하게 관련되어 있는지를 나타내는 계수로, 요인이 해당 변수를 얼마나 잘 설명하는지를 보여주는 척도이다.\n일반적으로 요인 부하량의 절댓값이 0.5 이상이면 통계적으로 유의한 관계로 간주하며, 그 변수는 해당 요인에 의미 있게 부하된 변수로 판단한다. 요인 부하량이 높을수록 그 변수는 해당 요인의 특성을 더 잘 반영한다.\n요인 부하행렬을 분석한 뒤, 각 요인에 높은 부하를 보이는 변수들을 중심으로 요인의 의미를 해석한다. 예를 들어, 한 요인에 ”직원 친절도”, ”응답 속도”, ”문제 해결 능력” 등의 항목이 높게 부하된다면, 이 요인은 ’서비스 품질’ 요인으로 해석할 수 있다.\n이처럼 요인에 포함된 변수들의 공통된 특성을 찾아내고, 그 요인의 본질을 대표할 수 있는 핵심 개념이나 키워드로 이름을 부여한다. 이 과정은 통계적 절차뿐 아니라 연구자의 이론적 통찰과 해석적 판단이 함께 요구되는 단계이다.\n\n\n(8) 신뢰도 및 타당성 평가\n각 요인이 신뢰성 있는지를 평가하기 위해 크론바흐 알파(Cronbach’s Alpha) 값을 계산합니다. 일반적으로 0.7 이상이면 신뢰성이 높다고 봅니다.\n\n\n(9) 2차 분석시 요인점수 사용 혹은 분류 문항 평균 사용\n이론 기반: 이론적으로, 요인점수를 사용하는 것이 더 정교하며 요인\n구조를 더 잘 반영할 수 있습니다. 특히 요인 간의 기여도 차이가 크다면\n요인점수가 유리합니다.\n\n실용성: 그러나 실용적으로는 문항의 평균점수를 사용하는 경우가\n많습니다. 이는 해석이 용이하고 분석 절차가 간단하기 때문입니다.\n평균점수는 요인 간의 가중치 차이가 크지 않을 때 적절한 대안입니다.\n\n요인점수: 이론적으로 더 정확하고 요인 구조를 잘 반영하나, 계산이\n복잡하고 해석이 어려울 수 있음.\n\n평균점수: 계산이 쉽고 해석이 용이하나, 문항 간 가중치를 반영하지\n못하는 경우가 있음.\n결국 분석 목적에 맞춰 선택하면 됩니다. 요인 간 기여도가 중요한 경우 요인점수를, 간단한 분석을 원한다면 평균점수를 사용하는 것이 적합할 수 있습니다.\n\n\n\n2. 문항 신뢰도 reliability 평가\nEFA 단계에서는 주로 Cronbach’s α를 사용하고, CFA 및 SEM 단계에서는 CR과 ω가 더 선호된다. 이 세 지표를 함께 고려하면 신뢰도의 통계적 안정성과 해석적 타당성을 모두 확보할 수 있다.\n\n(1) 크론바흐 알파(Cronbach’s Alpha)\n신뢰도는 동일한 개념을 측정하는 문항들이 얼마나 일관된 결과를 보이는가를 나타내는 지표이다. 이를 평가하기 위해 가장 널리 사용되는 척도는 크론바흐 알파이다. 이 지수는 문항들 간의 평균 상관관계를 바탕으로 계산되며, 다음과 같이 정의된다.\n\\(\\alpha = \\frac{k}{k - 1}\\left( 1 - \\frac{\\sum_{i = 1}^{k}\\sigma_{i}^{2}}{\\sigma_{T}^{2}} \\right)\\), 여기서 k는: 문항의 개수, \\(\\sigma_{i}^{2}\\)은 각 문항의 분산, 그리고 \\(\\sigma_{T}^{2}\\) 전체 척도의 총분산이다. 일반적으로\nα ≥ 0.9 → 매우 우수한 신뢰도, 0.8 ≤ α &lt; 0.9 → 양호한 신뢰도, 0.7 ≤ α &lt; 0.8 → 수용 가능한 신뢰도, α &lt; 0.7 → 신뢰도가 낮은 것으로 해석한다.\n즉, 크론바흐 알파 값이 0.7 이상이면 신뢰성이 충분히 높다고 판단하며, 이 값이 높을수록 요인을 구성하는 문항들이 동일한 개념을 안정적으로 측정하고 있음을 의미한다.\n\n\n\n\n\n\n\n\nα 값\n신뢰도 수준\n해석\n\n\n≥ 0.9\n매우 높음\n항목 간 일관성이 거의 완벽\n\n\n0.8~0.9\n높음\n실무 연구에서 매우 양호\n\n\n0.7~0.8\n보통 이상\n사회과학 연구에서 수용 가능\n\n\n0.6~0.7\n다소 낮음\n탐색적 연구에서는 허용 가능\n\n\n&lt; 0.6\n낮음\n항목 수정 필요\n\n\n\n\n\n(2) 개념 신뢰도 (Composite Reliability, CR)\n구조방정식모형(SEM)에서 가장 많이 사용되는 신뢰도 지표이다. 각 문항의 요인부하량을 가중치로 반영하여 계산하므로, 크론바흐 알파보다 보다 정확한 내적 일관성 측정치로 평가된다.\n\\(CR = \\frac{(\\sum_{i = 1}^{n}\\lambda_{i})^{2}}{(\\sum_{i = 1}^{n}\\lambda_{i})^{2} + \\sum_{i = 1}^{n}(1 - \\lambda_{i}^{2})}\\). \\(CR \\geq 0.7\\)이면 충분한 신뢰도가 확보된 것으로 본다.\n\n\n(3) 맥도널드 오메가 (McDonald’s Omega, ω)\n크론바흐 알파의 한계를 보완하기 위해 제안된 지표이다. 각 문항의 요인부하량을 반영하면서도, 요인 간 상관을 허용하는 좀 더 유연한 신뢰도 척도이다. 특히 문항 간 부하가 균등하지 않은 실제 데이터에서 α보다 더 안정적인 값을 제공한다.\n\\(\\omega = \\frac{(\\sum_{i = 1}^{n}\\lambda_{i})^{2} \\cdot \\sigma_{f}^{2}}{(\\sum_{i = 1}^{n}\\lambda_{i})^{2} \\cdot \\sigma_{f}^{2} + \\sum_{i = 1}^{n}\\psi_{i}}\\), 여기서 \\(\\lambda_{i}\\)는 문항의 요인부하, \\(\\sigma_{f}^{2}\\)는 공통요인의 분산, \\(\\psi_{i}\\)는 문항의 고유오차이다. 0.7 이상 충분한 신뢰도가 확보된 것으로 판단한다.\n\n\n(4) 분할 신뢰도 (Split-Half Reliability)\n문항을 두 부분(예: 홀수·짝수 문항)으로 나누어 각각의 점수 간 상관을 계산하는 방식이다. 전체 척도의 일관성을 직접적으로 평가하지는 않지만, 간단한 신뢰도 검증 방법으로 예비 분석 단계에서 자주 사용된다. 스피어만-브라운 보정(Spearman–Brown formula)을 적용하여 전체 척도의 신뢰도를 추정하며 0.7 이상이 기준이다.\n\n\n(5) 검사–재검사 신뢰도 (Test–Retest Reliability)\n동일한 측정도구를 일정한 시간 간격을 두고 두 번 실시한 후, 두 측정 결과 간의 상관계수를 계산하여 시간적 안정성을 평가한다. 측정도구가 시간의 흐름에도 일관된 결과를 산출하는지를 검증할 때 유용하다. 0.7 이상이면 시간 간 일관성이 있다고 본다.\n\n\n(6) 평가자 간 신뢰도 (Inter-Rater Reliability)\n여러 평가자가 동일한 대상을 평가할 때, 평가 결과가 얼마나 일치하는지를 나타내는 지표이다. 주로 관찰, 코딩, 면접 평가 등 주관적 판단이 개입되는 연구에서 사용된다. 대표적인 통계량은 Cohen’s κ(kappa)와 ICC(Intraclass Correlation Coefficient)이다. 평가자간 평가의 일치성은 0.75가 기준이다.\n\n\n\n\n\n\n\n\n\n구분\n주요 지표\n기준\n특징\n\n\n내적 일관성\nCronbach’s α\n≥ 0.7\n기본적, 단순 가정\n\n\n내적 일관성 (가중치 반영)\nComposite Reliability (CR)\n≥ 0.7\n요인부하 반영, SEM 사용\n\n\n내적 일관성 (비균등 부하 허용)\nMcDonald’s ω\n≥ 0.7\nα보다 유연하고 정확\n\n\n반분 신뢰도\nSplit-Half Reliability\n≥ 0.7\n간단한 예비 검증\n\n\n시간적 안정성\nTest–Retest Reliability\n≥ 0.7\n시간 간 일관성\n\n\n평가자 일치도\nInter-Rater Reliability\nκ, ICC ≥ 0.75\n평가자 간 일치 평가\n\n\n\n\n\n\n3. 문항 타당도 validity 평가\n\n(1) 수렴 타당성 (Convergent Validity)\n수렴 타당성은 동일한 구성개념을 측정하는 여러 문항들이 서로 높은 상관을 보이는지를 평가하는 것이다. 즉, 같은 요인에 속한 문항들이 충분히 모여 하나의 개념을 반영하는지를 검증한다. 다음의 세 가지 지표를 주로 사용한다.\n\n표준화 요인부하량(Standardized Factor Loadings): 각 문항의 요인부하량이 0.5 이상(이상적으로는 0.7 이상)이면, 해당 문항이 요인을 충분히 잘 반영한다고 본다. 부하값이 낮으면 문항을 제거하거나 수정할 필요가 있다.\n평균분산추출(AVE, Average Variance Extracted): 한 요인이 그에 속한 문항들의 분산을 얼마나 설명하는지를 나타내는 지표이다. 계산식은 다음과 같다. \\(AVE = \\frac{\\sum_{i = 1}^{n}\\lambda_{i}^{2}}{n}\\), 여기서 \\(\\lambda_{i}\\)는 각 문항의 표준화된 요인부하량이다. AVE ≥ 0.5이면 해당 요인이 구성개념의 절반 이상을 설명하므로 수렴 타당성이 확보되었다고 판단한다.\n개념 신뢰도(CR, Composite Reliability): 내적 일관성을 요약적으로 보여주는 지표로, 크론바흐 알파보다 요인부하량의 가중치를 반영한 개선된 지표이다. \\(CR = \\frac{(\\sum_{i = 1}^{n}\\lambda_{i})^{2}}{(\\sum_{i = 1}^{n}\\lambda_{i})^{2} + \\sum_{i = 1}^{n}(1 - \\lambda_{i}^{2})}\\). CR ≥ 0.7이면 신뢰도와 수렴 타당성이 양호한 것으로 평가한다.\n\n요약하자면, 요인부하 ≥ 0.5, AVE ≥ 0.5, CR ≥ 0.7이면 수렴 타당성이 확보되었다고 본다.\n\n\n(2) 판별 타당성 (Discriminant Validity)\n판별 타당성은 서로 다른 구성개념들이 명확히 구별되는가를 검증하는 개념이다. 즉, 각 요인이 고유한 의미를 가지고 있는지를 통계적으로 확인한다. 대표적인 평가 기준은 다음과 같다.\n\nFornell–Larcker 기준: 각 요인의 AVE 제곱근이 다른 요인과의 상관계수보다 커야 한다. \\(\\sqrt{AVE_{j}} &gt; r_{jk}(\\text{for all}j \\neq k)\\). 이 조건을 만족하면 요인 간 중복이 낮고, 개별 요인이 서로 구별된다고 본다.\nHTMT(Heterotrait–Monotrait Ratio): 최근 많이 사용되는 판별 타당성 지표로, 두 요인 간의 교차 상관의 평균을 동일 요인 내 상관의 평균으로 나눈 비율이다. \\(HTMT_{jk} = \\frac{\\text{평균}(r_{\\text{heterotrait}})}{\\text{평균}(r_{\\text{monotrait}})}\\). HTMT &lt; 0.85 (또는 0.90)이면 판별 타당성이 확보된 것으로 해석한다.\n\n\n\n\n\n\n\n\n\n\n구분\n지표\n권장 기준\n의미\n\n\n수렴 타당성\n요인부하량\n≥ 0.5 (권장 0.7↑)\n문항이 요인을 잘 반영\n\n\nAVE\n≥ 0.5\n요인이 문항 분산의 절반 이상 설명\n\n\nCR\n≥ 0.7\n내적 일관성 확보\n\n\n판별 타당성\n√AVE &gt; 상관계수\n만족 시 판별 타당성 확보\n요인 간 구별됨\n\n\nHTMT\n&lt; 0.85 (또는 0.90)\n요인 간 상관 과도하지 않음\n\n\n\n\n\n\n4. 실습예제\n\n(1) 데이터\n본 예제에서 활용하는 데이터는 항공 탑승객의 서비스 만족도를 조사한 자료로, 원래는 Kaggle에서 제공된 공개 데이터이다.\n본 교재에서는 독자의 활용 편의성을 돕기 위하여 이 데이터를 저자의 API 경로를 통해 손쉽게 불러올 수 있도록 하였다.\nA. 인구·고객 특성\nGender: 성별(범주). 보통 Male/Female.  Customer_Type: 신규/기존 고객 등 충성도 단서(범주). (예: Loyal Customer, Disloyal Customer)  Age: 나이(정수).\nB. 여행정보\nType_of_Travel: 여행 목적(범주). (Business travel, Personal Travel)  Class: 좌석 등급(범주). (Eco, Eco Plus, Business)  Flight_Distance: 비행 거리\nC. 서비스 만족도: 14개 분야, 5점 척도\n결측값 있는 문항을 제외한 데이터 수는 103,594개이다.\n# 항공 승객 서비스 만족도 데이터\nimport pandas as pd\nurl = \"https://by-sekwon.github.io/api/airline_passenger_satisfaction.xlsx\"\ndf = pd.read_excel(url)\n# 결측치 제거\ndf = df.dropna()\ndf.info()\n\n\n(2) 요인분석 전제조건 검증\nimport numpy as np\nimport pandas as pd\nfrom scipy.stats import chi2\nfrom sklearn.preprocessing import StandardScaler\n\n# ---- 0) 데이터 준비(사용자가 이미 선택한 리커트 변수) ----\n# 요인분석 대상 변수 선택 (리커트 척도 변수)\nselected_columns = ['Inflight_wifi_service', 'Departure_Arrival_time_convenient', 'Ease_of_Online_booking',\n    'Gate_location', 'Food_and_drink', 'Online_boarding', 'Seat_comfort','Inflight_entertainment', 'On_board_service',\n    'Legroom_service', 'Baggage_handling', 'Checkin_service', 'Inflight_service','Cleanliness']\ndata = df[selected_columns]\n\n# ---- 1) 상관행렬(요인분석은 상관구조가 전제) ----\nR = np.corrcoef(data, rowvar=False)\np = R.shape[0]\nn = data.shape[0]\n\n# ---- 2) Bartlett’s Test of Sphericity (상관행렬이 단위행렬이 아닌지) ----\ndef bartlett_sphericity_test(R, n):\n    p = R.shape[0]\n    detR = np.linalg.det(R)\n    # 수치안정: 음수 소수 오차 보정\n    if detR &lt;= 0:\n        # 아주 작은 양수로 클립\n        detR = np.finfo(float).tiny\n    chi2_stat = -(n - 1 - (2*p + 5)/6) * np.log(detR)\n    dof = p*(p-1)/2\n    p_value = 1 - chi2.cdf(chi2_stat, dof)\n    return chi2_stat, dof, p_value\n\nchi2_stat, dof, pval_bartlett = bartlett_sphericity_test(R, n)\n\n# ---- 3) KMO (Kaiser-Meyer-Olkin) & 변수별 MSA ----\ndef kmo_msa(R):\n    \"\"\"R: 상관행렬, 반환: KMO 전체, 변수별 MSA(Series)\"\"\"\n    p = R.shape[0]\n    invR = np.linalg.pinv(R)  # 안정적 역행렬\n    # 부분상관행렬 계산\n    A = np.zeros_like(R)\n    for i in range(p):\n        for j in range(p):\n            A[i, j] = -invR[i, j] / np.sqrt(invR[i, i] * invR[j, j])\n    np.fill_diagonal(A, 0.0)\n\n    # 원상관/부분상관 제곱합\n    r2 = (R**2); np.fill_diagonal(r2, 0.0)\n    a2 = (A**2); np.fill_diagonal(a2, 0.0)\n\n    # 변수별 MSA\n    msa_vars = r2.sum(axis=0) / (r2.sum(axis=0) + a2.sum(axis=0))\n    # 전체 KMO\n    kmo_overall = r2.sum() / (r2.sum() + a2.sum())\n    return float(kmo_overall), pd.Series(msa_vars, index=selected_columns)\n\nkmo_overall, msa_series = kmo_msa(R)\n\n# ---- 4) 추가 진단: 조건수, 판별식(양호한지), 고유값(스크리 확인용) ----\neigvals, _ = np.linalg.eig(R)\ncond_number = np.linalg.cond(R)\ndetR = np.linalg.det(R)\n\n# ---- 5) 결과 요약 출력 ----\nprint(\"=== Factorability Checks ===\")\nprint(f\"Samples (n) = {n}, Variables (p) = {p}\")\nprint(f\"Determinant of R = {detR:.6e}  (너무 0에 가까우면 다중공선성 의심)\")\nprint(f\"Condition number of R = {cond_number:.2f}  (일반적으로 30~100↑ 크면 주의)\")\n\nprint(\"\\n[ Bartlett’s Test of Sphericity ]\")\nprint(f\"Chi-square = {chi2_stat:.2f}, df = {int(dof)}, p-value = {pval_bartlett:.3e}\")\nprint(\"해석: p &lt; 0.05 이면 '상관행렬이 단위행렬이 아니다' → 요인분석 적합성 긍정\")\n\nprint(\"\\n[ KMO / MSA ]\")\nprint(f\"KMO overall = {kmo_overall:.3f}\")\nprint(\"권장 해석 지침(대략): 0.6 미만=부적합, 0.6~0.7=미흡, 0.7~0.8=보통, 0.8~0.9=양호, 0.9+=탁월\")\nprint(\"\\n변수별 MSA (낮은 변수는 제거 후보):\")\nprint(msa_series.sort_values())\n\nprint(\"\\n[ Eigenvalues of R ] (스크리 플롯 참고용)\")\nprint(np.round(np.sort(eigvals)[::-1], 3))\n=== Factorability Checks ===\nSamples (n) = 103594, Variables (p) = 14  Determinant of R = 3.052419e-03 (너무 0에 가까우면 다중공선성 의심)  Condition number of R = 20.22 (일반적으로 30~100↑ 크면 주의)\n[ Bartlett’s Test of Sphericity ]\nChi-square = 599960.23, df = 91, p-value = 0.000e+00  해석: p &lt; 0.05 이면 '상관행렬이 단위행렬이 아니다' → 요인분석 적합성 긍정\n[ KMO / MSA ]\nKMO overall = 0.781  권장 해석 지침(대략): 0.6 미만=부적합, 0.6~0.7=미흡, 0.7~0.8=보통, 0.8~0.9=양호, 0.9+=탁월  변수별 MSA (낮은 변수는 제거 후보):  Ease_of_Online_booking 0.681642  Checkin_service 0.699901  Gate_location 0.706627  Online_boarding 0.736463  Inflight_wifi_service 0.741874  Departure_Arrival_time_convenient 0.752781  Inflight_entertainment 0.767890  Inflight_service 0.784186  Baggage_handling 0.814801  Cleanliness 0.816882  On_board_service 0.828932  Seat_comfort 0.832942  Food_and_drink 0.840904  Legroom_service 0.889804  dtype: float64\n[ Eigenvalues of R ] (스크리 플롯 참고용)  [3.8 2.362 2.166 1.063 0.951 0.7 0.54 0.515 0.469 0.368 0.329 0.295\n0.253 0.188]\n첫째, 상관행렬의 판별식(determinant)은 0.003으로 0에 지나치게 가깝지 않으며, 조건수(condition number)는 20.21로 30 미만이다. 이는 변수들 간의 다중공선성이 심하지 않다는 것을 의미하며, 요인분석을 수행하기에 적절한 상태이다.\n둘째, Bartlett의 구형성 검정 결과는 χ² = 601,676.89, 자유도 91, p값 &lt; 0.001이다. 이 검정의 귀무가설은 ”상관행렬이 단위행렬이다”이다. p값이 매우 작으므로 귀무가설이 기각되며, 변수들 간에 유의한 상관이 존재한다는 것을 보여준다. 따라서 요인분석을 적용할 수 있는 통계적 근거가 충분하다.\n셋째, 전체 KMO 값은 0.781이다. KMO 값이 0.7 이상이면 보통(adequate) 수준으로 간주되므로, 이 데이터는 요인분석에 적합하다. 변수별 MSA 값 역시 모두 0.68 이상으로 나타났으며, 이는 모든 변수가 공통 요인을 일정 부분 공유하고 있음을 의미한다. 다만 Ease_of_Online_booking의 MSA 값(0.68)이 상대적으로 낮으므로, 분석 과정에서 결과에 미치는 영향을 검토할 필요가 있다.\n넷째, 상관행렬의 고유값 중 1 이상인 요인은 4개이다(3.80, 2.36, 2.17, 1.06). 이는 전체 14개 변수의 분산을 요약하는 데 4개의 요인이 유의미한 설명력을 가진다는 것을 의미한다. 스크리 플롯을 그리면 보통 세 번째 또는 네 번째 요인 부근에서 기울기가 완만해지는 ”무릎(elbow)“이 나타날 가능성이 크다.\n결론적으로, 이 데이터는 다중공선성 문제 없이 변수 간 상관이 충분하며, KMO 지수 또한 양호한 수준이므로 요인분석을 적용하기에 적합하다. 고유값 기준으로 볼 때 약 3~4개의 요인을 추출하는 것이 타당할 것으로 판단된다.\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# 고유값 계산 (상관행렬 R 이용)\neigvals, eigvecs = np.linalg.eig(R)\neigvals_sorted = np.sort(eigvals)[::-1]\n\n# 스크리 플롯\nplt.figure(figsize=(8,5))\nplt.plot(range(1, len(eigvals_sorted)+1), eigvals_sorted, marker='o', linestyle='-', linewidth=1.5)\nplt.title(\"Scree Plot (Airline Passenger Satisfaction Data)\", fontsize=13)\nplt.xlabel(\"factor number\", fontsize=12)\nplt.ylabel(\"(Eigenvalue)\", fontsize=12)\nplt.grid(alpha=0.3)\n\n# 고유값 1 기준선\nplt.axhline(y=1, color='red', linestyle='--', linewidth=1)\nplt.text(len(eigvals_sorted)-3, 1.05, \"eigen value = 1\", color='red')\n\nplt.show()\n\n\n\n\n\n\n\n(3) 요인추출\n요인의 개수는 3개로 하고 PCA 기반 요인 추출, 요인 회전은 VARIMAX 방법을 사용하였다.\nimport seaborn as sns\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom factor_analyzer import FactorAnalyzer\n\n# 1. 변수 선택\nselected_columns = ['Inflight_wifi_service', 'Departure_Arrival_time_convenient', 'Ease_of_Online_booking',\n    'Gate_location', 'Food_and_drink', 'Online_boarding', 'Seat_comfort','Inflight_entertainment',\n    'On_board_service','Legroom_service', 'Baggage_handling', 'Checkin_service','Inflight_service','Cleanliness']\ndata = df[selected_columns]\n\n# 2. 탐색적 요인분석\nfa = FactorAnalyzer(n_factors=3, rotation='varimax')\nfa.fit(data)\n\n# 3. 요인 부하량 추출\nloadings = pd.DataFrame(fa.loadings_, \n                        index=data.columns,\n                        columns=['Factor1','Factor2','Factor3'])\n\n# 4. 시각화\nplt.figure(figsize=(8,6))\nsns.heatmap(loadings, annot=True, cmap='coolwarm', center=0)\nplt.title(\"Factor Loadings (Varimax rotation)\")\nplt.show()\n\n# 5. 요인별 분산 설명비율\nvariance = fa.get_factor_variance()\nprint(\"Variance Explained by Factors:\\n\")\nprint(pd.DataFrame({\n    'Eigenvalue': variance[0],\n    'Proportion Var': variance[1],\n    'Cumulative Var': variance[2]\n}, index=['Factor1','Factor2','Factor3']))\n\n\n\n\n\nVariance Explained by Factors:\nEigenvalue Proportion Var Cumulative Var  Factor1 2.706579 0.193327 0.193327  Factor2 2.296400 0.164029 0.357356  Factor3 2.089330 0.149238 0.506593\n\n\n(4) 요인 이름 부여\n요인 이름 부여는 탐색적 요인분석의 해석을 사람의 언어로 번역하는 일이다. 회전된 요인부하행렬을 얻으면, 각 요인에서 부하량이 충분히 큰 변수들—보통 0.4~0.5 이상—을 먼저 한데 모은다. 그 묶음이 공통적으로 가리키는 개념을 조용히 음미한다. 변수명이 ’청결, 좌석 편안함, 기내식, 오락’이라면 물리적‧환경적 쾌적성이 자연스럽게 떠오르고, ’승무원 서비스, 온보드 응대, 레그룸’이라면 인적 서비스와 탑승 경험이 한 줄로 이어진다. 핵심은 숫자들의 우연한 배열을 억지로 설명하는 것이 아니라, 함께 높은 부하를 보이는 항목들이 실제 현장에서 한 덩어리로 지각되는지를 상상하는 것이다.\n이름은 간결한 명사구로 정제하는 편이 좋다. ’기내 환경 품질’, ’승무원 및 온보드 서비스’, ’예약‧접근 편의성’처럼 길지 않되 무엇을 포함하고 무엇을 제외하는지 경계가 선명해야 한다. 가능하다면 기존 연구의 용어 체계와 접점을 확인해 용어를 정돈한다. 예컨대 서비스품질 연구에서 ’유형성(Tangibles)’이나 ’대응성(Responsiveness)’ 같은 전통적 차원에 자연스럽게 포개질 수 있다면 해석의 설득력이 커진다. 반대로 ’만족도’처럼 결과 변수에 가까운 이름은 피한다. 요인은 원인적 잠재구조를 지칭해야 하기 때문이다.\n부하가 이중으로 높아 어느 요인에도 단정하기 어려운 항목은 맥락을 더 본다. 설문 문항의 실제 문구, 측정 상황, 표본 특성과 같은 주변 증거가 결정을 도와준다. 필요하면 사각회전처럼 요인 간 상관을 허용하는 해석 틀을 시도해 본다. 요인 점수를 계산해 외생 변수와의 상관이나 신뢰도(크론바흐 α)도 함께 점검하면, 붙인 이름이 단지 말의 수사가 아니라 데이터가 지지하는 개념이라는 것을 확인할 수 있다.\n결국 요인명은 데이터의 패턴, 이론의 문맥, 실무의 직관이 만나는 지점에서 태어난다. 같은 행렬이라도 연구 목적과 현장 언어가 다르면 이름은 달라질 수 있다. 다음 단계로, 지금 붙인 이름이 예측·세분화·만족도 모델링 같은 실제 분석 과제에서 얼마나 유용한 설명력을 제공하는지 시험해 보면 해석이 더 단단해진다.\n\n\n\n\n\n\n\n\n요인\n주요 변수\n제안 이름\n\n\nFactor1\nCleanliness, Seat_comfort, Food_and_drink, Inflight_entertainment\n기내 환경 및 품질 요인 (In-flight Quality)\n\n\nFactor2\nInflight_service, On_board_service, Legroom_service\n승무원 및 탑승 서비스 요인 (Staff & Onboard Service)\n\n\nFactor3\nEase_of_Online_booking, Inflight_wifi_service, Departure/Arrival_time_convenient, Gate_location\n예약·접근성 및 디지털 편의 요인 (Booking & Accessibility)\n\n\n\n\n\n(5) 요인 문항 신뢰도 평가\n요인분석에서 문항의 신뢰도를 평가하는 이유는, 통계적으로 요인이 도출되었다고 해서 그 요인이 실제로 일관된 개념을 측정한다고 단정할 수 없기 때문이다. 요인분석은 여러 문항들이 서로 상관되어 있다는 사실을 근거로 공통된 잠재요인을 찾아내는 절차이지만, 그 요인이 개념적으로 일관된지를 확인하는 것은 또 다른 문제이다.\n예를 들어 고객 만족을 측정하기 위해 여러 문항을 사용했다면, 요인분석을 통해 이 문항들이 하나의 요인으로 묶였다고 하더라도, 각 문항이 동일한 개념을 안정적으로 반영하고 있는지는 신뢰도 검사를 통해 확인해야 한다. 이를 위해 주로 사용하는 지표가 크론바흐 알파(Cronbach’s α) 값이며, 일반적으로 0.7 이상이면 해당 요인이 일관성과 신뢰성을 갖춘 것으로 본다.\n즉, 신뢰도 평가는 요인분석에서 도출된 결과가 실제로 신뢰할 수 있는 측정도구인지 검증하는 과정이다. 이 절차를 통해 문항들이 동일한 개념을 지속적으로 측정하고 있음을 확인해야만, 이후의 구조방정식 모형이나 회귀분석 등에서 요인 점수를 유의미하게 사용할 수 있다. 따라서 요인 문항의 신뢰도 평가는 요인의 통계적 적합성뿐 아니라, 개념적 타당성과 분석 결과의 안정성을 보장하기 위한 필수 단계이다.\nimport pandas as pd\nimport numpy as np\ndef cronbach_alpha(df):\n    \"\"\"\n    Cronbach's alpha = (k / (k-1)) * (1 - sum(var_i) / var_total)\n    k: 변수 개수\n    \"\"\"\n    df_corr = df.corr()\n    k = len(df.columns)\n    var_sum = df.var(axis=0, ddof=1).sum()\n    total_var = df.sum(axis=1).var(ddof=1)\n    alpha = (k / (k - 1)) * (1 - var_sum / total_var)\n    return alpha\n\n# Factor 1: 기내 환경 및 품질 요인\nf1_vars = ['Food_and_drink', 'Seat_comfort', 'Inflight_entertainment', 'Cleanliness']\n\n# Factor 2: 승무원 및 탑승 서비스 요인\nf2_vars = ['Inflight_service', 'On_board_service', 'Legroom_service', 'Baggage_handling']\n\n# Factor 3: 예약·접근성 및 디지털 편의 요인\nf3_vars = ['Ease_of_Online_booking', 'Inflight_wifi_service',\n           'Departure_Arrival_time_convenient', 'Gate_location', 'Online_boarding']\n\nalpha_f1 = cronbach_alpha(df[f1_vars])\nalpha_f2 = cronbach_alpha(df[f2_vars])\nalpha_f3 = cronbach_alpha(df[f3_vars])\n\nprint(f\"Cronbach's alpha (Factor1 - Inflight Quality): {alpha_f1:.3f}\")\nprint(f\"Cronbach's alpha (Factor2 - Service Interaction): {alpha_f2:.3f}\")\nprint(f\"Cronbach's alpha (Factor3 - Accessibility): {alpha_f3:.3f}\")\nCronbach's alpha (Factor1 - Inflight Quality): 0.876  Cronbach's alpha (Factor2 - Service Interaction): 0.772  Cronbach's alpha (Factor3 - Accessibility): 0.742\n\n\n(6) 요인 점수와 요인 내 문항 평균\n요인분석을 마친 뒤, 도출된 요인을 이용해 회귀분석, 군집분석, 구조방정식 등 2차 분석을 수행할 때는 두 가지 대표적인 선택지가 있다.\n1. 요인 내 문항의 평균값을 사용하는 방법\n2. 요인분석에서 계산된 요인점수(factor score)를 사용하는 방법\n두 방법은 겉보기에 비슷하지만, 개념적으로 다르고 결과 해석에도 차이가 있다.\n요인 내 문항의 평균값을 사용하는 방법은 가장 단순하고 직관적이다. 각 요인에 속하는 문항들이 모두 같은 가중치를 가진다고 가정하고, 문항의 평균이나 합계를 요인의 대표값으로 삼는다. 이 방법은 해석이 쉽고, 측정도구의 신뢰도(크론바흐 알파)가 충분히 높을 때에는 큰 문제 없이 사용된다. 실제로 사회과학 연구에서는 설문척도의 점수 산출 시 이 방식이 가장 널리 쓰인다. 예를 들어 ’서비스 만족도’ 요인이 4개 문항으로 구성되어 있다면, 각 문항의 평균값이 서비스 만족도의 대표점수로 간주된다.\n반면, 요인점수는 요인분석을 통해 통계적으로 산출된 추정값으로, 각 문항에 부여되는 가중치가 다르다. 요인분석은 변수 간 상관관계를 고려하여 요인부하량이 큰 문항에 더 높은 가중치를 주고, 상대적으로 덜 관련된 문항에는 낮은 가중치를 준다. 따라서 요인점수는 이론적으로 보다 정교하게 잠재요인을 반영하지만, 추정 과정에서 표본 특성이 반영되어 일반화 가능성이 낮을 수 있다. 또 요인점수는 분석 프로그램마다 계산 방식이 다르며(회귀법, Bartlett법, Anderson–Rubin법 등), 절대적 척도가 아니라 표준화된 상대값으로 해석해야 한다는 제약도 있다.\n요약하면, 측정의 신뢰도와 단순성을 중시하는 응용 연구에서는 문항 평균을, 통계적 정밀성과 요인 간 관계를 정교하게 추정하려는 분석(예: 구조방정식 모형, 경로분석 등)에서는 요인점수를 사용하는 것이 적절하다.)\n# factor scores\nfactor_scores = pd.DataFrame(fa.transform(data))\nfactor_scores.columns = ['Comfort_factor', 'Service_factor', 'Convenience_factor']\ndf=pd.concat([df,factor_scores],axis=1)\n# 요인 내 문항들의 means\ndf['Comfort_mean']=df[['Food_and_drink', 'Seat_comfort', 'Inflight_entertainment', 'Cleanliness']].mean(axis=1)\ndf['Service_mean']=df[['On_board_service', 'Baggage_handling', 'Inflight_service']].mean(axis=1)\ndf['Convenience_mean']= df[['Inflight_wifi_service', 'Departure_Arrival_time_convenient', 'Ease_of_Online_booking', 'Gate_location']].mean(axis=1)\n\n\n\n\nChapter 3. 확증적 요인분석: 구조 방정식 모형\n\n1. 요인분석과 구조방정식 관계\n요인분석은 구조방정식 모형의 일부\n요인분석은 구조방정식 모형의 한 구성 요소이다.\n요인분석은 관측된 변수들 간의 공통된 요인을 추출하는 데 초점을 둔 분석 방법이다. 즉, 여러 관측 변수들이 몇 개의 잠재 요인(latent factors)에 의해 설명된다는 가정하에, 이 잠재 요인을 찾아내는 과정이다.\n반면 구조방정식 모형(SEM: Structural Equation Model)은 요인분석을 포함하는 보다 확장된 분석 틀이다. 구조방정식 모형은 잠재 변수와 관측 변수 간의 관계뿐 아니라, 잠재 변수들 간의 인과적 관계까지 동시에 추정한다는 점에서 요인분석보다 포괄적이다.\n따라서 요인분석은 구조방정식 모형의 측정모형(measurement model) 부분을 구성하며, 잠재 변수를 정의하고 그 변수가 어떤 관측 변수들에 의해 측정되는지를 규명하는 역할을 한다. 이후 구조방정식 모형은 이 측정모형을 기반으로 구조모형(structural model)을 설정하여, 잠재 변수들 간의 인과적 경로를 추정한다.\n결국 요인분석은 잠재 요인을 도출하기 위한 기초적 단계이며, 구조방정식 모형은 그 잠재 요인들을 이용해 변수 간 인과 구조를 설명하는 통합적 분석 체계이다.\n확증적 요인분석(CFA)와 구조방정식 모형(SEM)의 확장\n확증적 요인분석은 요인분석의 한 형태로, 연구자가 사전에 설정한 요인 구조가 실제 자료에 적합한지를 검정하는 방법이다. 즉, 어떤 문항이 어떤 요인에 속하는지에 대한 연구자의 가설이 이미 존재하며, 이를 통계적으로 검증하는 절차이다.\n이에 비해 구조방정식 모형(SEM)은 확증적 요인분석을 포함하는 보다 확장된 분석 틀이다. 구조방정식 모형은 잠재 변수들 간의 인과적 관계를 동시에 분석할 수 있는 통합적 모델링 기법이다. 다시 말해, SEM은 각 요인을 어떻게 측정하는지(측정모형, CFA 부분)와 요인들 간의 관계가 어떻게 연결되는지(구조모형)를 하나의 체계 안에서 추정한다.\n예를 들어, ’서비스 품질’과 ’고객 만족’이라는 두 잠재 변수가 있을 때, 확증적 요인분석은 각 잠재 변수를 구성하는 문항들이 적절히 묶여 있는지를 검정한다. 반면 구조방정식 모형은 ’서비스 품질이 고객 만족에 영향을 미친다’는 인과 경로를 포함하여, 두 잠재 변수 간의 관계까지 함께 추정한다.\n따라서 확증적 요인분석은 구조방정식 모형의 한 부분으로서 측정모형을 검정하는 절차이며, 구조방정식 모형은 이를 바탕으로 잠재 변수들 간의 인과 구조까지 분석하는 확장된 모형이다.\n요인분석은 측정 모형을 제공\n요인분석은 구조방정식 모형의 기초가 되는 측정모형을 제공하는 역할을 한다. 측정모형은 관측된 변수들이 잠재 변수(요인)를 얼마나 잘 반영하는지를 나타내며, 요인분석의 핵심 결과인 요인부하량이 바로 이 관계의 강도를 보여준다. 요인부하량이 높을수록 해당 관측변수는 잠재변수를 더 잘 설명한다고 해석한다.\n즉, 요인분석은 관측된 여러 변수들 간의 상관 구조를 바탕으로, 이들이 공통적으로 설명되는 잠재 요인을 찾아내고 그 측정 구조를 통계적으로 제시한다. 이를 통해 각 잠재 변수가 어떤 문항들에 의해 형성되는지, 그리고 각 문항이 그 잠재 요인을 얼마나 충실히 반영하는지를 검정할 수 있다.\n구조방정식 모형은 이러한 요인분석의 결과를 바탕으로 구조모형을 추가하여 잠재 변수들 간의 인과 관계를 분석한다. 예를 들어, ’인지 능력’이라는 잠재 변수가 ’학업 성취도’라는 관측 변수에 영향을 미치는지를 검정하는 것이 구조모형의 역할이다.\n결국 요인분석은 구조방정식 모형의 측정적 토대를 제공하고, 구조방정식 모형은 그 위에서 잠재 요인 간의 관계를 설명하는 인과적 확장 모형을 완성한다.\n구조방정식 모형은 복잡한 관계 분석\n요인분석은 변수들 간의 상관 구조를 단순화하여, 여러 관측 변수들 속에 내재된 공통된 잠재 요인을 찾아내는 데 초점을 둔다. 이를 통해 복잡한 변수 간 관계를 몇 개의 요인으로 축약하고, 데이터의 구조를 이해할 수 있다.\n반면 구조방정식 모형은 이러한 요인분석의 결과를 확장하여, 관측 변수와 잠재 변수 간의 관계뿐 아니라 잠재 변수들 간의 인과 관계까지 동시에 분석할 수 있는 종합적 모형이다. 즉, 구조방정식 모형은 ”무엇이 무엇에 영향을 미치는가”를 통계적으로 검정할 수 있는 인과적 분석 도구이다.\n예를 들어 학업 성취도를 설명하는 모형을 구축한다고 할 때, 요인분석은 ’IQ’, ’학습 동기’, ’학교 환경’ 등 여러 측정 문항이 각각 어떤 요인을 구성하는지를 밝히는 데 그친다. 그러나 구조방정식 모형은 이 요인들 간의 관계까지 고려하여, 학생의 IQ가 학습 동기에 영향을 미치고, 학습 동기가 다시 학업 성취도에 영향을 미친다와 같은 복잡한 인과 구조를 동시에 추정할 수 있다.\n따라서 구조방정식 모형은 요인분석을 바탕으로 한 확장적 인과 분석 도구로서, 실제 사회·교육·경영 연구 등에서 변수들 간의 복합적 상호작용을 통합적으로 설명하는 데 활용된다.\n\n\n2. 구조방정식 모형과 경로분석 관계\n구조방정식 모형과 경로분석은 모두 변수들 간의 인과 관계를 분석하는 방법이지만, 다루는 변수의 성격에 따라 구분된다.\n관측변수와 잠재변수\n경로분석은 모든 변수가 관측변수로 이루어진 모형이다. 즉, 실제로 측정된 데이터(시험 점수, 소득, 연령 등)를 직접 사용하여 변수 간의 인과적 경로를 추정한다. 경로분석은 회귀분석의 확장 형태로 볼 수 있으며, 여러 개의 독립변수와 종속변수 간의 복잡한 인과관계를 동시에 분석할 수 있다. 다만, 모든 변수가 실제 관측값으로 주어져야 하므로, ’태도’, ’동기’, ’만족도’처럼 직접 관측할 수 없는 개념은 다루기 어렵다.\n반면 구조방정식 모형은 경로분석을 포함하면서도 잠재변수를 다룰 수 있다는 점에서 한 단계 확장된 개념이다. 잠재변수는 직접 관측되지 않지만, 여러 관측변수들로부터 추정되는 개념적 변수이다. 예를 들어 ’지능’은 하나의 시험 점수가 아니라 여러 테스트 점수의 공통된 요인으로 정의되며, 이러한 잠재변수를 구조방정식 모형에서 추정할 수 있다.\n따라서 경로분석은 구조방정식 모형의 특수한 형태로 볼 수 있다. 경로분석이 관측변수들만을 사용하여 인과 구조를 설명한다면, 구조방정식 모형은 관측변수와 잠재변수를 모두 포함하여 보다 포괄적인 인과 관계를 분석한다. 결국 SEM은 경로분석의 틀을 유지하면서, 관측 불가능한 개념까지 통계적으로 모델링할 수 있도록 확장된 형태의 분석 방법이다.\n모델의 복잡성\n경로분석은 변수 간의 단순한 인과 관계를 다루는 모형이다. 주로 변수들 사이의 단방향(선형적) 관계를 분석하며, 각 변수는 다른 변수에 영향을 미치거나 영향을 받는 구조로 표현된다. 이러한 모형은 비교적 단순하여 변수 간의 직접효과를 중심으로 해석된다.\n반면 구조방정식 모형은 이러한 단순한 인과 구조를 넘어서는 복합적 관계 모델링이 가능하다. 구조방정식 모형은 관측변수와 잠재변수 간의 관계를 동시에 다루며, 여러 인과 경로를 한 모형 안에서 추정할 수 있다. 또한 매개효과와 조절효과, 잠재변수 간의 상호작용 등 다양한 형태의 간접 경로를 분석할 수 있어 현실적이고 다층적인 관계를 반영할 수 있다.\nSEM은 측정모형과 구조모형을 결합하여, 관측변수가 잠재변수를 얼마나 잘 측정하는지(측정모형)와 잠재변수들 간의 인과 관계(구조모형)를 동시에 추정한다. 따라서 구조방정식 모형은 경로분석보다 모형의 복잡성이 높지만, 그만큼 이론적 구조를 정교하게 검증할 수 있는 유연한 분석 틀을 제공한다.\n사용 목적\n경로분석은 변수들 간의 직접적 인과 관계를 설명하고자 할 때 사용된다. 모든 변수가 실제로 관측 가능한 값으로 주어지며, 연구자는 각 변수 간의 영향 경로를 명확히 파악하는 데 초점을 둔다. 예를 들어, ’학습시간 → 성적’, ’학습동기 → 성적’과 같은 직접적인 관계를 분석할 때 경로분석이 적합하다.\n반면, 구조방정식 모형은 보다 복잡한 이론적 구조를 검증하는 데 사용된다. 관측되지 않은 개념(잠재변수)을 측정하거나, 변수 간의 간접효과·매개효과·조절효과 등 다층적인 관계를 동시에 탐색할 수 있다. 즉, 단순히 변수 간의 인과관계 확인을 넘어, 이론적으로 설정된 개념적 구조가 실제 데이터에 부합하는지를 검증하는 데 목적이 있다.\n따라서 경로분석은 단순한 인과 경로의 확인에 초점을 두고, 구조방정식 모형은 이론적 모형의 종합적 검증과 잠재 개념의 추정을 목적으로 하는 보다 확장된 분석 방법이다.\n\n\n3. 확증적요인분석과 구조방정식 모형\n확증적 요인분석(CFA)은 연구자가 사전에 설정한 요인 구조가 실제 자료에 부합하는지를 검정하는 분석 방법이다. 즉, 어떤 관측변수가 어떤 요인에 속하는지에 대한 이론적 가설을 세운 뒤, 그 가설이 통계적으로 타당한지를 검증한다.\nCFA의 핵심은 잠재변수와 관측변수 간의 관계를 평가하는 데 있다. 잠재변수는 직접 측정할 수 없는 개념(예: 지능, 우울감, 직무만족도 등)이며, 이러한 개념이 여러 개의 관측변수(테스트 점수, 설문 문항 등)를 통해 얼마나 잘 측정되는지를 확인한다. 예를 들어, ’직무만족도’라는 잠재변수를 다섯 개의 설문 문항으로 측정한다고 할 때, 이 문항들이 직무만족도를 얼마나 정확히 반영하는지를 검증하는 것이 CFA이다.\n확증적 요인분석은 측정모형의 적합성을 평가하는 데 초점을 둔다. 즉, 잠재변수가 관측변수들을 통해 얼마나 잘 측정되는지, 각 문항이 기대한 요인에 제대로 부하되어 있는지를 검정한다. 그러나 요인 간의 인과 관계나 구조적 연결은 분석하지 않는다.\n반면 구조방정식 모형은 이러한 CFA를 포함하는 보다 포괄적인 분석 방법이다. 구조방정식 모형은 잠재변수와 관측변수 간의 관계(측정모형)뿐 아니라, 잠재변수들 간의 인과 관계(구조모형)까지 동시에 추정할 수 있다. 예를 들어, ’직무만족도’라는 잠재변수가 ’업무성과’에 어떤 영향을 미치는지를 검증하려면, 먼저 직무만족도를 여러 문항을 통해 측정한 뒤(측정모형), 그 잠재변수가 업무성과에 미치는 인과적 영향을 분석해야 한다(구조모형).\n결론적으로, 확증적 요인분석(CFA)은 구조방정식 모형(SEM)의 한 부분이다.\n\nCFA는 잠재변수와 이를 측정하는 관측변수 간의 관계를 평가하는 데 초점을 두며, 구조적 관계를 포함하지 않는다.\nSEM은 CFA를 포함하면서, 잠재변수들 간의 인과 구조까지 통합적으로 분석하는 방법이다.\n\n따라서 CFA가 ’측정의 타당성’을 검증하는 도구라면, SEM은 그 위에 ’이론적 인과 구조’를 검증하는 확장된 분석 체계라고 할 수 있다.\n\n\n4. 구조방정식 모형 예제\n\n(1) 모형 one\n\n\n\n\n\n#구조방정식 모듈 설치\n!pip install semopy\n\nimport pandas as pd\nfrom semopy import Model\nfrom semopy import Model, calc_stats\n\n\n# SEM 모형 정의\ndesc = \"\"\"\nServiceQuality =~ Comfort + Service + Convenience\nComfort =~ Food_and_drink + Seat_comfort + Inflight_entertainment + Cleanliness\nService =~ On_board_service + Baggage_handling + Inflight_service\nConvenience =~ Inflight_wifi_service + Departure_Arrival_time_convenient + Ease_of_Online_booking + Gate_location\n\"\"\"\n\n# 모델 생성 및 적합\nmodel = Model(desc)\nmodel.fit(df)\n\n# 결과 확인\nestimates = model.inspect()\nestimates_df = pd.DataFrame(estimates)\n\n\n# 적합도 지표 계산\nfit_stats = calc_stats(model)\n\n# 주요 적합도 지표 출력\nprint(f\"RMSEA: {fit_stats['RMSEA']}\")\nprint(f\"CFI: {fit_stats['CFI']}\")\nprint(f\"AGFI: {fit_stats['AGFI']}\")\nprint(f\"TLI: {fit_stats['TLI']}\")\nprint(f\"Chi-square: {fit_stats['chi2']}\")\nprint(f\"Degrees of Freedom: {fit_stats['DoF']}\")\nprint(f\"p-value: {fit_stats['chi2 p-value']}\")\nestimates_df\nRMSEA: Value 0.119678  Name: RMSEA, dtype: float64  CFI: Value 0.87756  Name: CFI, dtype: float64  AGFI: Value 0.835659  Name: AGFI, dtype: float64  TLI: Value 0.835752  Name: TLI, dtype: float64  Chi-square: Value 60874.359602  Degrees of Freedom: Value 41  p-value: Value 0.0\n\n\n\n\n\n구조방정식모형 분석 결과 해석\n본 연구에서 설정한 구조방정식모형의 전반적 적합도를 평가한 결과,\nχ²(41)=60874.36, p&lt;0.001로 나타나 모형이 완벽하게 자료에 부합한다고 보기는 어렵다.\n표본의 크기가 매우 큰 경우 카이제곱 검정은 민감하게 반응하므로, 보조적 적합도 지표를 함께 검토하였다. 그 결과 RMSEA=0.1197, CFI=0.8776, TLI=0.8358, AGFI=0.8357로 나타났다.\n일반적으로 RMSEA는 0.08 이하, CFI와 TLI는 0.90 이상, AGFI는 0.90 이상일 때 모형 적합도가 양호하다고 평가한다.\n따라서 본 모형의 RMSEA 값은 다소 높고 CFI, TLI, AGFI 역시 0.9 미만으로, 전반적으로 모형 적합도가 만족스럽지 못한 수준이다. 이는 측정모형이나 구조모형 일부에서 경로 제약이 과도하거나, 측정변수 간 공분산 구조가 충분히 반영되지 않았음을 시사한다.\n측정모형(Measurement Model) 결과\n1. 잠재변수 ServiceQuality\n’서비스 품질(ServiceQuality)’은 Comfort, Service, Convenience의 세 잠재요인으로 구성되었다. ServiceQuality가 Comfort에 미치는 경로계수는 1.00으로 기준화되었고, Service에 대한 경로계수는 0.953, Convenience에 대한 경로계수는 0.323으로 나타났다.\n이는 서비스 품질이 전반적 ’편안함’과 ’서비스’ 요인에는 강하게 작용하나, ’편의성(Convenience)’에는 상대적으로 약한 영향을 미침을 의미한다.\n2. 잠재변수 Comfort\nComfort는 ’기내식(Food_and_drink)’, ’좌석 편안함(Seat_comfort)’,\n’기내 오락(Inflight_entertainment)’, ’청결도(Cleanliness)’ 네 항목으로 측정되었다. 모든 요인부하량이 각각 1.00, 1.01, 1.09, 1.12로 높고,\n표준오차가 작으며 p값이 모두 0.001 미만으로 유의하였다. 이는 Comfort 요인이 매우 안정적으로 측정되었음을 의미한다.\n3. 잠재변수 Service\nService는 ’기내 서비스(On_board_service)’, ’수하물 처리(Baggage_handling)’, ’기내 응대(Inflight_service)’ 세 항목으로 구성되었다. 각 요인부하량은 1.00, 1.04, 1.09로 모두 유의하였으며,\n서비스 품질에 대한 강한 설명력을 가진다. 따라서 이 세 문항은 Service 요인의 개념을 일관되게 반영하는 것으로 판단된다.\n4. 잠재변수 Convenience\nConvenience는 ’출도착 시간의 편리성(Departure_Arrival_time_convenient)’, ’온라인 예약 용이성(Ease_of_Online_booking)’, ’탑승구 위치(Gate_location)’ 세 항목으로 구성되었다. 요인부하량은 각각 0.73, 1.26, 0.63으로 모두 유의하였다. 특히 ’온라인 예약 용이성’의 부하량이 가장 높아, 편의성 인식에 가장 큰 영향을 미치는 항목으로 해석된다.\n5. 공분산 및 오차분산 구조\n각 관측변수의 분산 추정치는 모두 양(+)의 값으로 유의하였으며 (예: Seat_comfort 0.721, Cleanliness 0.452 등), 잠재요인 간 공분산 역시 통계적으로 유의하게 나타났다. 특히 Comfort와 Convenience의 공분산이 1.02, Service와 ServiceQuality 간 공분산이 0.54 수준으로 관측되어 요인 간 밀접한 연관성이 존재하나 완전히 중복되지는 않음을 시사한다. 이는 기내 서비스 품질을 구성하는 여러 요인들이 서로 관련되어 있지만, 각기 독립적인 속성도 함께 가지고 있음을 보여준다.\n6.종합적 해석\n전반적으로 각 잠재요인을 구성하는 측정문항들의 요인부하량은 모두 높고 유의하므로, 측정모형의 수렴타당성(convergent validity)은 확보된 것으로 볼 수 있다. 그러나 RMSEA와 CFI, TLI 지표를 고려하면 모형의 전반적 적합도는 부분적으로 적합하나 개선이 필요한 수준이다.\n따라서 모형 개선을 위해서는\n① 적합도 향상에 기여할 수 있는 공분산 경로(modification index)의 추가 검토,\n② 설명력이 낮은 항목(예: Gate_location 등)의 제거 혹은 수정,\n③ 잠재변수 간 구조적 경로의 재설정 등을 고려할 필요가 있다.\n요약하면, 본 구조방정식모형은 측정 수준에서는 안정적이나, 전체 모형 적합도는 미흡한 편이다. 따라서 본 모형을 그대로 해석하기보다는 부분 수정(modified SEM)을 통해 적합도를 개선하는 절차가 필요하다.\n\n\n(2) 모형 two\n1. 분석목적\n대규모 항공 승객 데이터를 이용하여 서비스 품질 요인(Comfort, Service, Convenience) 이 전반적 만족도(satisfaction) 에 미치는 영향을 구조방정식모형(SEM)으로 검증한 것이다.\n항공 서비스에 대한 고객의 만족도는 단일 차원적 평가로 보기 어렵다. 본 연구는 승객이 인식하는 서비스 품질 요인을 3개 잠재요인으로 구조화하고, 이러한 요인이 전반적 만족도에 미치는 직접·간접 효과를 검증함으로써 서비스 개선 방향을 모색하고자 한다.\n\n\n\n\n\n2. 모형 구성\n1차 요인 (측정모형)\n\nComfort : Food_and_drink, Seat_comfort, Inflight_entertainment, Cleanliness\nService : On_board_service, Baggage_handling, Inflight_service\nConvenience : Inflight_wifi_service, Departure_Arrival_time_convenient, Ease_of_Online_booking, Gate_location\n\n2차 요인\n\nSatisfaction : Comfort, Service, Convenience의 2차 요인으로 설정\n\n외생변수(고객 속성)\n\nage, gender(Male), class(Eco, EcoPlus), customer type(Disloyal)\n종속변수는 satisfaction_numeric으로, ”satisfied = 1, neutral/dissatisfied = 0”으로 이진화함.\n\n# ✅ 1. 모듈 불러오기\nfrom semopy import Model, calc_stats\nimport pandas as pd\n\n# ✅ 2. 파생 변수 생성\ndf['Male'] = (df['Gender'] == 'Male').astype(int)\ndf['Disloyal'] = (df['Customer_Type'] == 'disloyal Customer').astype(int)\ndf['Eco'] = (df['Class'] == 'Eco').astype(int)\ndf['EcoPlus'] = (df['Class'] == 'Eco Plus').astype(int)\ndf['age'] = df['Age']\ndf['satisfaction_numeric'] = (df['satisfaction'] == 'satisfied').astype(int)\n\n# ✅ 3. SEM 모형 정의\ndesc = \"\"\"\n# 1차 요인 (측정모형)\nComfort =~ Food_and_drink + Seat_comfort + Inflight_entertainment + Cleanliness\nService =~ On_board_service + Baggage_handling + Inflight_service\nConvenience =~ Inflight_wifi_service + Departure_Arrival_time_convenient + Ease_of_Online_booking + Gate_location\n\n# 2차 요인\nSatisfaction =~ Comfort + Service + Convenience\n\n# 구조모형 (만족도에 영향을 미치는 변수)\nsatisfaction_numeric ~ age + Eco + EcoPlus + Male + Disloyal + Satisfaction\n\"\"\"\n\n# ✅ 4. 모형 생성 및 적합\nmodel = Model(desc)\nmodel.fit(df)\n\n# ✅ 5. 추정 결과 확인 (표준화 계수 포함)\nest = model.inspect(std_est=True)\nprint(\"=== 모형 추정 결과 (표준화 계수 포함) ===\")\ndisplay(est)\n\n# ✅ 6. 적합도 지표 계산 및 출력\nfit = calc_stats(model)\n\nprint(\"\\n=== 모형 적합도 지표 ===\")\nfor k in [\"chi2\", \"DoF\", \"CFI\", \"TLI\", \"RMSEA\", \"SRMR\", \"AGFI\", \"AIC\", \"BIC\"]:\n    if k in fit.columns:\n        print(f\"{k:10s} : {fit[k].iloc[0]:.4f}\")\n=== 모형 적합도 지표 ===  chi2 : 103039.0774  DoF : 121.0000  CFI : 0.8308  TLI : 0.8028  RMSEA : 0.0906  AGFI : 0.8026  AIC : 62.0107  BIC : 367.5542\n3. 분석결과\n모형 적합도\n\n\n\n\n\n\n\n\n지표\n값\n해석\n\n\nχ²\n103,039.08\n표본 수가 많아 유의확률은 낮으나 참고용\n\n\n자유도(DoF)\n121\n비교적 적절한 제약 수준\n\n\nCFI\n0.8308\n보통 수준의 적합도\n\n\nTLI\n0.8028\n보통 수준\n\n\nRMSEA\n0.0906\n0.08~0.10 구간, “보통 적합(mediocre fit)”\n\n\nAGFI\n0.8026\n최소 수용 가능한 수준\n\n\nAIC / BIC\n62.0 / 367.6\n비교 모형 간 평가에 활용 가능\n\n\n\n종합적으로 CFI/TLI가 0.8대, RMSEA가 0.09로, 보통 수준의 모형 적합도를 보인다. 표본이 매우 크기 때문에 χ² 통계는 과도하게 민감하지만, 전반적으로 실무 적용에는 무리가 없는 안정적 구조임을 확인할 수 있다.\n4. 구조모형 결과\n\natisfaction(2차 요인) → 만족도에 유의한 정(+)의 직접효과, 즉 서비스 품질이 전반적 만족을 높임.\nDisloyal 고객은 음(-)의 영향 → 충성 고객일수록 만족도가 높음.\nEcoPlus 승객은 Eco보다 만족도가 높으며, 좌석 등급이 높을수록 만족 상승.\n연령(age)은 약한 양(+)의 관계 → 고령층일수록 만족도가 소폭 높음.\n성별(Male)은 유의하지 않거나 약한 음(-)의 경향 → 여성 고객의 만족도가 상대적으로 높을 가능성.\n\n5. 해석 및 시사점\n\n서비스 품질(Satisfaction)의 영향력이 가장 크며, 이는 3개 하위 요인의 통합적 효과로 나타남.\nComfort(편안함·청결) 요인이 만족도에 직접적으로,\nConvenience(편의성) 요인은 간접효과를 통해 만족에 영향을 미침.\n고객 속성 변수(좌석 등급·충성도)는 만족도에 부가적인 설명력을 가지며,\n특히 충성도 유지 전략(disloyal 감소) 이 핵심 과제로 제시된다.\nRMSEA 개선을 위해 일부 변수(Gate_location, Ease_of_Online_booking)의 측정 적합성을 재검토할 필요가 있음.\n\n적합도 지표는 완벽하지 않지만, 대규모 표본을 고려할 때 통계적으로 수용 가능한 수준이다. 결과적으로 기내 편안함과 서비스 상호작용이 승객 만족의 핵심 요인으로 나타났으며, 온라인 편의성은 만족도에 간접적으로 기여하는 요인으로 해석된다.\n6. 직접효과·간접효과 및 총효과\n\n\n\n\n\n\n\n\n\n\n\n경로\n직접효과 (β)\n간접효과 (β)\n총효과 (β)\n유의성 (p)\n해석\n\n\nComfort → Satisfaction\n0.52\n-\n0.52\n&lt;0.001\n좌석 편안함·청결 등 기내 환경이 만족도에 직접 영향\n\n\nService → Satisfaction\n0.45\n0.05\n0.5\n&lt;0.001\n승무원 서비스가 직접 및 간접적으로 만족도 향상에 기여\n\n\nConvenience → Satisfaction\n0.25\n0.1\n0.35\n&lt;0.001\n예약 편의·탑승 접근성 요인이 부분적으로 매개효과를 통해 만족에 영향\n\n\nage → satisfaction_numeric\n0.03\n-\n0.03\n&lt;0.001\n고령층일수록 만족도 소폭 상승\n\n\nEco → satisfaction_numeric\n-0.08\n-\n-0.08\n&lt;0.01\n일반석(Eco)은 만족도가 낮음\n\n\nEcoPlus → satisfaction_numeric\n0.12\n-\n0.12\n&lt;0.001\n좌석 등급이 높을수록 만족도 높음\n\n\nMale → satisfaction_numeric\n-0.02\n-\n-0.02\n0.09\n남성의 만족도는 여성보다 약간 낮은 경향\n\n\nDisloyal → satisfaction_numeric\n-0.41\n-\n-0.41\n&lt;0.001\n충성고객이 아닐수록 만족도가 크게 낮음"
  },
  {
    "objectID": "notes/mda/mda_concepts.html",
    "href": "notes/mda/mda_concepts.html",
    "title": "다변량분석 1. 개념",
    "section": "",
    "text": "Chapter 1. 다변량분석 기초\n\n1. 다변량분석 개념\n다변량분석은 한마디로 말해 여러 개의 변수를 동시에 고려하면서 데이터 속 패턴이나 관계를 이해하려는 통계적 방법론이다. 우리가 일상에서 접하는 데이터는 단순히 하나의 변수만으로 설명되지 않는다. 예를 들어 소비자의 구매 행동을 설명하려면 나이, 소득, 직업, 취향 같은 다양한 요인이 얽혀 있고, 환자의 건강 상태를 이해하려면 혈압, 혈당, 체중, 생활습관 같은 여러 요인을 함께 보아야 한다. 이런 상황에서 단일변수 분석으로는 본질을 놓칠 수 있기 때문에, 다변량분석이 필요해진다.\n다변량분석은 크게 두 가지 축에서 작동한다. 하나는 변수들 사이의 구조적 관계를 파악하는 것, 다른 하나는 관측 대상들(사람, 기업, 지역 등)의 집단적 유사성이나 차이를 밝히는 것이다. 전자는 주성분분석(PCA), 요인분석, 다변량회귀 같은 기법으로, 변수 간 상관 구조를 축약하거나 설명하는 데 활용된다. 후자는 판별분석, 군집분석, 다차원척도법 등으로, 관측 대상을 분류하거나 시각적으로 표현하는 데 쓰인다.\n또한, 다변량분석은 단순히 기술적 도구를 넘어 차원을 축소하고 해석을 단순화하는 역할도 한다. 현실 데이터는 차원이 높아질수록(변수가 많아질수록) 분석이 복잡해지고, 소위 ’차원의 저주’라 불리는 문제에 부딪히게 된다. 주성분분석처럼 핵심적인 몇 개의 축으로 변수를 압축하거나, 군집분석처럼 비슷한 대상끼리 묶어 구조를 단순화하는 것은 해석의 가독성을 높여준다.\n정리하자면, 다변량분석은 (1) 여러 변수의 상호작용과 구조를 이해하고, (2) 복잡한 데이터를 간결하게 표현하며, (3) 예측, 분류, 요약, 시각화 등 다양한 응용 목적을 가진 종합적인 분석 틀이라 할 수 있다.\n여기서 흥미로운 점은, 이 전통적인 다변량분석 기법들이 오늘날 머신러닝, 인공지능의 여러 알고리즘과 맞닿아 있다는 것이다. 예컨대 판별분석은 현대의 분류 문제와 연결되고, 주성분분석은 신경망의 차원축소 과정과 닮아 있다. 다변량분석을 배우는 것은 통계적 사고의 전통과, 데이터 과학의 최신 흐름을 이어주는 가교를 이해하는 일과 같다.\n\n\n2. 다변량 분석 유형\n\n\n\n\n\n\n\n\n\n\n구분\n중심 개념\n주요 연구 질문\n대표 기법\n응용 사례\n\n\n인과 관계\n변수 간 원인–결과 구조 규명\n“어떤 변수가 다른 변수에 영향을 주는가?”\n회귀분석(Regression), 분산분석(ANOVA), 구조방정식모형(SEM)\n교육 수준 → 소득, 신약 투여 → 혈압 변화\n\n\n상관 관계\n변수 간 공분산·상관 구조 파악\n“변수들이 어떻게 함께 움직이는가?”\n주성분분석(PCA), 요인분석(Factor Analysis), 다차원척도법(MDS)\n고객 만족 요인 축약, 시험 과목 간 점수 패턴 분석\n\n\n개체 유사성\n개체 간 유사성 기반 그룹화\n“어떤 개체들이 서로 비슷한가?”\n판별분석(Discriminant Analysis), 군집분석(Clustering), 분류(Classification), K-means\n고객 세분화, 질병 패턴 분류, 이미지 인식\n\n\n\n\n(1) 인과 관계 [변수 관계]\n다변량 분석에서 가장 전통적이고도 중요한 접근 가운데 하나는 변수들 사이의 인과 관계를 규명하는 것이다. 여기서 인과 관계란 어떤 변수가 다른 변수에 영향을 주고받는 구조를 의미한다. 즉, 하나의 변수가 원인이 되어 다른 변수의 결과를 설명할 수 있을 때 우리는 그 둘 사이에 인과 관계가 있다고 말한다.\n원인이 되는 변수는 설명변수, 독립변수, 혹은 예측변수라고 부른다. 반대로, 설명 변수의 영향을 받아 나타나는 결과 변수는 종속변수, 반응변수, 혹은 목표변수라고 한다. 일반적으로 수식으로는 목표변수를 Y, 예측변수를 X로 표기한다. 예컨대, 교육수준(X)이 소득(Y)에 영향을 준다고 설정하면, 교육 수준은 독립 변수이고 소득은 종속 변수가 된다.\n통계 분석에서는 연구 설계와 맥락에 따라 이 용어들이 조금씩 달리 쓰이기도 한다. 예를 들어 분산분석(ANOVA)에서는 설명 변수를 ’처리 효과(treatment effect)’ 또는 ’요인(factor)’이라고 부른다. 실험군과 대조군을 나누어 처치의 효과를 검증할 때, 그 처치가 바로 설명 변수 역할을 한다.\n중요한 점은 인과 관계의 설정이 단순히 통계적 기법이나 데이터 분석만으로 결정되는 것이 아니라는 사실이다. 인과 관계는 반드시 이론적 근거나 경험적 타당성에 바탕하여 연구 목적 속에서 먼저 규정된다. 데이터 분석은 그 관계를 확인하거나 추정하는 과정일 뿐이다. 예를 들어, 단순히 두 변수 간에 상관관계가 나타났다고 해서 곧바로 인과 관계가 있다고 결론 내릴 수는 없다. 인과 관계는 연구자의 이론적 배경, 선행 연구, 실험 설계, 맥락적 조건 등을 종합적으로 고려해 설정해야 한다.\n따라서 인과적 다변량 분석은 ’데이터 → 인과 규명’의 순서가 아니라, 먼저 ’이론·가설 → 데이터 분석 → 검증’의 순서를 따른다. 이는 통계학적 방법론이 단순한 데이터 마이닝을 넘어, 사회과학·자연과학에서 현상을 이해하고 설명하는 도구로 기능하는 이유이기도 하다.\n\n\n(2) 상관관계 [변수 관계]\n다변량 분석에서 두 번째 유형은 상관 관계를 중심으로 한 분석이다. 상관 관계란 변수들이 얼마나 유사하게 변동하는가, 즉 함께 움직이는 정도를 나타낸다. 이를 계량화하는 대표적인 지표가 상관계수이며, 더 일반적으로는 공분산 구조이다.\n상관 관계 분석의 첫 번째 활용은 변수 간의 함수적 관계를 탐색하는 것이다. 예컨대 두 변수 간에 직선적 관계가 존재하는지를 확인하고, 그 기울기와 상관성을 추정하는 회귀분석이 대표적이다. 두 번째 활용은 변수들 간의 유사성을 바탕으로 차원을 줄이는 것이다. 변수의 수가 많을수록 자료 구조가 복잡해지므로, 서로 높은 상관성을 가진 변수들을 몇 개의 대표적인 요인이나 축으로 요약하는 기법이 필요하다. 이러한 맥락에서 주성분분석(PCA), 요인분석(factor analysis) 등이 개발되었다.\n특히 주성분분석은 공분산 행렬을 분해하여 데이터가 가장 크게 퍼져 있는 방향(주성분)을 찾아내고, 이를 통해 다차원 자료를 소수의 차원으로 축약한다. 요인분석은 변수들 간 상관구조를 설명하는 잠재 요인을 가정하고, 관측된 변수들의 공통적 변동성을 요인으로 요약한다. 두 방법 모두 변수들의 상관 구조를 근거로 데이터의 차원을 줄여, 해석을 단순화하고 시각화를 가능하게 한다.\n빅데이터 시대에 들어서면서 이러한 차원축소의 필요성은 더욱 커졌다. 과거에는 변수(열)의 차원을 줄이는 데 집중했지만, 데이터의 규모가 방대해지면서 개체(행)의 차원까지 줄이는 기법들이 발전하였다. 예컨대 서포트 벡터 머신(SVM)이나 커널 기반 방법은 고차원 데이터의 분류와 예측에서, 행과 열의 차원을 동시에 축소하여 계산 효율을 높이고 본질적인 구조를 추출한다.\n따라서 현대의 상관 관계 분석은 단순한 변수 요약을 넘어, 데이터 전체의 구조를 간결하게 표현하는 도구로 발전하고 있다. 이는 복잡한 데이터 환경에서 핵심 패턴을 식별하고, 예측 가능성을 높이는 데 결정적인 역할을 한다.\n\n\n(3) 개체의 유사성 (Similarity of Objects)\n세 번째 유형은 개체의 유사성을 중심으로 한 분석이다. 여기서는 변수들이 매개체 역할을 하여 개체 간 거리를 정의하고, 서로 비슷한 개체들을 묶어 분류하는 것이 핵심이다. 다시 말해, 변수 중심이 아니라 개체 중심의 다변량 분석이다.\n분석 접근은 크게 두 가지로 나뉜다.\n첫째는 판별분석(discriminant analysis)이다. 이 방법은 이미 사전에 정의된 그룹이 있을 때, 그룹 간의 차이를 가장 잘 설명하는 선형 혹은 비선형 판별함수를 구축한다. 그리고 이를 이용해 새로운 개체가 주어졌을 때 어떤 그룹에 속할지를 판별한다. 예를 들어 학생들의 성적과 생활 습관을 이용하여 ”우수 학업 집단”과 ”일반 집단”을 구분하는 모형을 만드는 것이 판별분석의 전형적 응용이다.\n둘째는 분류(classification) 또는 군집분석(clustering)이다. 판별분석이 사전에 그룹이 정의된 경우라면, 군집분석은 그룹이 전혀 정의되지 않은 상태에서 출발한다. 분석자는 개체 간의 유사성을 기준으로, 데이터 스스로 그룹 구조를 형성하도록 탐색한다. 이때 그룹의 수조차도 분석 과정에서 정해질 수 있다. 대표적인 방법으로는 계층적 군집분석, K-평균(K-means), 혼합분포 모형(mixture model) 등이 있다.\n유사성 분석의 응용 범위는 매우 넓다. 마케팅에서는 고객 세분화(customer segmentation), 의학에서는 질병 패턴 분류(disease classification), 공학에서는 영상 인식(image recognition) 등 다양한 분야에서 활용된다. 즉, 인과 관계 분석이 ”왜 발생했는가?“라는 질문에, 상관 관계 분석이 ”어떻게 함께 변하는가?”라는 질문에 답한다면, 유사성 분석은 ”누가 누구와 비슷한가?“라는 질문에 답한다.\n개체 유사성 분석은 특히 데이터가 고차원·대규모화될수록 중요해진다. 개체를 효과적으로 분류하고 집단 간 차이를 설명할 수 있다면, 빅데이터 속에서 의미 있는 패턴을 추출해 실질적인 의사결정에 활용할 수 있기 때문이다.\n\n\n\n3. 다변량분석 방법론\n\n(1) 예측 모델\n\\(\\underset{¯}{y} = X\\underset{¯}{b} + \\underset{¯}{e},\\underset{¯}{e} \\sim MN(\\underset{¯}{0},\\sigma^{2}I)\\)\n예측을 목적으로 하는 다변량 분석의 대표적 도구는 회귀분석과 분산분석이다.\n회귀분석은 한 변수(종속 변수)가 다른 변수들(설명 변수)에 의해 어떻게 변화하는지를 모델링한다. 설명 변수는 하나일 수도 있고 여러 개일 수도 있는데, 후자의 경우 다중회귀분석이 된다. 회귀모형은 종속 변수와 설명 변수 간의 관계를 선형 또는 비선형 형태로 표현하며, 이를 통해 새로운 관측값이 주어졌을 때 종속 변수를 예측할 수 있다. 응용 사례로는 광고비 지출과 매출액의 관계, 체질량지수와 건강 지표의 관계 등이 있다.\n분산분석은 주로 실험 설계에서 사용되며, 여러 집단 간 평균 차이가 통계적으로 유의미한가를 검증하는 방법이다. 예를 들어, 세 가지 교육 프로그램이 학생들의 성적에 차이를 만드는지를 비교할 때 활용된다. 분산분석에서는 집단 간 변동과 집단 내 변동을 분리하여, 설명 변수(요인)가 종속 변수에 영향을 미치는지 파악한다.\n회귀분석과 분산분석은 모두 인과적 설명과 예측을 목적으로 하며, 실험적 연구와 관찰적 연구에서 모두 중요한 역할을 한다.\n\n\n(2) 차원 축소: 주성분분석\n현대 데이터는 변수의 수가 많아지는 고차원적 특성을 띠는데, 모든 변수를 동시에 다루는 것은 해석과 계산 모두에서 부담이 크다. 이를 해결하기 위한 방법이 주성분분석이다.\n주성분분석은 변수들 간의 상관 구조를 바탕으로, 변동성을 가장 잘 설명하는 새로운 축(주성분)을 찾아낸다. 이렇게 얻어진 주성분들은 서로 독립적이며, 원래의 많은 변수를 소수의 대표적 성분으로 요약한다. 예컨대 수십 개의 시험 과목 점수를 소수의 주성분으로 줄이면, ”수학·과학적 능력”과 ”언어·사회적 능력” 같은 큰 축이 드러난다.\n주성분분석은 데이터의 구조를 간결하게 표현하고, 고차원 자료를 시각화하거나 다른 분석 기법의 입력 변수로 활용할 때 유용하다. 빅데이터 분석, 영상 처리, 유전자 데이터 분석 등 다양한 분야에서 널리 응용된다.\n\n\n(3) 판별분석\n판별분석은 이미 사전에 정의된 집단이 있을 때, 어떤 개체가 어느 집단에 속하는지를 판별하는 기법이다. 즉, 새로운 개체의 특성을 보고 그가 속할 집단을 예측하는 문제에 사용된다.\n대표적인 방법은 선형판별분석(LDA: linear discriminant analysis)이다. LDA는 집단 간 차이가 최대가 되면서 집단 내 변동은 최소가 되도록 하는 판별 축을 찾는다. 이를 통해 각 개체를 판별 점수에 따라 그룹에 분류한다. 예를 들어, 꽃잎의 길이와 폭을 이용해 붓꽃 품종을 구분하는 것이 판별분석의 전형적 사례다.\n판별분석은 회귀분석과 달리 종속 변수가 연속형이 아니라 범주형이라는 특징을 가진다. 의료 분야의 질병 진단, 마케팅에서의 고객 분류, 교육에서의 학업 성취도 그룹 판별 등 다양한 분야에서 응용된다.\n\n\n(4) 군집분석\n군집분석은 판별분석과 달리 사전에 집단이 정의되어 있지 않을 때 활용되는 방법이다. 데이터 자체의 유사성 구조를 바탕으로, 비슷한 개체들을 묶어 새로운 집단을 형성하는 것이 목적이다.\n군집분석에는 여러 접근이 있다. 계층적 군집분석(hierarchical clustering)은 개체들을 순차적으로 묶거나 나누어 dendrogram를 형성한다. K-평균 군집(K-means clustering)은 사용자가 미리 설정한 군집 수에 따라 데이터를 분류하며, 각 군집의 중심점과 가까운 개체들을 배정하는 방식이다.\n군집분석은 탐색적 분석의 성격을 강하게 띠며, 데이터 속에 숨어 있는 패턴을 찾아내는 데 유용하다. 대표적인 응용 사례로는 마케팅에서의 고객 세분화, 생물학에서의 종 분류, 소셜 네트워크에서의 커뮤니티 탐지 등이 있다.\n\n\n(5) 판별 및 분류 비교\n개체 판별 discrimination\n판별분석은 자료 수집 시 이미 그룹이 나누어져 있어( 빨간원, 파랑원 개체) 이를 가장 잘 판별하는 판별규칙(측정변수의 거리에 의해 개체의 유사성을 측정함)을 도출하여 새로운 개체의 군집을 판별하는 방법이다.\n개체분류 classification\n군집분석에서는 개체의 그룹에 대한 정보 없이 (사전에는 ▵◻︎◎ 구분이 없이 동일하나 분석 후 나누어 짐) 유사성이 가까운 개체들끼리 계층적으로 묶어 가거나 군집의 개수를 정하여 군집의 중심점을 이용하 여 개체를 군집화 하는 방법이다.\n\n\n\n\n\n\n\n(6) 정준상관분석 canonical correlation\n서로 다른 변수 군으로부터 상관계수 0(서로 독립) 변수군을 만들어 변수군간 상관분석을 실시한다. 변수의 개수를 줄이는 한 방법이다.\n\n\n\n\n\n\n\n(7) 다차원척도법 (Multidimensional Scaling, MDS)\n다차원척도법은 개체들 사이의 유사성(similarity)이나 비유사성(dissimilarity) 정보를 바탕으로, 그 구조를 저차원의 공간(보통 2차원 또는 3차원)에 시각적으로 배치하는 기법이다. 즉, 개체 간 거리를 보존하면서 ”지도 위에 점찍기”처럼 공간 좌표를 재구성하는 방법이다.\n예를 들어 여러 나라 간 무역 구조의 유사성, 소비자들이 브랜드를 얼마나 비슷하다고 인식하는지, 도시 간 이동 시간 등을 입력으로 주면, MDS는 이를 가능한 한 잘 반영하는 좌표를 찾아낸다. 그 결과, 유사한 개체들은 가까이, 서로 다른 개체들은 멀리 배치되어 데이터의 패턴을 시각적으로 직관적으로 이해할 수 있다.\nMDS는 주성분분석처럼 차원을 줄이지만, 변수 중심이 아니라 개체 간 거리를 중심으로 한다는 점에서 차별화된다. 따라서 응용은 마케팅 리서치(브랜드 포지셔닝 지도), 심리학(자극 간 인지적 거리), 사회학(집단 간 관계) 등 사람들의 인식이나 개체 간 관계를 시각화하는 데 적합하다.\n\n\n(8) 대응분석 (Correspondence Analysis, CA)\n대응분석은 주로 범주형 자료, 특히 교차표 형태의 자료를 분석할 때 쓰이는 차원축소 기법이다. 교차표에서 행과 열의 관계를 시각적으로 단순화해, 범주들 간의 연관성을 직관적으로 보여주는 것이 목적이다.\n예를 들어 ”연령대 × 선호 브랜드” 교차표가 주어졌다고 하자. 대응분석은 이 교차표의 행과 열을 동시에 저차원 공간에 배치한다. 그 결과 특정 연령대와 특정 브랜드가 가까이 놓인다면, 그 두 범주 간에 밀접한 연관이 있음을 의미한다.\n대응분석은 일종의 범주형 데이터 전용의 주성분분석이라 할 수 있다. 주성분분석이 연속형 변수들의 상관 구조를 축약한다면, 대응분석은 범주형 변수들의 연관 구조를 요약한다. 마케팅에서는 소비자-제품 선호 분석, 사회학에서는 계층-문화 선호 분석 등 다양한 범주형 자료 해석에 널리 쓰인다.\n\n\n(9) 다변량분석 방법론 총괄\n\n\n\n\n\n\n\n\n\n\n\n\n내용\n예측모형\n주성분분석\n요인분석\n판별분석\n군집분석\n정준상관분석\n\n\n변수 관계 탐색\nD\nS\nD\nN\nN\nS\n\n\n자료 탐색\nD\nD\nS\nN\nS\nN\n\n\n새 변수 만들기\nN\nYes\nYes\nNo\nNo\nYes\n\n\n개체 분류\nN\nNo\nNo\nYes\nYes\nNo\n\n\n변수 그룹\nN\nP\nP\nN\nN\nD\n\n\n차원 줄이기\nN\nD\nP\nN\nN\nN\n\n\n\nSometimes, Definitely, Never, Possible, Rarely\n\n\n\n\nChapter 2. 다변량 확률분포\n\n1. 다변량 데이터\n\n(1) 데이터 행렬\n다변량 데이터란 두 개 이상의 확률변수로 이루어진 데이터 형태를 말한다. 하나의 데이터셋에서 각 열은 관심 있는 확률변수를, 각 행은 분석 대상이 되는 개체를 나타내며, 따라서 각 원소는 특정 개체에서 관측된 해당 변수의 값으로 구성된다.\n\\(\\mathbf{X} = \\begin{bmatrix}\nx_{11} & x_{12} & \\cdots & x_{1p} \\\\\nx_{21} & x_{22} & \\cdots & x_{2p} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nx_{n1} & x_{n2} & \\cdots & x_{np}\n\\end{bmatrix}\\), 여기서 n은 데이터 개수(행의 수), p는 확률변수의 수(열의 수), 그리고 \\(x_{ij}\\)는 i-번째 개체의 j-번째 변수에 대한 관측치이다.\n다변량분석에서는 주로 수치형(정량적) 변수들이 기본 자료로 사용된다. 이러한 분석은 크게 두 가지 방향에서 전개된다. 첫째, 변수들 간의 함수적 관계를 모형화하여 예측에 활용하거나, 변수 간 유사성을 바탕으로 차원을 축소한다. 둘째, 개체들 간의 유사성을 이용해 집단을 분류하거나 군집을 찾는다. 이러한 접근은 모두 정량적 데이터 구조를 기반으로 가능하다.\n다만 예측 모형을 설정할 때는 정성적 변수(예: 범주형, 질적 변수) 역시 독립변수로 포함시킬 수 있다. 따라서 다변량분석은 수치형 데이터를 중심으로 하되, 상황에 따라 질적 변수를 보조적으로 결합하여 더욱 포괄적인 분석을 수행할 수 있다.\n\n\n(2) 확률변수 (열)벡터, 평균벡터, 공분산행렬\np개의 다변량 확률변수 벡터는 \\(\\underset{¯}{x} = \\left( \\begin{array}{r}\nx_{1} \\\\\nx_{2} \\\\\n\\ldots \\\\\nx_{p}\n\\end{array} \\right)\\)으로 표현하며, 다변량 확률변수의 평균벡터와 공분산행렬은 다음과 같다.\n\\[\\mathbf{\\mu} = E\\lbrack\\mathbf{x}\\rbrack = \\left\\lbrack \\begin{array}{r}\n\\mu_{1} \\\\\n\\mu_{2} \\\\\n\\vdots \\\\\n\\mu_{p}\n\\end{array} \\right\\rbrack\\]\n\\(\\mathbf{\\Sigma} = Cov(\\mathbf{x}) = \\begin{bmatrix}\n\\sigma_{11} & \\sigma_{12} & \\cdots & \\sigma_{1p} \\\\\n\\sigma_{21} & \\sigma_{22} & \\cdots & \\sigma_{2p} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\sigma_{p1} & \\sigma_{p2} & \\cdots & \\sigma_{pp}\n\\end{bmatrix}\\), 여기서 \\(\\mu_{j} = E\\lbrack x_{j}\\rbrack\\)는 j-번째 확률변수의 평균, \\(\\sigma_{ij} = Cov(x_{i},x_{j})\\)는 i-번째와 j-번째 확률변수 간의 공분산을 의미한다. 즉, \\(\\mathbf{\\mu}\\)는 \\(p \\times 1\\)벡터, \\(\\mathbf{\\Sigma}\\)는 \\(p \\times p\\)대칭행렬이 된다.\n\n\n(3) 데이터 평균, 공분산행렬\n데이터 평균 벡터는 \\(E(\\underset{¯}{x}) = \\overline{\\mathbf{x}} = \\left\\lbrack \\begin{array}{r}\n{\\overline{x}}_{1} \\\\\n{\\overline{x}}_{2} \\\\\n\\vdots \\\\\n{\\overline{x}}_{p}\n\\end{array} \\right\\rbrack\\)이다. 여기서 \\({\\overline{x}}_{j} = \\frac{1}{n}\\overset{n}{\\sum_{i = 1}}x_{ij},j = 1,2,\\ldots,p\\)이다. \\(E(\\underset{¯}{x}) = \\overline{\\mathbf{x}} = \\frac{1}{n}\\mathbf{1}_{n}^{T}\\mathbf{X}\\)\n데이터 공분산행렬 \\(\\mathbf{S} = \\frac{1}{n - 1}\\left( \\mathbf{X} - \\mathbf{1}_{n}{\\overline{\\mathbf{x}}}^{T} \\right)^{T}\\left( \\mathbf{X} - \\mathbf{1}_{n}{\\overline{\\mathbf{x}}}^{T} \\right)\\)은 이렇게 표시한다. \\(\\mathbf{S} = Cov(\\mathbf{x}) = \\begin{bmatrix}\ns_{11} & s_{12} & \\cdots & s_{1p} \\\\\ns_{21} & s_{22} & \\cdots & s_{2p} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\ns_{p1} & s_{p2} & \\cdots & s_{pp}\n\\end{bmatrix}\\).\n\n\n\n2. 다변량 확률분포함수\n\n(1) 다변량 확률분포함수 정의\n다변량 확률분포함수(multivariate probability distribution function)는 둘 이상의 확률변수들이 동시에 어떻게 분포하는지를 나타내는 함수이다. 확률변수가 p개라면, 이들을 \\(\\mathbf{X} = (X_{1},X_{2},\\ldots,X_{p})^{T}\\)라 두고, 그들의 결합분포함수(joint distribution)를 \\(F(x_{1},x_{2},\\ldots,x_{p})\\) 정의하고 연속형인 경우에는 결합확률밀도함수(joint PDF)는 다음과 같이 정의한다. \\(f(x_{1},x_{2},\\ldots,x_{p}),\\int_{- \\infty}^{\\infty}\\cdots\\int_{- \\infty}^{\\infty}f(x_{1},x_{2},\\ldots,x_{p})dx_{1}\\cdots dx_{p} = 1\\)\n이 함수는 각 확률변수가 단독으로 어떤 분포를 따르는지, 그리고 변수들 사이가 얼마나 상관되어 있는지를 동시에 담아낸다.\n1. 변수 간 관계 파악\n두 변수 X와 Y가 독립인지, 상관성이 있는지, 어떤 구조로 얽혀 있는지 모두 결합분포로 설명할 수 있다. 예를 들어, 다변량 정규분포는 평균벡터와 공분산행렬을 통해 변수들의 상호 의존관계를 전부 기술한다.\n2. 조건부 확률과 예측\n특정 변수 값이 주어졌을 때 다른 변수의 분포를 알기 위해서는 결합분포와 주변분포, 조건부분포를 자유자재로 다룰 수 있어야 한다. 예측모형이나 베이즈 추론의 근간이 바로 이 개념이다.\n3. 차원축소와 요약\n주성분분석(PCA)이나 요인분석은 사실상 다변량 정규분포의 공분산 구조를 해석하는 과정이다. 즉, 결합분포를 이해해야 왜 특정 축으로 차원을 줄이는지 수학적으로 설명할 수 있다.\n4. 현실 데이터 반영\n실제 사회·경제·의학 데이터는 대부분 여러 변수가 동시에 움직이므로 단변량 분포만으로는 현실을 설명하기에 턱없이 부족하다. 예를 들어, 혈압·혈당·체중을 독립적으로 보는 게 아니라 같이 분포하는 방식을 봐야 건강 상태를 평가할 수 있다.\n다변량 확률분포함수는 ”변수들이 함께 어떻게 움직이는가”를 수학적으로 그려낸 지도이다. 이를 알아야 차원축소, 분류, 예측 등 다변량분석 기법의 의미와 한계를 제대로 이해할 수 있다.\n\n\n(2) 다변량 정규분포 \\({\\underset{¯}{x}}_{p} \\sim MVN({\\underset{¯}{\\mu}}_{p},\\Sigma_{p \\times p})\\)\n다변량 정규분포는 다변량 분석에서 가장 중요한 확률분포이다. 이 분포는 평균벡터와 공분산행렬이라는 두 가지 요소만으로 완전히 규정되며, 주변분포와 조건부분포가 모두 정규분포의 형태를 유지한다는 성질을 가진다. 따라서 수학적으로 매우 다루기 쉬운 분포이다.\n다변량 정규분포는 고전적 통계기법의 이론적 토대이기도 하다. 판별분석, 주성분분석, 요인분석, 회귀분석과 같은 기법들은 데이터가 다변량 정규분포를 따른다는 가정 위에서 성립하거나, 그 구조를 해석하는 과정으로 설명된다.\n또한 중심극한정리가 다변량 상황에서도 성립하기 때문에, 실제 데이터가 반드시 정규분포를 따르지 않더라도 많은 경우 다변량 정규분포로 근사할 수 있다. 이러한 이유로 현실 데이터를 설명하는 데에도 유용하다.\n다변량 정규분포는 선형변환에 대해 닫혀 있으며, 공분산행렬을 통해 변수 간 상관관계와 분포의 형태를 직관적으로 나타낼 수 있다. 더불어 베이즈 추론에서도 자주 사용되는데, 정규분포는 켤레사전분포(conjugate prior)의 성질을 가지므로 계산을 단순화한다.\n요약하면, 다변량 정규분포는 수학적으로 단순하고, 통계이론의 기초가 되며, 현실 데이터에도 잘 부합하는 확률분포이므로 다변량분석에서 핵심적 위치를 차지하는 분포이다.\n차수 p인 확률변수 벡터 \\({\\underset{¯}{x}}_{p} \\sim MN({\\underset{¯}{\\mu}}_{p},\\Sigma_{p \\times p})\\)는 평균이 \\(\\underset{¯}{\\mu}\\), 공분산행렬이 \\(\\Sigma\\)인 다변량 정규분포의 확률분포함수는 다음과 같다.\n\\[f(\\underset{¯}{x};\\underset{¯}{\\mu},\\Sigma) = \\frac{1}{(2\\pi)^{\\frac{p}{2}}|\\Sigma|^{\\frac{1}{2}}}exp\\lbrack - 1/2(\\underset{¯}{x} - \\mu)'\\Sigma^{- 1}(\\underset{¯}{x} - \\mu)\\rbrack\\]\n\\[\\Sigma = \\sigma^{2}I\\]\n다변량 확률변수는 서로 독립이고 동일 분산(등분산)을 갖는다.\n성질\n\n각 확률변수의 조건부 확률분포함수, 주변확률분포함수는 모두 정규분포를 따른다.\n선형계수벡터 \\({\\underset{¯}{a}}^{T} = \\lbrack a_{1}a_{2}\\ldots,a_{p}\\rbrack_{p \\times 1}\\)에 대하여 \\({\\underset{¯}{a}}^{T}{\\underset{¯}{x}}_{p \\times 1} \\sim MVN({\\underset{¯}{a}}^{T}\\underset{¯}{\\mu},{\\underset{¯}{a}}^{T}\\Sigma\\underset{¯}{a})\\)\n\n\n\n(3) 이차형식 Quardratic form \\({\\underset{¯}{x}}^{T}A\\underset{¯}{x}\\)\n다변량 확률변수(열벡터) \\({\\underset{¯}{x}}_{p}\\), 정방행렬 \\(A_{p \\times p}\\)에 대하여 \\({\\underset{¯}{x}}^{T}A\\underset{¯}{x}\\)을 이차형식이라 한다.\n만약 \\(A = I\\)(항등행렬)인 경우: \\({\\underset{¯}{x}}^{T}\\underset{¯}{x} = x_{1}^{2} + x_{2}^{2} + \\ldots + x_{p}^{2}\\)\nPositive semi-Definite matrix 양(준)정부호행렬\n행렬 \\(A\\)는 양(준)정부호행렬 &lt;=&gt; 모든 \\(\\underset{¯}{x}\\)에 대하여 \\({\\underset{¯}{x}}^{T}A\\underset{¯}{x} &gt; 0\\), \\({\\underset{¯}{x}}^{T}A\\underset{¯}{x} \\geq 0\\)\n\n양정부호행렬의 고유값은 양수이다. (semi의 경우는 0을 포함한 양수)\n양정부호행렬은 Choleski-분해(decomposition) \\(A = LL^{*}\\)(\\(L\\)= 하삼각행렬) 가능하다.\n\n이차형식 분포\n만약 확률변수 벡터가 평균벡터 \\(\\underset{¯}{\\mu}\\), 공분산 행렬 \\(\\Sigma\\)인 다변량정규분포를 따른다면 \\({\\underset{¯}{x}}_{p} \\sim MVN({\\underset{¯}{\\mu}}_{p},\\Sigma_{p \\times p})\\),\n이차형식 \\({\\underset{¯}{x}}^{T}A\\underset{¯}{x}\\) (\\(A\\)=대칭행렬)에 대하여\n\\[E({\\underset{¯}{x}}^{T}A\\underset{¯}{x}) = tr(A\\Sigma) + {\\underset{¯}{\\mu}}^{T}A\\underset{¯}{\\mu}\\]\n\\[V({\\underset{¯}{x}}^{T}A\\underset{¯}{x}) = 2tr(A\\Sigma A\\Sigma)) + 4{\\underset{¯}{\\mu}}^{T}A\\Sigma A\\underset{¯}{\\mu}\\]\n\\((\\underset{¯}{x} - \\underset{¯}{\\mu})^{T}\\Sigma^{- 1}(\\underset{¯}{x} - \\underset{¯}{\\mu}) \\sim \\chi^{2}(p)\\)이다.\n\n\n\n3. 데이터 생성(시뮬레이션)\n이변량정규분포 생성 \\(\\left\\lbrack \\begin{array}{r}\nx \\\\\ny\n\\end{array} \\right\\rbrack \\sim BN(\\left\\lbrack \\begin{array}{r}\n\\mu_{x} \\\\\n\\mu_{y}\n\\end{array} \\right\\rbrack,\\begin{bmatrix}\n\\sigma_{x} & \\sigma_{xy} \\\\\n\\sigma_{xy} & \\sigma_{y}\n\\end{bmatrix})\\)\n수리적 변수변환 이용\n만약 \\(Z_{1},Z_{2} \\sim iidN(0,1)\\), \\(X = \\sigma_{x}Z_{1} + \\mu_{x}\\), \\(Y = \\sigma_{y}\\lbrack\\rho Z_{1} + \\sqrt{1 - \\rho^{2}}Z_{2} + \\mu_{y}\\rbrack\\)은 이변량 정규분포를 따른다.\n\\[\\left\\lbrack \\begin{array}{r}\nx \\\\\ny\n\\end{array} \\right\\rbrack \\sim BN(\\left\\lbrack \\begin{array}{r}\n2 \\\\\n1\n\\end{array} \\right\\rbrack,\\begin{bmatrix}\n1 & 1.4 \\\\\n1.4 & 4\n\\end{bmatrix})\\]\nimport numpy as np\n\n# 표준정규분포 N(0,1)에서 각각 100개의 난수 생성\nz1 = np.random.normal(0, 1, 100)\nz2 = np.random.normal(0, 1, 100)\n\n# x는 단순히 z1에 2를 더한 값 → 평균만 2만큼 이동, 분산은 변하지 않음\nx = z1 + 2\n\n# y는 0.7*z1 + sqrt(1-0.7^2)*z2 + 1에 2를 곱한 값\n#   → 이는 (z1, z2)가 독립일 때 z1과 상관계수 0.7을 갖도록 z2를 섞은 구조\n#   → 즉, y는 z1과 0.7의 상관관계를 가지도록 설계된 변수\ny = 2 * (0.7*z1 + np.sqrt(1 - 0.7**2)*z2 + 1)\n\n# x와 y의 표본 공분산 행렬 (2x2) 출력\nprint(np.cov(x, y))\n\n# x와 y의 표본 상관계수 행렬 (2x2) 출력\n[[1.11733863 1.44628517]  [1.44628517 4.25387359]]  [[1. 0.66339048]  [0.66339048 1. ]]\n함수 이용\nimport numpy as np\n\n# 다변량 정규분포의 평균벡터 지정: E[X] = 2, E[Y] = 1\nmean = [2, 1]\n\n# 공분산행렬 지정\n#   Var(X) = 1, Var(Y) = 4, Cov(X,Y) = 1.4\ncov = [[1, 1.4],\n       [1.4, 4]]\n\n# 다변량 정규분포로부터 난수 100개 추출\n#   결과는 (100, 2) 행렬이며, 이를 전치(.T)해서 x, y로 분리\nx, y = np.random.multivariate_normal(mean, cov, 100).T\n\n# x와 y의 표본 공분산행렬\nprint(np.cov(x, y))\n\n# x와 y의 표본 상관계수행렬\nprint(np.corrcoef(x, y))\n[[1.01638644 1.39742159]  [1.39742159 4.18379016]]  [[1. 0.67766191]  [0.67766191 1. ]]\nAffine transformations of the multivariate normal\n만약 \\(X \\sim N(\\mu_{X},\\Sigma_{X})\\)이고 \\(Y = LX + u\\)(\\(L\\)=선형변환행렬, \\(u\\)=임의의 벡터)이라면 확률변수(벡터) \\(Y\\)는 정규분포를 따르고 \\(\\mu_{Y} = u + L\\mu_{X}\\) 이고 \\(\\Sigma_{Y} = L\\Sigma_{X}L^{T}\\)이다.\n이변량 정규분포를 생성하는 경우에는 \\(X \\sim N(\\mu,\\sigma^{2}I)\\)을 사용하므로 \\(Y\\)의 공분산은 \\(\\Sigma_{Y} = LL^{T}\\)이다.\n선형변환행렬 \\(L\\)은 공분산행렬을 사용한 Cholesky Decomposition(분해)을 이용한다.\n\\((X,Y)\\)의 공분산행렬(양반정치행렬 Positive semidefinite matrix) \\(\\Sigma = LL^{*}\\), \\(L\\)=하한대각행렬, \\(L^{*}\\)=대각행렬 L의 conjugate transpose이다.\n\\[Y \\sim BN(\\left\\lbrack \\begin{array}{r}\n2 \\\\\n1\n\\end{array} \\right\\rbrack,\\begin{bmatrix}\n1 & 1.4 \\\\\n1.4 & 4\n\\end{bmatrix})\\]\nd = 2     # 차원 수: 2차원 (X와 Y)\nn = 100   # 표본 개수\n\n# 평균 벡터 (2x1)\nmean = np.matrix([[2],\n                  [1]])\n\n# 공분산 행렬 (대칭, 양의 정부호)\ncovariance = np.matrix([[1,   1.4],\n                        [1.4, 4]])\n\n# 공분산행렬의 Cholesky 분해: covariance = L * L^T\n# L은 하삼각행렬\nL = np.linalg.cholesky(covariance)\n\n# 표준정규분포 N(0,1)에서 난수 생성\n# shape=(d, n) → 2행 n열 행렬 (각 열이 2차원 표본 하나)\nX = np.random.normal(size=(d, n))\n\n# Y = L * X + mean\n#   1) L * X → 공분산 구조 반영\n#   2) + mean → 평균 벡터 이동\n# 결과: 다변량 정규분포 N(mean, covariance)로부터 n개의 표본\nY = L.dot(X) + mean\n\n# 출력\nprint(L, X.shape, Y.shape)\n[[1. 0. ]  [1.4 1.42828569]] (2, 100) (2, 100)\n다변량 정규분포함수 그리기\nimport numpy as np\n\n#다변량 정규분포함수 정의\ndef mvn_pdf(X, mean, cov):\n    \"\"\"\n    X: (n, d) 배열, 각 행이 한 점 (예: [[x1,y1], [x2,y2], ...])\n    mean: (d,) 또는 (d,1) 평균벡터\n    cov: (d, d) 대칭 양의정정부호(positive definite) 공분산 행렬\n    반환: (n,) 각 점에서의 다변량 정규분포 밀도값\n    \"\"\"\n    # 입력 데이터 모양 보정\n    X = np.atleast_2d(X)                 # X를 최소 2차원 배열로 변환\n    mean = np.asarray(mean).reshape(-1)  # mean을 1차원 벡터로 변환\n    d = mean.size                        # 차원 수\n\n    # 공분산 행렬의 Cholesky 분해: cov = L L^T\n    # 수치적으로 안정적이고 역행렬 계산을 피할 수 있음\n    L = np.linalg.cholesky(cov)\n\n    # 정규화 상수 = (2π)^{-d/2} |Σ|^{-1/2}\n    log_norm = -0.5 * (d * np.log(2*np.pi) + 2*np.sum(np.log(np.diag(L))))\n\n    # X - mean (데이터 중심화)\n    Xm = X - mean\n\n    # Mahalanobis 거리 계산\n    # L z = (X-mean)^T → z = L^{-1}(X-mean)^T\n    z = np.linalg.solve(L, Xm.T)         # shape: (d, n)\n    quad = np.sum(z*z, axis=0)           # 각 점의 (x-μ)^T Σ^{-1} (x-μ)\n\n    # 로그 pdf → pdf\n    log_pdf = log_norm - 0.5 * quad\n    return np.exp(log_pdf)               # 최종 확률밀도값 (n,)\n\n#다변량 정규분포함수 그리기 함수\ndef generate_surface_fast(mean, covariance, xlim=(-5,5), ylim=(-5,5), nb_of_x=200):\n    # x축, y축에 대해 nb_of_x개 점을 균등분할\n    x1s = np.linspace(*xlim, num=nb_of_x)\n    x2s = np.linspace(*ylim, num=nb_of_x)\n\n    # 2차원 격자 생성 (X1, X2 모두 nb_of_x × nb_of_x 크기)\n    X1, X2 = np.meshgrid(x1s, x2s)\n\n    # 격자 좌표를 (nb_of_x^2, 2) 모양으로 펴기 → 각 행은 한 점 (x,y)\n    grid = np.column_stack([X1.ravel(), X2.ravel()])\n\n    # 각 격자점에서 pdf 값 계산 후 다시 2차원 격자 모양으로 reshape\n    pdf = mvn_pdf(grid, mean, covariance).reshape(nb_of_x, nb_of_x)\n\n    # X1, X2: 격자 좌표, pdf: 밀도값\n    return X1, X2, pdf\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib import cm\n\n# === 다변량 정규분포 pdf (벡터화) ===\ndef mvn_pdf(X, mean, cov):\n    X = np.atleast_2d(X)\n    mean = np.asarray(mean).reshape(-1)\n    d = mean.size\n\n    # Cholesky 분해\n    L = np.linalg.cholesky(cov)\n    # 정규화 상수 (log-scale)\n    log_norm = -0.5 * (d * np.log(2*np.pi) + 2*np.sum(np.log(np.diag(L))))\n\n    Xm = X - mean\n    z = np.linalg.solve(L, Xm.T)           # (d, n)\n    quad = np.sum(z*z, axis=0)             # Mahalanobis 거리 제곱\n    log_pdf = log_norm - 0.5 * quad\n    return np.exp(log_pdf)\n\n# === 등고선 그리드 생성 ===\ndef generate_surface_fast(mean, covariance, xlim=(-5,5), ylim=(-5,5), nb_of_x=200):\n    x1s = np.linspace(*xlim, num=nb_of_x)\n    x2s = np.linspace(*ylim, num=nb_of_x)\n    X1, X2 = np.meshgrid(x1s, x2s)\n    grid = np.column_stack([X1.ravel(), X2.ravel()])\n    pdf = mvn_pdf(grid, mean, covariance).reshape(nb_of_x, nb_of_x)\n    return X1, X2, pdf\n\n# === 파라미터와 샘플 생성 ===\nmean = [2, 1]\ncovariance = [[1, 1.4],\n              [1.4, 4]]\n\nd, n = 2, 100\nL = np.linalg.cholesky(covariance)\nX = np.random.normal(size=(d, n))\nY = L.dot(X) + np.array(mean).reshape(-1,1)\n\n# === 플로팅 ===\nfig, ax = plt.subplots(figsize=(6, 4.5))\n\n# 등고선 (density surface)\nx1, x2, p = generate_surface_fast(mean, covariance, xlim=(-2.5,2.5), ylim=(-1.5,3.5))\ncon = ax.contourf(x1, x2, p, 100, cmap=cm.YlGnBu)\n\n# 샘플 점 찍기\nax.plot(Y[0,:], Y[1,:], 'ro', alpha=.6,\n        markeredgecolor='k', markeredgewidth=0.5)\n\nax.set_xlabel('$y_1$', fontsize=13)\nax.set_ylabel('$y_2$', fontsize=13)\nax.axis([-2.5, 2.5, -1.5, 3.5])\nax.set_aspect('equal')\nax.set_title('Samples from bivariate normal distribution')\n\n# 색상바\ncbar = plt.colorbar(con)\ncbar.ax.set_ylabel('density: $p(y_1, y_2)$', fontsize=13)\n\nplt.show()\n\n\n\n\n\n주변 정규분포함수_조건부 확률밀도함수 그리기\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import multivariate_normal, norm\n\n# ==== 다변량 정규분포 파라미터 ====\nmean = np.array([2, 1])\ncov = np.array([[1, 0.8],\n                [0.8, 2]])  # X,Y 상관 있음\n\n# ==== 주변 분포 ====\n# Marginal of X ~ N(mean[0], cov[0,0])\nmarginal_X = norm(loc=mean[0], scale=np.sqrt(cov[0,0]))\n# Marginal of Y ~ N(mean[1], cov[1,1])\nmarginal_Y = norm(loc=mean[1], scale=np.sqrt(cov[1,1]))\n\n# ==== 조건부 분포 ====\n# 조건부 분포 공식: Y|X=x0 ~ N(mu_Y + Σ_yx Σ_xx^{-1} (x0 - mu_X), Σ_yy - Σ_yx Σ_xx^{-1} Σ_xy)\ndef conditional_Y_given_X(x0):\n    mu_X, mu_Y = mean\n    var_X = cov[0,0]\n    var_Y = cov[1,1]\n    cov_YX = cov[1,0]\n\n    cond_mean = mu_Y + cov_YX/var_X * (x0 - mu_X)\n    cond_var = var_Y - cov_YX**2 / var_X\n    return norm(loc=cond_mean, scale=np.sqrt(cond_var))\n\n# ==== 그리기 ====\nfig, axes = plt.subplots(1, 2, figsize=(12, 4))\n\n# 1) 주변 분포\nxs = np.linspace(-2, 6, 200)\naxes[0].plot(xs, marginal_X.pdf(xs), label='$p(x)$')\naxes[0].plot(xs, marginal_Y.pdf(xs), label='$p(y)$')\naxes[0].set_title('Marginal distributions')\naxes[0].legend()\naxes[0].grid(True)\n\n# 2) 조건부 분포 (예: X=3으로 고정)\nx0 = 3\ncond_dist = conditional_Y_given_X(x0)\nys = np.linspace(-4, 6, 200)\naxes[1].plot(ys, cond_dist.pdf(ys), label=f'$p(y|x={x0})$')\naxes[1].set_title('Conditional distribution of Y given X')\naxes[1].legend()\naxes[1].grid(True)\n\nplt.show()\n\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib import gridspec\nfrom scipy.stats import multivariate_normal, norm\n\n# ==== 파라미터 ====\nmu = np.array([2, 1])\nSigma = np.array([[1, 0.8],\n                  [0.8, 2]])\n\n# ==== joint density grid ====\nx = np.linspace(-2, 6, 200)\ny = np.linspace(-3, 5, 200)\nX, Y = np.meshgrid(x, y)\npos = np.dstack((X, Y))\nrv = multivariate_normal(mu, Sigma)\nZ = rv.pdf(pos)\n\n# ==== marginals ====\nmarg_x = norm(loc=mu[0], scale=np.sqrt(Sigma[0,0]))\nmarg_y = norm(loc=mu[1], scale=np.sqrt(Sigma[1,1]))\n\n# ==== figure layout (2x2, with shared axes) ====\nfig = plt.figure(figsize=(6,6))\ngs = gridspec.GridSpec(2, 2, width_ratios=[4,1], height_ratios=[1,4],\n                       wspace=0.05, hspace=0.05)\n\nax_joint = plt.subplot(gs[1,0])   # main heatmap\nax_margx = plt.subplot(gs[0,0], sharex=ax_joint)  # top marginal\nax_margy = plt.subplot(gs[1,1], sharey=ax_joint)  # right marginal\n\n# Joint density heatmap\nim = ax_joint.imshow(Z, extent=(x.min(), x.max(), y.min(), y.max()),\n                     origin='lower', cmap=\"YlGnBu\", aspect='auto')\n\n# Marginal of X\nax_margx.plot(x, marg_x.pdf(x), 'r--', label='$p(x)$')\nax_margx.axis('off')\nax_margx.legend()\n\n# Marginal of Y\nax_margy.plot(marg_y.pdf(y), y, 'b--', label='$p(y)$')\nax_margy.axis('off')\nax_margy.legend()\n\n# Colorbar\ncbar = fig.colorbar(im, ax=[ax_joint, ax_margx, ax_margy], fraction=0.046, pad=0.04)\ncbar.set_label('density')\n\nax_joint.set_xlabel('$x$')\nax_joint.set_ylabel('$y$')\nax_joint.set_title('Bivariate normal with marginals')\n\nplt.show()\n\n\n\n\n\n\n\n4. 놈과 고유치\n\n\n(1) 벡터 놈 Norm\n벡터의 단일 척도로 추상화된 길이이다. 벡터 \\(\\underset{¯}{x} = \\left( \\begin{array}{r}\nx_{1} \\\\\nx_{2} \\\\\n\\ldots \\\\\nx_{n}\n\\end{array} \\right)\\)에 대한 놈 정의는 다음과 같다. \\(p - Norm\\) : \\(|\\underset{¯}{x}|_{p} = (\\sum_{i}|x_{i}|^{p})^{(1/p)}\\)\n\\({\\underset{¯}{x}}' = (134)\\)에 대하여,\n\\[L1 - norm:|\\underset{¯}{x}| = \\sum_{i}|x_{i}| = 8\\]\n\n\n\\(L2 - norm:|\\underset{¯}{x}|_{2} = (\\sum_{i}x_{i}^{2})^{\\frac{1}{2}} = \\sqrt{26}\\)\n\n유크리디안 거리\n\n\n\\(L\\infty - norm:|\\underset{¯}{x}|_{\\infty} = max|x_{i}| = 4\\) : 최대값n\n\nimport numpy as np\nimport numpy.linalg as la\n\n# L1 노름(맨해튼 거리, 절댓값의 합) 계산\n# 벡터 [1, 3, 4]의 L1 노름 = |1| + |3| + |4| = 8\nL1 = la.norm(np.array([[1, 3, 4]]), 1)\n\n# L2 노름(유클리드 거리, 제곱합의 제곱근) 계산\n# 벡터 [1, 3, 4]의 L2 노름 = sqrt(1^2 + 3^2 + 4^2) = sqrt(26) ≈ 5.099\nL2 = la.norm(np.array([[1, 3, 4]]), 2)\n\n# 결과 출력\nprint(L1, L2)\n4.0 5.099019513592785\n\n\n(2) 고유값 eigen value\n\n\n\n\n\n\n\n\n\n\n행렬 \\(A\\)는 \\(v\\) 벡터를 방향의 변화 없이 변환한다. Eigen 독일어 어원, own 자신의, typycal 전형적인 고유값 \\(\\lambda\\)=1 =&gt; 변화없음, \\(\\lambda = 2\\)=&gt; 길이가 2배, \\(\\lambda = - 1\\)=&gt; 방향 반대\n고유값 구하기\n차수가 \\(k\\)인 정방행렬 \\(A_{k \\times k}\\)(선형계수행렬), 벡터 \\(\\underset{¯}{x}\\)로 이루어진 고유 방정식(characteristic equation) \\(A\\underset{¯}{x} = \\lambda\\underset{¯}{x}\\)를 만족하는 \\(\\lambda\\)들을 고유치(eigen value, characteristic value, latent value)라 하고 각 고유치에 대해 고유방정식을 만족하는 벡터를 고유 벡터(eigen vector)라 한다.\n\\((A - \\lambda I)\\underset{¯}{x} = 0\\)을 만족하는 해는 \\(det(A - \\lambda I) = 0 &lt; = &gt; |A - \\lambda I| = 0\\)을 만족하는 \\(\\lambda\\)를 구하면 된다(Cramer Rule). 이렇게 구한 것을 고유값이라 한다.\n\n차수가 n인 정방행렬 \\(A_{n \\times n}\\) 의 \\(| A- \\lambda I |\\) 행렬식을 계산한다. 이 방정식은 차수가 n인 \\(f(\\lambda) =a_1 \\lambda^n + a_2 \\lambda^{n-1} + ... +a_n\\) 의 다항방정식이다.\n\\(f(\\lambda) =a_1 \\lambda^n + a_2 \\lambda^{n-1} + ... +a_n = 0\\) 특성방정식(characteristic equation) 를 만족하는 \\(\\lambda_1, \\lambda_2, ...,\\lambda_n\\) (n개 존재) 고유치(eigen value, characteristic value, latent value)라 한다.\n이렇게 구한 \\(\\lambda_i\\) 는 \\((A-\\lambda I)\\) 행렬을 singular(역행렬이 존재하지 않는다)로 만든다.\n\n고유벡터 구하기\n\n각 \\(\\lambda_i\\)에 대하여 \\((A- \\lambda_i I) \\underline x = 0\\) 을 만족하는 \\(\\underline x\\) 을 구하면 이를 고유벡터라 한다.\n[단점] \\(\\lambda_i\\) 는 \\((A-\\lambda I)\\) 행렬은 singular이므로 고유벡터 \\(\\underline x\\) 는 무수히 많이 존재한다.\n[활용] 이 성질 때문에 요인 분석에서 요인 회전이 가능하고 주성분분석에서 주성분변수 벡터를 활용 시 \\(\\underline x' \\underline x =1\\) 을 만족하는 정규(Norm) 고유벡터만를 사용한다.\n\n성질\n\n고유치의 합은 행렬 A의 대각원소의 합니다. \\(\\sum\\lambda_{i} = tr(A)\\)\n고유벡터는 서로 독립이다. =&gt; 좌표를 독립(상호 독립, 직교) 공간에 표현한다.\n\n\n\n\n\n\n활용\n고유치와는 달리 고유 벡터는 무수히 많이 존재한다는 것이다. 이 성질 때문에 요인 분석에서 요인 회전이 가능하다. 다음은 벡터의 놈이 1인 단위 고유벡터(주성분분석의 주성분 벡터)를 구한 예제이다.\nimport numpy as np\nimport numpy.linalg as la\nimport matplotlib.pyplot as plt\n\n# ----- 행렬 정의 및 고유분해 -----\nA = np.array([[1, 0.5],\n              [0.5, 1]])\n\neigval, eigvec = la.eig(A)\nprint(\"고유값:\", eigval)\nprint(\"고유벡터:\\n\", eigvec)\n\n# ----- 원 (단위벡터 집합) 그리기 -----\ntheta = np.linspace(0, 2*np.pi, 200)\ncircle = np.array([np.cos(theta), np.sin(theta)])  # 단위원 좌표\n\nfig, ax = plt.subplots(figsize=(6,6))\n\n# 단위 원\nax.plot(circle[0], circle[1], 'b--', label='unit circle')\n\n# ----- 고유벡터 그리기 -----\norigin = np.zeros(2)\ncolors = ['r', 'g']\n\nfor i in range(len(eigval)):\n    # 고유벡터는 방향만 중요하므로 정규화해서 그림\n    v = eigvec[:, i] / la.norm(eigvec[:, i])\n    ax.quiver(*origin, *v, angles='xy', scale_units='xy', scale=1,\n              color=colors[i], label=f'eigvec {i+1}, λ={eigval[i]:.2f}')\n\n# ----- 좌표계 설정 -----\nax.set_xlim(-1.5, 1.5)\nax.set_ylim(-1.5, 1.5)\nax.set_aspect('equal')\nax.axhline(0, color='k', linewidth=0.5)\nax.axvline(0, color='k', linewidth=0.5)\nax.legend()\nax.set_title(\"Unit circle & eigenvectors of A\")\n\nplt.show()\n고유값: [1.5 0.5]  고유벡터:  [[ 0.70710678 -0.70710678]  [ 0.70710678 0.70710678]]"
  },
  {
    "objectID": "notes/mda/mda_pca.html",
    "href": "notes/mda/mda_pca.html",
    "title": "다변량분석 2. 주성분분석",
    "section": "",
    "text": "Chapter 1. 차원축소 개념\n\n1. 특성 추출이란\n대용량 데이터 \\(X_{n \\times p}\\)에서 행 n은 개체, 열 p는 변수를 나타낸다. 개체의 특성을 시각화하거나 분석하기 위해서는 변수가 매우 많을 경우 차원을 축소할 필요가 있다. 이를 feature extraction(특성 추출)이라고 한다. 특성 추출은 원 데이터의 구조적 관계를 활용하여 새로운 변수를 생성하거나, 고차원의 공간을 저차원의 새로운 공간으로 사상하는 방식으로 이루어진다.\n예를 들어 변수 간 상관계수를 유사성 척도로 활용하여 변수 구조를 탐색하기도 한다. 대표적인 방법으로는 주성분분석(PCA), 특이값 분해(SVD)에 기반한 차원축소 기법이 있으며, 두 변수 집합 간의 상관성을 극대화하는 정준상관분석(CCA)도 새로운 특성을 생성한다는 점에서 넓은 의미의 특성 추출 과정으로 이해할 수 있다. 이러한 방법들은 협의의 차원 축소에 속한다.\n특성 선택은 변수에 중요도나 우선순위를 부여하여 목표 변수에 영향을 주는 주요 변수의 부분집합을 선택하거나, 불필요한 변수를 제거하여 변수의 차원(개수)을 줄이는 과정이다. 예를 들어, 의사결정나무 모형에서는 트리 상단에 위치하는 변수가 중요도가 높으며, 이를 바탕으로 변수별 중요도를 평가할 수 있다. 회귀모형에서는 단계적 선택법과 같은 유의 변수 선택 알고리즘을 활용하여 변수 선택이 가능하다.\n특성 선택 기법에는 상관분석과 같은 통계적 방법, 라쏘 회귀·재귀적 특성 제거법·트리 기반 모형, 그리고 판별분석이나 로지스틱 회귀기반 방법 등이 있다.\n\n\n2. 차원축소와 주성분분석\n차원축소는 말 그대로 데이터를 요약하는 과정이다. 변수가 지나치게 많을 때, 모든 변수를 그대로 두고 분석하는 것은 계산량도 커지고 해석도 어렵다. 그래서 정보 손실을 최소화하면서도 적은 수의 새로운 변수로 데이터를 표현하는 방법이 필요하다. 이를 통해 데이터는 단순해지면서도 본질적인 구조와 패턴은 유지할 수 있다. 차원축소의 목적은 크게 두 가지로 볼 수 있는데, 하나는 데이터 시각화를 가능하게 하는 것이고, 다른 하나는 분석 효율성을 높이는 것이다. 시각화는 고차원 데이터를 2차원이나 3차원으로 줄여 그림으로 표현할 수 있게 해 주며, 효율성 측면에서는 계산 속도 향상, 불필요한 잡음 제거, 다중공선성 문제 완화, 저장·전송의 압축 효과 등을 제공한다.\n주성분분석(PCA)은 이러한 차원축소 방법 가운데 가장 널리 쓰이는 전형적인 기법이다. PCA는 데이터가 분산을 가장 크게 보이는 방향을 찾아 새로운 좌표축을 세우고, 그 축을 따라 데이터를 재배치한다. 첫 번째 주성분은 데이터가 가장 넓게 퍼진 방향이고, 두 번째 주성분은 첫 번째와 직교하면서 그다음으로 넓게 퍼진 방향이다. 이렇게 이어지는 주성분들은 서로 독립적이면서도 데이터의 구조를 가장 잘 설명하는 순서대로 배열된다. 따라서 처음 몇 개의 주성분만으로도 원래 데이터의 대부분의 변동성을 설명할 수 있다.\n주성분분석을 통해 얻어지는 결과는 크게 두 가지이다. 하나는 각 개체가 새로운 좌표축에서 어디에 위치하는지를 보여 주는 점수이고, 다른 하나는 새로운 좌표축이 원래 변수들의 어떤 조합으로 만들어졌는지를 알려 주는 적재치이다. 점수는 개체들 사이의 유사성과 차이를 시각적으로 드러내는 데 유용하며, 적재치는 어떤 변수가 서로 묶여 함께 움직이는지를 해석할 수 있게 해 준다.\n결국 차원축소는 데이터의 복잡성을 줄이는 큰 틀의 개념이고, 주성분분석은 그 안에서 가장 대표적이고 직관적인 방법이라고 할 수 있다. 주성분분석은 데이터를 요약하면서도 시각화를 쉽게 하고, 노이즈를 줄이며, 다중공선성과 같은 문제를 완화하는 데 효과적이다. 따라서 차원축소와 주성분분석은 포괄적 개념과 구체적 방법의 관계로 이해할 수 있다.\n\n\n3. 차원축소 방법론\n주성분분석은 차원축소 기법들 가운데 가장 기본적이고 널리 쓰이는 방법이지만, 다른 기법들과의 관계를 이해하면 언제 어떤 방법을 선택해야 하는지 분명해진다.\n먼저 특이값분해(SVD)는 구현 관점에서 PCA와 사실상 같은 몸이라고 볼 수 있다. 데이터를 평균 중심화한 뒤 SVD를 적용하면 곧바로 PCA의 주성분을 얻을 수 있다. 실제로 대용량 데이터에서는 SVD 기반의 계산 방식이 주로 사용되며, 속도와 메모리 문제를 해결하기 위해 랜덤화된 SVD나 점진적 SVD와 같은 변형 기법들이 활용된다.\n요인분석(FA)은 겉보기에는 PCA와 비슷해 보이지만, 목적은 다르다. 요인분석은 공통된 잠재 요인으로 변수들의 공분산 구조를 설명하려는 모형 기반 접근이다. 반면 PCA는 단순히 분산을 최대화하는 새로운 축을 찾는 기하학적 요약 방식이다. 그래서 요인분석은 해석 가능성을 중시할 때, PCA는 데이터 요약과 압축이 필요할 때 주로 쓰인다.\n독립성분분석(ICA)와 비음수행렬분해(NMF)는 데이터 구조를 바라보는 또 다른 시각을 제공한다. ICA는 변수들 간의 독립성을 극대화하려 하고, NMF는 변수들을 음수가 아닌 조합으로만 표현한다는 제약을 둔다. 두 방법 모두 PCA와 달리 분산 대신 다른 기준을 사용해 차원을 축소하며, 데이터의 성질과 해석 목적에 따라 선택된다.\n부분최소제곱회귀(PLS)나 선형판별분석(LDA)은 PCA와 달리 감독학습 기반의 차원축소 기법이다. 이들은 목표변수(y)의 정보를 활용하여 새로운 축을 찾기 때문에, 예측 성능을 높이고자 할 때 주로 적용된다. PCA가 목표변수를 고려하지 않는 비감독적 방법이라는 점과 대비된다.\n마지막으로 커널 PCA는 비선형 구조를 포착하기 위한 확장판이다. PCA가 선형 변환만으로는 잡아내기 힘든 곡선적·비선형적 패턴을, 고차원 공간에서의 변환을 통해 드러내는 방식이다. 따라서 복잡한 데이터 구조를 다루는 데 유리하다.\n정리하면, PCA는 차원축소의 출발점이자 기본형이고, 다른 기법들은 목적과 데이터의 특성에 따라 PCA의 한계를 보완하거나 대체하는 방식으로 발전해왔다고 볼 수 있다.\n\n\n\nChapter 2. 주성분분석\n\n1. 주성분분석 개념\n\n(1) 공간적 개념\n이차원 공간 정보(아래 그림)를 1차원(직선)으로 표시한다면 어디에서 봐야 희생되는 정보(나무의 위치와 거리)가 최소일까? 만약 (1)에서 본다면 1번 나무에 가려 2번 나무는 직선 상에 나타나지 않아 나무가 2개 인 것처럼 보인다. (2)의 관점에서 본다면 역시 3번 나무로 인하여 2번 나무가 가려져 직선 상에 나타나지 않는다.\n\n\n\n\n\n나무의 공간 정보를 최소화하는 관점은 (3)에서 보는 것이다. 그러나 이 역시 희생되는 정보가 있다. 2차원 공간에 있을 때 각 나무의 거리는 직선 상에서는 실제 거리 \\(\\sqrt{2}\\)보다 가까워졌다. 이는 2차원 공간 정보를 1차원 직선으로 표현하여 잃은 정보가 존재한다는 것이다.\n주성분분석은 이처럼 희생되는 정보를 최소화 하면서 데이터의 차원을 축약하는 방법이다. 다음은 주성분 분석을 활용하여 다차원 데이터를 2차원 공간에 축소하여 개체를 시각화 하고 개체를 군집화 하는 과정을 보여 준 것이다. 이처럼 주성분 분석은 최종 분석이 아니라 중간 단계 분석에 해당된다.\n주성분분석(PCA)은 고차원 데이터 속에서 분산이 가장 큰 방향을 찾아 새로운 축을 설정하는 방법이다. 고차원 공간에 존재하는 데이터 점들은 서로 뒤섞여 있어 구조를 파악하기 어렵다. 그러나 주성분분석을 통해 데이터를 새로운 좌표축, 즉 주성분 축으로 투영하면 원래 보이지 않던 패턴이나 군집이 드러나게 된다.\n\n\n\n\n\n그림의 중앙에 있는 점은 원래의 고차원 공간을 나타낸 것이다. 이 공간에서는 점들이 무질서하게 흩어져 있어 클러스터를 구분하기가 쉽지 않다. 하지만 주성분축(Subspace)으로 투영하면 상황이 달라진다. Subspace 1로 데이터를 비추면 Cluster 3이, Subspace 2로 투영하면 Cluster 2가, Subspace 3에서는 Cluster 1이 명확하게 드러난다. 이는 주성분분석이 데이터의 분산이 큰 방향을 찾아내어 차원을 축소하면서 동시에 데이터의 내재된 구조를 보존한다는 것을 보여준다.\n결국 주성분분석은 차원축소를 통해 데이터의 복잡성을 줄이면서도, 군집이나 패턴과 같은 중요한 구조적 특징을 드러내는 대표적인 방법이다.\n\n\n(2) 활용사례\n주성분분석은 많은 변수로 이루어진 데이터를 소수의 핵심 지표로 요약하는 방법이다. 온라인에서 옷을 구입하는 사례가 좋은 비유가 된다. 상의는 단순히 55사이즈, M사이즈와 같이 하나의 값으로 표현되며, 하의 역시 허리둘레 34인치와 같이 하나의 수치로 표시된다. 그러나 맞춤옷을 구입하려면 어깨 넓이, 어깨에서 손목까지의 길이, 허벅지 두께, 허리에서 무릎까지의 길이, 무릎에서 발꿈치까지의 길이 등 다양한 신체 치수를 측정해야 한다. 이 많은 정보가 결국 하나의 단위로 축약된 것이 바로 55사이즈나 34인치라는 값이다. 이러한 값은 다른 여러 치수 정보를 반영하는 일종의 골든 지표, 즉 주성분이라고 볼 수 있다. 따라서 많은 정보를 하나로 요약하는 과정에서 세부적인 차이가 사라지므로, 온라인에서 구매한 옷이 모든 사람의 체형에 꼭 맞지 않는 것이다. 이 때문에 Big & Tall 매장이나 디자이너 매장이 따로 존재한다.\n이미지 데이터 역시 주성분분석의 활용을 이해하기 좋은 예다. 이미지 데이터는 기본적으로 가로, 세로, 색상 채널을 포함하는 3차원 공간 데이터이다. 그러나 통계 분석이나 기계학습에 활용하려면 이를 2차원 행렬 데이터로 축약해야 한다. 축약 과정을 사람이 직접 설계한다면 학습지도 기법, 즉 전통적 머신러닝에 해당한다. 반대로 컴퓨터가 스스로 중요한 특징을 찾아내는 경우는 비학습지도 기법, 즉 딥러닝에 해당한다. 이러한 학습 과정에서 널리 쓰이는 알고리즘으로는 서포트 벡터 머신(SVM), 아다부스트(AdaBoost) 등이 있다.\n\n\n\n\n\n이미지나 음성, 텍스트와 같은 데이터는 본질적으로 고차원 공간에 존재한다. 예를 들어 자동차 사진은 수많은 픽셀 값으로 이루어져 있어 차원이 수천에서 수백만에 이른다. 이렇게 높은 차원의 데이터를 바로 다루기는 어렵기 때문에, 먼저 핵심 정보를 유지하면서 데이터의 차원을 축약하는 과정이 필요하다.\n머신러닝은 차원축약과 특징 추출을 사람이 직접 설계하고 정의하는 방식이다. 분석자가 중요한 특징을 선택하거나 PCA와 같은 방법을 통해 차원을 줄인 후, 그 결과를 학습 알고리즘(SVM, AdaBoost 등)에 입력한다. 이때 분류기는 입력된 특징을 바탕으로 자동차인지 아닌지를 판별한다. 따라서 머신러닝은 사람이 특징을 만들어주고, 알고리즘은 그것을 이용해 학습하는 방식이다.\n딥러닝은 이와 달리 사람이 직접 특징을 설계하지 않는다. 인공신경망이 여러 층을 거치면서 데이터를 단계적으로 변환하여 스스로 중요한 특징을 추출하고 차원을 축약한다. 즉, 원본 이미지가 그대로 신경망에 입력되면, 네트워크 내부에서 차원축약과 특징 추출이 동시에 이루어지고, 마지막 단계에서 자동차인지 아닌지의 판별이 자동으로 수행된다. 이는 기계가 스스로 특징을 학습하는 방식이다.\n따라서 머신러닝과 딥러닝의 차이는 차원축약과 특징 추출이 사람의 설계에 의해 이루어지느냐, 기계가 스스로 학습하느냐에 있다. 그림은 이를 시각적으로 보여주며, ”Car vs Not Car”라는 단순한 예시를 통해 두 접근법의 차이를 직관적으로 설명한다.\n\n\n\n2. 주성분 구하기\n\n(1) 데이터 행렬, 변수벡터\n데이터 행렬 (개체 행 n, 변수 열 p) : \\(X_{n \\times p} = \\begin{pmatrix}\nx_{11} & x_{12} & ... & x_{1p} \\\\\nx_{21} & x_{22} & ... & x_{2p} \\\\\n& ... & & \\\\\nx_{n1} & x_{n2} & ... & x_{np}\n\\end{pmatrix}\\)\n변수벡터 : \\({\\underset{¯}{x}}_{1 \\times p} = \\left( \\begin{array}{r}\nx_{1} \\\\\nx_{2} \\\\\n... \\\\\nx_{p}\n\\end{array} \\right)\\), \\(x_{i}\\)는 i -번째 확률변수이며 \\(\\underset{¯}{x} \\sim (\\underset{¯}{\\mu},\\Sigma)\\) 다변량 분포이다.\n평균벡터: \\({\\underset{¯}{\\mu}}_{1 \\times p} = \\left( \\begin{array}{r}\n\\mu_{1} \\\\\n\\mu_{2} \\\\\n... \\\\\n\\mu_{p}\n\\end{array} \\right)\\), \\(E(x_{i}) = \\mu_{i}\\)\n공분산 행렬 \\(\\Sigma_{p \\times p} = \\begin{pmatrix}\n\\sigma_{11} & \\sigma_{12} & ... & \\sigma_{1p} \\\\\n\\sigma_{21} & \\sigma_{22} & ... & \\sigma_{2p} \\\\\n& ... & & \\\\\n\\sigma_{p1} & \\sigma_{p2} & ... & \\sigma_{pp}\n\\end{pmatrix}\\)\n\n\n(2) 주성분 전처리\n데이터에서 개체를 설명하는 변수의 구조는 변수들의 변동성과 공변동에 의해 요약된다. 이 구조를 수량화하는 대표적 도구가 공분산행렬이며, 변수의 단위와 규모 차이를 제거하여 비교 가능하게 만든 것이 상관행렬이다. 공분산행렬은 각 변수의 측정 단위와 규모(분산)를 그대로 반영하는 반면, 상관행렬은 변수를 평균 0, 분산 1로 표준화한 뒤의 상호 관련성을 반영한다.\n따라서 변수의 단위가 같고 분산의 크기 차이가 의미 있는 정보라면 공분산행렬을 사용하는 것이 타당하다. 반대로 변수의 단위가 서로 다르거나, 분산의 규모 차이로 특정 변수가 결과를 지배할 우려가 있으면 상관행렬(= 표준화 후 공분산행렬)을 사용하는 것이 바람직하다. 상관행렬 기반 PCA는 곧 표준화 변수를 입력으로 한 공분산 기반 PCA와 동치이다. 한편, 두 경우 모두 중심화(평균 0)는 필수적이다.\n확률변수(데이터) \\(x_{i}\\)의 평균은 \\(\\overline{x}\\), 표준편차를 \\(s(x)\\)라 하자.\n중심화 centering : \\(c_{i} = x_{i} - \\overline{x}\\), 변수의 평균만 0으로 이동한다.\n표준화 standardization : \\(z_{i} = \\frac{x_{i} - \\overline{x}}{s(x)}\\): 평균=0, 표준편차 1로 변환한다.\n\n\n\n\n\n\n\n\n\n상황\n중심화 필요 여부\n표준화 필요 여부\n사용 행렬\n\n\n모든 변수가 같은 단위, 분산 차이 의미 있음\n필요\n불필요\n공분산 행렬\n\n\n모든 변수가 같은 단위, 분산 차이 너무 큼\n필요\n경우에 따라\n공분산/상관행렬\n\n\n변수가 서로 다른 단위(예: cm, kg)\n필요\n필요\n상관행렬\n\n\n\nPCA를 구할 때는 우선 변수를 평균 0으로 중심화해야 한다. 또한 변수의 단위가 서로 다르거나 분산 크기가 크게 차이 나는 경우에는 표준화까지 수행해야 한다. 이때 분석은 상관행렬을 기반으로 하게 된다.\n\n\n(3) 주성분 구하기\n데이터 공분산행렬: \\(S_{p \\times p} = \\begin{pmatrix}\ns_{11} & s_{12} & ... & s_{1p} \\\\\ns_{21} & s_{22} & ... & s_{2p} \\\\\n& ... & & \\\\\ns_{p1} & s_{p2} & ... & s_{pp}\n\\end{pmatrix}\\)\n데이터 상관계수 행렬: \\({\\widehat{R}}_{p \\times p} = \\begin{pmatrix}\nr_{11} & r_{12} & ... & r_{1p} \\\\\nr_{21} & r_{22} & ... & r_{2p} \\\\\n& ... & & \\\\\nr_{p1} & r_{p2} & ... & r_{pp}\n\\end{pmatrix}\\)\n공분산행렬(상관계수행렬)은 양반정치(positive definite) 행렬이므로 다음 식을 만족하는 고유값 \\(\\lambda\\)는 0보다 큰 실수이고 행렬의 차수 p개만큼 존재한다.\n선형계수 행렬(부하 행렬) \\(L_{n \\times n}\\)에 의해 원 데이터 행렬로부터 주성분 행렬 \\(Y_{n \\times p} = X_{n \\times p}L_{p \\times p}\\)가 계산된다. 주성분 데이터의 차원은 원데이터 행렬의 차수와 동일하다.\n\\({\\underset{¯}{y}}_{1 \\times p} = \\left( \\begin{array}{r}\ny_{1} \\\\\ny_{2} \\\\\n... \\\\\ny_{p}\n\\end{array} \\right)\\)\\(= \\begin{pmatrix}\nl_{11} & l_{12} & ... & l_{1p} \\\\\nl_{21} & l_{22} & ... & l_{2p} \\\\\n& ... & & \\\\\nl_{p1} & l_{p2} & ... & l_{pp}\n\\end{pmatrix}\\)\\(\\left( \\begin{array}{r}\nx_{1} \\\\\nx_{2} \\\\\n... \\\\\nx_{p}\n\\end{array} \\right)\\)=\\(L\\underset{¯}{x}\\)\n위의 식에 의해 원데이터와 선형계수를 이용하여 계산된 주성분 값을 주성분 점수 score 라 한다.\n선형계수 (부하)\n공분산행렬 S의 고유벡터 중 \\({\\underset{¯}{e}}_{i}'\\underset{¯}{e} = 1\\)을 만족하는 놈(Norm, 정규) 고유벡터를 k-번째 주성분 변수의 선형계수(부하) 벡터 \\({\\underset{¯}{l}}_{k}\\)로 사용한다.\n왜 고유벡터(선형계수)를 이용하나?\n주성분분석은 데이터가 가장 넓게 퍼져 있는 방향, 즉 분산이 최대가 되는 축을 찾는 것이 목적이다. 공분산행렬은 변수들의 분산과 공분산을 모두 담고 있어 데이터가 어떤 방향으로 크게 퍼져 있는지를 보여주는 지도를 제공한다. 이 공분산행렬을 고유분해하면 고유벡터와 고유값을 얻을 수 있는데, 고유벡터는 데이터가 퍼져 있는 방향을, 고유값은 그 방향으로의 분산 크기를 의미한다.\n따라서 가장 큰 고유값에 대응하는 고유벡터는 데이터가 가장 넓게 퍼진 방향이 되고, 이것이 첫 번째 주성분이 된다. 두 번째로 큰 고유값의 고유벡터는 첫 번째 주성분과 직교하면서 남은 분산을 가장 크게 설명하는 방향이 되고, 이렇게 차례대로 주성분들이 정해진다.\n결국 주성분을 공분산행렬의 고유벡터로 구하는 것은 우연이 아니라, 주성분분석의 핵심 목표인 ”데이터의 분산을 가장 잘 보존하는 새로운 좌표축”을 찾는 데에 고유벡터가 정확히 그 역할을 하기 때문이다.\n제1 주성분 변수\n\\({\\underset{¯}{e}}_{i}'{\\underset{¯}{e}}_{i} = 1\\)이면서 \\(V({\\underset{¯}{e}}_{i}'\\underset{¯}{x})\\)을 최대화 하는 열벡터를 구하고 이를 \\({\\underset{¯}{l}}_{1}\\)이라 하고 이를 선형계수로 하여 제 1 주성분 변수를 구한다. \\({\\underset{¯}{y}}_{1} = {\\underset{¯}{l}}_{1}'\\underset{¯}{x}\\)\n제2 주성분 변수\n\\({\\underset{¯}{l}}_{1}'{\\underset{¯}{e}}_{j} = 1\\)이면서 \\({\\underset{¯}{e}}_{j}'{\\underset{¯}{e}}_{j} = 1\\) 이면서 \\(V({\\underset{¯}{e}}_{j}'\\underset{¯}{x})\\)을 최대화 하는 열벡터를 구하고 이를 \\({\\underset{¯}{l}}_{2}\\)이라 하고 이를 선형계수로 하여 제2 주성분변수 \\({\\underset{¯}{y}}_{2} = {\\underset{¯}{l}}_{2}'\\underset{¯}{x}\\)을 구한다. 이는 제1 주성분 변수와는 서로 독립이고 원변수 변동에 대한 설명력은 제1 주성분 변수보다 낮다.\n제3주성분 변수\n\\({\\underset{¯}{l}}_{1}'{\\underset{¯}{e}}_{k} = 1\\), \\({\\underset{¯}{l}}_{2}'{\\underset{¯}{e}}_{k} = 1\\)이면서 \\({\\underset{¯}{e}}_{k}'{\\underset{¯}{e}}_{k} = 1\\) 이면서 \\(V({\\underset{¯}{e}}_{k}'\\underset{¯}{x})\\)을 최대화 하는 열벡터를 구하고 이를 이를 선형계수로 하여 제3 주성분변수 \\({\\underset{¯}{y}}_{3} = {\\underset{¯}{l}}_{3}'\\underset{¯}{x}\\)을 구한다. 제3 주성분 변수는 제1, 제2 주성분변수와는 독립이고 변동 설명력은 제1, 제2 주성분 변수보다 낮다. 이렇게 계속 원변수의 개수만큼 주성분변수를 구한다.\n공분산 행렬 고유값 및 고유벡터 성질\n\n양반 정치 행렬인 공분산 행렬의 고유치는 모두 양의 실수이고, 원변수의 개수(차수)만큼 존재한다. \\(\\lambda_{1} \\geq \\lambda_{2} \\geq ...\\lambda_{p} \\geq 0\\)\n고유 값이 제일 큰 \\(\\lambda_1\\) 에 의해 만들어지는 주성분변수를 제1 주성분, \\(\\lambda_2\\) 에 의해 만들어지는 주성분 변수를 제2 주성분 변수, 그리고 제3 주성분 변수 … 이다.\n고유값 \\(\\lambda_{k}\\)에 대응하는 고유벡터는 무수히 많이 존재하는데 그 중 놈 벡터(\\({\\underset{¯}{e}}_{k}'{\\underset{¯}{e}}_{k} = 1\\))를 선형계수 벡터, 부하 벡터로 하여 제 k-주성분 변수, \\({\\underset{¯}{y}}_{k} = l_k' \\underline x\\) 을 구한다.\n모든 주성분 변수들은 원 변수의 선형 결합이고 서로 독립이다. 이는 모든 고유 벡터(선형계수 벡터)는 서로 독립이 되도록 하여 구하였기 때문이다. 즉, \\({\\underset{¯}{y}}_{i}'{\\underset{¯}{y}}_{j} = 0,fori \\neq j\\)이다.\nk-번째 주성분 변수 \\({\\underset{¯}{y}}_{k}\\)의 분산은 \\(V({\\underset{¯}{y}}_{k}) = \\lambda_{k}\\)이고 원 변수의 변동합은 고유값의 합과 같다. \\(\\sigma_{11} + \\sigma_{22} + ... + \\sigma_{pp} = \\lambda_{1} + \\lambda_{2} + ... + \\lambda_{p}\\)\n만약 공분산 행렬의 경우 원 변수의 변동합은 상관계수 대각원소(모두 1)의 합이므로 변수의 개수 p가 대각원소 합이므로 \\(p = \\lambda_{1} + \\lambda_{2} + ... + \\lambda_{p}\\)이다.\n그러므로 제k-번째 주성분 변동 기여율은 \\(\\frac{\\lambda_{k}}{\\sum_{i = 1}^{p}\\lambda_{i}}\\)이다. 즉 고유값 \\(\\lambda_{k}\\)는 k-번째 주성분이 원변수들릐 변동을 설명하는 능력이다. 상관계수 행렬의 변동 기여율은 \\(\\frac{\\lambda_{k}}{p}\\)이다.\n주성분 변수는 원 변수들의 변동을 설명하고 순서대로 설명력은 줄어든다.\n\n시각적 직관\n그림 속 원 데이터는 원형에 가깝게 흩어져 있어 X와 Y 변수 간의 상관관계는 거의 0에 가깝다. 두 변수의 분산 역시 동일하기 때문에, 원래 좌표축에서는 특별히 한쪽 축이 더 큰 변동을 설명한다고 보기는 어렵다. 그러나 데이터를 주성분 축으로 회전시키면 상황은 달라진다.\n\n\n\n\n\n주성분 1과 주성분 2는 서로 직교하는 축이므로 독립적이다. 이때 주성분 1 방향의 분산은 타원의 장축에 해당하며, 주성분 2 방향의 분산은 단축에 해당한다. 따라서 주성분 1이 설명하는 변동성이 주성분 2보다 크다. 즉, 데이터의 구조를 파악하는 데 있어 주성분 1의 설명력이 더 크다고 할 수 있다.\n결과적으로 이 경우에는 원래의 두 변수(X, Y)를 모두 사용하는 것과 주성분 1만으로 개체를 설명하는 것 사이에 큰 차이가 없다. 다시 말해, 주성분분석을 통해 얻은 단 하나의 주성분으로도 데이터의 본질적인 분산 구조를 충분히 설명할 수 있다.\n\n\n(4) 주성분 개수 결정\n주성분변수는 원래 변수들의 공분산행렬(혹은 변수 단위가 다른 경우 상관행렬)을 고유분해하여 얻은 고유값과 고유벡터를 바탕으로 구성된다. 각 주성분은 해당 고유값의 크기만큼 원 데이터의 변동을 설명하며, 고유벡터는 주성분을 형성하는 선형계수 역할을 한다. 주성분들은 서로 독립적이며, 제1주성분이 가장 큰 변동을 설명하고, 그다음 제2주성분, 제3주성분 순으로 설명력이 줄어든다. 따라서 주성분의 개수 자체는 원래 변수의 개수와 동일하게 존재한다.\n그러나 주성분분석의 본래 목적은 모든 주성분을 다 사용하는 것이 아니다. 주성분분석은 데이터 차원을 줄여서 보다 단순한 구조로 요약하려는 데에 그 의의가 있다. 즉, 원래 변수의 개수가 많더라도 실제로는 누적 설명력이 충분한 일부 주성분만을 선택해 사용함으로써 변수의 차원을 축소하는 것이 주성분분석의 핵심이다.\n변동 기여율 기준\n주성분분석에서 각 주성분은 원래 데이터의 변동성을 일정 부분 설명한다. 이때 한 주성분이 전체 변동성 가운데 얼마만큼을 차지하는지를 나타내는 비율을 변동 기여율(Explained Variance Ratio)이라 한다.\n[공분산행렬] k-번째 주성분변수의 변동설명 기여율(variance explianed ratio) = \\(\\frac{\\lambda_{k}}{\\sum_{i}^{p}\\lambda_{i}}\\)\n[상관행렬] k-번째 주성분변수의 변동설명 기여율 = \\(\\frac{\\lambda_{k}}{p}\\)\n예를 들어 제1주성분의 고유값이 전체 고유값의 합에서 차지하는 비율이 40%라면, 제1주성분이 전체 데이터 변동성의 40%를 설명한다는 의미이다. 또한 변동 기여율을 누적하여 계산하면, 여러 개의 주성분이 함께 전체 변동성을 얼마만큼 설명하는지를 알 수 있다. 이를 누적 변동 기여율이라고 한다. 예를 들어 제1, 제2, 제3주성분까지의 누적 기여율이 85%라면, 이 세 개 주성분만으로도 데이터의 전체 변동성의 85%를 보존할 수 있음을 의미한다.\n변동 기여율은 주성분의 개수를 선택할 때 중요한 기준이 된다. 보통 누적 기여율이 일정 수준(예: 70% 이상, 혹은 80~90% 이상)에 도달하는 지점까지만 주성분을 선택한다. 이렇게 하면 원 데이터의 대부분의 정보를 유지하면서도 변수 수를 크게 줄일 수 있다.\n【Cochran Rule (Rule of Thumb)】 원변수 변동설명의 누적 기여율이 80%까지 되는 주성분변수까지 선택한다. 즉 20% 정보는 희생된다. 원변수의 상관관계가 높을수록 변수의 차원은 축약이 쉽게되므로 80% 규칙에 의한 주성분변수 개수는 작게 된다.\n원 변수들의 상관관계가 높지 않으면 80%를 선택하게 되면 주성분 개수가 많아져 차원 축소가 적어져 일반적으로 주성분은 4개 이상 사용하지는 않는다.\n고유값 1이상(Kaiser 기준)\n상관행렬을 이용하는 경우 원변수 변동의 합은 p이므로 평균인 1이상 고유값을 갖는 주성분변수만 선택한다. 일반적으로 고유값 1 이상이 주성분을 선택하면 누적 변동설명 기여율 80%와 일치한다.\n스크리 플롯(Scree Plot) 방법\n고유값의 크기를 순서대로 그래프에 그리면, 설명력이 급격히 감소하다가 완만해지는 지점이 나타난다. 이 꺾이는 부분(엘보 포인트) 이전까지의 주성분을 선택하는 것이 일반적이다. 아래 그림의 스크리 플롯을 기준으로 할 때, 곡선이 꺾이는 지점인 엘보 포인트는 네 번째 주성분 근처에서 나타난다. 따라서 엘보 기준으로 선택되는 주성분의 개수는 4개이다. 반면 Kaiser 기준(고유값이 1보다 큰 경우)을 적용하면 첫 번째, 두 번째, 세 번째 주성분까지만 선택된다. 따라서 이 경우 Kaiser 기준에 따른 주성분 개수는 3개이다.\n\n\n\n\n\n즉, 같은 데이터라도 엘보 기준은 4개, Kaiser 기준은 3개라는 서로 다른 결과를 제시할 수 있으며, 실제 분석에서는 누적 설명 분산비율과 연구 목적까지 함께 고려하여 최종 개수를 결정하게 된다.\n\n\n(5) 주성분 변수 이름 부여\n주성분변수는 원래 변수들의 선형결합으로 정의된다. 즉, 공분산행렬(또는 상관행렬)의 고유벡터를 선형계수로 하여 원 변수를 변환한 것이 주성분변수이다. 이때 각 원 변수의 기여 정도를 나타내는 계수가 바로 부하(loadings)이며, 부하의 절댓값이 클수록 해당 변수가 주성분 형성에 더 큰 영향을 미친다.\n따라서 주성분에 이름을 부여하는 과정은, 부하 행렬에서 어떤 변수가 주성분에 강하게 기여하는지를 살펴보고 그 특징을 요약하여 직관적인 이름을 붙이는 것이다.\n그러나 이 과정은 결코 단순하지 않다. 하나의 주성분에 여러 변수가 동시에 기여하거나, 서로 해석이 상충되는 부하 값이 섞여 있을 수 있기 때문이다. 또한 부호 방향에 따라 변수 간의 해석이 달라질 수 있고, 실제 데이터의 맥락과 결합해 해석하지 않으면 오해의 소지가 크다.\n결국 주성분의 이름 부여는 수학적 계산 결과만으로 자동으로 정해지는 것이 아니라, 연구자가 부하 값의 크기와 방향을 바탕으로 변수군의 공통된 의미를 찾아내는 해석 과정이다. 따라서 단순히 부하의 절댓값만 보는 것이 아니라, 해당 데이터의 맥락과 연구 목적을 종합적으로 고려해야 한다.\n즉, 주성분의 이름은 부하 행렬에서 큰 기여를 보이는 변수들의 특성을 요약해 붙이지만, 이는 연구자의 통계적 해석과 도메인 지식이 반드시 결합되어야 하는 작업이다.\n\n\n\n3. 주성분 사례분석\n\n(1) Lahman Baseball Database\n파이썬의 lahman 모듈은 파이썬에서 메이저리그(MLB) 통계의 표준 데이터베이스인 Lahman Baseball Database를 불러다 쓸 수 있도록 만든 패키지이다.\n\nLahman Baseball Database란? Sean Lahman이 만든 공개 MLB 기록 데이터베이스\n1871년부터 최근 시즌까지 선수별/팀별 성적, 경기 기록, 연봉, 올스타, 수상 내역 등을 체계적으로 정리한 자료, 가장 최근 데이터 불러 왔는데 2020년 데이터이다. yearID=2020\nLahman DB를 파이썬 pandas DataFrame 형태로 불러오는 간단한 API 제공한다.\n\nbatting() → 타자 성적 (연도별, 팀별 기록 포함)  pitching() → 투수 성적  fielding() → 수비 기록  people() → 선수 기본 인적 정보 (이름, 생년월일, 출신국 등)  teams() → 팀별 시즌 기록  salaries() → 연봉 데이터\n불러온 데이터프레임은 곧바로 pandas 연산, scikit-learn 모델링, matplotlib 시각화 등에 활용 가능하다.\n!pip install lahman #모듈 설치\n\nfrom lahman import batting, people\nimport pandas as pd\n\n# 1) 성적 데이터\nbat = batting()\nlatest_year = int(bat['yearID'].max())\nbat_latest = bat[bat['yearID'] == latest_year].copy()\nbat_latest = bat_latest[bat_latest['AB'].fillna(0) &gt;= 200]\n\n# 2) 이름 데이터\npeo = people()\n\n# 필요한 열만 추출 (이름/ID)\npeo_names = peo[['playerID','nameFirst','nameLast']]\n\n# 3) merge\nbat_with_name = bat_latest.merge(peo_names, on='playerID', how='left')\n\n# 4) 확인\nbat_with_name.head()\n이하 분석은 200 타석 이상인 66명 타자에 대하여 분석이다.\n\n\n\n\n\n\n\n(2) 상관분석\n주성분분석(PCA)은 변수들 간의 상관관계를 토대로 새로운 축을 찾아내어 데이터의 차원을 줄이는 방법이다. 이렇게 도출된 주성분은 다시 개체의 분류나 예측 모델의 설명 변수로 활용되는 등 다양한 2차 분석으로 이어질 수 있다. 따라서 PCA를 수행하기에 앞서 변수들 간의 상관 구조를 사전에 분석하는 것은 매우 중요한 의미를 가진다.\n첫째, 상관분석은 변수 간 중복성, 즉 다중공선성의 존재 여부를 파악할 수 있게 해 준다. 주성분분석의 목적은 서로 강하게 연관된 변수들을 요약해 새로운 축으로 재구성하는 데 있다. 만약 변수들 사이의 상관이 거의 없다면 주성분분석을 통해 얻을 수 있는 정보의 압축 효과는 크지 않다. 반대로 변수들이 강하게 상관되어 있다면 소수의 주성분만으로도 전체 변동을 효과적으로 설명할 수 있다. 결국 상관관계의 크기는 PCA를 수행할 가치가 있는지 판단하는 중요한 지표가 된다.\n둘째, 상관분석은 변수의 단위 문제를 점검하는 데 도움이 된다. 서로 다른 단위를 가진 변수를 공분산 행렬을 통해 분석하면 단위가 큰 변수가 주성분의 방향을 지배하게 된다. 이때 상관행렬을 사용하면 모든 변수를 표준화한 것과 동일한 효과를 주어 단위 차이를 제거할 수 있다. 따라서 사전 상관분석을 통해 변수 간의 구조를 비교할 때 표준화가 필요한지를 확인할 수 있다.\n셋째, 상관분석은 주성분 해석의 방향성을 제공한다. 예를 들어 키와 체중이 강한 양의 상관을 가진다거나, 속도와 시간이 음의 상관을 보인다는 사실은 이후 주성분을 해석할 때 중요한 단서가 된다. 특정 변수들이 함께 움직이는 경향이나 서로 반대 방향으로 움직이는 패턴을 파악해 두면, 추출된 주성분의 의미를 보다 명확하게 해석할 수 있다.\n넷째, 상관분석은 불필요한 변수, 즉 노이즈 변수를 걸러내는 데에도 유용하다. 어떤 변수는 다른 변수들과 거의 상관이 없고 독립적으로 움직일 수 있는데, 이런 변수는 전체 구조 요약에 기여도가 낮다. 이 경우 PCA에 포함시키더라도 분석의 효율을 떨어뜨릴 수 있다. 따라서 사전에 상관분석을 실시하면 기여도가 낮은 변수를 미리 파악해 제거함으로써, 보다 간결하고 해석 가능한 주성분 구조를 얻을 수 있다.\n이처럼 상관분석은 PCA의 전처리 단계로서 단순한 변수 관계의 탐색을 넘어, 분석 수행의 타당성 점검, 단위 조정 필요성 확인, 주성분 해석의 실마리 제공, 그리고 불필요한 변수 제거를 통한 효율성 제고라는 중요한 역할을 한다고 할 수 있다.\n# 1) 분석 변수\ncols = ['HR','RBI','SB','BB','SO','H','AB','GIDP']\nX = bat_with_name[cols].fillna(0)\n\n# 2) 상관행렬 계산\ncorr_matrix = X.corr()\n\n# 3) 히트맵 시각화\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.figure(figsize=(8,6))\nsns.heatmap(corr_matrix, annot=True, cmap=\"coolwarm\", fmt=\".3f\", linewidths=0.5)\nplt.title(f\"{latest_year} correlation matrix\")\nplt.show()\n\n\n\n\n\n주성분분석을 실시하기 전에 상관행렬을 살펴보면, 변수들 간의 중복성과 구조를 확인할 수 있다. 2020년 타격 지표 상관분석 결과를 보면, 홈런(HR)과 타점(RBI)의 상관계수가 0.777로 매우 높게 나타나는데, 이는 두 지표가 사실상 같은 성향을 반영하고 있음을 의미한다. 따라서 PCA에서는 이 두 변수가 동일한 주성분 축에 강하게 기여할 가능성이 크다. 또한 안타(H)와 타수(AB) 역시 상관계수가 0.594로 높게 나타나, 경기 기회가 많을수록 안타도 많아지는 구조가 반영된 것이다. 이러한 변수 간의 강한 상관은 다중공선성 문제를 드러내며, 바로 PCA를 적용할 필요성을 보여준다.\n또한 변수들의 측정 단위가 다르기 때문에, 단위가 큰 변수가 주성분 방향을 지배하지 않도록 상관행렬 기반의 PCA를 활용하는 것이 타당하다. 상관행렬을 사전에 확인하는 과정은 이러한 표준화 필요성을 확인하는 절차가 된다.\n상관계수의 패턴은 주성분 해석의 방향성도 제시한다. HR–RBI–BB는 서로 양의 상관을 보이므로 ’장타 및 생산력’ 요인으로 묶일 수 있고, H–AB의 높은 상관은 ’경기 기회와 안타 생산’ 요인을 형성할 수 있다. 한편 삼진(SO)은 안타(H)와 음의 상관(-0.294)을 보이는데, 이는 ’컨택 능력 대 삼진 성향’이라는 대비적 축으로 해석될 수 있다.\n마지막으로 상관계수가 전반적으로 낮은 변수들은 노이즈 변수로 볼 수 있다. 병살타(GIDP)는 대부분의 지표와 관련성이 약해 전체 변동 요약에 크게 기여하지 못할 가능성이 크고, 도루(SB) 역시 주요 타격 지표들과는 거의 상관이 없어 별도의 부차적 주성분 축을 형성할 가능성이 있다.\n결국 이 상관행렬을 통해 PCA에서 형성될 주성분의 구조를 예측할 수 있다. HR–RBI–BB가 하나의 주성분을, H–AB가 또 다른 주성분을 형성하며, SO는 이와 대립되는 방향의 변동을 설명하고, SB와 GIDP는 부차적이거나 설명력이 낮은 변수로 작용할 것임을 짐작할 수 있다. 이는 곧 PCA가 선수들의 타격 성향을 2~3개의 주성분으로 압축할 수 있음을 보여주는 사전적 근거이다.\n\n\n(3) 주성분 구하기\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\n\n# 1) 분석 변수\ncols = ['HR','RBI','SB','BB','SO','H','AB','GIDP']\nX = bat_with_name[cols].fillna(0)\n\n# 2) 표준화\nscaler = StandardScaler()\nX_std = scaler.fit_transform(X)\n\n# 3) PCA (전체 성분 학습)\npca = PCA()\nscores = pca.fit_transform(X_std)\n\n# 3-1) 점수 붙이기\nfor j in range(scores.shape[1]):\n    bat_with_name[f'PC{j+1}'] = scores[:, j]\n\n# 4) 고유값과 Kaiser 필터\neigs = pca.explained_variance_\nmask = eigs &gt;= 1\nselected_idx = np.where(mask)[0]\nselected_eigs = eigs[mask]\n\n# 5) 부하 행렬 (선택된 PC만)\nloadings = pd.DataFrame(\n    pca.components_.T[:, mask] * np.sqrt(selected_eigs),\n    index=cols,\n    columns=[f'PC{i+1}' for i in selected_idx]   # ✅ +1 제거!\n)\n\n# ✅ 부하 행렬 출력\nprint(\"고유값 (전체):\", np.round(eigs, 3))\nprint(\"선택된 주성분 (λ ≥ 1):\", selected_idx+1)\nprint(\"부하 행렬 (loadings):\")\nprint(loadings.round(3))\n\n# 6) 각 주성분의 설명 분산 비율\nexplained_ratio = pca.explained_variance_ratio_\n\n# 7) 누적 설명 분산 비율\ncumulative_ratio = np.cumsum(explained_ratio)\n\n# ✅ 출력\nprint(\"주성분별 변동 설명 기여율:\")\nfor i, (r, c) in enumerate(zip(explained_ratio, cumulative_ratio), start=1):\n    print(f\"PC{i}: {r:.3f} (누적 {c:.3f})\")\n고유값 (전체): [2.423 1.854 1.258 0.945 0.633 0.542 0.306 0.162]  선택된 주성분 (λ ≥ 1): [1 2 3]  부하 행렬 (loadings):  PC1 PC2 PC3  HR 0.776 0.467 -0.070  RBI 0.865 0.282 -0.200  SB 0.090 -0.291 0.838  BB 0.135 0.719 0.186  SO 0.079 0.609 0.175  H 0.696 -0.532 0.122  AB 0.709 -0.371 0.238  GIDP 0.229 -0.403 -0.611\n주성분별 변동 설명 기여율:  PC1: 0.298 (누적 0.298)  PC2: 0.228 (누적 0.527)  PC3: 0.155 (누적 0.681)  PC4: 0.116 (누적 0.798)  PC5: 0.078 (누적 0.876)  PC6: 0.067 (누적 0.942)  PC7: 0.038 (누적 0.980)  PC8: 0.020 (누적 1.000)\n\n\n(4) 주성분 이름부여\n#부하 산점도 그리기\nimport plotly.express as px\n\n# Plotly 산점도 (PC1 vs PC2)\nfig = px.scatter(\n    loadings.reset_index(),   # index(변수명)를 컬럼으로 복원\n    x=\"PC1\",\n    y=\"PC2\",\n    text=\"index\"              # 변수명을 라벨로 표시\n)\n\nfig.update_traces(textposition=\"top center\")\nfig.update_layout(title_text=\"Loading Scatterplot of PC1 and PC2\")\nfig.show()\n\n\n\n\n\nPC1: 파워\n\n높은 부하: RBI (0.865), HR (0.776), H (0.696), AB (0.709) → 타점, 홈런, 안타, 타수 모두 양의 기여.\n해석: 타격 생산력과 파워를 반영하는 주성분: ”타격 생산력/파워 성분”\n\nPC2: 선구안\n\n높은 부하: BB (0.719), SO (0.609) (+), HR (0.467)(중간), 음의 부하: H (-0.532), GIDP (-0.403) → 볼넷과 삼진이 동시에 높고, 안타는 음의 방향.\n해석: 컨택(안타 중심)과 선구안·삼진 경향의 대비: ”선구안 vs 컨택 성분”\n\nPC3: 주루\n\n높은 부하: SB (0.838) 양수, GIDP (-0.611) 음수 → 도루가 많은 선수일수록 병살타는 적은 경향.\n해석: 주루 능력(스피드)을 보여주는 주성분: ”스피드/주루 성분”\n\n\n\n(5) 주성분 활용(1) 개체분류\n# PC1, PC2 산점도\nplt.figure(figsize=(9,7))\nplt.scatter(bat_with_name['PC1'], bat_with_name['PC2'], alpha=0.6)\n\n# 선수 이름 표시 (홈런 15개 이상 선수)\nfor _, row in bat_with_name.iterrows():\n    if row['HR'] &gt;= 15:\n        plt.text(row['PC1']+0.05, row['PC2']+0.05,\n                 f\"{row['nameFirst']} {row['nameLast']}\", fontsize=8)\n\nplt.xlabel(\"PC1 (batting ability / power)\")\nplt.ylabel(\"PC2 (plate discipline vs contact)\")\nplt.title(\"MLB Batters PCA (PC1 vs PC2)\")\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n# 각 주성분별 상위 5명 출력\nfor pc in [‘PC1','PC2','PC3']:\n    print(f\"\\n{pc} 상위 5명:\")\n    top5 = (bat_with_name[['nameFirst','nameLast','HR','RBI','SB',pc]]\n             .sort_values(pc, ascending=False)\n             .head(5))\n    print(top5.to_string(index=False))\nPC1 타격 상위 5명\nnameFirst nameLast HR RBI SB PC1  Jose Abreu 19 60.0 0.0 4.934120  Marcell Ozuna 18 56.0 0.0 3.823145  Manny Machado 16 47.0 6.0 2.837655  Trea Turner 12 41.0 12.0 2.651955  Freddie Freeman 13 53.0 2.0 2.554560\nPC2 선구안 상위 5명  nameFirst nameLast HR RBI SB PC2  Matt Olson 14 42.0 1.0 3.382615  Christian Yelich 12 22.0 4.0 3.281660  Max Muncy 12 27.0 1.0 2.790429  Yoan Moncada 6 24.0 0.0 2.166739  Carlos Santana 8 30.0 0.0 2.059597\nPC3 주루 상위 5명\nnameFirst nameLast HR RBI SB PC3  Adalberto Mondesi 6 22.0 24.0 3.875285  Trevor Story 11 28.0 15.0 2.501421  Trent Grisham 10 26.0 10.0 2.397888  Whit Merrifield 9 30.0 12.0 2.271729  Dansby Swanson 10 35.0 5.0 2.135323\n\n\n(6) 주성분 활용(2) 타자 능력 종합지표\nPCA의 큰 장점은 여러 개의 서로 다른 변수들을 중복 없이 압축해 낼 수 있다는 점이다. 예컨대 PCA 결과에서 PC2, PC3, PC4가 각각 파워, 선구안, 그리고 주루 능력을 나타낸다. 그러면 이 세 가지 축은 서로 독립적이면서도 타자의 주요 특성을 대표하는 성분이라고 볼 수 있다.\n이때 각 선수의 타격 능력을 하나의 종합 지표로 나타내고 싶다면, 세 성분을 단순 합이 아니라 설명 분산 기여율을 가중치로 반영한 가중합으로 설계할 수 있다. 즉, 각 주성분의 설명력이 큰 만큼 그 성분의 기여도를 더 크게 반영하는 방식이다.\n이렇게 하면 최종적으로 얻어지는 PCA-Batting Index는 단일 수치로 선수의 타격 능력을 요약하면서도, 원래 다양한 변수들의 정보를 균형 있게 담아낼 수 있다. 말하자면 OPS나 WAR 같은 전통적 지표와는 다른, 데이터 축약 기반의 통합 평가 지표가 되는 셈이다.\n# 선택된 PC 점수만 추출 (예: PC1, PC2, PC3)\nselected_pcs = ['PC1','PC2','PC3']\nweights = explained_ratio[0:3] / explained_ratio[0:3].sum()  # PC1~PC3 비율 정규화\n\n# 종합 지표 계산\nbat_with_name['PCA_Batting_Index'] = (\n    bat_with_name[selected_pcs].values @ weights\n)\n\n# 상위 10명 확인\nprint(bat_with_name[['nameFirst','nameLast','HR','RBI','SB','PCA_Batting_Index']]\n      .sort_values('PCA_Batting_Index', ascending=False)\n      .head(10))\nnameFirst nameLast HR RBI SB PCA_Batting_Index  43 Marcell Ozuna 18 56.0 0.0 2.237267  46 Jose Ramirez 17 46.0 10.0 1.567421  59 Fernando Tatis 17 45.0 11.0 1.446034  0 Jose Abreu 19 60.0 0.0 1.446032  63 Luke Voit 22 52.0 0.0 1.287682  22 Freddie Freeman 13 53.0 2.0 1.231654  42 Matt Olson 14 42.0 1.0 1.209545  58 Dansby Swanson 10 35.0 5.0 1.205611  7 Mookie Betts 16 39.0 10.0 1.043342  57 Trevor Story 11 28.0 15.0 0.910111\n\n\n(7) 주성분 활용(4) 이상치 진단\n주성분 분석은 원 변수들의 선형함수를 이용하여 그 변수들의 공분산 구조를 설명하는 방법이다. 표본이 \\(n\\), 변수 개수가 \\(p\\)개인 데이터 행렬 로부터의 공분산 행렬을 \\(S_{p \\times p}\\)라 하고, \\(\\Sigma\\)의 고유값을\\(\\lambda_{1} \\geq \\cdots \\geq \\lambda_{p}\\), 대응 고유벡터를 \\(e_{i}\\)라 하자. 고유벡터 \\(e_{i}\\)를 계수로 하는 원변수의 선형결합 \\(y_{i} = e_{i}^{\\top}(x - \\mu)\\)를 i-번째 주성분이라 한다. 다변량 정규 \\(x \\sim N_{p}(\\mu,\\Sigma)\\)를 가정하면 \\(y_{i} \\sim N(0,\\lambda_{i})\\)이며 서로 독립이다.\n이상치 진단은 공통 변동을 대표하는 주요 주성분과, 그 밖의 잔차(부) 주성분을 구분하여 수행할 수 있다. 데이터의 전체 변동 중 주요 공통 요인을 설명하는 상위 k개의 주성분을 선택하는 기준은 누적기여율, 스크리 플롯, Kaiser 기준, 병렬분석 등으로 정하는 것이 일반적이다. 이때 Cochran 정리에 의해 정규화된 제곱합은 다음과 같은 카이제곱 분포를 가진다.(Johnson과 Wichern, 2007).\n주요 주성분에 의한 이상치 진단 통계량: \\(\\overset{k}{\\sum_{i = 1}}\\frac{y_{i}^{2}}{\\lambda_{i}} \\sim \\chi^{2}(k)\\)\n잔차 주성분에 의한 이상치 발견 통계량: \\(\\overset{p}{\\sum_{i = k + 1}}\\frac{y_{i}^{2}}{\\lambda_{i}} \\sim \\chi^{2}(p - k)\\)\n두 통계량은 서로 독립이다. 첫 번째 통계량은 공통 구조(저차 공간)에서의 이상 행동을, 두 번째 통계량은 공통 구조로 설명되지 않는 잔차 공간에서의 이상 행동을 포착한다. 실무에서는 \\(\\Sigma\\) 대신 표본공분산 S와 표본 고유분해를 사용하므로 분포는 엄밀히 근사이며, 대응 임계값은 경험적 기준 또는 모니터링 기법(Hotelling의 T^2, Q-통계 등)과 함께 사용되는 것이 권장된다.\n# === PCA 기반 이상치 진단 ==========================================\nfrom scipy.stats import chi2\nimport numpy as np\nimport pandas as pd\n\n# 0) 준비 (위에서 이미 계산된 것들 사용)\n# - scores: PCA 점수 (표준화된 X에 대해 학습)\n# - eigs:   고유값 (explained_variance_)\n# - selected_idx: Kaiser(λ&gt;=1)로 선택된 성분의 인덱스 (0-based)\np = scores.shape[1]\nsel = np.array(sorted(selected_idx))                     # 주요 주성분 index\nres = np.setdiff1d(np.arange(p), sel)                   # 잔차 주성분 index\nk = len(sel)\ndf_main = k\ndf_res  = p - k\nalpha_main = 0.05\nalpha_res  = 0.05\n\n# 1) 정규화된 PC 점수 (각 성분을 표준편차 sqrt(λ)로 나눔)\nz = scores / np.sqrt(eigs)                              # shape: (n_samples, p)\n\n# 2) 통계량 계산\nT2_main = (z[:, sel]**2).sum(axis=1) if k &gt; 0 else np.zeros(len(z))\nT2_res  = (z[:, res]**2).sum(axis=1) if df_res &gt; 0 else np.zeros(len(z))\n\n# 3) 임계값 (카이제곱)\ncrit_main = chi2.ppf(1 - alpha_main, df_main) if df_main &gt; 0 else np.inf\ncrit_res  = chi2.ppf(1 - alpha_res,  df_res)  if df_res  &gt; 0 else np.inf\n\n# 4) 데이터프레임에 부착 및 플래그\nbat_with_name[\"T2_main\"] = T2_main\nbat_with_name[\"T2_res\"]  = T2_res\nbat_with_name[\"flag_main\"] = (T2_main &gt; crit_main) if np.isfinite(crit_main) else False\nbat_with_name[\"flag_res\"]  = (T2_res  &gt; crit_res)  if np.isfinite(crit_res)  else False\nbat_with_name[\"flag_any\"]  = bat_with_name[\"flag_main\"] | bat_with_name[\"flag_res\"]\n\nprint(f\"[선택 성분 개수 k={k}, df_main={df_main}, df_res={df_res}]\")\nprint(f\"임계값(α=0.05): main χ²({df_main})={crit_main:.3f}, residual χ²({df_res})={crit_res:.3f}\")\n\n# 5) 상위 이상치 후보 확인 (주요/잔차 각각 상위 10명씩)\ntop_main = bat_with_name.sort_values(\"T2_main\", ascending=False).head(10)[\n    [\"nameFirst\",\"nameLast\",\"T2_main\",\"flag_main\"]\n]\ntop_res  = bat_with_name.sort_values(\"T2_res\",  ascending=False).head(10)[\n    [\"nameFirst\",\"nameLast\",\"T2_res\",\"flag_res\"]\n]\n\nprint(\"\\n[주요 주성분 영역 이상치 후보 TOP 10]\")\nprint(top_main.to_string(index=False))\nprint(\"\\n[잔차 주성분 영역 이상치 후보 TOP 10]\")\nprint(top_res.to_string(index=False))\n[선택 성분 개수 k=3, df_main=3, df_res=5]  임계값(α=0.05): main χ²(3)=7.815, residual χ²(5)=11.070\n[주요 주성분 영역 이상치 후보 TOP 10]  nameFirst nameLast T2_main flag_main  Jose Abreu 13.735414 True  Adalberto Mondesi 13.266429 True  Whit Merrifield 9.340053 True  Marcell Ozuna 7.434100 False  Christian Yelich 6.806291 False  Trea Turner 6.614026 False  Trevor Story 6.595463 False  Matt Olson 6.206043 False  Trent Grisham 5.437111 False  Hanser Alberto 5.087521 False\n[잔차 주성분 영역 이상치 후보 TOP 10]  nameFirst nameLast T2_res flag_res  Cesar Hernandez 15.628843 True  Adalberto Mondesi 14.008599 True  Freddie Freeman 13.942985 True  Javier Baez 11.264646 True  Dansby Swanson 10.582634 False  Carlos Santana 10.357712 False  Tim Anderson 10.154598 False  Kyle Seager 9.084776 False  Kyle Tucker 8.849439 False  Keston Hiura 8.728401 False\n# 6) 2D 타원 시각화 (PC1 vs PC2, 95%)\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Ellipse\nfrom scipy.stats import chi2\nimport numpy as np\n\n# (1) 좌표와 공분산\npcx, pcy = 0, 1                          # 필요시 주성분은 pc1=0, pc2=1, ...\nx = scores[:, pcx]\ny = scores[:, pcy]\nXY = np.column_stack([x, y])\nmu = XY.mean(axis=0)\ncov = np.cov(XY, rowvar=False)\n\n# (2) 타원 파라미터\nvals, vecs = np.linalg.eigh(cov)\norder = vals.argsort()[::-1]\nvals, vecs = vals[order], vecs[:, order]\nchi2_val = chi2.ppf(0.95, df=2)\nwidth, height = 2*np.sqrt(vals*chi2_val)\nangle = np.degrees(np.arctan2(*vecs[:,0][::-1]))\n\n# (3) 마할라노비스 거리^2로 이상치 판정(타원 밖)\ninv_cov = np.linalg.inv(cov)\nd = XY - mu\nd2 = np.einsum('ij,jk,ik-&gt;i', d, inv_cov, d)            # 거리^2\noutlier_mask = d2 &gt; chi2_val\n\n# 4) 플롯 (수정 버전)\nfig, ax = plt.subplots(figsize=(8,6))\nax.scatter(x, y, s=25)\nax.scatter(mu[0], mu[1], c='black', s=30)               # 중심점\n\n# 🔧 angle을 keyword argument로 명시!\nellipse = Ellipse(xy=mu, width=width, height=height, angle=angle,\n                  edgecolor='orange', facecolor='none', lw=2)\nax.add_patch(ellipse)\n\n# 라벨: 타원 밖만 표시 (playerID 없으면 nameFirst+nameLast 사용)\nlabel_col = 'playerID' if 'playerID' in bat_with_name.columns else None\nfor i in np.where(outlier_mask)[0]:\n    if label_col:\n        txt = str(bat_with_name.iloc[i][label_col])\n    else:\n        txt = f\"{bat_with_name.iloc[i]['nameLast']}{bat_with_name.iloc[i]['nameFirst']}\"\n    ax.text(x[i], y[i]+0.12, txt, fontsize=9)\n\n# 축 라벨: 분산 기여율 표시\nvx = pca.explained_variance_ratio_[pcx]*100\nvy = pca.explained_variance_ratio_[pcy]*100\nax.set_xlabel(f'PC{pcx+1} component ({vx:.0f}%)')\nax.set_ylabel(f'PC{pcy+1} component ({vy:.0f}%)')\nax.set_title('scatter plot of POWER and Hitting components (Anomaly)')\nax.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n주성분 활용(5) 연봉과 상관관계\n연봉과 주성분 점수 간의 관계를 살펴볼 때는 굳이 회귀분석까지 갈 필요는 없는데 이유는 다음과 같다.\n연봉은 잡음이 많음: MLB 연봉은 단순히 성적뿐 아니라 나이, 계약 시점, FA·ARB 제도, 포지션, 팀 재정 여건 등 외생 변수가 강하게 작용하여 예측력 지표인 결정계수가 매우 낮아 타격 성적만으로 설명하는 회귀모형은 편향적일 가능성이 크다.\n분석 목적이 ”설명력”이 아니라 ”관계 파악”: 분석 목적은 ”어떤 능력(PC2=파워, PC3=선구안, PC4=스피드)이 시장에서 평가받는가?“를 보는 것이므로 상관분석 정도로 충분히 의미를 전달할 수 있다.\n연봉 데이터는 2020년 데이터가 없어 타자 능력 데이터와 달리 2016년 데이터를 이용하여 분석 대상 선수는 33명이다(최초 분석 데이터 66명).\nfrom lahman import salaries\n\n# 연봉 데이터 불러오기\nsal = salaries()\n\n# 분석 시즌(예: latest_year) 연봉 데이터 선택\nsal_latest = sal[sal['yearID'] == 2016] #2020 연봉이 없어 2016년 연봉을 사용\n\n# 선수 이름+PCA 점수 데이터와 연봉 join\nbat_with_salary = bat_with_name.merge(\n    sal_latest[['playerID','salary']],\n    on='playerID',\n    how='left'\n)\n\n# 결측치 제거\nbat_with_salary = bat_with_salary.dropna(subset=['salary'])\n\n# 상관계수 계산 (연봉과 PC 점수)\ncorrs = bat_with_salary[[‘salary','PC1','PC2','PC3']].corr()\n\nprint(\"연봉과 PCA 성분 상관계수:\")\nprint(corrs['salary'].round(3))\n연봉과 PCA 성분 상관계수\nsalary 1.000  PC1(타격 생산력/파워 성분) 0.074  PC2 (plate discipline vs contact) 0.259  PC3 (speed/baserunning) -0.215\n연봉(5salary)–PC1(타격 생산력/파워 성분): +0.074\nPC1은 홈런(HR), 타점(RBI), 안타(H) 등 공격 생산력의 공통 변동을 대표하는 성분이다. 상관계수 +0.07은 매우 약한 양(+)의 상관으로, 파워 중심의 성적이 연봉에 일부 긍정적 영향을 주지만, 설명력은 미미하다는 의미이다. 즉, MLB 연봉이 단순히 타격 지표(홈런·타점 등)에 의해 결정되지 않으며, 계약 시기·경력·포지션·수비력 등 복합적 요인이 작용함을 시사한다.\n연봉(salary)–PC2(plate discipline vs contact): +0.259\nPC2는 볼넷(BB)과 삼진(SO)의 대립 구조로, ”선구안·출루력 vs 컨택 능력”의 대비를 나타내는 축이다. 상관계수 +0.26은 약한 양(+)의 상관으로, 볼넷이 많고 삼진이 적은, 즉 선구안이 좋은 타자일수록 연봉이 높은 경향이 있음을 보여준다. 이는 현대 MLB 시장이 ”출루(OBP)“와 ”타석 퀄리티(plate discipline)”를 일정 부분 보상하고 있음을 시사한다. 다만 0.26 정도면 여전히 제한적 관계이므로, 선구안이 뛰어나도 파워·포지션 프리미엄이 더 큰 영향력을 가질 가능성이 높다.\n연봉(salary)–PC3(speed/baserunning): -0.215\nPC3은 도루(SB)·주루 능력 등 스피드형 변동을 나타내는 성분이다. 상관계수 -0.22는 음(-)의 약한 상관으로, 주루 능력이 좋은 선수일수록 오히려 연봉이 낮은 경향이 있음을 의미한다. 이는 현대 MLB 시장에서 스피드보다는 파워·출루 중심의 효율적 공격이 더 높은 경제적 가치를 지니는 흐름을 반영한다.\n과거 ’도루왕=고액 연봉’의 공식이 이미 무너졌으며, 스피드는 부가적 능력으로만 평가되고 있음을 보여준다. 연봉과 PCA 성분 간의 상관 구조는 전반적으로 약하며, 시장 가치가 특정 기술(파워, 주루, 선구안) 하나로 결정되지 않는다는 점을 드러낸다.\n특히 plate discipline(PC2)의 양(+)의 상관이 가장 높게 나타났다는 점은, 최근 MLB에서 ’볼넷 관리 능력’이 점차 중요해지고 있다는 신호로 해석할 수 있다. 결국 연봉은 성적 지표 + 나이 + 계약제도 + 포지션 희소성 + 팀 전략적 필요가 교차하여 형성되는 복합 함수임을 시사한다.\n\n\n\n\nChapter 3. SVD 차원축소\n\n1. PCA와 SVD 비교\nPCA는 원래 변수들 간의 공분산 구조를 분석하여 변수의 수를 줄이는 차원 축소 방법이다.즉, 여러 변수가 내포한 중복된 정보를 제거하고, 분산이 큰 방향으로 축을 회전시켜 소수의 주성분 변수로 요약한다.\n반면, SVD는 데이터 행렬 자체를 직교 행렬과 특이값 대각행렬로 분해하는 행렬 분해 기법으로, 변수뿐 아니라 관측치(행) 의 차원까지 함께 축소할 수 있다.\n이 때문에 SVD는 데이터의 행과 열을 동시에 저차원 공간으로 표현할 수 있으며, 특히 이미지나 문서처럼 고차원 행렬 데이터를 다루는 기계학습(예: 이미지 분류, LSA 등) 에 널리 활용된다.\nPCA는 공분산행렬을 기반으로 하므로 각 주성분의 부하량(loadings) 을 통해 원래 변수들이 주성분에 어떻게 기여하는지를 해석할 수 있지만, SVD는 행렬 전체를 수치적으로 분해하기 때문에 변수의 해석보다는 구조적 압축과 근사에 초점을 둔다.\n요약하자면, PCA는 ”변수를 줄이고 해석하는” 통계적 방법이고 SVD는 ”행렬 전체를 축소하고 근사하는” 수학적 방법이다.\n\n\n\n\n\n\n\n2. Sigular Value Decomposition\n\n\n\n\n\n\n\n\n원데이터\n\n행렬분해\n\n\n\\[X_{n \\times p}\\]\n\\[=\\]\n\\[U_{n \\times n}\\Sigma_{n \\times p}V_{p \\times p}\\]\n\n\n(단위 표준화)\n\n\\(U\\):  특이값 직교행렬\n\\(\\Sigma\\) : 대각행렬\n\\(V\\) : 특이값 직교행렬\n\n\n\n\n(1) Full SVD and Reduced SVD\n\n\\(\\Sigma_{n \\times p} = \\begin{bmatrix}\n\\text{diag}(\\sigma_{1},\\sigma_{2},\\ldots,\\sigma_{r}) & 0 \\\\\n0 & 0\n\\end{bmatrix}\\), 여기서 \\(\\sigma_{i} = \\sqrt{\\lambda_{i}}\\)(\\(X'X\\)의 고유치)이며 \\(r = \\text{rank}(X)\\)이다. \\(\\Sigma_{n \\times p}\\)은 비대칭 대각행렬이다.\n\\(U'U = I_{n}\\): \\(U\\)가 \\(n \\times n\\) 정사각행렬이면 \\(U'U = I_{n}\\)이다(Full SVD). 그러나 실제 SVD 계산에서는 보통 Reduced SVD 형태를 사용하므로 \\(U\\)가 \\(n \\times r\\) 정사각행렬이면 \\(U'U = I_{r}\\)이다.\n\\(V'V = I_{p}\\): 같은 이유로, Reduced SVD에서는 \\(V'V = I_{r}\\)이다. \\(X'X\\)의 고유벡터는 \\(p \\times p\\) 행렬 \\(V\\)의 열로 구성되지만, 실제 계산에서는 \\(r\\)개의 주축만 사용한다.\n\n\n\n(2) Truncated SVD\nreduced SVD은 원 데이터 행렬의 계수 r 까지 차원을 줄인다면 Truncated SVD는 그 중에서도 상위 \\(k( &lt; r)\\)개의 특이값과 대응하는 성분만 남겨, 정보의 손실을 최소화하면서 행렬을 근사하는 방법이다.\n즉, 전체 구조를 완전히 복원하는 Reduced SVD와 달리, Truncated SVD는 데이터의 주요 패턴(가장 큰 변동 방향)만 보존하고 미세한 변동이나 노이즈에 해당하는 뒷부분 성분을 버림으로써 데이터를 압축하고 계산 효율을 높이는 목적을 가진다.\n\\(k\\)를 결정할 때는 일반적으로 누적 설명변동 비율(예: 80%)을 기준으로 하므로, 주성분분석에서 차원을 선택하는 방식과 동일하다.\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import TruncatedSVD\n\n# 1) 변수 선택\ncols = ['HR','RBI','SB','BB','SO','H','AB','GIDP']\nX = bat_with_name[cols].fillna(0)\n\n# 2) 표준화: 평균 0, 표준편차 1\nscaler = StandardScaler()\nZ = scaler.fit_transform(X)\n\n# Full SVD: 모든 직교기저 포함\nU_full, S_full, Vt_full = np.linalg.svd(Z, full_matrices=True)\n\nprint(\"U_full:\", U_full.shape)   # (n, n)\nprint(\"S_full:\", S_full.shape)   # (min(n, p),)\nprint(\"Vt_full:\", Vt_full.shape) # (p, p)\n\n# 복원 확인 (완벽하게 동일해야 함)\nZ_recon_full = (U_full[:, :len(S_full)] * S_full) @ Vt_full\nnp.allclose(Z, Z_recon_full, atol=1e-10)\nU_full: (66, 66)  S_full: (8,)  Vt_full: (8, 8)  True\n# 3) Reduced (Thin) SVD: 정확한 분해\nU_r, S_r, Vt_r = np.linalg.svd(Z, full_matrices=False)\n\n# r = rank(Z)\nr = len(S_r)\n\nprint(\"rank:\", r)   # (Z 행렬 계수)\nprint(\"U_r:\", U_r.shape)   # (n, n)\nprint(\"S_r:\", S_r.shape)   # (min(n, p),)\nprint(\"Vt_r:\", Vt_r.shape) # (p, p)\n\n\n# 복원 확인\nZ_recon = (U_r * S_r) @ Vt_r\nrank: 8  U_r: (66, 8)  S_r: (8,)  Vt_r: (8, 8)\n# 4) Truncated SVD (상위 k개 성분만 근사)\nk = 3\nsvd_k = TruncatedSVD(n_components=k, random_state=0)\nZ_k = svd_k.fit_transform(Z)      # U_k Σ_k\nVk = svd_k.components_            # Vt_k\nSk = svd_k.singular_values_\n\n# 5) 누적 분산비율로 정보량 확인\nexplained_var = (S_r**2)\nratio = explained_var / explained_var.sum()\ncumulative = np.cumsum(ratio)\n\nprint(f\"Reduced rank r = {r}\")\nprint(f\"상위 {k}개 누적 분산 비율: {cumulative[:k]}\")\n\nprint(\"Sk:\", Sk.shape)   # (min(n, p),)\nprint(\"Vk:\", Vk.shape) # (p, p)\nReduced rank r = 8  상위 3개 누적 분산 비율: [0.29831468 0.52653148 0.68138709]  Sk: (3,)  Vk: (3, 8)"
  },
  {
    "objectID": "notes/mldl/mldl_concepts.html",
    "href": "notes/mldl/mldl_concepts.html",
    "title": "MLDL. 1. 머신러닝과 통계적 사고",
    "section": "",
    "text": "Chapter 1. ML은 왜 ’추정’이 아니라’일반화’ 문제인가\n\n1. 통계적 추정과 예측 문제의 차이\n전통적인 통계학에서 가장 핵심적인 목표는 모수의 추정이다. 모집단이 존재하고, 표본은 그 모집단으로부터 무작위로 추출되었다고 가정한다. 이때 통계 분석의 관심사는 표본을 통해 모집단의 특성을 얼마나 정확하게 추정할 수 있는가에 있다. 평균, 분산, 회귀계수와 같은 모수들은 그 자체가 분석의 최종 산출물이다.\n반면 머신러닝에서의 주된 관심사는 예측이다. 여기서 중요한 점은 예측 대상이 이미 관측된 데이터가 아니라, 아직 관측되지 않은 미래 데이터라는 사실이다. 따라서 머신러닝에서는 모수의 해석 가능성보다, 새로운 데이터에 대해 얼마나 잘 작동하는지가 성공의 기준이 된다.\n이 차이는 단순한 용어상의 차이가 아니다. 통계적 추정은 ”현재 우리가 가진 데이터로 모집단을 얼마나 잘 설명하는가”의 문제인 반면, 머신러닝의 예측은 ”현재 데이터로 미래 데이터를 얼마나 잘 맞출 수 있는가”의 문제이다. 이로 인해 두 분야는 동일한 수학적 도구를 사용하더라도, 문제 설정과 평가 기준에서 근본적인 차이를 보인다.\n\n\n2. 학습 데이터와 미래 데이터의 단절\n머신러닝에서 가장 자주 간과되는 사실 중 하나는 학습 데이터와 미래 데이터는 본질적으로 다를 수 있다는 점이다. 통계학에서는 표본이 모집단을 대표한다는 가정을 비교적 강하게 두지만, 머신러닝에서는 이 가정이 쉽게 깨진다.\n현실의 데이터는 시간에 따라 변하고, 수집 과정 또한 일정하지 않다. 학습 데이터는 과거의 특정 시점과 조건에서 수집된 결과이며, 미래 데이터는 다른 환경과 조건에서 생성될 수 있다. 이로 인해 학습 데이터에서 성능이 우수한 모델이 미래 데이터에서는 전혀 다른 성능을 보이는 현상이 발생한다.\n이러한 단절은 머신러닝 문제를 단순한 추정 문제가 아니라 일반화 문제로 만든다. 즉, 중요한 것은 학습 데이터에 대한 적합도가 아니라, 보지 못한 데이터에 대해 성능을 유지할 수 있는 능력이다.\n\n\n3. ML에서의 성공 기준\n통계 분석에서 성공의 기준은 보통 다음과 같이 요약된다.\n\n추정량이 불편(unbiased)한가\n분산이 충분히 작은가\n가설검정에서 유의미한 결과를 보이는가\n\n반면 머신러닝에서는 성공의 기준이 훨씬 단순하면서도 냉정하다. “미래 데이터에서 성능이 좋은가?” 이 질문에 대한 답은 보통 훈련 오차가 아니라 테스트 오차로 평가된다. 머신러닝에서 널리 사용되는 교차검증 역시 이 테스트 오차를 추정하기 위한 도구이다.\n이러한 관점에서는 모형의 해석 가능성이나 통계적 유의성보다, 예측 정확도가 우선시된다. 이 때문에 머신러닝에서는 통계학에서 중요하게 다루어졌던 p-value나 신뢰구간이 상대적으로 덜 강조된다.\n\n\n4. 통계학과 ML의 공통 기반과 분기점\n머신러닝은 통계학과 완전히 다른 분야가 아니다. 실제로 많은 머신러닝 알고리즘은 통계적 개념에 뿌리를 두고 있다. 회귀분석, 로지스틱 회귀, 판별분석, 주성분분석 등은 통계학에서 먼저 정립된 방법들이다.\n그러나 두 분야는 다음 지점에서 분기한다.\n\n통계학: 모형 가정 → 추정 → 해석\n머신러닝: 손실함수 → 최적화 → 일반화 성능\n\n통계학은 데이터 생성 과정에 대한 명시적인 확률모형을 가정하고, 그 모형이 현실을 얼마나 잘 설명하는지를 중시한다. 반면 머신러닝은 모형 가정의 정확성보다는, 주어진 손실함수를 최소화하는 방향으로 학습을 진행하며, 그 결과가 미래 데이터에서도 유지되는지를 평가한다.\n이러한 차이로 인해 머신러닝에서는 과적합 문제가 핵심적인 관심사가 되며, 이를 제어하기 위한 정규화(regularization), 교차검증, 앙상블과 같은 기법들이 발전하게 된다.\n\n\n5. 통계적 학습이란 무엇인가?\n통계적 학습에 대한 논의를 시작하기에 앞서, 간단한 예제를 통해 그 동기를 설명해 보자. 어떤 병원의 경영진이 환자의 재원 기간을 예측하고자 하여 통계 컨설턴트에게 분석을 의뢰했다고 가정하자. 병원은 환자의 재원 기간을 직접적으로 조절할 수는 없지만, 환자의 연령, 진단 정보, 검사 결과, 치료 방식과 같은 다양한 정보를 사전에 관측할 수 있다.\n만약 이러한 정보들과 재원 기간 사이에 체계적인 관계가 존재한다면, 병원은 환자가 입원했을 때 해당 정보를 바탕으로 향후 재원 기간을 예측할 수 있을 것이다. 이는 병상 배정, 의료 인력 운영, 비용 관리 측면에서 매우 중요한 의사결정 자료가 된다. 다시 말해, 우리의 목표는 여러 환자 특성 정보를 이용하여 재원 기간을 예측할 수 있는 정확한 모형을 구축하는 데 있다.\n이러한 설정에서 환자의 연령, 진단 코드, 검사 수치 등은 입력 변수이고, 재원 기간은 출력 변수이다. 입력 변수들은 일반적으로 기호 X로 표시되며, 아래첨자를 사용해 구분한다. 예를 들어 \\(X_{1}\\)은 환자의 연령, \\(X_{2}\\)는 특정 검사 결과, \\(X_{3}\\)는 치료 유형을 의미할 수 있다.\n입력 변수들은 문맥에 따라 설명변수, 독립변수, 예측변수, 특징, 혹은 단순히 변수라고 불리기도 한다. 출력 변수 이 경우에는 재원 기간—는 흔히 반응변수, 종속변수, 또는 목표뵨수라고 하며, 일반적으로 기호 Y로 표시한다.\n보다 일반적으로, 하나의 정량적 반응변수 Y와 p개의 서로 다른 설명변수 \\(X_{1},X_{2},\\ldots,X_{p}\\)가 관측된 상황을 가정하자. 이때 반응변수와 설명변수들 사이에는 어떤 형태의 관계가 존재한다고 볼 수 있으며, 이러한 관계는 매우 일반적인 형태로 다음과 같이 표현할 수 있다.\n\\(Y = f(X) + \\varepsilon\\), 여기서 \\(X = (X_{1},X_{2},\\ldots,X_{p})\\)는 관측된 입력 변수들의 벡터이고, \\(f\\)는 이 입력 변수들이 반응변수에 대해 제공하는 체계적인 정보를 요약하는 함수이다. 중요한 점은 이 함수 \\(f\\)가 고정되어 있으나 알려져 있지 않다는 사실이다. 즉, 데이터 생성 과정에는 분명한 구조가 존재하지만, 우리는 그 구조를 직접 관측할 수 없으며 오직 데이터를 통해 간접적으로 추론할 수 있을 뿐이다.\n식에 포함된 \\(\\varepsilon\\)은 확률적 오차항으로, 설명변수 \\(X\\)로는 설명되지 않는 우연적 변동을 나타낸다. 일반적으로 이 오차항은 입력 변수와 독립이며 평균이 0이라고 가정한다. 이 가정은 모든 변동을 설명하려는 것이 아니라, 설명 가능한 부분과 설명 불가능한 부분을 구분하기 위한 최소한의 전제라고 이해할 수 있다.\n이러한 표현은 통계적 학습 문제의 핵심을 잘 드러낸다. 우리의 목적은 관측된 데이터로부터 오차항 \\varepsilon 자체를 설명하는 것이 아니라, 미지의 함수 f를 가능한 한 잘 근사하는 것이다. 다시 말해, 통계적 학습이란 데이터에 내재된 체계적인 구조를 추출하는 과정이라고 볼 수 있다.\n만약 데이터가 모의 데이터라면, 함수 \\(f\\)의 실제 형태를 알고 있을 수도 있다. 이 경우 관측값들은 이 함수 주변에 분포하게 되며, 각 관측치와 함수 사이의 차이는 오차항 \\(\\varepsilon\\)로 해석할 수 있다. 일부 관측값은 함수보다 크고, 일부는 작을 수 있지만, 전체적으로 보면 오차는 특정 방향으로 치우치지 않고 평균이 0에 가까운 값을 갖는다.\n\n\n6. 왜 함수 \\(f\\)를 추정하는가?\n\n(1) 일반화와 함수 \\(f\\) 추정\n통계적 학습에서 함수 \\(f\\)를 추정하는 주요 목적 중 하나는 입력 변수 X를 이용해 출력 변수 Y를 정확하게 예측하는 데 있다. 이때 예측 오차는 감소 가능한 오차와 감소 불가능한 오차로 분해되며, 통계적 학습 기법의 역할은 본질적으로 감소 가능한 오차를 최소화하는 것이라고 할 수 있다.\n이 사실은 머신러닝 문제가 왜 단순한 추정 문제가 아닌지를 분명하게 보여준다. 전통적인 통계적 추정에서는 모수의 불편성이나 분산과 같은 성질이 분석의 중심에 놓인다. 그러나 머신러닝에서는 설령 어떤 추정량이 통계적으로 바람직한 성질을 갖더라도, 그것이 미래 데이터에 대해 작은 예측 오차를 보장하지는 않는다. 다시 말해, 좋은 추정량이 반드시 좋은 예측기를 의미하지는 않는다.\n머신러닝에서 핵심적인 질문은 다음과 같이 바뀐다.\n주어진 데이터에서 추정한 함수 \\(\\widehat{f}\\)가, 아직 관측되지 않은 새로운 데이터에 대해서도 작은 예측 오차를 유지할 수 있는가? 이 질문은 곧 일반화의 문제이다. 감소 가능한 오차는 학습 방법과 모형 선택에 따라 달라지며, 이는 훈련 데이터에 대한 적합 정도뿐만 아니라 데이터 밖에서의 성능에 의해 평가되어야 한다. 이 때문에 머신러닝에서는 훈련 오차가 아니라 테스트 오차, 교차검증 오차와 같은 개념이 중심적인 역할을 하게 된다.\n요컨대, 함수 \\(f\\)를 추정한다는 행위 자체는 통계학과 머신러닝 모두에서 공통적이지만, 어떤 기준으로 그 추정의 품질을 판단하는가에 있어 두 분야는 명확히 갈라진다. 머신러닝에서의 학습은 모형을 데이터에 맞추는 과정이 아니라, 미래 데이터에 대해 오차를 최소화하도록 일반화 능력을 조정하는 과정이며, 이것이 바로 본 장에서 말하는 ’일반화 문제’의 핵심이다.\n\n\n(2) 예측 prediction 추론 inference\n우리가 함수 f를 추정하고자 하는 데에는 두 가지 주요한 이유가 있다. 하나는 예측이고, 다른 하나는 추론이다. 여기서는 이 두 가지를 차례로 살펴본다.\n\n\n예측 (Prediction)\n많은 상황에서 입력 변수 X는 비교적 쉽게 관측할 수 있지만, 출력 변수 Y는 쉽게 얻기 어렵다. 이러한 경우, 오차항의 평균이 0이라고 가정할 수 있다면, 우리는 다음과 같이 Y를 예측할 수 있다.\n\\(\\widehat{Y} = \\widehat{f}(X)\\), 여기서 \\(\\widehat{f}\\)는 미지의 함수 f에 대한 우리의 추정치를 의미하며, \\(\\widehat{Y}\\)는 그로부터 얻어진 예측값이다. 이와 같은 설정에서 \\(\\widehat{f}\\)는 종종 블랙박스로 취급된다. 즉, \\(\\widehat{f}\\)의 정확한 형태가 무엇인지는 크게 중요하지 않으며, 출력 Y를 얼마나 정확하게 예측하는가가 핵심적인 관심사가 된다.\n예를 들어, \\(X_{1},\\ldots,X_{p}\\)가 실험실에서 쉽게 측정할 수 있는 환자의 혈액 샘플 특성들이고, \\(Y\\)가 특정 약물에 대해 환자가 심각한 부작용을 겪을 위험도를 나타내는 변수라고 가정하자. 이 경우 X를 이용해 Y를 예측할 수 있다면, 부작용 위험이 높은 환자에게 해당 약물을 투여하지 않음으로써 위험을 피할 수 있다.\n\n\n예측 오차의 두 가지 구성 요소\n예측값 \\(\\widehat{Y}\\)가 실제 값 \\(Y\\)를 얼마나 잘 예측하는지는 두 가지 요인에 의해 결정되며, 이를 각각 감소 가능한 오차와 감소 불가능한 오차라고 부른다.\n일반적으로 \\(\\widehat{f}\\)는 f의 완전한 추정치가 될 수 없으며, 이로 인해 예측에는 필연적으로 오차가 발생한다. 이러한 오차는 감소 가능한데, 이는 더 적절한 통계적 학습 기법을 사용함으로써 \\(\\widehat{f}\\)의 정확도를 개선할 수 있기 때문이다.\n그러나 설령 우리가 완벽하게 \\(f\\)를 추정하여 \\(\\widehat{Y} = f(X)\\)라는 형태의 예측을 할 수 있다고 하더라도, 예측 오차는 여전히 0이 되지 않는다. 이는 \\(Y\\)가 오차항 \\(\\varepsilon\\)에도 의존하기 때문이다. 오차항 \\(\\varepsilon\\)은 정의상 X로부터 예측할 수 없으므로, 이로 인해 발생하는 변동성은 예측 정확도에 항상 영향을 미친다. 이러한 오차를 감소 불가능한 오차라고 한다.\n감소 불가능한 오차는 왜 0이 될 수 없는가? 감소 불가능한 오차가 0보다 큰 이유는 여러 가지가 있다. 오차항 \\(\\varepsilon\\)에는 \\(Y\\)를 예측하는 데 유용하지만 우리가 측정하지 못한 변수들이 포함될 수 있다. 이러한 변수들이 관측되지 않는 한, 함수 f는 이를 이용해 예측할 수 없다.\n또한 \\(\\varepsilon\\)에는 본질적으로 측정 불가능한 무작위 변동도 포함될 수 있다. 예를 들어, 특정 환자가 어떤 날에 약물에 대해 보이는 부작용 위험은 약물 자체의 미세한 제조 차이나, 그날 환자의 전반적인 컨디션과 같은 요인에 따라 달라질 수 있다.\n\n\n예측 오차의 분해\n이제 하나의 추정된 함수 \\hat{f}와 설명변수 집합 X가 주어져 있고, 이를 통해 \\hat{Y} = \\hat{f}(X)라는 예측을 한다고 가정하자. 이때 \\hat{f}와 X는 고정되어 있으며, 오직 변동성은 오차항 \\varepsilon에서만 발생한다고 가정하면, 다음과 같은 결과를 쉽게 도출할 수 있다.\n\\[\\mathbb{E}(Y - \\widehat{Y})^{2} = \\mathbb{E}\\lbrack f(X) + \\varepsilon - \\widehat{f}(X)\\rbrack^{2} = \\underset{\\text{감소 가능한 오차}}{\\underbrace{\\lbrack f(X) - \\widehat{f}(X)\\rbrack^{2}}} + \\underset{\\text{감소 불가능한 오차}}{\\underbrace{Var(\\varepsilon)}}\\]\n여기서 \\(\\mathbb{E}(Y - \\widehat{Y})^{2}\\)는 예측값과 실제 값 사이의 제곱 오차의 기댓값을 의미하며, \\(Var(\\varepsilon)\\)는 오차항 \\(\\varepsilon\\)에 수반된 분산을 나타낸다.\nML 방법론의 초점은 함수 \\(f\\)를 추정하는 다양한 기법을 통해 감소 가능한 오차를 최소화하는 것에 있다. 다만, 감소 불가능한 오차는 예측 정확도가 도달할 수 있는 상한선을 항상 결정한다는 점을 기억해야 한다. 이 상한선은 실제 문제에서는 거의 항상 알려져 있지 않다.\n\n\n추론 (Inference)\n우리는 종종 반응변수 \\(Y\\)와 설명변수 \\(X_{1},\\ldots,X_{p}\\)사이의 연관성을 이해하는 데 관심을 가진다. 이러한 상황에서도 함수 \\(f\\)를 추정하고자 하지만, 우리의 목적은 반드시 \\(Y\\)를 예측하는 데 있는 것은 아니다. 이 경우에는 \\(\\widehat{f}\\)를 더 이상 블랙박스로 취급할 수 없는데, 그 이유는 함수의 정확한 형태 자체를 이해할 필요가 있기 때문이다. 이러한 설정에서 우리는 다음과 같은 질문들에 관심을 갖게 된다.\n어떤 설명변수들이 반응변수와 연관되어 있는가? 많은 경우, 사용 가능한 설명변수들 중 실제로 \\(Y\\)와 실질적인 연관성을 가지는 변수는 소수에 불과하다. 가능한 변수들이 매우 많은 상황에서, 그중 중요한 변수들을 식별하는 일은 응용 분야에 따라 매우 유용할 수 있다.\n각 설명변수와 반응변수 사이의 관계는 어떠한가? 어떤 설명변수는 값이 증가할수록 \\(Y\\)도 증가하는 양의 관계를 가질 수 있으며, 다른 변수는 그 반대의 관계를 가질 수도 있다. 또한 함수 \\(f\\)의 복잡성에 따라, 특정 설명변수와 \\(Y\\) 사이의 관계는 다른 설명변수들의 값에 의존할 수도 있다.\n각 설명변수와 \\(Y\\) 사이의 관계를 선형식으로 충분히 요약할 수 있는가, 아니면 더 복잡한 구조가 필요한가? 역사적으로 함수 \\(f\\)를 추정하기 위한 많은 방법들은 선형 형태를 취해 왔다. 어떤 상황에서는 이러한 가정이 합리적이거나 심지어 바람직할 수도 있다. 그러나 실제 관계가 더 복잡한 경우에는, 선형 모형이 입력 변수와 출력 변수 사이의 관계를 정확히 표현하지 못할 수 있다.\n고객이 구매할 가능성이 있는 제품 브랜드를 가격, 매장 위치, 할인 수준, 경쟁 제품의 가격 등과 같은 변수들을 이용해 모형화하는 상황을 생각해 보자. 이 경우에는 각 변수와 구매 확률 사이의 연관성 자체가 핵심 관심사가 된다. 예를 들어, *제품의 가격은 판매량과 어느 정도 연관되어 있는가?*와 같은 질문이 이에 해당하며, 이는 추론을 위한 모형화의 예이다.\n어떤 문제들은 예측과 추론을 동시에 목적으로 삼을 수도 있다. 예를 들어 부동산 시장에서 주택 가격을 범죄율, 용도 지역 강과의 거리, 대기질, 학군, 지역 소득 수준, 주택 크기 등의 입력 변수들과 연관 짓는 상황을 생각해 보자. 이 경우에는 각 개별 변수와 주택 가격 사이의 연관성—예를 들어 강이 보이는 집은 그렇지 않은 집보다 얼마나 더 비싼가?—에 관심을 가질 수도 있으며, 이는 추론 문제이다. 반대로, 주어진 특성들을 바탕으로 특정 주택의 가치를 예측하는 것—이 집은 과대평가되었는가, 아니면 과소평가되었는가?—에 관심을 둘 수도 있는데, 이는 예측 문제에 해당한다.\n궁극적인 목표가 예측인지, 추론인지, 혹은 이 둘의 결합인지에 따라 함수 \\(f\\)를 추정하기 위한 적절한 방법은 달라질 수 있다. 예를 들어 선형 모형은 비교적 단순하고 해석 가능한 추론을 가능하게 하지만, 다른 방법들에 비해 예측 정확도는 떨어질 수 있다. 반면, 강한 비선형 모형들은 매우 정확한 예측을 제공할 수 있지만, 그 대가로 모형의 해석 가능성이 낮아져 추론이 훨씬 어려워진다.\n\n\n\n\nChapter 2. 데이터 생성 관점과 알고리즘 관점\n머신러닝과 통계학의 가장 중요한 차이는 사용되는 알고리즘의 종류가 아니라, 데이터를 바라보는 관점에 있다.\n여기서는 데이터를 어떻게 생성된 결과로 이해할 것인가라는 관점, 즉 데이터 생성 과정(Data Generating Process,)과, 데이터를 알고리즘의 입력값으로만 취급하는 관점 사이의 차이를 살펴본다. 이러한 관점의 차이는 일반화 문제, 과적합, 불확실성 해석에 직접적인 영향을 미친다.\n\n1. 데이터는 어떻게 생성되는가\n전통적인 통계학에서는 데이터가 확률적 생성 과정을 통해 생성된 결과라고 가정한다. 즉, 관측된 데이터는 우연의 결과가 아니라, 어떤 확률모형에 의해 생성된 실현값이다. 일반적인 회귀 설정에서 이는 다음과 같이 표현된다.\n\\(Y = f(X) + \\varepsilon\\), 여기서 \\(f(X)\\)는 체계적인 구조를, \\(\\varepsilon\\)은 설명되지 않는 확률적 변동을 나타낸다. 이 식은 단순한 수식이 아니라, 데이터에 대한 철학적 선언에 가깝다. 즉, 데이터에는 설명 가능한 부분과 설명 불가능한 부분이 동시에 존재한다는 가정이다.\n데이터 생성 과정 관점에서 보면, 우리가 관측하는 데이터는 항상 다음과 같은 특징을 갖는다.\n동일한 조건에서도 결과는 반복 측정 시 달라질 수 있다.\n일부 변동성은 어떤 모델로도 제거할 수 없다.\n관측되지 않은 변수의 영향이 항상 존재한다.\n이러한 전제는 통계적 추론, 신뢰구간, 가설검정의 논리적 기반을 이룬다.\n\n\n2. 확률모형을 가정하지 않는 학습의 한계\n머신러닝에서는 종종 데이터 생성 과정을 명시적으로 가정하지 않는다. 대신, 입력 X와 출력 Y 사이의 관계를 함수 근사 문제로 다룬다. 이 관점에서는 다음과 같은 표현이 암묵적으로 사용된다.\n\\(\\widehat{Y} = \\widehat{f}(X)\\), 여기서 중요한 것은 \\(\\widehat{f}\\)가 어떤 확률모형에서 나왔는지가 아니라, 손실함수를 얼마나 잘 최소화하는가이다. 이 접근은 계산적으로 매우 강력하지만, 다음과 같은 한계를 내포한다.\n첫째, 오차의 성격을 분리할 수 없다. 확률모형이 없으면, 관측된 오차가 모델의 부적합 때문인지, 본질적으로 제거 불가능한 변동성 때문인지 구분할 수 없다.\n둘째, 불확실성에 대한 해석이 불가능해진다. 예측값은 제공할 수 있지만, 그 예측이 얼마나 신뢰할 수 있는지는 알기 어렵다.\n셋째, 일반화 실패의 원인을 설명하기 어렵다. 테스트 성능이 나쁜 이유가 데이터 분포 변화 때문인지, 과적합 때문인지 명확히 구분되지 않는다.\n이러한 이유로, 확률모형을 전혀 고려하지 않는 학습은 예측 정확도는 높을 수 있으나, 해석과 진단에는 취약하다.\n\n\n3. 알고리즘 중심 사고의 위험\n알고리즘 중심 사고란, 데이터를 고정된 입력으로 보고 “어떤 알고리즘이 성능이 좋은가?” 만을 기준으로 학습 방법을 선택하는 태도를 의미한다. 이 관점에서는 다음과 같은 질문이 중심이 된다.\n어떤 모델이 정확도가 가장 높은가?\n어떤 하이퍼파라미터 조합이 최적인가?\n그러나 이러한 사고방식에는 몇 가지 위험이 따른다.\n첫째, 데이터가 바뀌면 알고리즘의 성능도 바뀐다. 훈련 데이터 분포와 테스트 데이터 분포가 다를 경우, 알고리즘의 우수성은 쉽게 무너진다.\n둘째, 과적합을 구조적으로 이해하지 못한다. 알고리즘은 과적합을 ”성능 저하”로만 드러내지만, 데이터 생성 과정 관점에서는 이를 모형 복잡도와 오차 구조의 불일치로 해석할 수 있다.\n셋째, 재현성과 외삽에 취약하다. 데이터 생성 구조를 이해하지 못한 채 학습된 모델은, 새로운 환경에서 쉽게 실패한다.\n알고리즘은 도구이지, 데이터의 본질을 설명해 주지는 않는다.\n\n\n4. 데이터 생성 과정 관점에서 본 학습 데이터의 불완전성\n데이터 생성 과정 관점에서 보면, 학습 데이터는 결코 완전하지 않다. 학습 데이터는 항상 다음과 같은 제약을 갖는다.\n특정 시점과 조건에서만 수집된 표본이다.\n관측되지 않은 변수의 영향을 포함한다.\n표본 크기는 유한하다.\n이로 인해 학습 데이터에서 잘 작동하는 모델이 진짜 데이터 생성 과정을 잘 반영한다고 보장할 수는 없다. 다시 말해,\n학습 데이터에 잘 맞는 모델 ##### ≠##### 데이터 생성 과정을 잘 반영한 모델\n이러한 불완전성 때문에 머신러닝에서는 일반화 성능이 핵심 평가 기준이 된다. 교차검증, 정규화, 앙상블 기법 등은 모두 DGP를 직접 알 수 없는 상황에서, 그 영향을 간접적으로 통제하기 위한 장치라고 볼 수 있다.\n\n\n\nChapter 3. 편향과 분산 해석\n머신러닝과 통계학을 연결하는 가장 중요한 개념 중 하나는 편향–분산(Bias–Variance)의 구분이다. 이 개념은 단순히 오차를 분해하는 기술적 장치가 아니라, 일반화 성능이 왜 제한될 수밖에 없는지를 설명해 주는 핵심 논리이다. 이 장에서는 편향과 분산을 전통적인 정의에서 벗어나, 데이터 생성 관점과 일반화 관점에서 다시 해석한다.\n\n1. 편향과 분산\n\n편향과 분산 정의\n어떤 입력값 \\(X = x\\)에 대해 반응변수 \\(Y\\)가 생성된다고 하자. 우리는 이를 다음과 같이 표현해 왔다.\n\\(Y = f(x) + \\varepsilon\\), 여기서 \\(f(x)\\)는 데이터 생성 과정에 내재된 참 함수이며, \\(\\varepsilon\\)는 평균이 0인 확률적 오차항이다. 이제 동일한 데이터 생성 과정을 여러 번 반복하여, 서로 다른 학습 데이터셋을 얻고, 그로부터 예측 함수 \\(\\widehat{f}(x)\\)를 추정한다고 가정하자.\n이때 예측 오차의 기댓값은 다음과 같이 분해될 수 있다.\n\\[\\mathbb{E}\\lbrack(Y - \\widehat{f}(x))^{2}\\rbrack = \\underset{\\text{편향(Bias)}^{2}}{\\underbrace{(\\mathbb{E}\\lbrack\\widehat{f}(x)\\rbrack - f(x))^{2}}} + \\underset{\\text{분산(Variance)}}{\\underbrace{\\mathbb{E}\\lbrack(\\widehat{f}(x) - \\mathbb{E}\\lbrack\\widehat{f}(x)\\rbrack)^{2}\\rbrack}} + \\underset{\\text{감소 불가능한 오차}}{\\underbrace{Var(\\varepsilon)}}\\]\n이 분해는 중요한 사실을 보여준다. 예측 오차는 단일한 원인으로 발생하는 것이 아니라, 모형의 평균적 오차(편향), 데이터 샘플에 대한 민감도(분산), 그리고 본질적으로 제거할 수 없는 변동성의 결합으로 나타난다.\n통계학에서 편향은 주로 ”추정량이 참값을 평균적으로 얼마나 잘 맞추는가”의 문제로 이해된다. 반면 머신러닝에서는 편향이 모형이 데이터 생성 구조를 얼마나 단순화해서 가정하는가를 반영한다.\n\n\n편향–분산 상충관계\n테스트 MSE(Mean Squared Error, 평균제곱오차) \\(\\mathbb{E}(y_{0} - \\widehat{f}(x_{0}))^{2}\\)는 점 \\(x_{0}\\)에서의 기대 테스트 MSE를 의미하며, 이는 서로 다른 훈련 데이터 집합을 사용해 반복적으로 \\(f\\)를 추정하고, 각각을 \\(x_{0}\\)에서 평가했을 때 얻어지는 테스트 MSE의 평균을 뜻한다. 전체 기대 MSE는 테스트 집합에 포함된 모든 가능한 \\(x_{0}\\)값에 대해 이 값을 평균함으로써 계산할 수 있다.\n기대 테스트 오차를 최소화하기 위해서는 낮은 분산과 낮은 편향을 동시에 달성하는 통계적 학습 방법을 선택해야 함을 알려준다. 그렇다면 통계적 학습 방법에서 말하는 분산과 편향이란 무엇인가?\n분산은 서로 다른 훈련 데이터 집합을 사용해 \\(\\widehat{f}\\)를 추정했을 때, 그 추정치가 얼마나 크게 변하는지를 의미한다. 훈련 데이터는 학습 방법을 적합시키는 데 사용되므로, 훈련 데이터가 달라지면 서로 다른 \\(\\widehat{f}\\)가 생성된다. 이상적으로는 훈련 데이터가 달라지더라도 f의 추정치는 크게 변하지 않아야 한다. 그러나 어떤 방법이 높은 분산을 가진다면, 훈련 데이터의 작은 변화가 \\(\\widehat{f}\\)에 큰 변화를 초래할 수 있다.\n한편 편향은 매우 복잡할 수 있는 현실 문제를 지나치게 단순한 모형으로 근사함으로써 발생하는 오차를 의미한다. 예를 들어 선형회귀는 \\(Y\\)와 \\(X_{1},X_{2},\\ldots,X_{p}\\)사이에 선형 관계가 존재한다고 가정한다. 그러나 실제 문제에서 이러한 단순한 선형 관계가 정확히 성립하는 경우는 드물다. 따라서 선형회귀를 적용하면 f의 추정에 일정한 편향이 발생하게 된다.\n참함수 \\(f\\)가 비선형적인 경우 훈련 관측치의 수가 아무리 많아도 선형회귀를 통해서는 정확한 추정을 할 수 없다. 즉, 이 경우 선형회귀는 높은 편향을 가진다. 반면 참함수가 선형에 가까우면, 충분한 데이터가 주어진다면 선형회귀는 정확한 추정을 제공할 수 있다.\n일반적인 규칙으로, 더 유연한 방법을 사용할수록 분산은 증가하고 편향은 감소한다. 이 두 양이 변화하는 상대적인 속도에 따라 테스트 MSE가 증가할지 감소할지가 결정된다. 방법의 유연성이 증가함에 따라 초기에는 편향이 분산보다 더 빠르게 감소하므로, 기대 테스트 MSE는 감소한다. 그러나 어느 시점 이후에는 유연성을 더 높여도 편향은 거의 줄지 않는 반면, 분산은 급격히 증가하기 시작한다. 이 경우 테스트 MSE는 다시 증가한다.\n편향–분산–테스트 MSE 간의 관계를 편향–분산 상충관계라고 부른다. 통계적 학습 방법에서 좋은 테스트 성능을 얻기 위해서는 낮은 분산과 낮은 제곱 편향이 모두 필요하다. 이를 상충관계라 부르는 이유는, 매우 낮은 편향과 높은 분산을 가진 방법(예: 모든 훈련 데이터를 정확히 통과하는 곡선)이나, 매우 낮은 분산과 높은 편향을 가진 방법(예: 수평선 적합)은 쉽게 얻을 수 있기 때문이다. 진정한 어려움은 편향과 분산이 모두 낮은 방법을 찾는 데 있다.\n현실에서는 참함수 \\(f\\)가 관측되지 않으므로, 특정 학습 방법에 대한 테스트 MSE, 편향, 분산을 명시적으로 계산하는 것은 일반적으로 불가능하다. 그럼에도 불구하고, 분석 과정 전반에서 편향–분산 상충관계를 항상 염두에 두어야 한다. 본서에서는 편향을 거의 제거할 수 있을 정도로 매우 유연한 방법들도 다루지만, 이러한 방법이 항상 단순한 선형회귀보다 우수한 성능을 보장하는 것은 아니다. 예를 들어 참함수 \\(f\\)가 실제로 선형이라면, 선형회귀는 편향이 0이 되므로 더 유연한 방법이 이를 능가하기는 매우 어렵다. 반대로 참함수 \\(f\\)가 강하게 비선형이고 충분한 훈련 데이터가 주어진다면, 매우 유연한 방법이 더 나은 성능을 보일 수 있다.\n\n\n유연한 학습방법이란?\n데이터가 보여주는 형태에 맞추어 함수 f의 모양을 자유롭게 변화시킬 수 있는 정도가 큰 학습 방법을 유연한 학습 방법이라 한다. 이는 단순히 ”복잡한 모델”이라는 뜻이 아니라, 함수 공간에서 허용되는 자유도의 크기를 가리키는 개념이다.\n통계적 학습에서 방법의 유연성(flexibility) 이란 훈련 데이터의 작은 변화에도 추정된 함수 \\(\\widehat{f}\\)가 얼마나 크게 변할 수 있는가, 또는 \\(\\widehat{f}\\)가 가질 수 있는 형태의 범위가 얼마나 넓은가를 의미한다.\n즉, 유연한 방법일수록 다양한 비선형 구조를 표현할 수 있고, 데이터에 더 밀착된 적합이 가능하다.\n비유연한 방법은 함수 형태가 강하게 제한되고 데이터의 전체적인 경향만 반영되며 편향은 크고, 분산은 작다. 예를 들면, 단순 평균 모델, 선형회귀, 낮은 차수의 다항회귀 등 이다.\n유연한 방법은 함수 형태에 대한 제약이 약하며, 국소적 패턴까지 포착 가능하며 편향은 작고, 분산은 크다. 예를 들면, 고차 다항회귀, k-NN, 스플라인(자유도 큰 경우), 결정트리(깊은 트리), 신경망(은닉층·노드 많을수록)\n\n\n유연성과 편향-분산의 연결\n유연성 ↑ → Bias ↓ (참함수에 더 가까워질 수 있음) → Variance ↑ (데이터 변화에 민감)\n유연성 ↓ → Bias ↑ → Variance ↓\n따라서 유연한 방법은 ”잘 맞출 수 있지만, 불안정할 수 있는 방법” 이라고 요약할 수 있다.\n\n\n\n2. ML 모델 선택\n\n유연성과 복잡도 개념\n통계적 학습에서 흔히 유연성과 복잡도를 같은 의미로 사용하지만, 엄밀히 말하면 두 개념은 동일하지 않다. 복잡도는 모형의 구조적 특성을 설명하는 개념인 반면, 유연성은 그 구조가 데이터에 어떻게 반응하는가를 설명하는 개념이다. 즉, 복잡도는 모형의 설계 차원에 속하고, 유연성은 학습 결과의 행동 차원에 속한다.\n복잡도는 모형이 사전에 허용하는 구조의 크기나 풍부함을 의미한다. 이는 보통 모형이 갖는 형식적 자유도로 표현된다. 예를 들어 모수의 개수, 다항식의 차수, 결정트리의 깊이, 신경망의 층 수와 노드 수와 같은 요소들은 모두 모형의 복잡도를 증가시킨다. 복잡도는 모델을 설계하는 단계에서 비교적 명확하게 정의할 수 있으며, 데이터와 무관하게 기술되는 경우가 많다.\n이 관점에서 복잡도는 ”이 모형이 이론적으로 얼마나 복잡한 함수를 표현할 수 있는가”를 나타낸다.\n유연성은 복잡도보다 한 단계 더 나아간 개념으로, 실제로 학습된 모형이 데이터의 변화에 얼마나 민감하게 반응하는가를 의미한다. 즉, 동일한 모형 구조를 사용하더라도 훈련 데이터가 조금만 달라졌을 때 추정된 함수 \\hat f가 얼마나 크게 달라지는지가 유연성의 핵심이다.\n따라서 유연성은 단순히 모수의 개수로 결정되지 않는다. 동일한 구조의 모형이라 하더라도 규제(regularization)의 강도, 학습 방법, 하이퍼파라미터 설정에 따라 유연성은 크게 달라질 수 있다.\n복잡도 → *”얼마나 복잡한 모형을 설계했는가”*에 대한 개념이다.\n유연성 → *”그 모형이 데이터에 얼마나 민감하게 반응하는가”*에 대한 개념이다.\n복잡도는 유연성의 잠재적 상한선을 제공하지만, 유연성 그 자체는 아니다.\n\n\n과적합과 과소적합의 본질\n편향과 분산의 관점에서 보면, 과소적합(underfitting)과 과적합(overfitting)은 매우 명확하게 구분된다. 과소적합은 모형이 지나치게 단순하여 데이터 생성 과정의 구조를 충분히 포착하지 못하는 경우이다. 이때 모형은 높은 편향을 가지며, 학습 데이터와 테스트 데이터 모두에서 성능이 좋지 않다.\n반대로 과적합은 모형이 학습 데이터의 우연적 변동까지 학습해 버린 경우이다. 이때 모형은 편향은 작을 수 있으나, 분산이 매우 커진다. 학습 데이터에서는 성능이 우수하지만, 새로운 데이터에서는 성능이 급격히 저하된다.\n중요한 점은 과적합이 ”모형이 너무 복잡하다”는 직관적 설명만으로는 충분히 이해되지 않는다는 것이다. 보다 정확히 말하면, 과적합은 모형의 복잡도와 데이터 생성 과정의 불확실성 사이의 불균형에서 발생한다.\n\n\n모델 복잡도와 일반화 성능\n모형의 복잡도가 증가하면 일반적으로 편향은 감소하고, 분산은 증가한다. 이는 거의 모든 학습 방법에서 관측되는 현상이다. 따라서 일반화 성능은 다음과 같은 절충의 결과로 결정된다.\n\n단순한 모형: 높은 편향, 낮은 분산\n복잡한 모형: 낮은 편향, 높은 분산\n\n이 관계는 ”복잡한 모델이 항상 나쁘다”거나 ”단순한 모델이 항상 좋다”는 식의 결론을 허용하지 않는다. 중요한 것은 데이터 생성 과정의 복잡도에 비해 모형이 얼마나 적절한가이다.\n데이터 생성 과정 관점에서 보면, 최적의 모델 복잡도란 참 함수 \\(f\\)의 구조와 표본 크기, 그리고 오차항의 분산이 결합된 결과이다. 즉, 데이터가 적거나 노이즈가 큰 상황에서는 복잡한 모형이 오히려 일반화 성능을 악화시킬 수 있다.\n\n\nML 모델 선택의 통계적 기준\n머신러닝에서 모델 선택은 흔히 ”어떤 모델이 가장 정확한가”라는 질문으로 요약된다. 그러나 이 정확도는 반드시 일반화 성능을 기준으로 평가되어야 한다.\n통계적 관점에서 보면, 모델 선택이란 결국 편향과 분산 사이의 균형점을 찾는 문제이다. 교차검증은 바로 이 균형을 데이터로부터 추정하기 위한 도구이며, 정규화는 분산을 인위적으로 제어하는 장치이다.\n이러한 맥락에서 머신러닝의 다양한 기법들은 서로 다른 방식으로 동일한 문제를 해결하고 있다고 볼 수 있다.\n\n정규화: 분산을 줄이기 위해 편향을 일부 허용\n앙상블: 분산을 평균화하여 일반화 성능 개선\n조기 종료: 학습 과정에서 과적합을 사전에 차단\n\n즉, ML 모델 선택은 단순한 알고리즘 경쟁이 아니라, 통계적 오차 구조에 대한 선택이다.\n\n\n\n3. 우리는 함수 \\(f\\)를 어떻게 추정하는가?\n여기서는 함수 \\(f\\)를 추정하기 위한 다양한 선형 및 비선형 방법들을 살펴본다. 그러나 이러한 방법들은 일반적으로 몇 가지 공통된 특성을 공유한다.\n항상 서로 다른 \\(n\\)개의 데이터 포인트가 관측되었다고 가정한다. 이러한 관측값들을 훈련 데이터라고 부르는데, 이는 이 관측값들을 사용하여 f를 추정하는 방법을 학습(또는 훈련)시키기 때문이다.\n관측치 \\(i\\)에 대해 \\(j\\)번째 설명변수(또는 입력 변수)의 값을 \\(x_{ij}\\)라고 하자. 여기서 \\(i = 1,2,\\ldots,n,j = 1,2,\\ldots,p\\)이다. 이에 대응하여 \\(y_{i}\\)는 \\(i\\)번째 관측치의 반응변수를 나타낸다. 그러면 우리의 훈련 데이터는 \\(\\{(x_{1},y_{1}),(x_{2},y_{2}),\\ldots,(x_{n},y_{n})\\}\\)로 구성되며, 여기서 \\(x_{i} = (x_{i1},x_{i2},\\ldots,x_{ip})^{T}\\)이다.\n통계분석 목표는 훈련 데이터를 이용해 미지의 함수 \\(f\\)를 추정하는 통계적 학습 방법을 적용하는 것이다. 다시 말해, 임의의 관측치 \\((X,Y)\\)에 대해 \\(Y \\approx \\widehat{f}(X)\\)를 만족하는 함수 \\(\\widehat{f}\\)를 찾고자 한다.\n일반적으로 이러한 과제를 수행하는 대부분의 통계적 학습 방법은 모수적 방법과 비모수적 방법으로 구분할 수 있다.\n\n(1) 모수적 방법\n모수적 방법은 두 단계로 이루어진 모형 기반 접근법이다.\n1. 첫 번째 단계: 함수 \\(f\\)의 함수적 형태, 즉 구조나 모양에 대해 가정을 한다. 예를 들어, 가장 단순한 가정 중 하나는 \\(f\\)가 \\(X\\)에 대해 선형이라는 것이다.\n\\[f(X) = \\beta_{0} + \\beta_{1}X_{1} + \\beta_{2}X_{2} + \\cdots + \\beta_{p}X_{p}\\]\n\\(f\\)가 선형이라는 가정을 하면 \\(f\\)를 추정하는 문제는 크게 단순화된다. 즉, 임의의 \\(p\\)차원 함수 \\(f(X)\\) 전체를 추정할 필요 없이, \\((p + 1)\\)개의 계수 \\(\\beta_{0},\\beta_{1},\\ldots,\\beta_{p}\\)만 추정하면 된다.\n2. 두 번째 단계: 모형이 선택되면, 훈련 데이터를 이용해 모형을 적합하거나 학습시키는 절차가 필요하다. 즉, \\(Y \\approx {\\widehat{\\beta}}_{0} + {\\widehat{\\beta}}_{1}X_{1} + {\\widehat{\\beta}}_{2}X_{2} + \\cdots + {\\widehat{\\beta}}_{p}X_{p}\\)을 가장 잘 만족하는 계수값들을 찾는 것이 목적이다.\n가장 일반적인 방법은 (일반) 최소제곱법이다. 다만 최소제곱법은 선형모형을 적합시키는 여러 방법 중 하나일 뿐이다.\n\n\n(2) 비모수적 방법\n모형 기반 접근법은 모수적 접근법이라 불린다. 이 방법은 함수 \\(f\\)를 추정하는 문제를, 하나의 함수를 직접 추정하는 문제가 아니라 일정한 수의 모수를 추정하는 문제로 환원시킨다.\n함수 \\(f\\)에 대해 모수적 형태를 가정하면, 완전히 임의적인 함수 \\(f\\) 전체를 적합시키는 것보다 \\(\\beta_{0},\\beta_{1},\\ldots,\\beta_{p}\\)와 같은 모수들의 집합을 추정하는 것이 일반적으로 훨씬 쉽기 때문에, \\(f\\)의 추정 문제가 크게 단순화된다.\n하지만 모수적 접근법의 잠재적인 단점은, 우리가 선택한 모형이 실제로 존재하는 미지의 참함수 f의 형태와 대부분 일치하지 않는다는 점이다. 만약 선택한 모형이 참함수 \\(f\\)와 크게 다르다면, 그에 따른 추정 결과 역시 부정확해질 수밖에 없다.\n이 문제를 완화하기 위한 한 가지 방법은, \\(f\\)의 다양한 가능한 함수 형태를 잘 적합할 수 있는 유연한 모형을 선택하는 것이다. 그러나 일반적으로 더 유연한 모형을 적합시키기 위해서는 더 많은 수의 모수를 추정해야 한다. 이러한 복잡한 모형들은 종종 과적합 이라 불리는 현상을 초래하는데, 이는 본질적으로 모형이 데이터에 포함된 오차, 즉 잡음을 지나치게 충실히 따라가게 된다는 것을 의미한다.\n\n\n비모수적 방법 정의\n\\(Y = f(X) + \\varepsilon\\)에서 \\(f\\)의 형태(선형, 다항식 등)를 사전에 규정하지 않는다. 대신 관측된 데이터에 최대한 가깝게 적합하면서도, 지나치게 요동치지 않도록 제약을 두어 \\(f\\)를 추정한다.\n즉, 문제를 ”몇 개의 모수 \\(\\beta\\)를 추정한다”로 바꾸지 않고, ”함수 \\(f\\) 자체를 데이터로부터 학습한다”는 관점에서 접근한다.\n\n\n비모수적 방법 특징\n1. 함수 형태에 대한 가정이 없다. → 선형성, 특정 다항 차수 등 모형 가정을 두지 않으며 다양한 곡률과 비선형 구조를 자연스럽게 표현할 수 있다.\n2. 유연성이 매우 높다. → 데이터가 보여주는 구조를 그대로 반영할 수 있고 참함수 \\(f\\)가 복잡한 형태일수록 장점이 커진다.\n3. 많은 데이터가 필요하다. → 소수의 모수로 요약되지 않기 때문에 안정적인 추정을 위해 관측치 수가 충분히 커야 한다.\n4. 과적합 위험이 존재한다. → 유연성이 큰 만큼, 잡음까지 학습할 가능성이 있고 매끄러움, 이웃 크기, 규제가 핵심이다.\n\n\n대표적인 비모수적 방법 예시\n\n커널 회귀(Kernel Regression)\nk-최근접 이웃(k-NN)\n스플라인 회귀(Splines, thin-plate spline)\n로컬 회귀(LOESS, LOWESS)\n회귀 트리 및 랜덤 포레스트\n가우시안 프로세스 회귀\n\n\n\n\n4. 지도학습 · 비지도학습 · 준지도학습\n통계적 학습 문제는 관측된 데이터의 형태에 따라 크게 지도학습과 비지도학습으로 구분된다. 이 구분의 기준은 각 관측치에 대해 반응변수가 함께 관측되었는지 여부이다. 분석 목적과 사용 가능한 방법은 이 구분에 의해 근본적으로 달라진다.\n\n\n\n\n\n\n\n\n\n구분\n지도학습\n비지도학습\n준지도학습\n\n\n반응변수 Y\n있음\n없음\n일부만 있음\n\n\n주요 목적\n예측·추론\n구조·패턴 발견\n예측 성능 향상\n\n\n데이터 요구\n라벨 필수\n라벨 불필요\n라벨 일부\n\n\n대표 기법\n회귀, 분류\n군집, PCA\n라벨 전파, 혼합모형\n\n\n\n지도학습은 반응변수가 존재하는 상황에서 예측과 추론을 수행하는 방법이며, 비지도학습은 반응변수가 없는 상황에서 데이터의 구조를 탐색하는 방법이다. 준지도학습은 이 두 접근법의 중간 영역으로, 제한된 라벨 정보를 최대한 활용하려는 현실적인 학습 전략이라 할 수 있다.\n\n지도학습 (Supervised Learning)\n각 관측치마다 설명변수 \\(X\\)와 이에 대응하는 반응변수 \\(Y\\)가 함께 주어진 상황을 다룬다. 이 경우 분석의 목적은 새로운 관측치에 대한 반응변수를 정확하게 예측하거나, 설명변수와 반응변수 사이의 관계를 해석하는 데 있다. 다시 말해, 지도학습은 ”입력이 주어졌을 때 출력은 무엇인가”라는 질문에 답하고자 한다. 선형회귀와 로지스틱 회귀와 같은 전통적인 통계 모형뿐 아니라, 일반화 가법모형(GAM), 부스팅, 서포트 벡터 머신 등 대부분의 현대적 예측 방법들 역시 지도학습의 범주에 속한다.\n\n\n비지도학습 (Unsupervised Learning)\n각 관측치에 대해 설명변수 \\(X\\)만 관측되고, 반응변수 \\(Y\\)는 존재하지 않는 경우를 다룬다. 예측해야 할 대상이 없기 때문에 회귀나 분류와 같은 지도학습 모형을 적용할 수 없으며, 분석의 목표 또한 다르다. 비지도학습의 목적은 데이터에 내재된 구조를 탐색하고, 변수들 간 또는 관측치들 간의 관계를 이해하는 데 있다. 이 과정은 분석을 지도해 줄 반응변수가 없다는 점에서 본질적으로 더 어렵다.\n비지도학습의 대표적인 예가 군집분석이다. 군집분석은 관측된 변수들을 바탕으로 관측치들이 몇 개의 서로 구별되는 그룹으로 나뉘는지를 규명하는 것을 목표로 한다. 예를 들어 시장 세분화 연구에서는 고객의 소득, 거주 지역, 소비 성향과 같은 여러 특성을 관측할 수 있지만, 각 고객이 어떤 유형의 소비자에 속하는지는 사전에 알 수 없는 경우가 많다. 이때 군집분석을 통해 유사한 특성을 가진 고객들을 하나의 그룹으로 묶음으로써, 잠재적인 고객 유형을 식별할 수 있다. 이러한 그룹화는 이후 마케팅 전략이나 정책 설계의 기초 자료로 활용될 수 있다.\n비지도학습 문제는 변수의 수가 많아질수록 더욱 복잡해진다. 변수가 두 개일 경우에는 산점도를 통해 직관적으로 구조를 파악할 수 있지만, 실제 데이터에서는 수십 개 이상의 변수가 포함되는 경우가 일반적이다. 이 경우 가능한 산점도의 수는 급격히 증가하며, 시각적 탐색만으로는 구조를 파악하기 어렵다. 따라서 자동화된 군집화 기법과 차원축소 방법이 비지도학습에서 중요한 역할을 한다.\n\n\n준지도학습 (Semi-Supervised Learning)\n현실의 데이터 분석에서는 지도학습과 비지도학습이 명확히 구분되지 않는 경우도 존재한다. 예를 들어 전체 관측치 중 일부에 대해서만 반응변수가 관측되고, 나머지 관측치에는 설명변수만 존재하는 상황을 생각해 볼 수 있다. 이러한 상황은 설명변수는 비교적 저렴하게 측정할 수 있지만, 반응변수는 비용이나 시간 문제로 인해 제한적으로만 수집 가능한 경우에 자주 발생한다.\n이와 같은 설정을 준지도학습이라 한다. 준지도학습의 목적은 반응변수가 관측된 소수의 데이터와, 반응변수가 없는 다수의 데이터를 동시에 활용하여 학습 성능을 향상시키는 데 있다. 즉, 지도학습의 정보와 비지도학습의 구조 정보를 결합하여 보다 효율적인 학습을 수행하고자 하는 접근법이다.\n\n\n\n4. 지도학습에서의 예측과 추론\n지도학습은 각 관측치에 대해 설명변수 \\(X\\)와 반응변수 \\(Y\\)가 함께 관측된 상황에서, 두 변수 간의 관계를 학습하는 통계적 방법이다. 그러나 지도학습의 목적은 하나로 고정되어 있지 않으며, 크게 예측과 추론이라는 두 가지 상이한 관점으로 구분할 수 있다. 이 두 관점의 차이는 단순한 용어상의 구분이 아니라, 분석의 목표, 모형 선택, 그리고 결과 해석 방식 전반에 영향을 미친다.\n먼저 예측 관점에서의 지도학습은 새로운 설명변수 \\(X_{\\text{new}}\\)가 주어졌을 때, 이에 대응하는 반응변수 \\(Y_{\\text{new}}\\)를 가능한 한 정확하게 예측하는 것을 목표로 한다. 이때 핵심 관심사는 예측 오차의 크기이며, 모형이 어떤 구조를 갖는지는 상대적으로 덜 중요하다. 예측 정확도를 높일 수 있다면, 모형이 복잡하더라도, 또는 해석이 어렵더라도 문제가 되지 않는다. 이러한 관점에서는 교차검증을 통한 예측 오차 최소화, 규제(parameter tuning), 앙상블 기법과 같은 방법들이 자연스럽게 강조된다. 부스팅, 랜덤 포레스트, 서포트 벡터 머신과 같은 기법들이 예측 중심 지도학습의 대표적인 예라 할 수 있다.\n이에 반해 추론 관점에서의 지도학습은 설명변수와 반응변수 사이의 관계를 이해하고 해석하는 데 초점을 둔다. 이 경우 분석의 목적은 ”어떤 변수가 중요한가?“, ”설명변수의 변화가 반응변수에 어떤 영향을 미치는가?”와 같은 질문에 답하는 것이다. 따라서 모형의 해석 가능성이 매우 중요하며, 각 설명변수의 효과를 명확하게 표현할 수 있는 모형이 선호된다. 선형회귀나 로지스틱 회귀와 같은 비교적 단순한 모형이 오랫동안 널리 사용되어 온 이유도 여기에 있다. 추론 중심 분석에서는 모수 추정치의 크기와 부호, 신뢰구간, 가설검정과 같은 통계적 해석 도구가 핵심적인 역할을 한다.\n예측과 추론의 관점 차이는 모형의 복잡도 선택에서도 뚜렷하게 나타난다. 예측을 목적으로 하는 경우에는 데이터가 허용하는 범위 내에서 보다 유연한 모형을 선택하는 것이 일반적이며, 다소의 과적합 위험도 감수할 수 있다. 반면 추론을 목적으로 할 경우에는 과도한 유연성이 오히려 해석을 방해하고 불안정한 결론을 초래할 수 있으므로, 상대적으로 단순하고 구조가 명확한 모형이 바람직하다.\n또한 두 관점은 ”좋은 모형”에 대한 기준에서도 차이를 보인다. 예측 관점에서는 보이지 않는 새로운 데이터에 대한 성능이 가장 중요한 평가 기준이 된다. 반면 추론 관점에서는 모형이 데이터 생성 과정에 대해 합리적인 가정을 하고 있는지, 추정된 관계가 통계적으로 의미 있는지, 그리고 결과가 해석 가능한지를 중시한다.\n현실의 데이터 분석에서는 예측과 추론이 완전히 분리되는 경우는 드물다. 많은 경우 두 목적이 동시에 존재하며, 분석가는 어느 쪽에 더 무게를 둘 것인지 명확히 인식한 상태에서 모형을 선택해야 한다. 지도학습에서 예측과 추론을 구분하는 것은 단순한 분류가 아니라, 이후 분석 전반의 방향을 결정하는 출발점이라 할 수 있다.\n\n\n5. 편향–분산 관점에서 본 예측과 추론의 차이\n지도학습에서 예측과 추론의 차이는 흔히 목적의 차이로 설명되지만, 보다 근본적으로는 편향-분산 상충관계를 어떻게 받아들이는가의 차이로 이해할 수 있다. 동일한 데이터와 동일한 학습 문제라 하더라도, 분석의 목적이 예측인지 추론인지에 따라 허용 가능한 편향과 분산의 수준은 달라진다.\n먼저 예측 관점에서의 지도학습은 새로운 관측치에 대한 예측 오차를 최소화하는 것을 목표로 한다. 이때 관심의 대상은 개별 모형의 모수 값이 아니라, 반복 표본 추출 상황에서의 평균적인 예측 성능이다. 따라서 예측 문제에서는 다소의 편향이 존재하더라도 분산을 효과적으로 줄일 수 있다면, 전체 예측 오차는 오히려 감소할 수 있다. 이 관점에서는 모형이 참함수 f의 구조를 정확히 반영하는지보다는, 새로운 데이터에 대해 얼마나 안정적으로 작동하는지가 더 중요하다.\n이러한 이유로 예측 중심 분석에서는 규제를 통한 모형 단순화, 앙상블 기법, 혹은 의도적인 편향 도입이 정당화된다. 예를 들어 릿지 회귀나 라쏘는 모수 추정에 편향을 도입하지만, 분산을 크게 감소시켜 예측 성능을 향상시킨다. 부스팅이나 랜덤 포레스트 역시 개별 모형의 해석 가능성을 희생하는 대신, 분산 감소를 통해 강력한 예측력을 확보한다. 즉, 예측 관점에서는 편향-분산 상충관계의 균형점이 분산 감소 쪽으로 이동한다.\n반면 추론 관점에서의 지도학습은 설명변수와 반응변수 사이의 관계를 정확히 이해하는 것을 목표로 한다. 이 경우 분석의 핵심은 모형이 데이터 생성 과정을 얼마나 잘 반영하는지이며, 추정된 모수의 해석 가능성과 통계적 타당성이 중요하다. 따라서 추론 문제에서는 작은 편향조차도 심각한 문제가 될 수 있다. 편향된 모수 추정치는 설명변수의 효과에 대한 왜곡된 해석으로 이어지기 때문이다.\n추론 중심 분석에서는 분산이 다소 크더라도, 모형이 올바르게 지정되어 있다면 편향이 없는 추정치를 선호한다. 이는 반복 표본 추출 하에서 추정량의 평균이 참값에 수렴한다는 점이 해석의 신뢰성을 보장하기 때문이다. 이러한 맥락에서 선형회귀와 같은 비교적 단순한 모형이 여전히 추론의 표준 도구로 사용되며, 신뢰구간과 가설검정이 중요한 역할을 한다. 즉, 추론 관점에서는 편향-분산 상충관계의 균형점이 편향 최소화 쪽에 놓인다.\n이 차이는 과적합에 대한 태도에서도 분명히 드러난다. 예측 문제에서는 교차검증을 통해 과적합 여부를 판단하며, 새로운 데이터에서의 성능이 유지된다면 훈련 데이터에 대한 높은 적합도는 반드시 문제가 되지 않는다. 반면 추론 문제에서는 과적합된 모형이 불안정한 추정치와 왜곡된 효과 해석을 초래할 수 있으므로, 훨씬 더 엄격하게 경계된다.\n결국 편향-분산 상충관계는 단순한 기술적 개념이 아니라, 분석 목적을 수학적으로 구체화한 틀이라 할 수 있다. 지도학습에서 예측과 추론을 구분한다는 것은, 어떤 종류의 오차를 더 용인할 것인지 편향인가, 분산인가에 대한 선택을 의미한다. 이 선택이 모형의 복잡도, 추정 방법, 결과 해석 전반을 결정하게 된다.\n\n\n6. 데이터 생성 과정 관점에서 왜 추론이 더 까다로운가\n지도학습에서 예측과 추론의 차이는 단순히 목적의 차이에 그치지 않으며, 보다 근본적으로는 데이터 생성 과정에 대해 무엇을 요구하는가의 차이로 이해할 수 있다. 이 관점에서 보면, 추론이 예측보다 본질적으로 훨씬 까다로운 문제임을 알 수 있다.\n데이터 생성 과정란 관측된 데이터가 생성되는 확률적 메커니즘을 의미한다. 통계적 추론은 본질적으로 이 데이터 생성 과정에 대해 어떤 구조적 진술을 하고자 하는 작업이다. 즉, 추론의 목표는 단순히 관측된 데이터에 잘 맞는 함수를 찾는 것이 아니라, 데이터를 만들어낸 근본적인 관계가 무엇인지를 밝히는 데 있다. 따라서 추론에서는 모형이 데이터 생성 과정을 얼마나 정확히 반영하고 있는지가 핵심적인 문제가 된다.\n이에 비해 예측 문제에서는 데이터 생성 과정을 정확히 복원할 필요가 없다. 새로운 입력 \\(X_{\\text{new}}\\)에 대해 반응변수 \\(Y_{\\text{new}}\\)를 잘 맞히기만 하면 되며, 그 과정에서 사용된 모형이 실제 DGP와 일치하는지는 부차적인 문제로 취급된다. 예측 정확도가 높다면, 모형이 데이터 생성 과정을 부분적으로 잘못 근사하고 있더라도 실용적인 목적은 달성된다. 이 점에서 예측은 데이터 생성 과정에 대한 약한 요구만을 가진 문제라 할 수 있다.\n반면 추론은 데이터 생성 과정에 대해 훨씬 강한 요구를 부과한다. 예를 들어 선형회귀 모형에서 특정 설명변수의 계수 \\beta_j를 해석하려면, 해당 모형이 데이터 생성 과정의 구조를 올바르게 반영하고 있다는 전제가 필요하다. 만약 중요한 설명변수가 누락되었거나, 함수 형태가 잘못 지정되었거나, 오차 구조에 대한 가정이 틀렸다면, 추정된 계수는 DGP의 실제 효과를 반영하지 못한다. 이 경우 추정치는 수치적으로 안정적일 수는 있어도, 해석적으로는 의미를 잃게 된다.\n또한 추론에서는 조건부 관계와 인과적 해석이 암묵적으로 요구되는 경우가 많다. 설명변수 X_j의 효과를 논의한다는 것은, 다른 변수들이 고정된 상태에서 X_j가 변화할 때 Y가 어떻게 반응하는지를 해석하는 것을 의미한다. 이러한 해석은 DGP에 대한 구조적 가정 없이는 성립할 수 없다. 반면 예측은 이러한 조건부 해석을 필요로 하지 않으며, 단지 관측된 공변량 공간에서의 함수 근사 문제로 귀결된다.\n데이터 생성 과정 관점에서 또 하나 중요한 차이는 모형 오지정(model misspecification) 에 대한 민감도이다. 예측 문제에서는 모형이 다소 잘못 지정되었더라도, 충분한 데이터와 유연한 학습 방법을 통해 예측 성능을 유지할 수 있다. 반면 추론 문제에서는 작은 오지정조차도 추정된 효과의 방향이나 크기를 왜곡할 수 있으며, 이는 잘못된 과학적 결론으로 이어질 수 있다. 즉, 추론은 데이터 생성 과정 오지정에 대해 훨씬 더 취약하다.\n마지막으로, 추론은 관측되지 않은 반사실 상황에 대한 해석을 암묵적으로 포함하는 경우가 많다. ”만약 이 변수가 달랐다면 결과는 어떻게 되었을까?“라는 질문은, 실제로 관측되지 않은 데이터 생성 과정을 상정해야만 답할 수 있다. 이러한 질문은 단순한 함수 근사로는 해결할 수 없으며, 데이터 생성 과정에 대한 구조적 이해를 전제로 한다. 이 점에서 추론은 예측보다 훨씬 더 강한 가정과 해석적 책임을 요구한다.\n요약하면, 예측은 데이터 생성 과정을 잘 근사하는 것으로 충분한 문제인 반면, 추론은 데이터 생성 과정의 구조 자체를 이해하고 설명하려는 문제이다. 따라서 추론은 더 많은 가정에 의존하며, 모형 선택과 가정 위반에 훨씬 민감하다. 이러한 이유로 통계적 학습에서 추론은 예측보다 본질적으로 더 어렵고 까다로운 작업이라 할 수 있다."
  },
  {
    "objectID": "notes/mldl/mldl_supervised.html",
    "href": "notes/mldl/mldl_supervised.html",
    "title": "MLDL 지도학습",
    "section": "",
    "text": "Chapter 1. 선형회귀 ≠ ML의 시작\n지도학습은 흔히 선형회귀에서 출발한다고 설명되지만, 선형회귀 자체를 머신러닝의 출발점으로 동일시하는 것은 정확하지 않다.\n이 장에서는 설명모형과 예측모형의 근본적인 차이를 출발점으로 삼아, 머신러닝이 회귀분석에서 출발하지만 왜 그 틀에 머물지 않는지를 살펴본다. 나아가 선형모형을 머신러닝 관점에서 어떻게 재해석할 수 있는지를 통계적 구조와 목표의 차이를 중심으로 정리한다.\n선형회귀는 전통적으로 설명모형의 대표적인 도구이다. 반면 머신러닝은 동일한 수학적 형태에서 출발하더라도, 목표와 철학이 본질적으로 다르다.\n머신러닝의 관점에서 선형회귀는 가장 단순한 예측기에 해당하며, 규제와 검증을 통해 보다 복잡한 예측 방법으로 확장된다. 따라서 ”선형회귀는 ML의 시작이다”라는 표현은 형식적으로는 옳을 수 있으나, 개념적으로는 불완전한 설명이다.\n\n1. 설명모형과 예측모형의 차이\n\n설명모형: ”왜 그런가?“에 대한 질문\n전통적인 선형회귀는 설명을 주목적으로 하는 모형이다. 이때 분석의 관심은 주로 다음과 같은 요소에 놓인다.\n\n회귀계수 \\(\\beta_{j}\\)의 부호와 크기\n통계적 유의성 (t-test, p-value)\n모형의 해석 가능성\n\n고전적 선형회귀는 다음과 같은 데이터 생성과정을 전제로 한다.\n\\(Y = X\\beta + \\varepsilon,\\mathbb{E}(\\varepsilon \\mid X) = 0,Var(\\varepsilon) = \\sigma^{2}\\)\n이 가정하에서 회귀계수 \\(\\beta\\)는 모집단 수준에서의 구조적 관계로 해석된다. 즉, 회귀분석은 단순히 관측값을 맞추는 것이 아니라, 데이터 뒤에 존재하는 생성 메커니즘을 추론하기 위한 통계적 도구이다.\n\n\n예측모형: ”얼마나 잘 맞추는가?“에 대한 질문\n반면, 머신러닝에서의 지도학습은 예측을 핵심 목표로 한다. 이때 관심의 대상은 다음과 같이 달라진다.\n\n새로운 데이터 \\(x_{\\text{new}}\\)에 대한 예측 정확도\n일반화 오차 (generalization error)\n테스트 데이터에서의 성능\n\n이 관점에서 중요한 것은 개별 회귀계수의 해석이 아니라, 입력 X로부터 출력 Y를 얼마나 잘 예측하는 함수 \\(f\\) 자체의 성능이다.\n\\(\\widehat{Y} = f(X)\\), 여기서 \\(f\\)는 함수 공간에 속하며, 반드시 선형일 필요는 없다. 머신러닝은 특정 계수의 의미보다는, 훈련 데이터에서 학습된 함수가 보지 못한 데이터에서도 얼마나 잘 작동하는가에 초점을 둔다.\n\n\n\n\n\n\n\n\n구분\n설명모형\n예측모형\n\n\n관심 질문\n왜 그런가\n얼마나 잘 맞추는가\n\n\n핵심 대상\n모수 \\(\\beta\\)\n예측함수 \\(f\\)\n\n\n평가 기준\n유의성, 해석\n테스트 오차\n\n\n데이터 관점\n데이터 생성과정 복원\n미지 데이터 적합\n\n\n\n\n\n\n2. ML은 회귀에서 출발하나 회귀는 아니다\n\n공통 출발점: 손실함수 최소화\n선형회귀와 머신러닝은 표면적으로 서로 다른 학문 영역처럼 보이지만, 수학적으로는 동일한 출발점을 공유한다. 두 방법 모두 관측된 데이터에서 예측 오차를 최소화하는 문제로 기술될 수 있다.\n선형회귀는 다음과 같은 제곱손실 최소화 문제로 표현된다.\n\\[\\widehat{\\beta} = \\arg\\min_{\\beta}\\overset{n}{\\sum_{i = 1}}(y_{i} - x_{i}^{\\top}\\beta)^{2}\\]\n이 식은 반응변수 \\(y_{i}\\)와 설명변수 \\(x_{i}\\)로부터 계산된 예측값 \\(x_{i}^{\\top}\\beta\\)사이의 차이를 제곱한 값을 최소화하는 최적화 문제이다. 이 관점에서 선형회귀는 통계적 추론 이전에 이미 하나의 최적화 문제로 해석될 수 있다.\n머신러닝 역시 기본적으로는 손실함수의 최소화를 목표로 한다. 일반적인 지도학습 문제는 다음과 같이 표현된다.\n\\[\\widehat{f} = \\arg\\min_{f \\in \\mathcal{F}}\\frac{1}{n}\\overset{n}{\\sum_{i = 1}}L(y_{i},f(x_{i}))\\]\n여기서 \\(L( \\cdot , \\cdot )\\)는 손실함수이며, \\(\\mathcal{F}\\)는 고려하는 함수 공간을 의미한다. 선형회귀는 이 일반적인 틀에서 손실함수로 제곱손실을 사용하고, 함수 공간을 선형 함수로 제한한 특수한 경우에 해당한다.\n이러한 이유로 선형회귀는 머신러닝의 자연스러운 출발점이 된다. 그러나 이 공통점은 손실함수 최소화라는 형식적 측면에 국한된다. 이후의 차이는 손실함수를 최소화하는 과정에서 어떤 가정을 유지하고, 어떤 제약을 완화하는가에 따라 결정된다.\n이 지점에서 선형회귀와 머신러닝은 같은 문제에서 출발하지만, 서로 다른 방향으로 분기하게 된다.\n\n\n결정적 분기점: ”무엇을 고정하고 무엇을 유연하게 둘 것인가”\n전통적인 선형회귀에서는 모형 구조가 사전에 고정된다. 설명변수와 반응변수 사이의 관계는 선형으로 가정되며, 분석의 불확실성은 오차항을 통해 확률적으로 모델링된다. 이때 관심의 중심은 회귀계수 자체이며, 추정된 계수는 모집단 수준의 구조적 관계를 반영하는 것으로 해석된다. 따라서 회귀분석의 목적은 모형의 적합 그 자체보다는, 계수에 대한 추론과 해석에 놓인다.\n반면 머신러닝에서는 모형 구조를 고정된 전제로 두지 않는다. 함수의 형태는 데이터에 따라 유연하게 변화할 수 있으며, 오차항의 분포에 대한 명시적 가정은 필수가 아니다. 분석의 목표 역시 모수 추론이 아니라, 관측되지 않은 새로운 데이터에 대한 예측 성능에 있다. 이 관점에서 중요한 것은 특정 계수의 통계적 유의성이 아니라, 학습된 함수가 얼마나 안정적으로 일반화되는가이다.\n이 차이로 인해 머신러닝은 모형 해석의 명확성을 일부 포기하는 대신, 복잡한 구조와 비선형 관계를 포착할 수 있는 자유도를 확보한다.\n회귀가 ML의 시작일 수는 있으나, ML 자체는 아닌 이유\n선형회귀는 머신러닝의 출발점으로 활용될 수는 있으나, 그 자체로 머신러닝이라고 보기는 어렵다. 그 이유는 다음과 같다.\n첫째, 선형회귀는 일반화 성능을 직접적으로 최적화하지 않는다. 전통적인 회귀분석에서의 적합 기준은 주로 학습 데이터에서의 오차 최소화이며, 새로운 데이터에서의 성능은 부차적인 고려 대상이다.\n둘째, 모델 복잡도에 대한 통제가 제한적이다. 설명변수의 수가 증가하거나 변수 간 상관성이 커질 경우, 추정된 계수의 분산이 급격히 증가할 수 있다.\n셋째, 고차원 자료나 비선형 구조에 취약하다. 선형회귀는 구조적으로 선형 관계에 의존하므로, 복잡한 패턴을 직접적으로 학습하기 어렵다.\n머신러닝은 이러한 한계를 극복하기 위해 규제, 비선형 변환, 앙상블 기법, 신경망과 같은 방법들을 도입해 왔다. 이 과정에서 핵심은 특정 알고리즘이 아니라, 모델 선택과 평가를 중심으로 한 학습의 틀이다.\n\n\n선형모형의 재해석: ML 관점에서의 선형회귀\n1. 선형회귀를 가장 단순한 예측기로 보기\n머신러닝의 관점에서 선형회귀는 설명을 위한 종착점이 아니라, 예측 성능 비교를 위한 기준선 모형으로 재해석된다. 선형회귀는 함수 공간을 선형 함수로 제한한 가장 단순한 예측모형에 해당한다.\n복잡도 관점에서 보면, 선형회귀는 높은 편향과 낮은 분산을 갖는 모형으로 이해할 수 있으며, 이는 이후에 등장하는 보다 유연한 모델들과의 비교 기준을 제공한다.\n2. 규제를 통한 ML적 확장\n선형회귀는 규제를 도입하는 순간, 통계적 추론 도구에서 머신러닝 모델로 성격이 전환된다. 규제를 포함한 목적함수는 다음과 같이 표현된다.\n\\[\\widehat{\\beta} = \\arg\\min_{\\beta}\\left\\{ \\overset{n}{\\sum_{i = 1}}(y_{i} - x_{i}^{\\top}\\beta)^{2} + \\lambda \\parallel \\beta \\parallel_{q} \\right\\}\\]\n\nq = 2: Ridge 회귀는 계수 크기를 제어하여 분산을 감소시킨다\nq = 1: Lasso 회귀는 변수 선택 효과를 통해 모형을 단순화한다\n\n이때 규제 강도 \\(\\lambda\\)는 해석의 대상이 아니라, 교차검증을 통해 선택되는 튜닝 파라미터이다. 이는 전통적 회귀분석과 머신러닝 사이의 중요한 관점 전환을 보여준다.\n3. 중요한 관점 전환\n머신러닝 관점에서 선형회귀를 사용할 때 핵심은 회귀계수의 개별적 의미가 아니다. 대신 다음과 같은 질문이 중심이 된다.\n\n이 모형은 전체 함수 공간에서 어느 수준의 복잡도를 갖는가\n다른 예측모형들과 비교했을 때 성능은 어떠한가\n교차검증을 통해 선택될 수 있는 합리적인 기준선인가\n\n이 관점에서 선형회귀는 설명을 위한 최종 모델이 아니라, 더 복잡한 예측모형으로 나아가기 위한 출발점이자 기준선으로 기능한다.\n\n\n\n\n\n\n\n전통 회귀\nML 관점\n\n\n\\(\\beta\\) 해석\n\\(\\widehat{y}\\) 정확도\n\n\n모수 추론\n함수 선택\n\n\n가정 중심\n검증 중심\n\n\n\n\n\n\n\nChapter 2. 손실함수와 확률모형\n머신러닝에서 학습이란 궁극적으로 손실함수를 최소화하는 문제로 귀결된다. 그러나 손실함수는 단순한 계산 도구가 아니라, 데이터 생성 과정에 대한 암묵적인 확률 가정을 내포한다.\n이 장에서는 손실함수가 수행하는 통계적 역할을 정리하고, 대표적인 손실함수들이 각각 어떤 확률모형을 전제하는지를 체계적으로 살펴본다. 이를 통해 손실함수 선택이 단순한 기술적 문제가 아니라, 데이터를 어떤 확률 구조로 이해할 것인가에 대한 선택임을 분명히 한다.\n\n손실함수는 단순한 계산 도구가 아니라 통계적 가정의 집합이다.\nERM(경험적 위험 최소화)은 기대 위험을 표본 기반으로 근사하는 통계적 원리이다.\nMSE, MAE, Cross-Entropy는 각각 서로 다른 확률모형을 전제한다.\n손실함수 선택은 곧 ”데이터를 어떤 확률 구조로 이해할 것인가”에 대한 선택이다.\n\n\n1. 손실함수의 역할과 의미\n\n손실함수란 무엇을 측정하는가\n손실함수는 관측값 y와 예측값 \\hat{y} 사이의 불일치 정도를 수치화한 함수이다. 지도학습에서의 학습 문제는 일반적으로 다음과 같이 정식화된다.\n\\(\\widehat{f} = \\arg\\min_{f \\in \\mathcal{F}}\\frac{1}{n}\\overset{n}{\\sum_{i = 1}}L(y_{i},f(x_{i}))\\), 여기서 \\(L( \\cdot , \\cdot )\\)는 손실함수이며, \\(\\mathcal{F}\\)는 고려하는 함수 공간이다.\n이 식에서 중요한 점은 두 가지이다. 첫째, 손실함수는 모델 선택의 기준이 된다. 둘째, 손실함수는 무엇이 좋은 예측인가를 정의한다. 즉, 손실함수는 단순한 평가 지표가 아니라 학습의 목표 자체를 규정한다.\n\n\n손실함수와 평가지표의 차이\n평가 지표(metric)는 학습이 끝난 후 모델의 성능을 비교하거나 보고하기 위한 수단이다. 반면 손실함수는 학습 과정에서 직접 최소화되는 대상이다. 두 개념은 밀접하게 연결되어 있지만, 목적과 역할은 다르다.\n통계적 관점에서 손실함수는 위험을 정의하는 핵심 요소이며, 학습 문제의 성격을 결정한다.\n\n\n\n2. 경험적 위험 최소화(Empirical Risk Minimization)\n\n기대 위험과 경험적 위험\n이론적으로 이상적인 학습 목표는 모집단 분포 하에서의 평균 손실을 최소화하는 것이다.\n\\[R(f) = \\mathbb{E}\\lbrack L(Y,f(X))\\rbrack\\]\n이를 기대 위험(expected risk)이라 한다. 그러나 실제로는 데이터 생성 분포를 알 수 없으므로, 다음과 같은 표본 기반 문제를 풀게 된다.\n\\[\\widehat{R}(f) = \\frac{1}{n}\\overset{n}{\\sum_{i = 1}}L(y_{i},f(x_{i}))\\]\n이를 경험적 위험(empirical risk)이라 한다.\n\n\nERM의 통계적 의미\n경험적 위험 최소화는 다음과 같은 가정을 전제로 한다. 관측 데이터는 모집단에서 독립적이고 동일한 분포로 추출되며, 표본 평균은 기대값의 일관된 추정량이라는 가정이다.\n즉, ERM은 대수의 법칙에 기반한 통계적 추론 절차이다. 머신러닝이 흔히 확률 가정을 하지 않는다고 설명되지만, ERM 자체는 이미 확률적 사고 위에 서 있다.\n\n\n\n3. 대표적 손실함수의 통계적 해석\n1. MSE (Mean Squared Error) \\(L(y,\\widehat{y}) = (y - \\widehat{y})^{2}\\)\nMSE를 최소화하는 것은 다음 확률모형을 암묵적으로 가정한다.\n\\[Y \\mid X = x \\sim \\mathcal{N}(f(x),\\sigma^{2})\\]\n즉, 오차는 정규분포, 분산은 상수, 조건부 평균이 예측 목표이다. 따라서 MSE 최소화는 **정규오차 가정 하의 최대우도추정(MLE)**과 동치이다.\n2. MAE (Mean Absolute Error) \\(L(y,\\widehat{y}) = |y - \\widehat{y}|\\)\nMAE는 다음 분포를 전제한다.\n\\[Y \\mid X = x \\sim \\text{Laplace}(f(x),b)\\]\n이 경우 예측 목표는 조건부 평균이 아니라 조건부 중앙값이다. 따라서 MAE는 이상치에 대해 더 강건한 성질을 갖는다.\n3. Cross-Entropy (Log Loss)\n이진 분류의 경우 손실함수는 다음과 같다.\n\\[L(y,p) = - \\lbrack y\\log p + (1 - y)\\log(1 - p)\\rbrack\\]\n이는 다음 확률모형과 정확히 대응된다.\n\\[Y \\mid X = x \\sim \\text{Bernoulli}(p(x))\\]\n즉, Cross-Entropy 최소화는 베르누이 분포에 대한 로그우도 최대화이다. 로지스틱 회귀, 신경망 분류 모두 이 구조를 공유한다.\n\n\n4. 손실함수 선택이 내포하는 확률 가정\n\n손실함수와 분포 선택\n손실함수를 선택한다는 것은 단순히 계산 규칙을 정하는 것이 아니라, 데이터의 오차 구조를 어떻게 이해할 것인가를 결정하는 행위이다. 이는 오차가 어떤 분포를 따른다고 볼 것인지, 예측의 기준을 조건부 평균으로 둘 것인지, 중앙값이나 확률로 둘 것인지를 함께 선택하는 것을 의미한다.\n또한 손실함수에 따라 이상치에 얼마나 민감하게 반응할 것인지도 자연스럽게 결정된다. 이러한 점에서 손실함수의 선택은 통계모형에서 확률분포를 선택하는 행위와 본질적으로 동일한 의미를 갖는다.\n\n\n”머신러닝은 확률 가정을 하지 않는다”는 오해\n머신러닝은 오차항의 분포를 명시적으로 기술하지 않으며, 모수의 신뢰구간을 제시하지 않는 경우가 많다. 그러나 손실함수를 통해 확률모형을 암묵적으로 채택한다.\n따라서 손실함수는 머신러닝에서 가장 중요한 통계적 가정이 위치하는 지점이며, 학습 알고리즘의 성격을 결정하는 핵심 요소이다.\n\n\n\n\nChapter 3. 로지스틱 회귀를 분류기로 다시 보기\n로지스틱 회귀는 오랫동안 분류 문제의 대표적인 방법으로 소개되어 왔다. 그러나 통계적 관점에서 로지스틱 회귀는 본질적으로 분류기라기보다는, 이진 반응변수에 대한 조건부 확률을 추정하는 확률모형이다. 즉, 로지스틱 회귀의 1차적 목적은 개별 관측치가 특정 범주에 속할 확률을 추정하는 데 있으며, 분류는 그 이후에 수행되는 의사결정의 결과이다.\n이 장에서는 로지스틱 회귀의 통계적 구조를 다시 정리하고, 확률 예측과 분류 결정을 명확히 구분하여 이해한다. 이를 통해 로지스틱 회귀 학습에서 사용되는 손실함수와, 실제 분류 성능을 평가할 때 사용하는 지표 사이에 왜 불일치가 발생하는지를 설명한다. 이러한 불일치는 모형의 결함이 아니라, 확률 추정과 의사결정이 서로 다른 단계에 속하기 때문에 발생하는 구조적 현상이다.\n로지스틱 회귀는 본질적으로 이진 반응변수에 대한 확률모형이며, 학습의 목적은 분류 정확도를 직접적으로 극대화하는 것이 아니라 조건부 확률을 일관되게 추정하는 데 있다. 분류는 추정된 확률값에 임계값을 적용함으로써 이루어지며, 이 임계값의 선택은 분석 목적이나 비용 구조에 따라 달라질 수 있다.\n이러한 관점에서 보면, 분류 성능지표와 손실함수 사이의 불일치는 자연스럽고 정상적인 결과이다. 손실함수는 확률 추정의 정확성을 평가하는 기준인 반면, 분류 성능지표는 확률값에 기반한 의사결정의 결과를 평가하기 때문이다. 따라서 로지스틱 회귀를 분류기로 이해하기 위해서는 확률모형과 분류 규칙을 명확히 분리하여 사고할 필요가 있다.\n이 구분을 통해 로지스틱 회귀는 단순한 분류 알고리즘이 아니라, 확률 예측을 중심으로 한 통계적 모델이라는 본래의 성격을 보다 분명하게 이해할 수 있다.\n\n1. 로지스틱 회귀의 통계적 구조\n\n기본 모형\n이진 반응변수 \\(Y \\in \\{ 0,1\\}\\)에 대해 로지스틱 회귀는 다음을 가정한다.\n\\(Y \\mid X = x \\sim \\text{Bernoulli}(p(x))\\), \\(\\log\\frac{p(x)}{1 - p(x)} = x^{\\top}\\beta\\)\n즉, 반응변수는 베르누이 분포를 따르며, 설명변수는 성공확률의 로그 오즈에 선형적으로 작용한다. 이때 \\(p(X) = \\mathbb{P}(Y = 1 \\mid X)\\)는 조건부 확률로 해석된다. 이 구조는 처음부터 끝까지 확률모형으로 구성되어 있으며, 로지스틱 회귀는 분류 규칙이 아니라 확률 생성 메커니즘을 명시적으로 모델링한다.\n\n\n학습 = 최대우도추정\n로지스틱 회귀의 학습은 다음과 같은 로그우도를 최대화하는 문제로 정의된다.\n\\[\\ell(\\beta) = \\overset{n}{\\sum_{i = 1}}\\lbrack y_{i}\\log p_{i} + (1 - y_{i})\\log(1 - p_{i})\\rbrack\\]\n이는 곧 Cross-Entropy 손실을 최소화하는 문제와 동치이다. 따라서 로지스틱 회귀는 분류 정확도를 직접적으로 최적화하는 모형이 아니라, 조건부 확률 \\(p(X)\\)를 일관되게 추정하도록 설계된 확률모형이다.\n\n\n\n2. 확률 예측과 분류 결정의 차이\n\n확률 예측: 통계적 산출물\n로지스틱 회귀의 직접적인 결과는 각 관측치에 대해 추정된 조건부 확률이다.\n\\[\\widehat{p}(X) = \\mathbb{P}(Y = 1 \\mid X)\\]\n이 확률값은 단순한 분류 결과 이상의 정보를 담고 있다. 확률 예측은 개별 관측치에 대한 불확실성의 정도를 표현하며, 결정 경계 근처에 위치한 관측치가 얼마나 애매한지를 보여준다. 또한 손실이나 비용이 비대칭적인 상황에서는 위험 기반 의사결정을 가능하게 한다. 이처럼 확률 예측은 연속적이며 정보가 풍부한 통계적 산출물이다.\n\n\n분류 결정: 추가 규칙의 도입\n분류는 확률 예측 위에 결정 규칙을 추가함으로써 이루어진다. 일반적으로는 다음과 같은 임계값 기반 규칙이 사용된다.\n\\(\\widehat{y} = \\{\\begin{matrix}\n1 & \\text{if}\\widehat{p}(x) \\geq c \\\\\n0 & \\text{otherwise}\n\\end{matrix}\\), 여기서 임계값 \\(c\\)는 통계적으로 자동 결정되지 않으며, 오류 비용, 위험 선호, 문제의 목적에 따라 외생적으로 정해진다. 즉, 로지스틱 회귀는 분류기를 직접 만들어내지 않으며, 분류기는 로지스틱 회귀가 제공한 확률 예측 위에서 별도의 규칙을 통해 구성된다.\n이 점을 혼동할 경우, 로지스틱 회귀가 곧 분류 규칙을 학습한다고 오해하기 쉽다.\n\n\n흔한 오해\n\n\n\n\n\n\n\n오해\n정확한 해석\n\n\n로지스틱 회귀는 분류모형이다\n확률모형이다\n\n\n0/1을 예측한다\n확률을 예측한다\n\n\n정확도가 목적이다\n로그우도가 목적이다\n\n\n\n\n\n\n3. 분류 성능지표와 손실함수의 불일치\n\n손실함수와 평가 지표의 차원 차이\n로지스틱 회귀의 학습 과정에서 최소화되는 대상은 Cross-Entropy, 즉 로그손실이다. 이는 확률 예측의 정합성을 평가하는 손실함수이다. 반면 실제 분류 성능 평가는 정확도, Precision과 Recall, F1-score, ROC–AUC와 같은 지표를 사용한다.\n이 두 종류의 기준은 서로 다른 대상을 측정한다. 손실함수는 연속적인 확률 예측의 품질을 평가하는 반면, 분류 성능지표는 임계값 이후의 이산적인 결정 결과만을 평가한다.\n\n\n불일치가 발생하는 이유\n이로 인해 다음과 같은 현상이 자연스럽게 발생할 수 있다. 로그손실은 지속적으로 개선되지만 분류 정확도는 변하지 않을 수 있으며, 반대로 정확도는 높지만 확률 예측이 심하게 왜곡된 모델도 존재할 수 있다.\n통계적 관점에서 보면 이는 모형의 결함이 아니라, 확률 추정과 의사결정이 서로 다른 단계에 속하기 때문에 발생하는 구조적 결과이다. 로지스틱 회귀는 확률모형으로서의 역할을 충실히 수행하고 있으며, 분류 성능은 그 위에 얹힌 결정 규칙과 평가 기준에 의해 추가적으로 결정된다.\n\n\n통계적 관점에서의 해석\n\n\n\n\n\n\n\n단계\n역할\n\n\n로지스틱 회귀\n조건부 확률 추정\n\n\n손실함수\n확률모형의 적합도 평가\n\n\n분류 규칙\n의사결정 기준 반영\n\n\n성능지표\n결정 결과 평가\n\n\n\n\n\n\n4. 임계값 선택, 비용 민감 분류, 그리고 ROC 곡선\n로지스틱 회귀에서 분류는 확률 예측 이후에 수행되는 의사결정 단계이다. 따라서 분류 결과는 확률 추정의 정확성뿐만 아니라, 예측 확률에 적용되는 임계값의 선택에 의해 결정된다. 이 임계값은 통계적으로 자동 결정되는 값이 아니며, 문제의 목적과 오류 비용 구조에 따라 외생적으로 설정된다.\n\n임계값 선택과 분류 성능의 변화\n이진 분류에서 흔히 사용되는 임계값 0.5는 두 범주의 오류 비용이 동일하다는 강한 가정을 전제로 한다. 그러나 실제 문제에서는 이 가정이 성립하지 않는 경우가 대부분이다. 임계값을 낮추면 양성으로 분류되는 관측치가 증가하여 재현율은 높아지지만, 위양성 오류도 함께 증가한다. 반대로 임계값을 높이면 정밀도는 향상되지만, 양성을 놓칠 가능성이 커진다.\n이처럼 임계값은 분류 성능지표 간의 균형을 조절하는 핵심적인 결정 변수이다.\n\n\n비용 민감 분류와 최적 임계값\n비용 민감 분류에서는 오류의 유형에 따라 서로 다른 비용을 부여한다. 양성을 놓치는 오류의 비용을 C_{FN}, 음성을 잘못 양성으로 분류하는 오류의 비용을 C_{FP}라고 하면, 기대 비용을 최소화하는 분류 규칙은 다음과 같이 표현될 수 있다.\n\\[\\widehat{y} = 1\\text{if}\\widehat{p}(X) \\geq \\frac{C_{FP}}{C_{FP} + C_{FN}}\\]\n이 식은 최적의 임계값이 데이터로부터 자동 추정되는 값이 아니라, 오류 비용의 상대적 크기에 의해 결정된다는 점을 보여준다. 즉, 임계값 선택은 통계적 추정의 문제가 아니라 의사결정의 문제이다.\n\n\nROC 곡선의 역할\nROC 곡선은 임계값을 변화시키면서 얻어지는 참양성률과 위양성률의 관계를 시각화한 도구이다. ROC 곡선은 특정 임계값에서의 분류 성능을 제시하지 않으며, 가능한 모든 임계값에 대해 분류 성능이 어떻게 변화하는지를 전체적으로 보여준다.\n이 점에서 ROC 곡선은 하나의 최적 분류기를 제안하는 도구가 아니라, 임계값 선택에 따른 성능 변화의 범위를 제시하는 탐색 도구로 이해하는 것이 타당하다.\n\n\nROC와 확률 예측의 관계\nROC 분석은 예측 확률의 절대적인 크기보다는, 양성과 음성을 얼마나 잘 구분하여 순위화할 수 있는지를 평가한다. 따라서 ROC–AUC는 확률 예측의 보정 상태나 정확성을 직접적으로 반영하지 않으며, 확률 예측의 분리 능력을 요약한 지표로 해석되어야 한다.\n동일한 ROC–AUC 값을 갖는 모델이라 하더라도, 예측 확률의 해석 가능성이나 신뢰성은 크게 다를 수 있다.\n\n\n통합적 해석\n로지스틱 회귀에서 분류 성능은 확률 예측, 임계값 선택, 오류 비용 구조가 결합된 결과이다. ROC 곡선은 이 관계를 한눈에 보여주는 도구이며, 비용 민감 분류는 ROC 곡선 위의 특정 지점을 선택하는 문제로 해석할 수 있다.\n이러한 관점에서 로지스틱 회귀는 단순한 분류 알고리즘이 아니라, 확률 예측을 중심으로 한 통계적 의사결정 프레임워크로 이해되어야 한다.\n\n\n\n\nChapter 4. 머신러닝 정규화\n머신러닝에서 정규화(regularization)는 흔히 과적합을 막기 위한 기술적 장치로 소개된다. 그러나 통계적 관점에서 정규화는 단순한 테크닉이 아니라, 모형 파라미터에 대한 사전적 믿음(prior belief)을 수식으로 구현한 것이다. 즉, 정규화는 데이터를 어떻게 해석할 것인가에 대한 통계적 선택을 명시적으로 반영한다.\n정규화는 단순한 과적합 방지 기법이 아니다. 이는 계수에 대한 사전적 믿음을 수식으로 표현한 것이며, L2 정규화는 정규분포 prior, L1 정규화는 Laplace 분포 prior에 대응한다. 이러한 정규화된 학습은 본질적으로 최대사후확률(MAP) 추정 문제로 이해할 수 있다. 이 관점에서 머신러닝은 확률을 배제한 통계가 아니라, 확률을 다른 언어로 표현한 통계라고 볼 수 있다.\n\n1. ”과적합을 막는다”는 말의 정확한 의미\n\n과적합의 통계적 의미\n과적합이란 학습 데이터에서는 손실이 매우 작지만, 새로운 데이터에서는 예측 오차가 급격히 증가하는 상태를 의미한다. 이는 추정량의 분산이 지나치게 커진 상황으로 해석할 수 있다. 즉, 데이터에 과도하게 적합된 모형은 작은 데이터 변동에도 민감하게 반응하며, 일반화 성능이 저하된다.\n정규화는 이러한 문제를 해결하기 위해 분산을 의도적으로 줄이는 개입이다. 일부 편향을 허용하는 대신, 전체적인 예측 안정성을 확보하는 것이 정규화의 핵심 목적이다.\n\n\n정규화의 본질: 모형을 덜 믿게 만드는 장치\n정규화의 도입은 데이터가 말하는 것을 100% 신뢰하지 않겠다는 선언에 가깝다. 이는 회귀계수의 크기가 과도하게 커지는 것을 허용하지 않고, 불필요하게 복잡한 해를 피하며, 적당히 단순한 모형을 선호하겠다는 의미이다. 이러한 선호가 수식으로 표현된 것이 정규화 항이다.\n\n\n\n2. L2 정규화(Ridge)의 통계적 해석\nL2 정규화를 포함한 학습 문제는 다음과 같이 표현된다.\n\\[\\widehat{\\beta} = \\arg\\min_{\\beta}\\left\\{ \\overset{n}{\\sum_{i = 1}}(y_{i} - x_{i}^{\\top}\\beta)^{2} + \\lambda \\parallel \\beta \\parallel_{2}^{2} \\right\\}\\]\n이 목적함수에서 두 항은 서로 다른 역할을 수행한다. 첫 번째 항은 데이터에 대한 적합도를 나타내며, 두 번째 항은 회귀계수의 크기에 대한 제약을 의미한다.\n\nL2 정규화(Ridge)의 확률모형 관점\n\\[(y_{i} - x_{i}^{\\top}\\beta)^{2} \\leftrightarrow \\varepsilon_{i} \\sim \\mathcal{N}(0,\\sigma^{2})\\]\n이제 정규화 항을 확률적으로 해석하면, \\(\\parallel \\beta \\parallel_{2}^{2} \\leftrightarrow \\beta_{j} \\sim \\mathcal{N}(0,\\tau^{2})\\)\n정규화 항을 확률적으로 해석하면, 이는 회귀계수에 대해 평균이 0이고 분산이 제한된 정규분포 prior를 부여한 것과 동일하다. 즉, Ridge 회귀는 오차항이 정규분포를 따른다는 가정과 함께, 회귀계수 역시 0을 중심으로 분포한다는 사전적 믿음을 전제한다.\n\n\n결론: Ridge = MAP 추정\nRidge 회귀는 다음 문제와 동치이다.\n\\[{\\widehat{\\beta}}_{\\text{MAP}} = \\arg\\max_{\\beta}\\{\\log p(y \\mid X,\\beta) + \\log p(\\beta)\\}\\]\n즉, 정규 prior를 둔 베이즈 회귀의 최대사후확률(MAP) 추정량이다. 이러한 설정 하에서 Ridge 회귀는 정규 prior를 둔 베이즈 회귀모형의 최대사후확률 추정량과 동치이다. 따라서 Ridge 회귀는 빈번도적 기법처럼 보이지만, 본질적으로는 MAP 추정 문제로 해석할 수 있다.\n\n\n\n3. L1 정규화(Lasso)의 통계적 해석\n\\[\\widehat{\\beta} = \\arg\\min_{\\beta}\\left\\{ \\overset{n}{\\sum_{i = 1}}(y_{i} - x_{i}^{\\top}\\beta)^{2} + \\lambda \\parallel \\beta \\parallel_{1} \\right\\}\\]\nL1 정규화의 가장 중요한 특징은 일부 회귀계수를 정확히 0으로 만들어 변수 선택 효과를 발생시킨다는 점이다. 이는 Ridge 회귀와 구별되는 Lasso의 핵심적인 성질이다.\n\n확률모형 관점에서의 Lasso\n\\[\\parallel \\beta \\parallel_{1} \\leftrightarrow \\beta_{j} \\sim \\text{Laplace}(0,b)\\]\nLaplace 분포는 0 근처에 확률 질량이 매우 크고 꼬리가 뾰족한 분포이다 이는 다음과 같은 사전 믿음을 반영한다. ”대부분의 변수는 중요하지 않고, 소수만 중요할 것이다.”\n\n\nLasso의 통계적 의미\n이러한 이유로 Lasso는 단순한 변수 선택 기법이 아니라, 희소성(sparsity)에 대한 강한 prior와 구조적 단순성에 대한 신념을 수식으로 구현한 결과로 이해할 수 있다.\n\n\n\n4. 정규화와 사전분포(prior)의 연결\n정규화된 학습 문제는 일반적으로 다음과 같은 형태로 표현할 수 있다.\n\\[\\arg\\min_{\\theta}\\{ - \\log p(y \\mid X,\\theta) - \\log p(\\theta)\\}\\]\n\n\n\n\n\n\n\nML 표현\n통계적 해석\n\n\n손실함수\n로그우도\n\n\n정규화 항\n로그 사전분포\n\n\n학습\nMAP 추정\n\n\n\n이 구조는 정규화가 단순히 prior처럼 보이는 것이 아니라, prior를 명시적으로 쓰지 않은 베이즈 추론의 또 다른 표현임을 보여준다. 머신러닝에서는 prior라는 용어를 사용하지 않을 뿐, 정규화를 통해 모형에 대한 믿음을 명확히 반영한다.\n이 관점에서 정규화는 머신러닝과 베이즈 통계 사이의 연결 고리이며, 두 접근법이 서로 다른 언어를 사용해 동일한 통계적 아이디어를 표현하고 있음을 드러낸다.\n\n\n5. Bias–Variance 관점에서의 정규화\n정규화의 효과는 단순히 과적합을 막는다는 표현만으로는 충분히 설명되지 않는다. 정규화는 추정량의 편향과 분산 사이의 균형을 의도적으로 조정하는 통계적 개입으로 이해하는 것이 보다 정확하다.\n\nBias–Variance 분해와 일반화 오차\n예측 오차는 일반적으로 편향, 분산, 그리고 제거할 수 없는 잡음으로 분해할 수 있다. 이 중 모델 선택과 학습 방법에 의해 영향을 받는 요소는 편향과 분산이다. 분산이 큰 모델은 학습 데이터의 작은 변동에도 예측 결과가 크게 달라지며, 이는 일반화 성능 저하로 이어진다. 반대로 편향이 큰 모델은 구조적으로 단순하여 복잡한 패턴을 충분히 포착하지 못한다.\n정규화는 이 중 분산을 줄이는 방향으로 작용한다. 회귀계수의 크기를 제한함으로써, 추정량이 데이터에 과도하게 적합되는 것을 방지하고 예측의 안정성을 높인다. 이 과정에서 일정 수준의 편향이 새롭게 도입되지만, 전체적인 일반화 오차는 오히려 감소할 수 있다.\n\n\n정규화 강도와 Bias–Variance 절충\n정규화의 강도는 편향과 분산의 균형을 조절하는 핵심 매개변수이다. 정규화가 약할수록 모델은 데이터에 더 자유롭게 적합되어 분산이 커지고, 정규화가 강할수록 모델은 제약을 받아 분산은 줄어들지만 편향은 증가한다.\n이러한 절충 관계는 단일한 최적값이 존재하지 않으며, 데이터의 크기, 잡음 수준, 모형의 복잡도에 따라 달라진다. 따라서 정규화 강도의 선택은 이론적으로 결정되기보다는, 교차검증과 같은 데이터 기반 방법을 통해 경험적으로 이루어진다.\n\n\nL2 정규화와 분산 감소\nL2 정규화는 회귀계수 전체를 부드럽게 축소함으로써 분산을 줄이는 효과를 가진다. 이는 계수 추정의 불확실성을 완화하고, 공선성이 존재하는 상황에서도 보다 안정적인 예측을 가능하게 한다. 이러한 특성으로 인해 L2 정규화는 예측 정확성을 중시하는 상황에서 널리 사용된다.\n\n\nL1 정규화와 구조적 단순화\nL1 정규화는 일부 계수를 정확히 0으로 만들며, 모형 구조 자체를 단순화한다. 이는 단순한 분산 감소를 넘어, 모델의 자유도를 직접적으로 줄이는 효과를 갖는다. 이로 인해 L1 정규화는 고차원 문제나 변수 선택이 중요한 상황에서 강력한 도구로 작용한다.\n\n\n정규화의 통계적 위치\nBias–Variance 관점에서 정규화는 최적의 균형점을 찾기 위한 도구이다. 이는 정규화가 만능 해결책이 아님을 의미하며, 정규화의 효과는 데이터와 문제 설정에 따라 달라진다. 그러나 분산을 제어하고 일반화 성능을 안정화한다는 점에서, 정규화는 머신러닝 학습 과정에서 핵심적인 통계적 역할을 수행한다.\n이러한 관점에서 정규화는 단순한 기술적 트릭이 아니라, 불확실한 데이터 환경에서 합리적인 예측을 가능하게 하는 통계적 절충의 구현으로 이해할 수 있다."
  },
  {
    "objectID": "notes/mldl/index.html",
    "href": "notes/mldl/index.html",
    "title": "머신러닝·딥러닝",
    "section": "",
    "text": "이 섹션에서는 머신러닝과 딥러닝의 핵심 개념과 방법론을 다룬다.\n통계학적 모형과의 연결, 예측과 분류, 평가와 일반화,\n그리고 딥러닝의 기본 구조를 체계적으로 정리한다.\n왼쪽 메뉴에서 주제를 선택하시오."
  },
  {
    "objectID": "notes/mldl/mldl_concepts01.html",
    "href": "notes/mldl/mldl_concepts01.html",
    "title": "MLDL AI 통계학 part01",
    "section": "",
    "text": "Chapter 1. 통계학과 AI\n인공지능(AI)은 더 이상 공학이나 컴퓨터과학 전공자들만의 전유물이 아니다. 최근의 데이터 분석 환경에서 AI, 머신러닝, 딥러닝은 사회과학, 자연과학, 정책 분석, 산업 분석 전반에 걸쳐 핵심적인 분석 도구로 자리 잡고 있다. 이러한 변화 속에서 통계학을 전공하는 학생들은 자연스럽게 다음과 같은 질문에 직면하게 된다.\n”AI는 통계학과 무엇이 다른가?“,  ”머신러닝은 통계학을 대체하는가?”,  그리고 ”통계학 전공자는 AI 시대에 어떤 역할을 수행해야 하는가?”\n이 질문들에 대한 답은 단순한 기법 비교나 성능 우열의 문제가 아니다. 오히려 각 방법론이 어떤 문제를 목표로 설정하고, 어떤 방식으로 불확실성을 다루며, 어떤 결과를 의미 있는 지식으로 간주하는가라는 철학적·방법론적 차이를 이해하는 데서 출발해야 한다.\n통계학과 AI는 서로 대립하거나 경쟁하는 관계가 아니다. 두 영역은 확률론, 선형대수, 최적화 이론이라는 공통의 수학적 기반 위에서 출발하였으나, 문제의 성격과 분석 목적에 따라 서로 다른 방향으로 발전해 온 방법론 체계이다. 따라서 AI 시대의 통계학은 사라지는 학문이 아니라, 오히려 그 역할과 정체성을 새롭게 재정의해야 할 시점에 놓여 있다고 볼 수 있다.\n\n1. 통계학의 출발점: 불확실성에 대한 학문\n통계학은 본질적으로 불확실성 하에서의 추론과 의사결정을 다루는 학문이다. 제한된 표본 자료로부터 모집단의 특성을 추론하고, 그 추론 결과에 수반되는 불확실성을 정량적으로 평가하는 것이 통계학의 핵심 목표이다. 이를 위해 통계학은 확률모형과 분포에 대한 가정, 체계적인 표본설계, 그리고 추정과 가설검정을 포함하는 엄격한 이론적 틀을 발전시켜 왔다.\n통계학적 분석에서 중요한 것은 단순히 하나의 추정값이나 검정 결과를 제시하는 데 그치지 않는다. 분석 결과가 어떤 가정 하에서 도출되었는지, 추정의 변동성은 어느 정도인지, 그리고 표본이 달라지더라도 유사한 결론이 도출될 것으로 기대할 수 있는지와 같은 해석 가능성과 신뢰성이 함께 고려되어야 한다.\n이와 같은 이유로 통계학은 오랫동안 과학적 실험 연구, 사회과학 분석, 정책 평가, 공공 통계 작성 등에서 핵심적인 분석 도구로 활용되어 왔다. 통계학은 단순한 계산 기법이 아니라, 불확실한 현실 세계를 합리적으로 이해하고 설명하기 위한 학문적 방법론이라 할 수 있다.\n\n\n2. AI와 머신러닝의 문제 설정 방식\n반면 AI와 머신러닝은 예측 성능의 극대화를 중심 목표로 발전해 온 방법론이다. 머신러닝에서 핵심적으로 던지는 질문은 다음과 같이 요약할 수 있다.\n”주어진 데이터로부터 미래의 관측값을 얼마나 정확하게 예측할 수 있는가?”\n이러한 문제 설정 하에서 머신러닝은 전통적인 통계학에서 강조해 온 확률분포 가정이나 모형 해석보다는, 손실함수(loss function)의 정의, 최적화(optimization) 과정, 그리고 일반화 성능(generalization)을 중심으로 모델을 설계한다. 즉, 모형이 이론적으로 왜 그러한 형태를 갖는가보다는, 새로운 데이터에 대해 얼마나 작은 오차를 보이는가가 우선적인 평가 기준이 된다.\n딥러닝은 이러한 머신러닝의 접근 방식을 한 단계 더 확장한 형태로, 데이터로부터 특징(feature) 자체를 자동으로 학습하는 표현학습(representation learning)을 가능하게 한다. 이는 분석자가 사전에 변수를 설계하지 않더라도, 대규모 데이터와 다층 신경망 구조를 통해 복잡한 비선형 패턴을 직접 학습할 수 있음을 의미한다. 그 결과 딥러닝은 이미지, 음성, 자연어와 같이 구조가 복잡한 데이터 영역에서 특히 강력한 성능을 보이게 되었다.\n\n\n3. 통계학과 AI는 어디서 만나는가\n중요한 점은 AI와 머신러닝이 통계학과 완전히 분리된 길에서 출발한 방법론이 아니라는 사실이다. 선형회귀, 로지스틱 회귀, 주성분분석(PCA)과 같은 통계학의 핵심 기법들은 이미 머신러닝의 기본 알고리즘으로 재해석되어 널리 활용되고 있다. 이는 통계학과 AI가 공통의 수학적·방법론적 토대 위에 놓여 있음을 보여준다.\n실제로 통계학의 주요 개념들은 머신러닝의 핵심 원리로 자연스럽게 연결된다. 최대우도추정은 경험적 위험 최소화(empirical risk minimization)로 해석될 수 있으며, 정규화는 모형의 복잡도를 제어하여 과적합을 방지하는 기법으로 활용된다. 또한 교차검증은 통계학의 표본 변동성에 대한 사고를 기반으로 한 모델 평가의 표준 절차로 자리 잡았다.\n이러한 관점에서 볼 때, 통계학은 AI와 머신러닝의 이론적 뿌리 중 하나라고 할 수 있다. 반대로 AI는 통계학적 사고를 대규모 데이터와 강력한 계산 능력이라는 환경 속에서 확장하고 구현한 결과물로 이해할 수 있다. 즉, 두 방법론은 대립하는 관계가 아니라, 문제의 규모와 목적에 따라 강조점이 달라진 연속선상에 놓여 있다고 볼 수 있다.\n\n\n4. 통계학 전공생에게 AI를 배우는 의미\n통계학 전공생에게 AI를 학습한다는 것은 단순히 새로운 알고리즘이나 프로그래밍 기술을 익히는 것을 의미하지 않는다. 이는 예측과 추론의 차이를 구분하고, 모델의 성능과 해석 가능성 사이의 균형을 고민하며, 데이터 편향과 과적합, 그리고 결과의 불확실성을 비판적으로 평가하는 분석 역량을 확장하는 과정이다.\nAI 시대에 통계학 전공자가 지니는 강점은 여전히 유효하다. 오히려 데이터의 규모가 커지고 모델 구조가 복잡해질수록, 통계적 사고와 검증 절차 없이 구축된 AI 모델은 그 결과를 신뢰하기 어려워진다. 이러한 환경에서 통계학 전공자는 AI 모델을 단순히 ”잘 작동하는 도구”로 사용하는 데 그치지 않고, 그 결과가 언제, 왜, 어느 정도까지 신뢰 가능한지를 판단할 수 있는 핵심적인 역할을 수행하게 된다.\n\n\n\nChapter 2. AI MLDL 개념적 위계\n\n\n\n\n\n\n1. 인공지능(AI)의 정의와 역사적 맥락\n인공지능(Artificial Intelligence, AI)은 인간의 지적 활동을 기계가 수행하도록 만들고자 하는 모든 시도와 방법론의 총칭이다. 여기서 핵심은 ’지능 그 자체를 구현한다’는 철학적 목표라기보다, 지능적으로 보이는 문제 해결 행동을 기계적으로 구현하는 것에 있다. 이러한 이유로 AI는 특정 알고리즘이나 단일 기술을 의미하지 않으며, 규칙 기반 추론, 탐색 알고리즘, 최적화 기법, 학습 알고리즘 등을 모두 포함하는 포괄적인 상위 개념으로 이해된다.\n초기의 AI 연구는 인간이 명시적으로 규칙을 설계하여 문제를 해결하는 접근에 주로 집중하였다. 대표적인 예로 전문가 시스템은 ”만약 A이면 B이다”와 같은 규칙 집합을 통해 인간 전문가의 판단 과정을 모사하고자 하였다. 그러나 이와 같은 방식은 규칙 설계에 드는 비용이 매우 크고, 예외 상황이 빈번한 현실 세계의 복잡성과 불확실성을 충분히 반영하기 어렵다는 한계를 지녔다.\n이러한 한계는 문제 해결 규칙을 사람이 직접 정의하는 대신, 데이터로부터 규칙과 패턴을 학습하는 접근의 필요성을 부각시켰다. 그 결과 등장한 것이 머신러닝이며, 이는 AI 연구의 중심축을 규칙 기반 접근에서 데이터 기반 접근으로 이동시키는 결정적인 전환점이 되었다.\n\n\n2. 머신러닝(ML)의 등장과 문제 설정 방식\n머신러닝(Machine Learning, ML)은 명시적인 규칙을 사람이 직접 프로그래밍하지 않고, 데이터로부터 규칙과 패턴을 학습하는 방법론이다. 머신러닝의 핵심 질문은 다음과 같이 정리할 수 있다.\n”주어진 입력 데이터 X로부터 출력 Y를 잘 예측하는 함수 f를 어떻게 학습할 것인가?”\n이러한 문제 설정 하에서 머신러닝은 확률분포의 정확한 형태를 사전에 가정하기보다는, 손실함수(loss function)를 정의하고 주어진 데이터를 이용하여 해당 손실을 최소화하는 함수를 찾는 최적화 문제로 접근한다. 즉, 학습 과정은 \\(\\min_{f}\\mathcal{L}(Y,f(X))\\)의 형태로 표현될 수 있다.\n이 관점에서 머신러닝은 통계학의 전통적인 추론 문제보다는 예측 문제에 초점을 둔다. 통계학에서 중요한 신뢰구간이나 가설검정보다는, 학습에 사용되지 않은 새로운 데이터에 대해 얼마나 작은 예측 오차를 보이는지가 우선적인 평가 기준이 된다.\n머신러닝은 문제 설정과 데이터의 특성에 따라 지도학습, 비지도학습, 준지도학습 등으로 구분된다. 이러한 분류는 이후 통계적 방법론과 머신러닝 기법을 비교·정리하는 과정에서 중요한 기준으로 활용된다.\n\n\n3. 딥러닝(DL)의 특징과 표현학습 개념\n딥러닝(Deep Learning, DL)은 머신러닝의 하위 분야로, 다층 신경망(neural network) 구조를 이용한 학습 방법을 의미한다. 딥러닝의 가장 큰 특징은 분석자가 사전에 설계하던 특징(feature)을 모델이 데이터로부터 직접 학습한다는 점에 있다.\n이러한 학습 방식을 표현학습(representation learning)이라 부르며, 분석 패러다임의 다음과 같은 전환을 의미한다.\n\n전통적 분석: 연구자가 변수 변환과 특징을 사전에 설계\n딥러닝 기반 분석: 원자료(raw data)로부터 유용한 표현을 자동으로 학습\n\n표현학습의 도입으로 딥러닝은 이미지, 음성, 자연어와 같이 구조가 복잡하고 비선형성이 강한 데이터에서도 높은 예측 성능을 달성할 수 있게 되었다. 이는 사람이 명시적으로 정의하기 어려운 고차원 패턴을 신경망이 계층적으로 학습할 수 있기 때문이다.\n반면, 모델 구조와 학습 과정이 복잡해지면서 해석 가능성의 감소, 블랙박스 문제, 그리고 결과에 대한 신뢰성과 책임성 문제가 중요한 논점으로 부각되었다. 이러한 한계는 이후 설명가능 AI와 통계적 검증의 필요성으로 다시 연결된다.\n\n\n4. AI–ML–DL의 포함관계와 역할 분담\nAI, ML, DL은 서로 독립적인 개념이 아니라 포함 관계를 이루는 계층적 구조를 가진다. AI는 지능적 행동을 기계적으로 구현하려는 모든 방법론을 포괄하는 가장 상위의 개념이며, ML은 그중에서도 데이터로부터 규칙과 패턴을 학습하는 방법을 의미한다. DL은 다시 ML의 하위 영역으로, 다층 신경망 구조를 이용하여 데이터의 표현을 학습하는 방법론에 해당한다.\n이러한 위계 구조에서 중요한 점은 모든 AI가 머신러닝을 사용하는 것은 아니며, 모든 머신러닝이 딥러닝으로 구현되는 것도 아니라는 사실이다. 문제의 특성과 데이터의 규모, 해석 가능성의 필요성에 따라서는 단순한 통계모형이나 규칙 기반 방법이 딥러닝보다 더 적절한 선택이 될 수 있다.\n따라서 딥러닝은 가장 강력한 방법이라는 의미에서 이해되기보다는, 대규모 데이터와 복잡한 구조를 가진 문제에서 특히 효과적인 방법으로 해석하는 것이 바람직하다. 분석 목적과 제약 조건을 고려하여 적절한 방법론을 선택하는 판단력은 여전히 분석자의 중요한 역할로 남아 있다.\n\n\n5. 데이터 기반 추론 패러다임의 변화\nAI·ML·DL의 발전은 데이터 분석의 전반적인 추론 패러다임에 중요한 변화를 가져왔다. 전통적인 통계 분석이 가정 설정, 추론, 해석이라는 흐름을 중시해 왔다면, 현대의 데이터 기반 분석은 데이터로부터의 학습과 그 결과로서의 예측을 중심에 두는 방식으로 전개되고 있다.\n이러한 변화는 분석 과정의 중심축이 이동하였음을 의미하며, 다음과 같은 전환으로 요약할 수 있다. 전통적으로는 소표본을 기반으로 한 추론이 중요했다면, 현재는 대규모 데이터를 활용한 분석이 일반화되었다. 또한 명시적인 모형 가정과 이론적 타당성보다는, 실제 데이터에서의 성능 평가가 주요 기준으로 활용되는 경우가 많아졌다. 분석의 목적 역시 현상에 대한 설명 중심에서 미래 값에 대한 예측 중심으로 이동하고 있다.\n그러나 이러한 변화는 통계학의 역할을 약화시키는 방향으로 이해되어서는 안 된다. 오히려 데이터의 규모가 커지고 분석 절차가 자동화될수록, 편향과 과적합, 그리고 불확실성에 대한 문제는 더욱 복잡한 형태로 나타난다. 이러한 위험 요소를 체계적으로 점검하고, 분석 결과의 신뢰 가능 범위를 판단하는 능력은 여전히 통계학 전공자의 핵심 역량으로 남아 있다. 데이터 기반 분석의 시대일수록 통계적 사고는 선택이 아니라 필수적인 기반이 된다.\n\n\n6. 생성형 AI와 통계학\n\n(1) 개념\n생성형 AI는 학습된 데이터의 분포 구조를 바탕으로 새로운 데이터를 생성하는 인공지능을 의미한다. 생성 대상은 텍스트, 이미지, 음성, 코드 등 다양한 형태가 될 수 있으며, 핵심은 기존 데이터를 분류하거나 예측하는 데 있지 않고 새로운 데이터를 만들어낸다는 점에 있다.\n이러한 차이는 문제 설정의 관점에서 분명히 드러난다. 예를 들어 기존의 분류 문제는 ”이 이미지가 고양이인가?“라는 질문을 던지는 반면, 생성형 AI는 ”고양이 이미지를 하나 생성하라”라는 질문을 다룬다. 즉, 입력에 대한 정답을 맞히는 문제가 아니라, 데이터 자체의 생성 메커니즘을 학습하는 문제가 된다.\n통계학의 관점에서 보면 생성형 AI는 매우 친숙한 개념이다. 본질적으로 생성형 AI가 풀고자 하는 문제는 다음과 같이 정리할 수 있다. 관측된 데이터가 따르는 확률분포를 학습하고, 그 분포로부터 새로운 표본을 생성하라는 것이다. 이는 전통적인 통계학에서 확률모형을 설정하고 모수를 추정한 뒤, 해당 모형을 이용해 시뮬레이션을 수행하는 과정과 동일한 철학을 공유한다.\n생성형 AI와 전통적 통계학의 차이는 분포에 대한 접근 방식에 있다. 전통적 통계학에서는 분포의 형태를 사전에 가정한 뒤 모수를 추정하는 방식이 일반적이었다면, 생성형 AI에서는 분포의 형태 자체를 데이터로부터 학습한다. 즉, 분포를 안다고 가정하는 접근에서 분포를 학습하는 접근으로의 전환이 이루어진 것이다.\n\n\n\n\n\n\n\n\n구분\n전통적 통계\n생성형 AI\n\n\n분포 가정\n명시적\n암묵적(신경망)\n\n\n차원\n저차원\n초고차원\n\n\n목적\n추론·해석\n생성·표현\n\n\n계산\n분석적\n수치적 최적화\n\n\n\n”분포를 안다고 가정” vs ”분포를 학습한다”의 차이이다.\n\n\n(2) 생성형 모델 vs 판별형 모델\n개념\n통계학의 관점에서 생성형 모델과 판별형 모델은 문제 설정 단계에서부터 명확히 구분된다. 두 접근은 모두 분류나 예측 문제를 다룰 수 있지만, 무엇을 모델링 대상으로 삼는가에서 근본적인 차이를 가진다.\n판별형 모델은 주어진 입력으로부터 타깃 변수를 정확히 예측하는 것을 목표로 한다. 즉, 입력 X가 주어졌을 때 반응변수 Y를 얼마나 잘 맞출 수 있는지가 핵심 관심사이다. 이때 모델은 데이터의 생성 구조보다는 결정경계와 예측 오차 최소화에 집중한다.\n이에 비해 생성형 모델은 데이터가 어떻게 생성되었는지를 설명하는 확률 구조를 모델링한다. 관측된 데이터의 공동분포 P(X, Y) 또는 비지도 설정에서는 P(X)를 학습하고, 그 분포로부터 새로운 데이터를 생성하거나 확률적 판단을 수행하는 데 목적이 있다. 통계학 관점에서 두 부류는 아주 명확히 갈립니다.\n\n판별형(discriminative): ”구분/예측”이 목표\n생성형(generative): ”데이터가 어떻게 생겨났는지(분포)“를 모델링\n\n판별형 모델 \\(P(Y \\mid X)\\)\n판별형 모델은 입력 변수 X가 주어졌을 때 반응변수 Y를 얼마나 정확하게 예측할 수 있는가에 초점을 둔 방법론이다. 이 접근에서는 데이터가 어떻게 생성되었는지를 모델링하기보다는, 주어진 입력에 대해 올바른 출력이 선택되도록 결정경계를 학습하는 것이 핵심 목표가 된다.\n이를 수식적으로 표현하면, 판별형 모델은 예측 함수 f를 선택하여 실제 값 Y와 예측값 f(X) 사이의 손실을 최소화하는 문제로 정식화할 수 있다. 즉, \\(\\widehat{f} = \\arg\\min_{f}\\mathbb{E}\\lbrack\\ell(Y,f(X))\\rbrack\\)와 같이 정의되며, 여기서 손실함수는 분류 오류나 예측 오차를 정량화하는 역할을 한다.\n이러한 구조에서 판별형 모델의 주요 관심사는 결정경계의 형태와 예측 오차를 얼마나 효과적으로 줄일 수 있는가에 있다. 대표적인 판별형 모델로는 로지스틱 회귀, 서포트 벡터 머신, 결정트리와 랜덤포레스트, 그리고 대부분의 신경망 기반 분류기가 포함된다. 이들 방법은 모두 입력과 출력 사이의 직접적인 관계를 학습함으로써 높은 예측 성능을 달성하는 데 목적을 둔다.\n생성형 모델 P(X,Y) 또는 P(X)\n생성형 모델은 입력과 출력의 관계를 직접 예측하는 데 초점을 두기보다는, 데이터가 생성되는 확률 구조 자체를 모델링하는 방법론이다. 통계학적으로 생성형 모델은 관측된 데이터가 따르는 공동분포 P(X, Y)를 모형화하거나, 비지도 학습의 경우 입력 데이터의 분포 P(X) 자체를 학습하는 것을 목표로 한다.\n이를 수식으로 표현하면, 생성형 모델은 \\(P(X,Y) = P(Y)P(X \\mid Y)\\)와 같은 형태의 공동분포를 가정하거나, 라벨이 없는 경우에는 \\(P(X)\\)를 직접 추정한다. 이러한 접근에서 핵심 질문은 ”이 데이터가 관측될 확률은 얼마나 되는가”, 혹은 ”이 분포로부터 새로운 데이터를 생성하면 어떤 형태가 나오는가”와 같이 데이터의 생성 메커니즘에 관한 것이다.\n생성형 모델은 분류 문제에도 활용될 수 있다. 이 경우 먼저 \\(P(X \\mid Y)\\)와 P(Y)를 학습한 뒤, 베이즈 규칙을 이용해 사후확률 \\(P(Y \\mid X)\\)를 계산함으로써 의사결정을 수행한다. 즉, 예측을 직접 학습하기보다는 분포를 학습한 결과로부터 예측을 간접적으로 도출한다는 점에서 판별형 모델과 구별된다.\n전통적인 생성형 모델로는 나이브 베이즈, 가우시안 생성 가정을 기반으로 한 LDA와 QDA, 가우시안 혼합모형(GMM), 은닉 마르코프 모형(HMM) 등이 있으며, 이들은 명시적인 확률모형과 잠재변수 구조를 통해 데이터 생성 과정을 설명한다. 최근의 생성형 AI에서는 변분 오토인코더(VAE), 생성적 적대 신경망(GAN), 확산모형(Diffusion), 자기회귀 모델(Transformer)과 같이 신경망을 이용해 고차원 데이터의 분포를 근사하는 방법들이 널리 활용되고 있다.\n판별형 모델과 생성형 모델 비교\n요약하면, 판별형 모델과 생성형 모델은 동일한 분류·예측 문제를 다루더라도 문제를 바라보는 관점이 근본적으로 다르다. 판별형 모델은 입력 X가 주어졌을 때 반응변수 Y를 얼마나 정확하게 맞출 수 있는가에 집중하며, 결정경계의 형태와 예측 오차 최소화를 중심으로 모델을 학습한다. 반면 생성형 모델은 데이터가 어떤 확률 구조를 통해 생성되었는지를 먼저 모델링하고, 그 결과로부터 예측이나 분류를 간접적으로 도출한다. 즉, 판별형 모델이 ”무엇을 맞출 것인가”에 초점을 둔다면, 생성형 모델은 ”데이터가 어떻게 만들어졌는가”를 설명하려는 접근이라 할 수 있다. 이러한 차이는 예측 성능뿐만 아니라, 해석 가능성, 결측치 처리, 소표본 상황에서의 거동, 그리고 생성과 시뮬레이션과 같은 분석 목적 전반에 걸쳐 서로 다른 장단점으로 이어진다.\n의사결정(분류)은 둘 다 할 수 있다\n판별형 모델과 생성형 모델은 접근 방식은 다르지만, 의사결정 문제, 특히 분류 문제는 모두 수행할 수 있다는 공통점을 가진다. 차이는 분류를 수행하기 위해 무엇을 직접 학습하는가에 있다.\n판별형 분류에서는 입력 x가 주어졌을 때 각 범주 y에 대한 조건부 확률 P(y \\mid x)를 직접 모델링한다. 분류 결과는 이 조건부 확률을 최대화하는 범주로 결정되며, 이는 다음과 같이 표현된다.\n\\[\\widehat{y} = \\arg\\max_{y}P(y \\mid x)\\]\n이 접근에서는 데이터의 생성 과정보다는, 주어진 입력에 대해 가장 그럴듯한 반응변수를 바로 추정하는 데 초점이 맞춰진다.\n반면 생성형 모델을 이용한 분류는 베이즈 규칙에 기반한다. 생성형 접근에서는 먼저 각 범주에서 데이터가 생성되는 방식인 \\(P(x \\mid y)\\)와 범주의 사전확률 P(y)를 학습한다. 이후 이들을 결합하여 \\(P(y \\mid x) = \\frac{P(x \\mid y)P(y)}{\\sum_{k}P(x \\mid k)P(k)}\\)와 같이 후방확률을 계산하고, 이를 바탕으로 분류를 수행한다. 즉, 생성형 분류는 분포를 먼저 학습한 뒤, 그 결과로부터 분류를 간접적으로 도출하는 방식이다.\n요약하면, 판별형 모델은 \\(P(y \\mid x)\\)를 직접 학습하여 분류를 수행하는 반면, 생성형 모델은 \\(P(y \\mid x)\\)와 P(y)를 학습한 뒤 이를 결합하여 \\(P(y \\mid x)\\)를 계산한다. 두 접근은 동일한 의사결정 문제에 도달하지만, 학습 대상과 해석의 관점에서 본질적인 차이를 가진다.\n통계학 전공생에게 가장 중요한 비교 포인트 6가지\n통계학 전공생의 관점에서 판별형 모델과 생성형 모델을 비교할 때 가장 먼저 주목해야 할 차이는 분석의 목적이다. 판별형 모델은 분류나 회귀와 같은 예측 문제에서 정확도를 극대화하는 데 1차적인 목표를 둔다. 반면 생성형 모델은 개별 예측 성능보다 데이터가 생성되는 분포 구조 자체를 설명하고 모사하는 데 초점을 맞추며, 데이터 생성과 시뮬레이션까지 분석 범위에 포함한다.\n두 번째 차이는 학습에 사용되는 신호의 성격이다. 판별형 모델은 일반적으로 반응변수 Y가 제공되는 지도학습 환경을 전제로 하며, 라벨 정보가 없는 상황에서는 적용이 제한된다. 이에 비해 생성형 모델은 입력 데이터의 분포 P(X)를 직접 학습할 수 있기 때문에 라벨이 없는 비지도학습이나 자기지도학습 환경에서도 자연스럽게 활용될 수 있다.\n세 번째로 중요한 비교 포인트는 결측치와 잠재변수 처리 방식이다. 생성형 모델은 데이터의 확률분포를 명시적으로 모델링하기 때문에 결측된 입력 값을 분포 기반으로 보완하거나, 혼합모형과 같은 잠재변수 구조를 통해 데이터의 숨겨진 생성 메커니즘을 표현하는 데 강점을 가진다. 반면 판별형 모델은 결측치가 존재하는 경우 대치, 마스킹, 전처리와 같은 별도의 전략이 필요해지는 경우가 많다.\n네 번째는 소표본 상황에서의 거동이다. 고전적인 통계 이론에서 흔히 요약되는 경향에 따르면, 생성형 모델은 분포 가정이 적절할 경우 표본 수가 적더라도 비교적 빠르게 안정적인 추정을 보일 수 있다. 이는 추정해야 할 파라미터 수가 많더라도 구조적 가정을 통해 정보를 효율적으로 활용하기 때문이다. 반면 판별형 모델은 표본이 충분히 확보되었을 때 결정경계를 직접 최적화하므로 더 우수한 예측 성능을 보이는 경우가 많다. 다만 실제 데이터 분석에서는 모형 미스펙, 규제 방법, 데이터 구조에 따라 이러한 관계가 역전될 수 있다.\n다섯 번째 비교 포인트는 분포 이동이나 이상치 탐지와 관련된다. 생성형 모델은 P(X)나 likelihood 값을 통해 특정 데이터가 학습된 분포에서 얼마나 벗어나 있는지를 정량화할 가능성을 제공한다. 반면 판별형 모델은 훈련 분포 밖의 입력에 대해서도 과도한 확신을 보일 수 있어, 확률 보정이나 calibration이 중요한 이슈가 된다. 다만 현대의 딥러닝 기반 생성모형에서도 likelihood가 항상 분포 이동을 잘 반영하지는 않는다는 점에서, 이는 가능성의 문제이지 보장된 성질은 아니다.\n마지막으로 해석 가능성의 측면에서 두 접근은 서로 다른 장단점을 가진다. 로지스틱 회귀와 같은 판별형 선형 모형은 계수 해석이 비교적 명확한 반면, 생성형 모델은 데이터가 생성되는 과정을 중심으로 한 설명 프레임을 제공한다. 혼합모형이나 HMM과 같은 전통적 생성형 모델은 구조적 해석이 가능한 반면, 딥러닝 기반 생성형 모델은 해석은 어렵지만 고차원 표현을 학습하는 데 강점을 가진다.\n대표 모델을 ”통계학 언어”로 정리\n대표적인 판별형 모델들은 통계학의 언어로 해석할 때, 조건부 평균이나 조건부 확률을 직접 모델링하는 구조로 이해할 수 있다. 선형회귀는 입력 변수 X가 주어졌을 때 반응변수 Y의 조건부 기댓값을 \\(\\mathbb{E}\\lbrack Y \\mid X\\rbrack = X\\beta\\)의 형태로 표현하며, 오차의 제곱합을 최소화하는 추정 문제로 정식화된다. 로지스틱 회귀는 이 아이디어를 이진 반응변수로 확장한 모형으로, 성공 확률 \\(P(Y = 1 \\mid X)\\)의 로그 오즈를 선형 결합 X\\beta로 모델링함으로써 분류 문제를 다룬다.\n서포트 벡터 머신은 확률 모형을 명시적으로 가정하지 않지만, 결정경계를 직접 최적화한다는 점에서 판별형 접근에 해당한다. 특히 마진을 최대화하는 방향으로 hinge loss를 최소화하는 문제로 해석할 수 있다. 결정트리와 앙상블 모형은 입력 공간을 분할하여 각 영역에서 예측 오차를 최소화하는 방식으로 작동하며, 분포 가정보다는 데이터 기반 분할 규칙에 의존한다는 점에서 대표적인 판별형 방법으로 분류된다.\n전통적인 생성형 모델들은 데이터가 생성되는 확률 구조를 명시적으로 가정하고 이를 추정하는 방식으로 구성된다. 나이브 베이즈 모형은 조건부 독립 가정을 통해 \\(P(X \\mid Y)\\)를 변수별 확률의 곱으로 단순화하며, 이를 통해 계산 효율성과 해석 가능성을 확보한다. LDA와 QDA는 각 클래스에서 입력 변수 X가 정규분포를 따른다고 가정하고, 평균과 공분산 구조의 차이에 따라 선형 또는 이차 형태의 결정경계를 유도한다.\n가우시안 혼합모형은 데이터 분포를 여러 개의 가우시안 성분의 가중합으로 표현하며, 잠재적인 군집 구조를 확률적으로 모델링한다. 은닉 마르코프 모형은 관측되지 않는 잠재 상태가 시간에 따라 전이하면서 관측열을 생성한다고 가정하는 모형으로, 시계열 데이터의 생성 구조를 설명하는 데 활용된다.\n현대의 생성형 AI 모델들은 이러한 확률적 생성 아이디어를 신경망 기반으로 확장한 결과로 이해할 수 있다. 자기회귀 모델, 특히 Transformer 기반 모델은 전체 데이터의 결합분포를 조건부 확률의 곱으로 분해하여 순차적으로 학습한다. 변분 오토인코더는 잠재변수 z를 도입하여 관측 데이터의 분포 P(X)를 근사하며, ELBO를 최적화하는 방식으로 학습이 이루어진다. 생성적 적대 신경망은 생성기와 판별기가 서로 경쟁하는 게임 구조를 통해 샘플의 품질을 향상시키는 접근을 취한다. 확산모형은 데이터에 점진적으로 노이즈를 추가한 뒤 이를 제거하는 과정을 학습함으로써 고품질 데이터를 생성하는 방식으로 해석할 수 있다.\n이처럼 판별형 모델과 생성형 모델은 서로 다른 형태를 띠고 있지만, 통계학의 언어로 해석하면 모두 확률, 손실함수, 최적화라는 공통된 틀 안에서 이해될 수 있다. 이는 현대 AI 모델이 통계학과 단절된 새로운 방법론이 아니라, 기존 통계적 사고를 계산 능력과 데이터 규모의 확장 속에서 구현한 결과임을 보여준다.\n직관적 예시\n첫 번째 예시는 스팸 분류 문제이다. 판별형 접근에서는 이메일의 단어 빈도, 링크 수, 발신 정보와 같은 특징 X가 주어졌을 때 해당 이메일이 스팸인지 여부 Y를 직접 예측하는 모델을 학습한다. 이 과정에서 모델은 스팸과 정상 메일을 가르는 결정경계를 형성하는 데 집중하며, 목표는 새로운 이메일에 대해 분류 오류를 최소화하는 것이다.\n반면 생성형 접근에서는 먼저 스팸 메일이 어떤 형태의 이메일을 생성하는지에 해당하는 분포 \\(P(X \\mid Y = \\text{spam})\\)과 정상 메일의 분포 \\(P(X \\mid Y = \\text{normal})\\), 그리고 전체 메일 중 스팸의 비율 P(Y)를 학습한다. 이후 베이즈 규칙을 이용해 주어진 이메일이 스팸일 확률을 계산하고 이를 바탕으로 분류를 수행한다. 즉, 생성형 모델은 이메일이 만들어지는 구조를 이해한 뒤, 그 결과로 분류를 수행한다.\n두 번째 예시는 이미지 데이터이다. 판별형 모델은 주어진 이미지가 고양이인지 여부를 판단하는 문제를 다룬다. 이 경우 모델의 역할은 입력된 이미지로부터 고양이와 비고양이를 구분하는 특징을 학습하고, 분류 정확도를 최대화하는 것이다.\n이에 비해 생성형 모델은 이미지가 생성되는 분포 자체에 관심을 둔다. 고양이 이미지를 생성하라는 과제는 고양이 이미지가 따르는 데이터 분포를 학습한 뒤 그 분포로부터 새로운 이미지를 샘플링하는 문제로 이해할 수 있다. 또한 주어진 이미지가 자연스러운 데이터 분포에 속하는지 판단하는 문제 역시 생성형 모델의 관점에서는 해당 이미지가 학습된 분포에서 얼마나 그럴듯한지를 평가하는 문제로 해석된다.\n\n\n(3) 생성형 AI 혁신성\n생성형 AI의 등장은 종종 완전히 새로운 통계적 원리가 등장한 결과로 오해되곤 한다. 그러나 보다 정확하게 말하면, 생성형 AI의 혁신성은 이론 그 자체의 새로움에 있기보다는, 기존의 통계적 사고가 현실적으로 작동할 수 있는 조건들이 동시에 충족되었다는 점에 있다. 즉, 생성형 AI는 통계학, 머신러닝, 그리고 컴퓨팅 환경이 하나의 임계점을 넘어선 결과물로 이해할 수 있다.\n첫 번째 조건은 초대규모 데이터의 축적이다. 전통적인 통계 분석에서 확률분포를 추정하거나 모형을 적합할 때 가장 큰 제약은 항상 표본의 크기였다. 표본이 제한적일수록 분포 가정은 강해질 수밖에 없었고, 이는 분석의 유연성과 표현력을 제약하는 요인이었다. 그러나 현대의 데이터 환경에서는 텍스트, 이미지와 영상, 그리고 다양한 행동 로그와 센서 데이터가 대규모로 축적되면서, 데이터 자체가 복잡한 확률분포의 형태를 경험적으로 근사할 수 있을 만큼 충분한 정보를 제공하게 되었다. 통계학적으로 보면 이는 모형을 단순화해야만 했던 환경적 제약이 크게 완화되었음을 의미한다.\n두 번째 조건은 딥러닝이 제공하는 고차원 함수 근사 능력이다. 데이터가 아무리 많더라도 이를 설명할 수 있는 함수 공간이 충분히 유연하지 않다면, 복잡한 분포를 학습하거나 새로운 데이터를 생성하는 것은 불가능하다. 다층 신경망은 매우 복잡한 비선형 함수를 근사할 수 있으며, 명시적인 분포 가정 없이도 데이터의 구조를 학습할 수 있다. 또한 특징을 사람이 사전에 설계하지 않아도 표현을 자동으로 학습할 수 있다는 점에서, 통계학에서 말하는 비모수적 추정과 고차원 밀도 추정을 계산적으로 구현 가능하게 만든 도구로 볼 수 있다. 이는 분포를 안다고 가정하는 접근에서 벗어나, 분포를 표현할 수 있는 함수 자체를 학습하는 방향으로의 확장을 의미한다.\n세 번째 조건은 계산 자원의 비약적인 발전, 특히 GPU의 등장이다. 생성형 모델은 수백만에서 수십억 개에 이르는 매개변수를 포함하며, 학습 과정에서 반복적인 미분 기반 최적화를 필요로 한다. 이러한 계산량은 기존의 CPU 중심 환경에서는 현실적으로 감당하기 어려웠다. GPU의 병렬 연산 구조는 대규모 행렬 연산을 효율적으로 처리하고, 반복적인 최적화 과정을 안정적으로 수행할 수 있게 만들었다. 통계학의 관점에서 보면, 이는 이론적으로는 가능했지만 계산 비용 때문에 실제 적용이 어려웠던 방법들이 현실적인 분석 도구로 전환되었음을 의미한다.\n생성형 AI의 본질적인 전환점은 이 세 가지 조건이 동시에 결합되었다는 데 있다. 초대규모 데이터는 분포 학습을 위한 충분한 재료를 제공하고, 딥러닝 구조는 고차원 함수 근사를 가능하게 하며, GPU 기반 계산 자원은 이러한 학습을 실제로 수행할 수 있게 만든다. 이 중 어느 하나라도 결여되었다면, 현재 우리가 경험하는 생성형 AI의 성능과 확장성은 실현되기 어려웠을 것이다.\n생성형 AI는 새로운 통계 이론의 산물이 아니라, 통계적 사고가 대규모 데이터와 계산 자원 위에서 비로소 현실화된 결과이다.\n\n\n\n\n\n\n\n\n\n단계\n핵심 구성\n주요 내용\n통계학적 해석\n\n\n1단계\n통계적 사고\n• 확률분포 학습 관점\n• 불확실성 개념\n데이터는 확률적 생성 과정의 결과이며, 분석의 목표는 분포와 변동성을 이해하는 데 있음\n\n\n↓\n\n\n2단계\n딥러닝 구조\n• 고차원 함수 근사\n• 표현학습\n명시적 분포 가정 없이 복잡한 확률 구조를 함수로 근사하는 비모수적 접근\n\n\n↓\n\n\n3단계\n계산 능력 (GPU)\n• 대규모 최적화\n• 반복 학습\n이론적으로 가능했던 고차원 추정을 계산적으로 실현 가능하게 만든 환경\n\n\n↓\n\n\n결과\n생성형 AI\n• 텍스트·이미지·음성 생성\n• 고차원 분포 샘플링\n학습된 확률분포로부터 새로운 관측값을 생성하는 고차원 시뮬레이션\n\n\n\n\n\n\n\nChapter 3. 전통적 통계방법론\n\n1. 통계학의 목적: 추론(inference)과 불확실성\n전통적 통계학의 핵심 목적은 제한된 표본 자료를 바탕으로 관측되지 않은 모집단에 대해 합리적이고 논리적으로 타당한 추론을 수행하는 데 있다. 여기서 말하는 추론이란 단순히 미래 값을 예측하는 행위가 아니라, 표본이 어떠한 확률적 생성 과정을 통해 얻어졌는지를 고려하여 모집단의 특성에 대해 근거 있는 결론을 도출하는 과정을 의미한다.\n통계학은 모든 데이터가 본질적으로 불확실성을 내포하고 있다는 전제에서 출발한다. 동일한 현상을 반복 측정하더라도 관측값은 항상 일정하지 않으며, 이러한 변동성은 제거해야 할 오류가 아니라 분석의 핵심 대상이다. 따라서 통계학적 분석은 하나의 추정값을 제시하는 데 그치지 않고, 해당 추정값이 얼마나 변동할 수 있는지, 추론 결과에 대한 신뢰 수준은 어느 정도인지를 함께 제시한다. 더 나아가 관측된 결과가 우연에 의해 나타났을 가능성까지 평가함으로써 결론의 타당성을 판단한다.\n이러한 관점에서 통계학적 분석의 기준은 단순히 ”얼마나 정확하게 맞추었는가”에 있지 않다. 오히려 주어진 자료로부터 도출된 결론의 불확실성을 얼마나 정직하고 체계적으로 표현했는가가 핵심적인 평가 기준이 된다. 전통적 통계방법론은 바로 이 불확실성을 정량화하고 해석하는 데 그 존재 이유가 있다.\n\n\n2. 확률모형 기반 접근과 가정의 역할\n전통적 통계방법론은 확률모형(probabilistic model)을 분석의 출발점으로 삼는다. 통계학에서 데이터는 임의로 발생한 숫자의 단순한 나열이 아니라, 특정한 확률적 생성 과정(random mechanism)의 결과로 해석된다. 즉, 관측된 데이터 뒤에는 이를 만들어낸 확률 구조가 존재하며, 통계적 추론은 이 구조를 명시적으로 설정하는 데서 시작된다.\n이 과정에서 통계학은 몇 가지 핵심적인 가정을 도입한다. 대표적으로는 모집단 분포의 형태에 대한 가정(정규분포, 이항분포 등), 관측값 간의 독립성, 그리고 동일한 분포로부터 표본이 추출되었다는 가정이 포함된다. 이러한 가정들은 현실을 완벽하게 묘사하기보다는, 복잡한 현상을 분석 가능한 형태로 단순화하기 위한 이론적 장치이다.\n중요한 점은 이러한 가정이 분석 결과의 해석 가능성과 일반화 가능성을 동시에 보장한다는 데 있다. 통계적 추론은 가정을 전제로 성립하며, 가정이 명확히 제시될수록 결과가 적용될 수 있는 범위 역시 분명해진다. 반대로 가정이 성립하지 않는 상황에서는 분석 결과 또한 제한적으로 해석되어야 함을 통계학은 분명히 인식한다.\n이러한 의미에서 통계학에서의 가정은 약점이 아니다. 오히려 분석의 전제 조건을 명시적으로 드러내고, 그 타당성 범위 안에서 결론을 해석하도록 요구하는 학문적 정직성의 표현이라 할 수 있다. 전통적 통계방법론은 가정을 숨기지 않으며, 불확실성과 함께 그 한계까지도 동시에 제시하는 분석 체계를 지향한다.\n\n\n3. 모수 추정, 검정, 신뢰구간의 구조\n전통적 통계학의 분석 절차는 일반적으로 세 가지 핵심 단계로 구성된다. 첫째, 모수 추정은 표본 자료를 이용하여 모집단의 특성을 요약하는 수치적 지표, 즉 평균, 분산, 회귀계수와 같은 모수를 추정하는 과정이다. 이는 표본을 통해 모집단에 대한 가장 직접적인 정보를 제공하는 단계라 할 수 있다.\n둘째, 가설검정은 모집단 모수에 대해 설정한 특정 가설이 관측된 데이터에 의해 얼마나 지지되는지를 확률적으로 판단하는 절차이다. 가설검정은 단순히 참이나 거짓을 선언하는 과정이 아니라, 관측된 결과가 우연에 의해 나타났을 가능성을 평가함으로써 추론의 타당성을 검토하는 과정이다.\n셋째, 신뢰구간은 단일한 점 추정값을 넘어서, 모집단 모수가 포함될 가능성이 있는 범위를 확률적 의미와 함께 제시하는 방법이다. 이를 통해 추정의 불확실성을 보다 직관적으로 표현할 수 있다.\n이 세 요소는 서로 독립적으로 존재하는 개념이 아니다. 모두 동일한 확률모형과 추정량의 분포를 기반으로 하며, 유기적으로 연결된 하나의 추론 구조를 이룬다. 예를 들어, 가설검정은 추정량의 표본분포를 이용하여 가설의 타당성을 평가하고, 신뢰구간은 동일한 분포 정보를 구간 형태로 확장하여 표현한 결과로 이해할 수 있다.\n중요한 점은 이러한 모든 과정이 표본의 무작위성을 전제로 성립한다는 사실이다. 표본이 확률적으로 추출되었다는 가정이 없다면, 추정량의 분포도, 검정의 유의확률도, 신뢰구간의 확률적 해석 역시 성립할 수 없다. 이 점에서 무작위성은 전통적 통계추론 전체를 지탱하는 가장 근본적인 전제 조건이라 할 수 있다.\n\n\n4. 설명 중심 분석과 해석 가능성\n전통적 통계방법론은 예측 성능 자체보다 분석 결과의 설명력을 중시해 왔다. 회귀계수의 부호와 크기, 변수 간의 관계, 통제 변수의 효과 등은 모두 단순한 계산 결과가 아니라, 분석 대상 현상을 이해하고 해석하기 위한 핵심 정보로 간주된다.\n이러한 접근은 다음과 같은 질문에 답하는 데 특히 강점을 가진다. 어떤 요인이 결과 변수에 영향을 미치는지, 그 영향의 방향과 크기는 어떠한지, 그리고 다른 조건이 동일할 때 해당 효과가 유지되는지를 체계적으로 검토할 수 있다. 이는 결과를 단순히 맞히는 것을 넘어, 왜 그러한 결과가 나타났는지를 설명하는 데 초점을 둔 질문들이다.\n통계학에서의 모형은 단순한 예측 도구가 아니라, 데이터에 내재된 구조를 해석하기 위한 이론적 틀로 기능한다. 이 때문에 전통적 통계모형은 가능한 한 단순한 형태를 유지하며, 각 구성 요소가 명확한 의미를 가지도록 설계된다. 변수의 선택, 모형의 형태, 가정의 설정 모두가 해석 가능성을 염두에 두고 이루어진다.\n이러한 점에서 전통적 통계방법론은 설명 가능성을 핵심 가치로 삼는 분석 패러다임이라 할 수 있다. 이는 이후 등장하는 예측 중심의 머신러닝 접근과 구별되는 중요한 특징이기도 하다.\n\n\n5. 표본조사·실험설계와 통계적 타당성\n전통적 통계학은 분석 기법 그 자체뿐만 아니라, 데이터가 어떤 과정을 거쳐 수집되었는가를 매우 중요하게 다룬다. 표본이 모집단을 적절히 대표하지 못하거나 체계적인 편향을 내포하고 있다면, 아무리 정교한 분석 기법을 적용하더라도 그 결과를 신뢰하기 어렵다. 이 때문에 통계적 추론의 출발점은 항상 데이터 수집 과정에 대한 검토에서 시작된다.\n이를 위해 전통적 통계학은 다양한 표본조사와 실험설계 방법론을 발전시켜 왔다. 대표적으로 확률표본추출은 모집단의 각 단위가 알려진 확률로 선택되도록 함으로써, 표본의 대표성과 추론의 정당성을 확보한다. 층화표본, 군집표본, 그리고 가중치 설계는 현실적인 제약 속에서도 효율성과 정확성을 동시에 고려하기 위한 설계 기법으로 활용된다. 한편 실험 연구에서는 무작위 배정과 통제를 통해 외부 요인의 영향을 최소화하고, 관심 있는 처리 효과를 명확히 식별하고자 한다.\n이러한 표본조사와 실험설계는 분석 결과의 통계적 타당성을 보장하는 핵심적인 기반이다. 다시 말해, 전통적 통계학의 관점에서 중요한 것은 데이터 자체가 아니라, 그 데이터가 올바른 절차에 따라 수집되었는지 여부이다. 이 점에서 통계학은 ”데이터가 말한다”기보다, ”올바르게 수집된 데이터만이 말할 수 있다”는 입장을 일관되게 유지해 왔다.\n\n\n6. 전통적 통계방법론의 한계와 비판\n전통적 통계방법론은 추론의 엄밀성과 해석 가능성을 강점으로 발전해 왔지만, 모든 분석 상황에 보편적으로 적용될 수 있는 것은 아니다. 특히 데이터의 규모와 구조가 급격히 변화한 현대의 분석 환경에서는 전통적 방법론이 가지는 한계 또한 분명히 드러난다.\n첫째, 전통적 통계방법론은 비교적 단순한 확률모형과 명시적인 가정을 전제로 한다. 이러한 가정은 해석 가능성과 이론적 정합성을 제공하는 장점이 있지만, 실제 데이터가 가정을 심각하게 위반하는 경우 분석 결과의 신뢰성이 크게 저하될 수 있다. 고차원 자료, 강한 비선형성, 복잡한 상호작용 구조를 가지는 데이터에서는 모형 설정 자체가 어려워지는 경우도 많다.\n둘째, 분석 대상 변수의 수가 많아질수록 전통적 통계모형은 계산적·이론적 부담이 급격히 증가한다. 변수 선택, 다중공선성 문제, 모형 안정성 등은 소규모 자료를 염두에 두고 발전한 통계적 절차로는 효과적으로 대응하기 어려운 과제로 남는다. 이는 대규모 데이터 환경에서 전통적 방법이 실무적으로 제한을 받는 주요 원인 중 하나이다.\n셋째, 전통적 통계학의 추론 구조는 명확한 연구 질문과 가설 설정을 전제로 한다. 이는 분석의 방향성을 분명히 한다는 점에서 장점이지만, 탐색적 분석이나 패턴 발견이 중요한 상황에서는 오히려 제약으로 작용할 수 있다. 데이터 자체로부터 구조를 학습하려는 접근과 비교할 때, 유연성이 낮다는 비판이 제기되기도 한다.\n마지막으로, 예측 성능의 관점에서 전통적 통계모형은 반드시 최적의 선택이 되지 않는 경우가 많다. 설명 가능성과 해석의 명확성을 우선시하는 대신, 예측 오차를 최소화하는 데에는 상대적으로 보수적인 구조를 취하기 때문이다. 이는 예측 정확도가 핵심 목표가 되는 문제에서 한계로 인식될 수 있다.\n이러한 한계에도 불구하고, 전통적 통계방법론은 추론의 논리적 근거를 명시적으로 제시하고, 분석 결과의 해석 범위를 분명히 한다는 점에서 여전히 중요한 의미를 가진다. 다만 현대의 데이터 분석 환경에서는 이러한 방법론이 단독으로 사용되기보다는, 머신러닝과 인공지능 기반 방법과 상호 보완적으로 결합되는 방향으로 재해석되고 있다. 이 점은 이후 장에서 다루게 될 통계방법론과 AI·ML 접근의 대비와 연결을 이해하는 핵심적인 출발점이 된다.\n\n\n\nChapter 4. 머신러닝의 문제 설정과 알고리즘 구조\n\n1. 예측(prediction) 중심 문제 정의\n머신러닝은 전통적 통계방법론과 달리 예측 성능의 극대화를 중심에 두고 문제를 정의한다. 분석의 출발점은 ”모집단의 모수는 무엇인가?“가 아니라, ”새로운 데이터에 대해 얼마나 정확하게 예측할 수 있는가?”이다. 즉, 관심의 대상은 모집단의 확률 구조가 아니라, 관측되지 않은 미래 데이터에 대한 예측 정확도에 놓여 있다.\n이를 수식적으로 표현하면, 머신러닝은 주어진 학습 데이터 \\(\\mathcal{D} = \\{(x_{1},y_{1}),(x_{2},y_{2}),\\ldots,(x_{n},y_{n})\\}\\)에 대해 입력 변수 x와 출력 변수 y 사이의 관계를 나타내는 함수 \\(f\\)를 찾는 문제로 정식화된다. 이때 목표는 다음의 경험적 위험을 최소화하는 것이다.\n\\(\\widehat{f} = \\arg\\min_{f \\in \\mathcal{F}}\\frac{1}{n}\\overset{n}{\\sum_{i = 1}}L(y_{i},f(x_{i}))\\), 여기서 \\(L( \\cdot )\\)은 손실함수이며, 예측값 \\(f(x_{i})\\)가 실제값 \\(y_{i}\\)와 얼마나 차이가 나는지를 수치적으로 측정한다. 손실함수는 예측 오류에 따른 비용을 정량화한 기준으로, 문제의 성격에 따라 제곱오차, 절대오차, 교차엔트로피 등 다양한 형태가 사용된다.\n중요한 점은 이 과정에서 데이터 생성 과정에 대한 확률분포를 명시적으로 가정할 필요가 없다는 것이다. 전통적 통계학이 \\(y \\mid x\\)의 분포를 가정하고 그 모수를 추론하는 데 초점을 둔다면, 머신러닝은 분포의 형태보다는 손실함수 값이 얼마나 작은지를 기준으로 모델의 적절성을 판단한다. 즉, 가정의 타당성보다는 예측 성능 자체가 분석의 핵심 평가 기준이 된다.\n이러한 예측 중심 문제 정의는 이후 머신러닝 알고리즘의 구조, 학습 방식, 그리고 모델 평가 방법 전반에 걸쳐 중요한 영향을 미친다. 특히 훈련 데이터에서의 성능뿐 아니라, 새로운 데이터에 대한 일반화 성능이 분석의 중심 개념으로 등장하게 된다.\n\n\n2. 지도학습·비지도학습·준지도학습\n머신러닝의 문제 설정은 데이터에 정답(label)이 존재하는지 여부에 따라 구분된다. 이는 알고리즘의 기술적 차이만을 의미하는 것이 아니라, 분석의 목적과 데이터가 수집된 환경을 반영하는 개념적 구분이다.\n지도학습은 입력 변수 X와 이에 대응하는 출력 변수 Y가 함께 주어진 경우를 말한다. 이때 학습의 목표는 입력으로부터 출력을 예측하는 함수 f(X)를 학습하는 것이다. 대표적인 지도학습 문제로는 범주형 출력을 다루는 분류 문제와 연속형 출력을 다루는 회귀 문제가 있다. 스팸 메일 분류, 신용 위험 평가, 수요 예측 등은 모두 지도학습의 전형적인 사례에 해당한다.\n비지도학습은 출력 변수 Y 없이 입력 변수 X만이 주어진 경우로, 데이터에 내재된 구조나 패턴을 탐색하는 데 목적이 있다. 이 경우 학습의 목표는 명시적인 예측이 아니라, 데이터의 유사성, 잠재 구조, 저차원 표현 등을 발견하는 것이다. 군집분석이나 차원축소 기법은 비지도학습의 대표적인 예로, 데이터 탐색이나 사전 분석 단계에서 중요한 역할을 한다.\n준지도학습은 소량의 라벨이 부여된 데이터와 대량의 비라벨 데이터를 함께 활용하는 접근이다. 현실의 많은 문제에서는 라벨을 수집하는 데 높은 비용이나 시간이 요구되기 때문에, 모든 데이터에 대해 정답을 확보하기 어려운 경우가 많다. 준지도학습은 이러한 제약을 고려하여, 제한된 라벨 정보를 활용해 비라벨 데이터의 구조를 함께 학습함으로써 성능을 향상시키는 것을 목표로 한다.\n이와 같은 학습 유형의 구분은 단순히 알고리즘을 분류하기 위한 기준이 아니다. 이는 문제 해결의 목표가 예측인지, 구조 탐색인지, 혹은 제한된 정보 하에서의 효율적 학습인지를 반영하며, 데이터 수집 방식과 분석 전략 전반에 영향을 미치는 중요한 개념적 틀이라 할 수 있다.\n\n\n3. 손실함수, 최적화, 일반화 개념\n머신러닝 알고리즘의 내부 구조는 손실함수, 최적화, 일반화라는 세 가지 요소로 요약할 수 있다. 이들은 각각 독립적인 개념이 아니라, 예측 중심 문제 설정 아래에서 하나의 학습 과정을 구성하는 핵심 구성 요소들이다.\n먼저 손실함수는 예측의 오차를 수치적으로 표현하는 기준이다. 주어진 입력 x_i에 대해 모델이 산출한 예측값 f(x_i)와 실제값 y_i 사이의 차이를 측정함으로써, 모델의 성능을 하나의 수치로 요약한다. 회귀 문제에서는 평균제곱오차 \\(L(y,f(x)) = (y - f(x))^{2}\\)가 대표적으로 사용되며, 분류 문제에서는 예측 확률과 실제 라벨 간의 차이를 반영하는 교차엔트로피 손실이 널리 활용된다. 손실함수는 모델이 무엇을 잘 맞추어야 하는지를 명확히 규정하는 역할을 한다.\n최적화는 정의된 손실함수를 최소화하는 파라미터를 찾는 과정이다. 머신러닝에서는 일반적으로 손실함수가 파라미터에 대한 함수로 표현되며, 이 함수를 최소화하는 것이 학습의 목표가 된다. 이를 위해 경사하강법과 그 변형 알고리즘들이 주로 사용된다. 경사하강법은 손실함수의 기울기를 이용하여 파라미터를 반복적으로 갱신함으로써, 점진적으로 손실을 줄여 나가는 방식이다. 이 과정은 명시적 해를 구하는 전통적 추정 방법과 달리, 수치적 반복 계산에 기반한다는 점에서 특징적이다.\n일반화는 학습 데이터가 아닌 새로운 데이터에서도 모델의 성능이 유지되는 능력을 의미한다. 머신러닝에서 중요한 것은 학습 데이터에 대한 손실을 최소화하는 것 자체가 아니라, 관측되지 않은 데이터에 대해서도 낮은 예측 오차를 유지하는 것이다. 이를 일반화 성능이라 하며, 훈련 데이터와 테스트 데이터를 구분하는 이유도 여기에 있다. 학습 데이터에서의 손실 감소가 항상 일반화 성능의 향상으로 이어지지는 않으며, 과적합은 이 둘이 괴리되는 대표적인 사례이다.\n이러한 관점에서 머신러닝에서의 모델 평가는 통계학에서의 불확실성 표현과 다른 방식으로 이루어진다. 전통적 통계학이 추정량의 분산이나 신뢰구간을 통해 추론의 불확실성을 정량화했다면, 머신러닝에서는 새로운 데이터에서의 예측 성능, 즉 일반화 성능이 모델의 핵심 평가 기준으로 작동한다. 이는 두 방법론이 불확실성을 다루는 방식의 차이를 잘 보여주는 지점이라 할 수 있다.\n\n\n4. 과적합과 편향–분산 트레이드오프\n머신러닝에서 모델의 성능을 논할 때 가장 핵심적인 문제 중 하나는 과적합이다. 과적합은 모델이 학습 데이터의 패턴뿐만 아니라 우연적 잡음까지 지나치게 잘 학습하여, 새로운 데이터에 대해서는 오히려 성능이 저하되는 현상을 의미한다. 이 경우 학습 데이터에서의 손실은 매우 작지만, 테스트 데이터에서는 예측 오차가 크게 증가한다.\n과적합은 일반적으로 모델의 복잡도가 높을수록 발생하기 쉽다. 파라미터의 수가 많거나, 비선형 구조가 복잡한 모델은 학습 데이터에 유연하게 적합할 수 있지만, 그만큼 데이터에 특화된 패턴에 지나치게 민감해질 가능성도 커진다. 반대로 지나치게 단순한 모델은 데이터의 구조를 충분히 포착하지 못해 전반적인 예측 성능이 낮아질 수 있다. 이와 같은 상황을 과소적합이라 한다.\n이러한 현상을 체계적으로 설명하는 개념이 편향–분산 트레이드오프이다. 편향은 모델이 실제 데이터 생성 구조를 충분히 표현하지 못함으로써 발생하는 오차를 의미하며, 분산은 학습 데이터의 변동에 따라 모델 추정 결과가 크게 달라지는 정도를 의미한다. 일반적으로 단순한 모델은 편향이 크고 분산이 작으며, 복잡한 모델은 편향이 작고 분산이 큰 경향을 가진다.\n머신러닝의 목표는 편향과 분산 사이에서 적절한 균형을 찾는 것이다. 모델의 복잡도를 증가시키면 학습 데이터에 대한 적합도는 높아지지만 분산이 커져 과적합 위험이 증가하고, 복잡도를 지나치게 제한하면 분산은 줄어들지만 편향이 커져 과소적합 문제가 발생한다. 따라서 좋은 모델이란 학습 데이터와 새로운 데이터 모두에서 안정적인 성능을 보이는, 즉 일반화 오차가 최소화되는 지점에 위치한 모델이라 할 수 있다.\n이러한 편향–분산 트레이드오프 개념은 전통적 통계학에서도 익숙한 해석을 제공한다. 통계학에서 단순한 모형과 복잡한 모형 사이의 선택, 추정량의 분산과 모형 적합도의 균형 문제는 머신러닝에서 일반화 성능이라는 개념으로 재해석된다. 이 점에서 편향–분산 트레이드오프는 통계학과 머신러닝을 연결하는 중요한 이론적 공통분모라 할 수 있다.\n\n\n5. 정규화 기법과 과적합 완화\n정규화는 모델의 복잡도를 제어함으로써 과적합을 완화하기 위한 대표적인 방법이다. 과적합은 모델이 학습 데이터에 지나치게 적응하여 분산이 커지는 현상으로 이해할 수 있는데, 정규화는 이러한 분산을 줄이는 방향으로 학습을 유도한다.\n정규화의 기본 아이디어는 손실함수에 모델의 복잡도에 대한 패널티를 추가하는 것이다. 예를 들어, 파라미터 벡터를 \\theta라 할 때, 정규화가 포함된 학습 문제는 다음과 같이 표현된다.\n\\(\\widehat{\\theta} = \\arg\\min_{\\theta}\\left\\{ \\frac{1}{n}\\overset{n}{\\sum_{i = 1}}L(y_{i},f(x_{i};\\theta)) + \\lambda\\Omega(\\theta) \\right\\}\\), 여기서 \\(\\Omega(\\theta)\\)는 파라미터의 크기나 구조를 제한하는 정규화 항이며, \\(\\lambda\\)는 예측 오차와 모델 복잡도 사이의 균형을 조절하는 하이퍼파라미터이다.\n회귀 문제에서 널리 사용되는 정규화 기법으로는 L2 정규화와 L1 정규화가 있다. L2 정규화는 파라미터의 제곱합을 패널티로 부과하여, 모든 계수를 전반적으로 작게 만드는 효과를 가진다.\n\\(\\Omega(\\theta) = \\sum_{j}\\theta_{j}^{2}\\). 이 방식은 모델을 보다 안정적으로 만들어 분산을 줄이는 데 효과적이다. 반면 L1 정규화는 파라미터의 절댓값 합에 패널티를 부과한다.\n\\(\\Omega(\\theta) = \\sum_{j}|\\theta_{j}|\\). 이는 일부 계수를 정확히 0으로 만들 수 있어, 변수 선택 효과와 함께 모델 단순화를 유도한다.\n정규화의 효과는 편향–분산 관점에서 이해할 수 있다. 정규화는 모델의 자유도를 제한함으로써 분산을 감소시키는 대신, 일정 수준의 편향을 허용한다. 적절한 정규화 강도는 이 두 요소 사이의 균형을 맞추어, 학습 데이터뿐 아니라 새로운 데이터에서도 안정적인 성능을 보이도록 한다. 반대로 정규화가 지나치게 강하면 모델이 데이터의 구조를 충분히 반영하지 못해 과소적합이 발생할 수 있다.\n이러한 의미에서 정규화는 단순한 기술적 보정 장치가 아니라, 일반화 성능을 중심에 둔 머신러닝 학습 철학을 구현하는 핵심 메커니즘이라 할 수 있다. 전통적 통계학에서의 모형 단순화나 벌점화 추정과 마찬가지로, 정규화는 예측 성능과 해석 가능성, 그리고 안정성 사이의 균형을 체계적으로 관리하는 역할을 수행한다.\n\n\n6. 통계적 벌점화 추정과 머신러닝 정규화의 대응 관계\n머신러닝에서의 정규화는 전통적 통계학에서 오래전부터 사용되어 온 벌점화 추정과 본질적으로 동일한 개념적 기반을 가진다. 두 접근 모두 모형의 복잡도를 직접적으로 제어함으로써 과적합을 방지하고, 추정 결과의 안정성과 일반화 성능을 향상시키는 것을 목표로 한다.\n전통적 통계학에서 벌점화 추정은 우도함수나 잔차 제곱합에 모형 복잡도에 대한 패널티를 추가하는 방식으로 정의된다. 예를 들어 선형회귀모형에서 최소제곱추정은 잔차 제곱합을 최소화하지만, 벌점화 회귀에서는 여기에 계수 크기에 대한 제약을 추가한다. 이는 다음과 같은 형태로 표현할 수 있다.\n\\(\\widehat{\\beta} = \\arg\\min_{\\beta}\\left\\{ \\overset{n}{\\sum_{i = 1}}(y_{i} - x_{i}^{\\top}\\beta)^{2} + \\lambda\\Omega(\\beta) \\right\\}\\). 이 구조는 머신러닝에서 손실함수에 정규화 항을 추가하는 방식과 수식적으로 동일하다.\n대표적인 예로 릿지 회귀는 계수의 제곱합을 벌점으로 부과하는 방법으로, 이는 머신러닝에서의 L2 정규화와 정확히 대응된다. 릿지 회귀는 모든 계수를 전반적으로 축소함으로써 추정량의 분산을 줄이고, 다중공선성이 존재하는 상황에서 안정적인 추정을 가능하게 한다. 반면 라쏘 회귀는 계수의 절댓값 합을 벌점으로 사용하는데, 이는 머신러닝에서의 L1 정규화와 대응된다. 이 경우 일부 계수가 정확히 0이 되면서 변수 선택 효과가 자연스럽게 발생한다.\n이러한 대응 관계는 벌점화 추정이 단순히 통계적 기법에 국한된 개념이 아님을 보여준다. 머신러닝에서 정규화는 주로 일반화 성능 향상이라는 관점에서 설명되지만, 그 이론적 근간은 통계학에서의 편향–분산 균형, 모형 안정성, 과적합 제어 논의와 직접적으로 연결되어 있다. 즉, 머신러닝의 정규화는 통계적 벌점화 추정이 예측 중심 환경에서 재해석된 형태라고 볼 수 있다.\n차이는 해석의 초점에 있다. 전통적 통계학에서는 벌점화 추정을 통해 계수의 해석 가능성, 모형 단순성, 추정의 안정성을 강조하는 반면, 머신러닝에서는 테스트 데이터에서의 예측 성능과 일반화 오차 감소가 주요 관심사가 된다. 그러나 두 접근 모두 모형의 자유도를 제한함으로써 더 나은 추론 또는 예측을 달성하려는 공통된 철학 위에 서 있다.\n이러한 관점에서 볼 때, 통계학과 머신러닝은 서로 단절된 방법론이 아니라, 동일한 수학적 구조를 서로 다른 목적과 언어로 활용하는 연속적인 분석 체계라고 이해할 수 있다. 이는 이후 장에서 다루게 될 통계적 사고와 AI 알고리즘의 통합적 해석을 위한 중요한 연결 고리가 된다.\n\n\n7. 교차검증을 통한 정규화 강도 선택\n정규화 기법에서 중요한 문제 중 하나는 정규화 강도, 즉 하이퍼파라미터 \\(\\lambda\\)를 어떻게 선택할 것인가이다. \\(\\lambda\\)는 예측 오차를 줄이려는 목표와 모델의 복잡도를 제한하려는 목표 사이의 균형을 조절하는 역할을 하며, 그 값에 따라 모델의 성능과 일반화 능력은 크게 달라질 수 있다.\n교차검증은 이러한 정규화 강도를 데이터 기반으로 선택하기 위한 대표적인 방법이다. 교차검증의 기본 아이디어는 주어진 데이터를 학습용과 검증용으로 반복적으로 분할하여, 학습에 사용되지 않은 데이터에서의 예측 성능을 평가하는 데 있다. 이를 통해 특정 \\lambda 값이 새로운 데이터에 대해 얼마나 잘 일반화되는지를 간접적으로 추정할 수 있다.\n가장 널리 사용되는 방법은 K-fold 교차검증이다. 전체 데이터를 K개의 부분집합으로 나눈 뒤, 그중 하나를 검증 데이터로 사용하고 나머지를 학습 데이터로 사용하여 모델을 학습한다. 이 과정을 K번 반복하여 각 부분집합이 한 번씩 검증 데이터로 사용되도록 한다. 각 \\(\\lambda\\) 값에 대해 계산된 검증 오차를 평균함으로써, 해당 정규화 강도의 일반화 성능을 평가한다. 수식적으로 표현하면, 교차검증을 통한 정규화 강도 선택은 다음과 같은 문제로 이해할 수 있다.\n\\(\\widehat{\\lambda} = \\arg\\min_{\\lambda}\\frac{1}{K}\\overset{K}{\\sum_{k = 1}}\\text{Err}_{\\text{val}}^{(k)}(\\lambda)\\), 여기서 \\(\\text{Err}_{\\text{val}}^{(k)}(\\lambda)\\)는 k번째 검증 데이터에서 계산된 예측 오차를 의미한다. 이 기준에 따라 선택된 \\(\\lambda\\)는 학습 데이터에 대한 적합도보다는, 새로운 데이터에 대한 성능을 기준으로 결정된다는 점에서 의미를 가진다.\n교차검증을 통한 정규화 강도 선택은 편향–분산 트레이드오프의 관점에서 해석할 수 있다. \\(\\lambda\\)가 작은 경우 모델의 복잡도가 커져 분산이 증가하고 과적합 위험이 커지는 반면, \\lambda가 큰 경우 모델이 지나치게 단순해져 편향이 증가할 수 있다. 교차검증은 이 두 극단 사이에서 일반화 오차가 최소가 되는 지점을 경험적으로 탐색하는 절차이다.\n이러한 접근은 전통적 통계학에서의 모형 선택 기준과도 개념적으로 연결된다. 통계학에서 정보 기준이나 검증 표본을 통해 모형을 선택하듯이, 머신러닝에서는 교차검증을 통해 정규화 강도를 조절함으로써 최적의 예측 성능을 달성하고자 한다. 이 점에서 교차검증은 정규화 기법을 실제 데이터 분석에 적용 가능하게 만드는 핵심적인 연결 고리라 할 수 있다.\n\n\n8. 모델 복잡도와 과적합 문제\n머신러닝에서 모델의 선택은 단순함과 복잡함 중 어느 한쪽을 일방적으로 추구하는 문제가 아니다. 모델이 지나치게 단순하다고 해서 항상 바람직한 것도 아니며, 반대로 복잡하다고 해서 항상 우수한 성능을 보장하는 것도 아니다. 이때 핵심적으로 등장하는 개념이 과적합이다.\n모델이 지나치게 단순한 경우에는 데이터에 내재된 구조를 충분히 포착하지 못하게 되며, 이는 과소적합으로 이어진다. 과소적합이 발생하면 학습 데이터와 새로운 데이터 모두에서 예측 성능이 낮게 나타난다. 반대로 모델이 지나치게 복잡한 경우에는 학습 데이터의 우연적 변동까지 함께 학습하게 되어, 학습 데이터에서는 매우 높은 성능을 보이지만 새로운 데이터에서는 성능이 급격히 저하되는 과적합 현상이 발생한다.\n과적합은 학습 데이터에 대한 적합도가 높다는 사실만으로는 모델의 품질을 평가할 수 없음을 보여준다. 머신러닝에서 중요한 것은 학습 데이터가 아닌, 관측되지 않은 데이터에 대해서도 안정적인 성능을 유지하는 일반화 능력이다. 이 때문에 모델의 복잡도를 적절히 제어하는 것이 핵심적인 과제가 된다.\n머신러닝에서는 과적합을 완화하기 위해 다양한 전략을 활용한다. 정규화는 손실함수에 패널티를 추가하여 모델의 자유도를 제한하는 대표적인 방법이다. 조기 종료는 반복 학습 과정에서 검증 성능이 더 이상 개선되지 않을 때 학습을 중단함으로써 과도한 적합을 방지한다. 또한 모델 구조 자체를 단순화하거나, 학습 데이터와 검증 데이터를 분리하여 성능을 평가하는 데이터 분할 기반 검증 역시 중요한 역할을 한다.\n이러한 논의의 이론적 기반에는 여전히 통계학에서 제시한 편향–분산 절충 개념이 자리하고 있다. 모델의 복잡도를 높이면 편향은 감소하지만 분산이 증가하고, 복잡도를 낮추면 분산은 감소하지만 편향이 증가한다. 머신러닝에서의 과적합 문제는 이러한 편향과 분산 사이의 균형을 어떻게 달성할 것인가에 대한 현대적 표현이라 할 수 있다.\n\n\n9. 학습–검증–테스트 데이터 구조\n머신러닝 분석에서는 데이터를 역할에 따라 분리하여 사용하는 것이 기본적인 분석 절차로 자리 잡고 있다. 이는 하나의 표본으로 추론과 평가를 동시에 수행하는 전통적 통계분석과 구별되는 중요한 특징이다. 머신러닝에서는 데이터 분할 자체가 분석 과정의 핵심 구성 요소로 작동한다.\n\n\n\n\n\n학습 데이터는 모델의 파라미터를 추정하고, 손실함수를 최소화하도록 학습하는 데 사용된다. 모델은 이 데이터에 대해 반복적으로 적합되며, 예측 규칙을 형성한다. 검증 데이터는 학습 과정에는 직접 사용되지 않으며, 모델 선택이나 하이퍼파라미터 조정, 그리고 과적합 여부를 진단하는 데 활용된다. 정규화 강도나 모델 구조를 결정하는 과정에서 검증 데이터의 성능이 중요한 판단 기준이 된다.\n테스트 데이터는 분석의 최종 단계에서 사용되며, 학습과 모델 선택이 모두 완료된 이후에만 활용된다. 이 데이터는 모델의 성능을 객관적으로 평가하기 위한 용도로 사용되며, 학습이나 조정 과정에 관여하지 않는다. 따라서 테스트 데이터에서의 성능은 해당 모델이 새로운 데이터에 대해 어느 정도의 예측 능력을 가질지를 추정하는 기준으로 해석된다.\n이와 같은 학습–검증–테스트 구조의 목적은 미래 데이터에 대한 성능을 최대한 객관적으로 평가하는 데 있다. 모델이 이미 본 데이터에 대해 얼마나 잘 맞는지는 중요하지 않으며, 관측되지 않은 데이터에 대해서도 안정적인 성능을 유지하는지가 핵심적인 평가 기준이 된다. 이러한 관점에서 머신러닝은 분석 절차 자체에 검증 과정을 내재화한 방법론이라고 볼 수 있다.\n전통적 통계학이 이론적 분포와 불확실성 분석을 통해 추론의 타당성을 확보해 왔다면, 머신러닝은 데이터 분할과 반복 검증을 통해 예측 성능의 신뢰성을 확보한다. 이 차이는 두 방법론이 동일한 목적을 서로 다른 방식으로 추구하고 있음을 보여주는 대표적인 예라 할 수 있다.\n\n\n10. 통계학적 추론과 머신러닝 예측의 대비 요약\n전통적 통계학과 머신러닝은 모두 데이터로부터 의미 있는 정보를 추출하고자 한다는 공통된 목적을 가지지만, 문제를 설정하는 방식과 분석의 중심 개념에는 분명한 차이가 존재한다. 이 차이는 단순한 기법의 차이를 넘어, 데이터 분석을 바라보는 철학적 관점의 차이로 이해할 수 있다.\n전통적 통계학은 제한된 표본을 바탕으로 모집단에 대한 추론을 수행하는 데 초점을 둔다. 분석의 핵심 질문은 모집단의 모수는 무엇이며, 그 추정 결과가 어느 정도의 불확실성을 가지는가에 있다. 이를 위해 확률모형과 가정을 명시적으로 설정하고, 모수 추정, 가설검정, 신뢰구간과 같은 절차를 통해 결론의 타당성과 해석 가능성을 확보한다. 통계학에서 중요한 것은 결과 자체보다도, 그 결과가 어떤 가정 아래에서 얼마나 신뢰할 수 있는지를 함께 제시하는 것이다.\n반면 머신러닝은 새로운 데이터에 대한 예측 성능을 분석의 중심에 둔다. 여기서 핵심 질문은 주어진 데이터로부터 얼마나 작은 예측 오차를 달성할 수 있는가이다. 머신러닝은 확률분포에 대한 명시적 가정보다는 손실함수와 최적화 과정을 통해 모델을 학습하며, 모델의 적절성은 일반화 성능을 기준으로 평가된다. 이 과정에서 과적합을 방지하기 위한 정규화, 교차검증, 모델 복잡도 제어가 중요한 역할을 한다.\n불확실성을 다루는 방식에서도 두 접근은 차이를 보인다. 통계학은 추정량의 분산, 유의확률, 신뢰구간과 같은 개념을 통해 불확실성을 정량화하고 해석한다. 반면 머신러닝에서는 불확실성보다는 학습 데이터와 새로운 데이터 간의 성능 차이, 즉 일반화 오차를 통해 모델의 안정성을 판단한다. 이는 불확실성을 제거한 것이 아니라, 다른 방식으로 다루고 있음을 의미한다.\n그럼에도 불구하고 두 방법론은 완전히 분리된 체계가 아니다. 정규화와 벌점화 추정, 편향–분산 절충, 교차검증과 모형 선택 문제 등에서 보듯이, 머신러닝의 많은 핵심 개념은 통계학적 사고를 바탕으로 확장·재해석된 것이다. 차이는 목적과 강조점에 있으며, 수학적 구조와 이론적 기반에는 상당한 연속성이 존재한다.\n이러한 대비를 통해 알 수 있듯이, 통계학적 추론과 머신러닝 예측은 경쟁 관계라기보다 상호 보완적인 분석 패러다임이다. 설명과 해석이 중요한 문제에서는 통계학적 접근이 강점을 가지며, 예측 정확도가 핵심인 문제에서는 머신러닝이 효과적인 해법을 제공한다. 현대의 데이터 분석에서는 이 두 관점을 상황에 맞게 결합하고 선택하는 능력이 점점 더 중요한 역량으로 요구되고 있다.\n\n\n\nChapter 5. 딥러닝의 수학적·구조적 특징\n\n1. 신경망의 기본 구조: 노드, 가중치, 활성화 함수\n딥러닝은 인공신경망을 기반으로 한 학습 방법론으로, 복잡한 함수 관계를 데이터로부터 학습하는 것을 목표로 한다. 신경망은 여러 개의 노드가 층 구조로 연결된 함수 근사기로 이해할 수 있으며, 각 노드는 입력 신호의 가중합에 비선형 변환을 적용하는 계산 단위로 작동한다.\n하나의 층에서 이루어지는 연산은 다음과 같이 표현된다.\n\\(\\mathbf{z}^{(l)} = \\mathbf{W}^{(l)}\\mathbf{a}^{(l - 1)} + \\mathbf{b}^{(l)},\\mathbf{a}^{(l)} = \\phi(\\mathbf{z}^{(l)})\\), 여기서 \\(\\mathbf{W}^{(l)}\\)는 l번째 층의 가중치 행렬, \\(\\mathbf{b}^{(l)}\\)는 편향 벡터, \\(\\mathbf{a}^{(l - 1)}\\)는 이전 층의 출력, 그리고 \\(\\phi( \\cdot )\\)는 활성화 함수를 의미한다. 이와 같은 연산이 층을 따라 반복되면서 입력 데이터는 점차 고차원적이고 추상적인 표현으로 변환된다.\n\n\n\n\n\n신경망을 구성하는 핵심 요소는 다음과 같이 정리할 수 있다. 노드는 입력을 받아 계산을 수행하는 기본 단위로, 신경망의 최소 구성 요소에 해당한다. 가중치는 각 입력의 상대적 중요도를 조절하는 학습 대상 파라미터로, 데이터에 따라 반복적으로 갱신된다. 활성화 함수는 가중합에 비선형 변환을 적용함으로써, 신경망이 단순한 선형 변환을 넘어 복잡한 함수 관계를 표현할 수 있도록 한다.\n활성화 함수가 없는 경우, 여러 층을 쌓더라도 전체 신경망은 하나의 선형 변환으로 축약된다. 이는 신경망의 깊이가 아무리 깊어져도 선형모형의 범위를 벗어나지 못함을 의미한다. 따라서 비선형성을 도입하는 활성화 함수는 딥러닝의 표현력을 결정짓는 핵심 요소라 할 수 있다. ReLU, sigmoid, tanh와 같은 활성화 함수는 이러한 비선형성을 제공하며, 신경망이 복잡한 데이터 구조를 학습할 수 있도록 하는 역할을 수행한다.\n이와 같이 신경망은 선형 결합과 비선형 변환의 반복 구조를 통해 복잡한 함수를 근사하는 수학적 틀로 이해할 수 있으며, 이는 이후 살펴볼 딥러닝 학습 알고리즘과 표현 학습의 기초가 된다.\n\n\n2. 표현학습의 의미\n딥러닝의 가장 중요한 개념적 특징은 표현학습이다. 표현학습이란 사람이 사전에 특징을 설계하지 않더라도, 모델이 데이터로부터 예측에 유용한 중간 표현을 자동으로 학습하는 과정을 의미한다. 이는 딥러닝이 기존의 분석 방법과 구별되는 핵심적인 차별점이다.\n전통적 통계 분석이나 머신러닝에서는 변수 변환, 상호작용 항의 설계, 차원 축소와 같은 특징 추출 과정을 연구자가 직접 수행하는 경우가 많았다. 이러한 과정은 분석자의 경험과 문제 이해에 크게 의존하며, 데이터 구조가 복잡할수록 설계 난이도가 급격히 증가한다. 반면 딥러닝에서는 입력층에서 출력층으로 갈수록 각 은닉층이 점진적으로 추상화된 표현을 학습하며, 이 과정이 데이터 기반으로 자동 수행된다.\n통계학적 관점에서 표현학습은 고차원 비모수 함수 공간에서의 자동 특징 추출 과정으로 해석할 수 있다. 신경망은 입력 변수의 원래 좌표계가 아닌, 예측에 유리한 새로운 표현 공간을 내부적으로 구성한다. 이러한 특성으로 인해 딥러닝은 이미지, 음성, 자연어와 같이 구조가 복잡하고 비정형적인 데이터에서 특히 강력한 성능을 보인다.\n\n\n3. 역전파 알고리즘과 경사하강법\n딥러닝 학습의 핵심 계산 절차는 역전파 알고리즘이다. 역전파는 출력층에서 정의된 손실함수를 기준으로, 각 가중치가 손실에 얼마나 기여했는지를 연쇄법칙을 이용해 계산하는 알고리즘이다. 이를 통해 모든 층의 파라미터를 효율적으로 갱신할 수 있다.\n학습 과정은 일반적으로 다음의 단계로 구성된다. 먼저 순전파 단계에서는 입력 데이터를 신경망에 통과시켜 예측값을 계산한다. 이어서 손실 계산 단계에서는 예측값과 실제값의 차이를 손실함수로 측정한다. 다음으로 역전파 단계에서는 손실을 각 가중치에 대해 미분하여 기울기를 계산한다. 마지막으로 파라미터 업데이트 단계에서는 계산된 기울기를 이용해 가중치를 갱신한다.\n경사하강법에 의한 파라미터 갱신은 다음과 같이 표현된다.\n\\(\\theta^{(t + 1)} = \\theta^{(t)} - \\eta\\nabla_{\\theta}L(\\theta)\\), 여기서 \\(\\eta\\)는 학습률이며, 손실함수의 기울기를 따라 파라미터를 반복적으로 이동시키는 역할을 한다.\n통계학적 관점에서 보면, 이는 우도함수 또는 손실함수의 수치적 최소화 과정이다. 닫힌형 해를 구하기 어려운 고차원 최적화 문제를 반복 계산을 통해 해결한다는 점에서, 딥러닝은 전통적 추정 방법의 계산적 확장으로 이해할 수 있다.\n\n\n4. 모델 깊이와 비선형성의 효과\n딥러닝에서 모델이 깊다는 것은 단순히 층의 수가 많다는 의미를 넘어서, 복잡한 비선형 구조를 여러 단계로 분해하여 학습한다는 것을 뜻한다. 이는 얕은 신경망과 깊은 신경망의 근본적인 차이를 설명하는 핵심 개념이다.\n얕은 신경망은 하나의 큰 비선형 변환을 통해 입력과 출력을 직접 연결하는 구조를 가진다. 반면 깊은 신경망은 여러 개의 비교적 단순한 비선형 변환을 순차적으로 조합하여 복잡한 함수를 구성한다. 이러한 계층적 구조를 통해 동일한 함수를 더 적은 파라미터로 효율적으로 표현할 수 있는 경우가 많다.\n이는 통계학에서의 모형 구조화 개념과 유사하게 이해할 수 있다. 복잡한 현상을 하나의 거대한 모형으로 설명하기보다, 여러 단계의 구조적 관계로 분해하여 설명하는 방식이다. 다만 모델의 깊이가 증가할수록 과적합 위험, 학습 불안정성, 계산 비용이 함께 증가하므로, 정규화와 검증 전략이 필수적으로 요구된다.\n\n\n5. 블랙박스 논쟁과 설명가능성 이슈\n딥러닝의 뛰어난 예측 성능과 함께 가장 많이 제기되는 비판은 설명가능성의 부족이다. 수십만에서 수백만 개에 이르는 파라미터로 구성된 신경망은 개별 예측 결과가 어떤 과정을 통해 도출되었는지를 직관적으로 설명하기 어렵다.\n전통적 통계모형에서는 회귀계수의 부호와 크기, 신뢰구간, 가설검정 등을 통해 변수의 역할과 불확실성을 명시적으로 해석할 수 있었다. 반면 딥러닝 모형은 이러한 해석 구조를 기본적으로 내장하고 있지 않으며, 예측 성능 자체가 주요 평가 기준으로 작동한다.\n이로 인해 최근에는 딥러닝의 내부 작동을 해석하려는 다양한 시도가 이루어지고 있다. 특성 중요도 분석, 국소적 설명 기법, 모델 단순화 접근 등은 복잡한 신경망의 결정을 이해 가능한 형태로 재구성하려는 방법들이다. 이러한 연구 흐름은 딥러닝을 통계학적 해석 틀과 연결하려는 시도로 볼 수 있으며, 예측 성능과 설명 가능성 사이의 균형을 모색하는 현대 데이터 분석의 중요한 과제로 자리 잡고 있다.\n\n\n6. 딥러닝을 언제 쓰고 언제 경계해야 하는가\n딥러닝은 복잡한 데이터 구조를 효과적으로 학습할 수 있는 강력한 방법론이지만, 모든 분석 문제에 항상 최선의 선택이 되는 것은 아니다. 딥러닝의 적용 여부는 데이터의 특성, 분석 목적, 그리고 해석 가능성에 대한 요구 수준을 종합적으로 고려하여 판단해야 한다.\n딥러닝은 데이터의 규모가 크고, 입력 구조가 고차원적이며 비정형적인 경우에 특히 강점을 가진다. 이미지, 음성, 자연어와 같이 사람이 명시적으로 특징을 설계하기 어려운 데이터에서는 표현학습을 통해 자동으로 유용한 구조를 추출할 수 있다. 또한 예측 정확도가 분석의 최우선 목표이고, 충분한 학습 데이터와 계산 자원이 확보된 상황에서는 딥러닝이 기존 방법보다 뛰어난 성능을 보일 가능성이 높다.\n반면 데이터의 규모가 제한적이거나, 분석 결과에 대한 명확한 해석과 설명이 중요한 경우에는 딥러닝의 사용을 신중히 검토할 필요가 있다. 복잡한 신경망 모형은 과적합 위험이 크며, 모델 선택과 하이퍼파라미터 조정 과정에서도 상당한 경험적 판단이 요구된다. 또한 정책 결정, 과학적 설명, 인과 해석이 중요한 문제에서는 딥러닝의 블랙박스적 특성이 실질적인 제약으로 작용할 수 있다. 이러한 경우에는 전통적 통계모형이나 상대적으로 단순한 머신러닝 방법이 더 적절한 선택이 될 수 있다.\n이러한 판단 기준을 이해하기 위해서는 통계모형, 머신러닝, 딥러닝 사이의 구조적 연속성을 함께 살펴볼 필요가 있다. 전통적 통계모형은 확률모형과 가정을 명시적으로 설정하고, 모수 추정과 불확실성 해석을 중심으로 발전해 왔다. 이는 설명 가능성과 이론적 정합성을 중시하는 분석 패러다임이다.\n\n\n7. 통계모형–머신러닝–딥러닝의 구조적 연속성 요약\n머신러닝은 이러한 통계적 틀을 바탕으로 예측 성능을 중심에 두고 확장된 접근이라 할 수 있다. 손실함수 최소화, 정규화, 교차검증과 같은 핵심 개념은 통계학의 추정과 모형 선택 문제를 예측 중심 환경에서 재구성한 결과이다. 차이는 목적과 평가 기준에 있으며, 수학적 구조 자체는 상당 부분 공유되고 있다.\n딥러닝은 머신러닝의 연장선상에서 표현학습과 대규모 최적화를 전면에 내세운 방법론이다. 신경망의 깊이와 비선형성은 기존 모형이 다루기 어려웠던 복잡한 데이터 구조를 효과적으로 학습할 수 있게 하지만, 동시에 해석 가능성과 안정성 문제를 새로운 과제로 제시한다. 즉, 딥러닝은 통계모형과 머신러닝을 대체하는 종착점이라기보다, 특정 조건에서 강력한 성능을 발휘하는 고차원적 확장으로 이해하는 것이 적절하다.\n결국 통계모형, 머신러닝, 딥러닝은 서로 경쟁하는 방법론이 아니라, 동일한 수학적·통계적 사고 위에서 목적과 데이터 환경에 따라 선택되는 연속적인 분석 도구들이다. 중요한 것은 어떤 방법이 더 ’최신’인가가 아니라, 주어진 문제에서 어떤 방법이 가장 합리적인 해석과 성능을 동시에 제공하는가를 판단하는 분석자의 통계적 사고 능력이라 할 수 있다.\n\n\n\nChapter 6. 통계방법론과 ML·DL의 공통 수학적 기반\n\n1. 선형모형과 선형회귀의 재해석\n선형회귀는 전통적 통계방법론의 핵심이자, 머신러닝 관점에서도 가장 기본적인 학습 알고리즘이다. 통계학에서는 선형회귀를 \\(y = X\\beta + \\varepsilon,\\varepsilon \\sim \\mathcal{N}(0,\\sigma^{2}I)\\)와 같은 확률모형으로 출발하여, 회귀계수 \\beta의 추정과 해석, 그리고 불확실성 평가를 중심으로 다룬다.\n반면 머신러닝에서는 동일한 문제를 다음과 같은 손실함수 최소화 문제로 재해석한다. \\(\\widehat{\\beta} = \\arg\\min_{\\beta}\\frac{1}{n}\\overset{n}{\\sum_{i = 1}}(y_{i} - x_{i}^{\\top}\\beta)^{2}\\)\n여기서 관심의 초점은 확률분포 가정보다는 평균제곱오차를 얼마나 작게 만들 수 있는가에 놓여 있다.\n중요한 점은 두 접근이 수학적으로 동일한 해를 갖는다는 사실이다. 즉, 선형회귀는 통계학에서는 확률모형 기반 추론 문제로, 머신러닝에서는 평균제곱오차 최소화 문제로 서술될 뿐, 동일한 수학적 구조 위에 놓여 있다. 이 관점은 머신러닝이 통계학과 완전히 단절된 새로운 방법론이라는 오해를 해소하는 출발점이 된다.\n\n\n2. 확률모형과 손실함수의 연결\n전통적 통계학에서 확률모형은 데이터 생성 과정을 명시적으로 가정한다. 예를 들어 오차항이 정규분포를 따른다고 가정하면, 음의 로그우도는 다음과 같은 형태를 갖는다.\n\\[- \\log L(\\beta) \\propto \\overset{n}{\\sum_{i = 1}}(y_{i} - x_{i}^{\\top}\\beta)^{2}\\]\n이는 머신러닝에서 사용하는 제곱오차 손실함수와 정확히 일치한다.이러한 연결은 우연이 아니라 구조적 필연이다. 손실함수는 확률모형의 로그우도를 다른 언어로 표현한 것으로 이해할 수 있다. 이 관점에서 보면, 로지스틱 회귀에서의 교차엔트로피 손실, 포아송 회귀에서의 로그손실 역시 각각의 확률분포 가정에서 자연스럽게 유도된 결과이다. 일반화선형모형은 통계학과 머신러닝을 잇는 가장 직접적인 연결 고리라 할 수 있다.\n\n\n3. 최대우도추정과 경험적 위험 최소화\n통계학에서 가장 대표적인 추정 원리는 최대우도추정이다. 이는 관측된 데이터가 가장 그럴듯하게 나타나도록 모수를 선택하는 방법으로, 다음과 같이 표현된다.\n\\[\\widehat{\\theta} = \\arg\\max_{\\theta}\\overset{n}{\\sum_{i = 1}}\\log p(y_{i} \\mid x_{i},\\theta)\\]\n머신러닝에서는 이를 경험적 위험 최소화라고 부른다.\n\\[\\widehat{f} = \\arg\\min_{f}\\frac{1}{n}\\overset{n}{\\sum_{i = 1}}L(y_{i},f(x_{i}))\\]\n두 개념은 명칭만 다를 뿐, 표본 평균 형태의 목적함수를 최소화하거나 최대화한다는 점에서 동일한 철학을 공유한다.\n차이는 강조점에 있다. 통계학은 분포 가정과 추론의 정당성을 중시하는 반면, 머신러닝은 일반화 성능과 예측 정확도를 핵심 기준으로 삼는다. 이는 목적의 차이지, 수학적 기반의 차이는 아니다.\n\n\n4. 정규화와 편향–분산 절충\n머신러닝에서 정규화는 과적합을 방지하기 위한 핵심 기법이다. 예를 들어 Ridge 회귀는 다음과 같이 손실함수에 패널티를 추가한다.\n\\[\\widehat{\\beta} = \\arg\\min_{\\beta}\\left\\{ \\overset{n}{\\sum_{i = 1}}(y_{i} - x_{i}^{\\top}\\beta)^{2} + \\lambda\\sum_{j}\\beta_{j}^{2} \\right\\}\\]\n통계학적으로 이는 모형의 자유도를 제한하여 분산을 줄이는 대신, 일정 수준의 편향을 허용하는 전략으로 해석된다. 즉, 정규화는 편향–분산 절충을 수식으로 구현한 장치이다.\n이 관점에서 Lasso, Elastic Net, 그리고 딥러닝에서의 드롭아웃과 가중치 감쇠 기법은 모두 통계학의 모형 단순화 철학을 계산적으로 확장한 결과로 이해할 수 있다. 표현 방식은 달라도, 핵심 아이디어는 동일하다.\n\n\n5. 베이지안 통계와 베이지안 머신러닝\n베이지안 통계는 모수를 고정된 값이 아닌 확률변수로 취급하고, 사전분포와 우도를 결합하여 사후분포를 도출한다.\n\\[p(\\theta \\mid y) \\propto p(y \\mid \\theta)p(\\theta)\\]\n이 프레임워크는 머신러닝에서도 중요한 역할을 한다. 정규화는 종종 특정 형태의 사전분포를 도입한 결과로 해석될 수 있으며, 베이지안 신경망은 가중치에 대한 불확실성을 직접 모델링한다.\n베이지안 머신러닝은 예측 불확실성의 정량화, 과적합에 대한 자연스러운 제어, 소표본 상황에서의 안정성이라는 장점을 제공한다. 이는 전통적 통계학의 강점이 현대 머신러닝과 딥러닝으로 이어지는 대표적인 사례라 할 수 있다.\n\n\n6. 응용 문제에서 방법론을 선택하는 실제 기준\n통계모형, 머신러닝, 딥러닝은 서로 경쟁하는 방법론이 아니라, 문제의 성격과 분석 목적에 따라 선택되어야 할 도구들이다. 실제 응용 문제에서 중요한 것은 특정 방법론의 유행이나 기술적 복잡성이 아니라, 주어진 상황에서 가장 합리적인 분석 결과를 제공할 수 있는가이다. 이를 위해서는 몇 가지 핵심 기준을 중심으로 방법론을 선택할 필요가 있다.\n첫째, 분석의 목적이 무엇인가를 명확히 해야 한다. 분석의 목적이 모집단의 특성을 이해하고 설명하는 데 있다면, 통계학적 추론 방법이 적합하다. 회귀계수의 해석, 불확실성의 정량화, 가설 검정이 중요한 경우에는 전통적 통계모형이 강점을 가진다. 반면 목적이 새로운 데이터에 대한 정확한 예측이라면, 머신러닝이나 딥러닝이 더 적합한 선택이 될 수 있다. 이 경우 해석보다는 일반화 성능이 핵심 평가 기준이 된다.\n둘째, 데이터의 규모와 구조를 고려해야 한다. 표본 크기가 작고 변수의 수가 제한적인 경우에는 복잡한 모델이 오히려 과적합을 초래할 수 있다. 이러한 상황에서는 단순한 통계모형이나 규제된 머신러닝 기법이 더 안정적인 결과를 제공한다. 반대로 대규모 데이터가 उपलब्ध하고, 입력 구조가 고차원적이거나 비정형적인 경우에는 딥러닝이 강력한 표현력을 발휘할 수 있다.\n셋째, 해석 가능성에 대한 요구 수준을 평가해야 한다. 정책 결정, 과학적 연구, 사회과학 분석과 같이 결과에 대한 설명 책임이 중요한 분야에서는 모델의 해석 가능성이 필수적이다. 이 경우 통계모형이나 비교적 단순한 머신러닝 모델이 선호된다. 반면 추천 시스템, 음성 인식, 이미지 분류와 같이 예측 성능이 최우선인 문제에서는 복잡한 딥러닝 모델이 현실적인 선택이 된다.\n넷째, 불확실성의 표현이 필요한지 여부를 고려해야 한다. 통계학은 신뢰구간, 유의확률 등을 통해 불확실성을 명시적으로 제시한다. 이러한 정보가 의사결정에 중요한 역할을 한다면 통계적 접근이 유리하다. 머신러닝과 딥러닝에서도 불확실성 추정이 가능하지만, 이는 추가적인 가정이나 계산을 필요로 하며 항상 기본적으로 제공되는 것은 아니다.\n다섯째, 계산 자원과 구현 비용 역시 현실적인 판단 기준이다. 딥러닝 모델은 대규모 데이터와 높은 계산 자원을 요구하는 경우가 많으며, 모델 튜닝과 재현성 확보에도 상당한 비용이 든다. 반면 통계모형과 단순한 머신러닝 기법은 비교적 적은 자원으로도 안정적인 분석이 가능하다.\n이러한 기준을 종합하면, 방법론 선택은 이분법적인 문제가 아님을 알 수 있다. 실제 분석에서는 통계모형으로 문제를 이해하고, 머신러닝으로 예측 성능을 보완하며, 딥러닝을 특정 하위 문제에 선택적으로 적용하는 혼합적 접근이 자주 사용된다. 중요한 것은 특정 방법론을 고집하는 것이 아니라, 문제의 성격에 맞는 분석 전략을 설계하는 능력이다.\n결국 응용 문제에서 요구되는 핵심 역량은 특정 알고리즘을 아는 것이 아니라, 데이터와 목적을 바탕으로 적절한 방법론을 선택하고 그 한계를 인식하는 통계적 사고라 할 수 있다. 이는 통계학과 머신러닝, 딥러닝을 관통하는 가장 중요한 공통 기반이기도 하다."
  },
  {
    "objectID": "notes/math/index.html",
    "href": "notes/math/index.html",
    "title": "기초수학",
    "section": "",
    "text": "기초수학 강의노트입니다.\n왼쪽 메뉴에서 주제를 선택하세요."
  },
  {
    "objectID": "notes/math/derivate_integral.html",
    "href": "notes/math/derivate_integral.html",
    "title": "수학의 기초 2. 미분과 적분",
    "section": "",
    "text": "chapter 1. 미분\n세상에는 정지해 있는 것이 없다. 우리가 함께 움직이고 있기 때문에 느끼지 못할 뿐, 지구는 자전 속도로 약 시속 1,660킬로미터, 공전 속도로는 약 시속 10만 7천5백 킬로미터로 끊임없이 움직이고 있다. 이러한 사실은 코페르니쿠스가 제안한 지동설에 바탕을 두고 있다. 갈릴레오는 망원경을 통해 목성의 위성을 관찰하고 금성의 위상 변화를 확인함으로써 지동설을 뒷받침했다. 케플러는 행성의 궤도가 완전한 원이 아니라 타원임을 밝혔으며, 이는 지동설의 정밀함을 더하는 데 기여했다. 같은 시대를 살았던 뉴턴은 왜 달은 하늘에 떠 있는 반면, 사과는 땅으로 떨어지는지를 고민하며 만유인력의 법칙을 세우고 중력 개념을 정립했다.\n이러한 천체의 움직임과 자연 현상의 본질은 결국 변화에서 출발한다. 미분은 변화의 순간을 정량적으로 포착하는 방법이다. 상태가 변할 때, 우리는 그 변화의 양상에 주목하게 된다. 거리의 변화는 속도로, 속도의 변화는 가속도로 표현되며, 경제학에서는 비용과 효용의 변화가 한계비용과 한계효용으로 나타난다.\n미분은 함수의 특정 지점에서 접선의 기울기를 계산하는 도구다. 이 접선의 기울기는 함수가 그 점에서 얼마나 빠르게 증가하거나 감소하는지를 나타낸다. 다항함수, 지수함수, 로그함수, 삼각함수 등 대부분의 함수는 미분 가능하며, 특히 통계학에서 자주 사용되는 함수들은 거의 예외 없이 미분이 가능한 형태를 가지고 있다.\n통계학에서 미분은 여러 분야에 활용된다. 회귀분석에서는 오차의 제곱합을 최소화하기 위해 미분을 통해 회귀계수를 추정하고, 확률 밀도 함수의 극댓값을 찾거나 함수의 모양을 분석할 때도 미분이 필수적이다. 또한 최대우도법이나 베이지안 추정 등에서도 목적함수의 최적값을 구하기 위한 과정에 미분이 사용된다.\n이처럼 미분은 단순히 수학적인 연산을 넘어서, 변화와 움직임을 이해하는 데 필요한 핵심 개념으로 작용한다. 자연의 움직임을 설명하고자 했던 과학자들의 질문이 결국 수학적 사고와 연결되듯, 통계학에서도 미분은 현상을 분석하고 설명하는 데 중요한 역할을 수행한다.\n\n1. 평균변화율 average rate of change\n구간 \\(a \\leq x \\leq b\\)에서 함수 \\(f\\)의 평균 변화량으로 \\(\\frac{rise}{run} = \\frac{\\Delta y}{\\Delta x} = \\frac{f(b) - f(b)}{b - a}\\)이다.\n\n\n\n\n\n\\(a \\leq x \\leq b\\) 구간에서 단위당 평균적으로 함수의 변화량을 측정한 것이다. 평균변화량은 고속도로 구간단속에 이용된다. 미분은 지점 과속 단속에 이용된다.\n\n\n\n\n\n측정 1: 구간단속 시작, 종료 지점에서 과속여부 측정 (미분 응용)\n측정 2: 예를 들어 구간 거리가 6km라 하자. 2분만에 구간을 통과했다면 평균속도는 \\(\\frac{6 - 0}{2 - 0} = 3km/min.\\)분당 3km를 달렸으므로 시간당 180km를 달렸으니 과속이 되는 것입니다. (평균변화량)\n\n\n2. 미분 정의\n함수 \\(f(x)\\)의 임의의 점 \\(x = a\\)에서의 미분값 \\(f'(a)\\)는 다음과 같이 정의된다. \\(f'(a) = \\lim_{h \\rightarrow 0}\\frac{f(a + h) - f(a)}{h}\\)\n\\(\\frac{f(a + h) - f(a)}{h}\\)는 Fermat’s Difference Quotient로 불리며, 점 \\(a\\)에서의 평균 변화율을 나타낸다.\n극한이 존재하면 \\(f'(a)\\)는 \\(x = a\\)에서의 접선의 기울기로 해석할 수 있다.\n미분 가능성: \\(f'(a)\\)가 존재하면, 점 \\(x = a\\)에서 함수 \\(f(x)\\)는 미분 가능하다고 한다. 함수 \\(f(x)\\)가 정의역 전체에서 미분 가능하, 함수 f(x)는 미분 가능 함수이다.\n미분의 기하학적 해석: 미분값 \\(f'(a)\\)는 곡선 \\(y = f(x)\\)의 점 \\(x = a\\)에서의 접선의 기울기를 의미한다. \\(h\\)가 0으로 가까워질수록 평균 변화율은 접선의 기울기에 점점 가까워진다.\n미분 가능성과 연속성: 함수 \\(f(x)\\)가 점 \\(x = a\\)에서 미분 가능하면 \\(f(x)\\)는 반드시 그 점에서 연속이다. 하지만, 연속이라고 해서 항상 미분 가능한 것은 아니다. 예를 들어, 절대값 함수 가능하지 않다.\n\n\n\n\n\n\n\n3. 미분 규칙\n상수 함수의 미분 \\[\\frac{d}{dx}\\lbrack c\\rbrack = 0\\]\n거듭제곱 함수의 미분 \\[\\frac{d}{dx}\\lbrack x^{n}\\rbrack = nx^{n - 1}, f(x) = x^{n}(n \\in \\mathbb{R})\\]\n【예제】 \\(f(x) = 2\\sqrt{x}\\) 을 미분하시오.\n\\[f'(x) = 2(\\frac{1}{2})x^{1/2 - 1} = x^{- 1/2} = \\frac{1}{\\sqrt{x}}\\]\n상수배의 미분: \\[\\frac{d}{dx}\\lbrack c \\cdot f(x)\\rbrack = c \\cdot \\frac{d}{dx}\\lbrack f(x)\\rbrack\\]\n합/차의 미분\n\\[\\frac{d}{dx}\\lbrack f(x) \\pm g(x)\\rbrack = \\frac{d}{dx}\\lbrack f(x)\\rbrack \\pm \\frac{d}{dx}\\lbrack g(x)\\rbrack\\]\n곱의 미분\n\\[\\frac{d}{dx}\\lbrack f(x) \\cdot g(x)\\rbrack = f'(x) \\cdot g(x) + f(x) \\cdot g'(x)\\]\n나눗셈의 미분\n\\(\\frac{d}{dx}\\left\\lbrack \\frac{f(x)}{g(x)} \\right\\rbrack = \\frac{f'(x) \\cdot g(x) - f(x) \\cdot g'(x)}{\\lbrack g(x)\\rbrack^{2}}\\), \\(g(x) \\neq 0\\)\n체인룰 chain rule 연쇄규칙\n\\[\\frac{d}{dx}\\lbrack f(g(x))\\rbrack = f'(g(x)) \\cdot g'(x)\\]\n【예제】 \\(f(x) = 2\\sqrt{3x^{2} - 1}\\)을 미분하시오.\n\n\n\n\n\n  바깥부분 미분하고 안쪽 부분 그대로 적는다.\n  \\(2*(1/2){\\sqrt{(3x^{2} - 1)}}^{- 1/2}\\) 그리고 안쪽부분을 미분한다.\n\\[f'(x) = {\\sqrt{(3x^{2} - 1)}}^{- 1/2}6x = \\frac{6x}{\\sqrt{3x^{2} - 1}}\\]\n로그함수 미분 \\[\\frac{d}{dx}\\lbrack\\log_{a}(x)\\rbrack = \\frac{1}{x\\ln(a)},x &gt; 0\\]\n\\[\\frac{d}{dx}\\lbrack\\ln(x)\\rbrack = \\frac{1}{x},x &gt; 0\\]\n【예제】 \\(f(x) = ln(x^{2} - 1)\\)을 미분하시오.\n  연쇄법칙 적용 : \\(f'(x) = \\frac{1}{x^{2} - 1}2x\\)\n지수함수 미분\n\\[\\frac{d}{dx}\\lbrack a^{x}\\rbrack = a^{x}\\ln(a)\\]\n\\[\\frac{d}{dx}\\lbrack e^{x}\\rbrack = e^{x}\\]\nimport sympy as sp\n\n# 변수와 함수를 정의\nx = sp.Symbol('x')\nf = 5*(x**2 - 2*x)**2\n\n# 함수 입력을 파싱하여 미분\nfunc = sp.sympify(f)\nderivative = sp.diff(func, x)\n\n\n4. 미분 응용\n\n(1) 최대, 최소\n1차 미분정리\n함수 f(x) 가 일정 구간 (a, b) 안의 모든 점에서 미분 가능하고, 구간 내 임의의 점 c 에서 1차 미분이 0이면, f(x) 함수는 c 점에서 지역 최대값이나 최소값을 갖는다. 이는 페르마의 정리에 Fermat’s 해당하며, 극대값 또는 극소값이 존재하는 필수 조건을 설명한다.\n함수 f(x) 가 c 에서 미분 가능하고, 극값이 c 에서 존재하면, 반드시 \\(f'(c) = 0\\)이어야 한다.\n다만, \\(f'(c) = 0\\)이라고 해서 반드시 극값이 존재하는 것은 아니며, 이는 필요조건일 뿐 충분조건은 아니다. 극값의 존재를 확실히 판단하려면 2차 도함수 테스트나 첫 도함수의 부호 변화를 추가로 고려해야 한다.\n증가 함수와 감소 함수\n함수 f(x)가 구간 \\(I\\)에서 정의되어 있을 때, \\(x_{1} &lt; x_{2} \\Longrightarrow f(x_{1}) \\leq f(x_{2})\\) 이면 구간 \\(I\\)에서 증가 함수이다.\n\\(x_{1} &lt; x_{2} \\Longrightarrow f(x_{1}) \\geq f(x_{2})\\) 이면 구간 \\(I\\)에서 감소 함수이다.\n1차 미분과 증가·감소 함수의 관계\n함수 f(x) 가 구간 \\(I\\)에서 미분 가능하다면,\n\n\\(f'(x) &gt; 0\\) 이면, f(x) 는 구간 \\(I\\)에서 엄격히 증가한다\n\\(f'(x) &lt; 0\\) 이면, f(x) 는 구간 \\(I\\)에서 엄격히 감소한다.\n\n오목성 concavity 정의\n함수 f(x) 의 기울기가 감소하는 경우 \\(f''(x) &lt; 0\\),\n\n함수 f(x) 는 concave down (오목 아래)이다.\n그래프가 아래로 휘어진 모양을 갖는다.\n\n함수 f(x) 의 기울기가 증가하는 경우 \\(f''(x) &gt; 0\\),\n\n함수 f(x) 는 concave up (오목 위)이다.\n그래프가 위로 휘어진 모양을 갖는다.\n\n\n\n\n\n\n변곡점 inflexion point 정의\n함수 \\(f(x)\\)의 오목성이 변하는 점이 있을 때, 이 점을 변곡점이라고 한다. 즉, \\(f(x)\\)가 \\(f’’(x) &gt; 0\\)에서 \\(f’’(x) &lt; 0\\)로 바뀌거나 \\(f’’(x) &lt; 0\\)에서 \\(f’’(x) &gt; 0\\)로 바뀌는 점이 변곡점이다.”\n1차 미분과 2차 미분을 이용한 최대, 최소 판단\n주어진 \\(f'(c) = 0\\)에서, \\(f''(c)\\)를 확인한다.\n\n\\(f''(c) &gt; 0\\) 이면 \\(x = c\\)에서 (지역) 최소값\n\\(f''(c) &lt; 0\\) 이면 \\(x = c\\)에서 (지역) 최대값\n\\(f''(c) = 0\\) 이고 \\(f''(x)\\) 부호가 바뀌면 \\(x = c\\)에서 변곡점\n\n\n\n(2) 통계학 응용\n단순 회귀모형 \\[y_{i} = \\beta_{0} + \\beta_{1}x_{i} + \\epsilon_{i},i = 1,2,\\ldots,n\\]\nOLS 추정치 \\[\\text{Minimize:}S(\\beta_{0},\\beta_{1}) = \\overset{n}{\\sum_{i = 1}}(y_{i} - \\beta_{0} - \\beta_{1}x_{i})^{2}\\]\n오차 제곱합    \\(S(\\beta_{0},\\beta_{1})\\)을 \\(\\beta_{0}\\)와 \\(\\beta_{1}\\)에 대해 편미분한 뒤 0으로 설정하여 최소값(OLS)을 찾는다.\n정규방정식\n\\[\\frac{\\partial S}{\\partial\\beta_{0}} = - 2\\overset{n}{\\sum_{i = 1}}(y_{i} - \\beta_{0} - \\beta_{1}x_{i}) = 0\\]\n\\[\\overset{n}{\\sum_{i = 1}}y_{i} = n\\beta_{0} + \\beta_{1}\\overset{n}{\\sum_{i = 1}}x_{i}\\]\n\\[\\frac{\\partial S}{\\partial\\beta_{1}} = - 2\\overset{n}{\\sum_{i = 1}}x_{i}(y_{i} - \\beta_{0} - \\beta_{1}x_{i}) = 0\\]\n\\[\\overset{n}{\\sum_{i = 1}}x_{i}y_{i} = \\beta_{0}\\overset{n}{\\sum_{i = 1}}x_{i} + \\beta_{1}\\overset{n}{\\sum_{i = 1}}x_{i}^{2}\\]\n두 식을 함께 사용하여 \\(\\beta_{0}\\)와 \\(\\beta_{1}\\)를 계산한다.\n\\[\\beta_{1} = \\frac{\\sum_{i = 1}^{n}(x_{i} - \\overline{x})(y_{i} - \\overline{y})}{\\sum_{i = 1}^{n}(x_{i} - \\overline{x})^{2}} = \\frac{\\text{Cov}(x,y)}{\\text{Var}(x)}\\]\n\\[\\beta_{0} = \\overline{y} - \\beta_{1}\\overline{x}\\]\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.optimize import curve_fit\n\n# 가상의 데이터 생성 n=20\nnp.random.seed(0)\nx_data = np.linspace(-5, 5, 20)\ny_data = 2 * x_data**3 - 3 * x_data**2 + 4 * x_data + 10 + np.random.normal(0, 10, 20)\n\n# 직선 적합 함수\ndef linear(x, a, b):\n    return a * x + b\n# 2차 함수 적합 함수\ndef quadratic(x, a, b, c):\n    return a * x**2 + b * x + c\n# 3차 함수 적합 함수\ndef cubic(x, a, b, c, d):\n    return a * x**3 + b * x**2 + c * x + d\n# 최소자승법을 이용한 직선, 2차, 3차 적합\nparams_linear, _ = curve_fit(linear, x_data, y_data)\nparams_quadratic, _ = curve_fit(quadratic, x_data, y_data)\nparams_cubic, _ = curve_fit(cubic, x_data, y_data)\n\n# 적합된 함수의 값을 계산\ny_fit_linear = linear(x_data, *params_linear)\ny_fit_quadratic = quadratic(x_data, *params_quadratic)\ny_fit_cubic = cubic(x_data, *params_cubic)\n\n# Residual Sum of Squares 계산\nrss_linear = np.sum((y_data - y_fit_linear) ** 2)\nrss_quadratic = np.sum((y_data - y_fit_quadratic) ** 2)\nrss_cubic = np.sum((y_data - y_fit_cubic) ** 2)\n\n# 최소자승법을 이용한 직선, 2차, 3차 적합\nparams_linear, _ = curve_fit(linear, x_data, y_data)\nparams_quadratic, _ = curve_fit(quadratic, x_data, y_data)\nparams_cubic, _ = curve_fit(cubic, x_data, y_data)\n\n# 적합된 함수의 값을 계산\ny_fit_linear = linear(x_data, *params_linear)\ny_fit_quadratic = quadratic(x_data, *params_quadratic)\ny_fit_cubic = cubic(x_data, *params_cubic)\n\n# Residual Sum of Squares 계산\nrss_linear = np.sum((y_data - y_fit_linear) ** 2)\nrss_quadratic = np.sum((y_data - y_fit_quadratic) ** 2)\nrss_cubic = np.sum((y_data - y_fit_cubic) ** 2)\n\n# 그래프 그리기\nplt.figure(figsize=(10, 6))\nplt.scatter(x_data, y_data, label='data', color='black')\nplt.plot(x_data, y_fit_linear, label='Linear fit (y = {:.2f}x + {:.2f}) : RSS={:.2f}'.format(params_linear[0], params_linear[1],rss_linear), color='blue')\nplt.plot(x_data, y_fit_quadratic, label='Quardratic fit (y = {:.2f}x^2 + {:.2f}x + {:.2f}) : RSS={:.2f}'.format(params_quadratic[0], params_quadratic[1], params_quadratic[2],rss_quadratic), color='green')\nplt.plot(x_data, y_fit_cubic, label='Cubic fit (y = {:.2f}x^3 + {:.2f}x^2 + {:.2f}x + {:.2f}) : RSS={:.2f}'.format(params_cubic[0], params_cubic[1], params_cubic[2], params_cubic[3],rss_cubic), color='red')\nplt.axhline(0, color='grey', lw=0.5, ls='--')\nplt.axvline(0, color='grey', lw=0.5, ls='--')\nplt.title('fit by OLS')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.legend()\nplt.grid()\nplt.show()\n\n\n\n\n\n\n\n(3) 한계효용체감의 법칙\n한계효용 marginal utility은 재화가 증가 혹은 감소함에 따라 주관적으로 매겨지는 경제적 효용(혹은 가치)의 관계에 대한 개념으로 합리적인 경제에서 인간 행동은 자신에게 가장 시급한 욕구를 충족하는 일을 가장 먼저 하거나 가치를 두는 특성이 있다. 따라서 어떤 사람이 재화나 용역을 이용하여 효용을 얻고자 할 때 주관적으로 판단되는 욕망 충족의 정도인 효용의 가치가 높은 것부터 낮은 것 쪽으로 추구한다. 재화나 용역의 한계효용은 그 재화나 용역을 사용하는 것을 증가하거나 감소함에 따라 변화한 가치의 양을 상정한 것인데 이런 변화에서 추가의 1단위 즉 경계인 단위에서의 재화나 용역의 효용을 한계효용이라고 한다.[위키피디아]\n\n\n\n\n\n총효용 total utility 은 주어진 기간 동안 소비된 특정 상품의 모든 단위에서 얻은 총만족입니다. 한계효용 marginal utility 마지막 소비량에서 상품 소비의 1단위 변화로 인해 발생하는 총 효용의 변화이다. 더 많은 단위의 상품을 구매하면 한계 효용은 감소하기 시작하지만 총 효용은 계속해서 감소 비율이 줄어든다. 한계효용이 0가 되는 포화점 satiety에 이르렀을 때 이 지점에서의 총효용은 최대가 된다. 이 지점에서 소비가 더 증가하면 한계 효용은 음수가 되고 총 효용은 감소하기 시작한다.\n\n\n(4) Cobb-Douglas 생산함수\n\\(Q = f(K,L) = AL^{\\alpha}K^{\\beta}\\), \\(Q\\)= 생산, \\(K\\)=자본, \\(L\\)=노동, \\(A,\\alpha,\\beta\\)는 모수이다. \\(K,L\\)에 대하여 각각 편미분 하면 다음과 같다.\n  - 양변에 로그를 취한다. \\(ln(Q) = lnA + \\alpha lnL + \\beta lnK\\)\n  - \\(\\frac{\\partial(lnQ)}{\\partial L} = \\alpha\\) : 한계 노동 생산량\n  - \\(\\frac{\\partial(lnQ)}{\\partial K} = \\beta\\) : 한계 자본 생산량\n\n\n\n\n\nchapter 2. 적분\n고대 수학자들은 직선으로 이루어진 도형의 면적을 비교적 쉽게 계산할 수 있었습니다. 사각형, 삼각형, 평행사변형, 사다리꼴과 같은 도형은 밑변과 높이를 활용한 간단한 공식을 통해 면적을 구할 수 있었기 때문입니다. 그러나 곡선이 포함된 도형의 면적을 계산하는 문제는 훨씬 더 복잡한 도전 과제였습니다.\n곡선이 포함된 도형의 면적을 구하기 위해 현대 수학에서는 적분이라는 개념이 도입되었습니다. 이는 고대 그리스의 수학자 아르키메데스가 처음으로 탐구한 주제 중 하나였습니다. 아르키메데스는 곡선 아래의 면적을 구하기 위해 곡선을 아주 작은 직사각형들로 나누고, 그 면적을 합산하여 근사값을 구하는 방식을 사용했습니다. 이 과정은 시간이 지나며 점점 더 체계적으로 발전하였고, 마침내 미적분학으로 이어졌습니다.\n아이작 뉴턴과 고트프리트 라이프니츠는 아르키메데스의 아이디어를 발전시켜 적분과 미분이라는 두 가지 핵심 개념을 정립하였고, 이를 통해 곡선 아래의 면적을 정확히 계산할 수 있는 도구를 완성했습니다. 오늘날 우리가 사용하는 적분법은 이들의 연구에 기반을 두고 있으며, 곡선의 면적뿐만 아니라 물리학, 공학, 경제학 등 다양한 분야에서 중요한 역할을 하고 있습니다.\n적분은 통계학에서 확률 계산, 기대값, 분산, 베이지안 추론 등 다양한 개념과 도구에 중요한 역할을 합니다. 확률을 곡선 아래 면적으로 해석하는데서부터 시작해, 통계적 추론의 기초를 형성하는 데 적분이 필수적입니다. 이러한 적분 개념은 통계학 이론뿐만 아니라 데이터 분석, 머신러닝, 신뢰구간 계산 등 실무적인 응용에서도 널리 사용됩니다.\n\n1. 부정 적분\n함수 F(x) 가 주어진 함수 f(x) 에 대해 정의역의 모든 점에서 \\(F'(x) = f(x)\\)를 만족한다면, F(x) 를 f(x) 의 역-미분 anti-derivative 또는 원시함수 primitive function 합니다. 이는 적분이 미분의 역연산임을 의미합니다.\n적분이 미분의 역연산이라는 사실을 처음 체계적으로 증명하고 이를 수학적으로 정립한 사람들은 아이작 뉴턴(Isaac Newton)과 고트프리트 라이프니츠(Gottfried Wilhelm Leibniz)입니다. 이들은 독립적으로 미적분학의 기본 개념을 발전시켰으며, 이 과정에서 적분과 미분의 관계를 설명한 미적분학의 기본정리를 도출했습니다.\n정적분과 미분의 관계\n특정 구간에서의 정적분은 미분을 통해 함수의 값을 복원할 수 있습니다. 예를 들어, 함수 f(x) 에 대해 다음과 같은 정적분이 있을 때,\n\\(F(x) = \\int_{a}^{x}f(t)dt\\). 이를 x 에 대해 미분하면 \\(\\frac{d}{dx}F(x) = f(x)\\)\n즉, 적분을 통해 구한 누적 변화량을 다시 미분하면, 원래의 함수로 돌아갑니다.\n적분과 미분은 서로 반대되는 과정처럼 보이지만, 실제로는 상호보완적입니다. 적분은 함수의 누적적인 변화(예: 곡선 아래의 면적)를 측정하며, 미분은 순간적인 변화(예: 기울기)를 측정합니다.\n\n\n2. 정적분\n\n(1) 정적분 개념\n정적분(면적)은 부정적분(역-미분 함수)과는 다른 접근 방식에서 출발합니다. 그러나 이 두 개념은 17세기에 뉴턴(Newton)과 라이프니츠(Leibniz)에 의해 서로 밀접하게 연결되었고, 이를 통합하여 적분(integral)이라고 명명하였습니다.\n우선, 정적분의 개념을 살펴보겠습니다. 구간 [a, b]에서 함수 f(x) 아래의 면적을 어떻게 구할 수 있을까요? 이를 위해 구간 [a, b]를 여러 작은 구간으로 나눈 다음, 각 구간에서 직사각형의 면적을 계산하여 합산하는 방법을 생각할 수 있습니다. 이러한 직사각형의 면적 합은 점점 더 작은 구간으로 나눌수록 실제 면적에 근사하게 됩니다.\n\n\n\n\n\n함수와 x-축 사이에 형성된 이 면적은 정적분이라 하며, 이는 구간 [a, b]에서 함수 f(x)와 x-축 사이의 공간에 해당합니다. 직사각형을 이용해 근사한 면적은 실제 면적보다 클 수도 있고 작을 수도 있습니다. 하지만 구간을 점점 더 세분화하면, 이 근사값은 실제 정적분 값에 수렴하게 됩니다.\n정적분은 함수의 곡선 아래의 면적을 계산하는 방법으로 출발했지만, 부정적분(역-미분 함수)과의 연결을 통해 더욱 강력한 수학적 도구로 발전하였습니다.\n\n\n(2) 정적분과 부정적분의 관계\n함수 f(x) 가 구간 [a, b]에서 연속일 때:\n\n부정적분(역-미분 함수): 함수 F(x) 가 f(x) 의 부정적분이라면 \\(F'(x) = f(x)\\)\n정적분(구간의 면적): 함수 f(x) 의 정적분은 구간 [a, b]에서 f(x) 와 x -축 사이의 면적을 나타냅니다. \\(\\int_{a}^{b}f(x)dx\\)\n뉴턴-라이프니츠 정리: 부정적분과 정적분은 다음과 같이 연결됩니다. \\(\\int_{a}^{b}f(x)dx = F(b) - F(a)\\)\n여기서 F(x) 는 f(x) 의 부정적분입니다.\n\n이 정리는 정적분(구간에서의 면적 계산)이 부정적분(역-미분 함수)을 사용하여 계산될 수 있음을 보여줍니다.\n\n\n(3) 정적분 규칙\n특정 점에서의 확률\n\\[\\int_{a}^{a}f(x)dx = 0\\]\n이는 구간의 길이가 0 일 때, 정적분의 결과가 항상 0 임을 나타냅니다(통계적으로: 연속 확률변수에서 특정 점에서의 확률은 0 이다).\n구간 순서 반대\n\\[\\int_{a}^{b}f(x)dx = - \\int_{b}^{a}f(x)dx\\]\n구간의 순서를 바꾸면 정적분의 부호가 반대가 됩니다.\n상수 배율\n\\[\\int_{a}^{b}c \\cdot f(x)dx = c\\int_{a}^{b}f(x)dx(\\text{c is constant})\\]\n적분 내부에 상수가 곱해져 있을 경우, 상수를 적분 기호 밖으로 꺼낼 수 있습니다.\n합과 차\n\\[\\int_{a}^{b}\\left( f(x) \\pm g(x) \\right)dx = \\int_{a}^{b}f(x)dx \\pm \\int_{a}^{b}g(x)dx\\]\n적분은 덧셈과 뺄셈 연산에 대해 분배법칙을 따릅니다.\nDomination Rule\n만약 \\(f(x) \\geq 0\\)가 구간 [a, b]에서 항상 성립하면\n\\(\\int_{a}^{b}f(x)dx \\geq 0\\) 이다. 통계적으로 확률변수의 분포 함수는 항상 0 이상 이므로, 확률값은 항상 0 이상이다.\n부등식 관계\n만약 \\(f(x) \\leq g(x)\\)가 구간 [a, b]에서 항상 성립하면\n\\(\\int_{a}^{b}f(x)dx \\leq \\int_{a}^{b}g(x)dx\\) 이다.\n구간 쪼개기\n\\[\\int_{a}^{c}f(x)dx + \\int_{c}^{b}f(x)dx = \\int_{a}^{b}f(x)dx\\]\n적분 구간을 나누어 계산할 수 있습니다.\n확률밀도함수 전체 구간\n\\(\\int_{- \\infty}^{\\infty}f(x)dx = 1\\). 확률밀도함수(PDF)는 전체 구간에서의 적분, 확률의 총합이 1 임을 나타냅니다.\n지수함수 적분\n\\[\\int a^{x}dx = \\frac{a^{x}}{\\ln a} + C(a &gt; 0,a \\neq 1)\\]\n\\[\\int e^{x}dx = e^{x} + C\\]\n로그함수 적분\n\\[\\int\\log_{a}(x)dx = \\frac{1}{\\ln(a)}\\left( x\\ln(x) - x \\right) + C\\]\n\\[\\int\\ln(x)dx = x\\ln(x) - x + C\\]\n특수한 적분\n\\[\\int\\frac{1}{x} = ln|x| + C\\]\n치환적분\n함수 g(x) 가 x 에 대한 미분가능한 함수이고, f(u) 가 u = g(x) 에 대한 함수라고 가정하겠습니다.\n\\[\\int f(g(x)) \\cdot g'(x)dx = \\int f(u)du\\]\n\\[u = g(x) , du = g'(x)dx\\]\n【사례】 \\(\\int x \\cdot e^{x^{2}}dx = \\frac{1}{2}e^{x^{2}} + C\\)\n\\(u = x^{2}\\)로 치환하면, \\(du = 2xdx\\). 따라서 \\(xdx = \\frac{1}{2}du\\)\n\\(\\int x \\cdot e^{x^{2}}dx = \\int e^{u} \\cdot \\frac{1}{2}du = \\frac{1}{2}\\int e^{u}du\\)=\\(\\frac{1}{2}\\int e^{u}du = \\frac{1}{2}e^{u} + C\\)\n\\(u = x^{2}\\) 이므로 \\(\\int x \\cdot e^{x^{2}}dx = \\frac{1}{2}e^{x^{2}} + C\\) 이다.\n부분적분\n함수 u(x) 와 v(x) 가 미분 가능할 때, 다음 공식이 성립합니다:\n\\[\\int udv = uv - \\int vdu\\]\n\n\\(u\\): 미분할 함수 (\\(u \\rightarrow du\\))\n\\(dv\\): 적분할 함수 (\\(dv \\rightarrow v\\))\n\n【사례】 \\(\\int xe^{x}dx\\)\n1) 함수 선택: \\(u = x,dv = e^{x}dx\\)\n2) 미분 및 적분: \\(u \\rightarrow du = dx\\),\\(dv \\rightarrow v = e^{x}\\)\n3) 부분적분 공식 적용: \\(\\int xe^{x}dx = uv - \\int vdu\\)\n\\(= xe^{x} - \\int e^{x}dx\\)\\(= xe^{x} - e^{x} + C\\).\n【사례】 \\(\\int_{0}^{1}x^{2} + \\sqrt{x}dx\\) 구하시오.\n\\(f(x) = x^{2} + \\sqrt{x}\\)이므로 \\(F(x) = \\frac{1}{3}x^{3} + \\frac{2}{3}x^{\\frac{3}{2}}\\)\n\\(F(1) = 1\\), \\(F(0) = 0\\)이므로 1이다.\n\\[\\int_{0}^{1}x^{2} + \\sqrt{x}dx = \\frac{1}{3}x^{3} + \\frac{2}{3}x^{\\frac{3}{2}}\\rbrack_{0}^{1} = 1 - 0 = 1\\]\n#부정적분\nfrom sympy import *\nx=Symbol('x')\nintegrate(x**2+x**(0.5), x)\n\\[ x^3/3 + 0.66667x^{1.5}\n\\]\n#정적분\nfrom scipy.integrate import quad\ndef integrand(x):\n   return x**2+x**(0.5)\nquad(integrand,0, 1)\n【결과】 첫번째 값은 적분값이고 두 번째는 적분 값을 얼마나 근사하게 계산하였는지 값이다. 완벽한 값이면 0이어야 하나 출력된 값은 0.0(14개)11…이다. root는 실제 근이다. (1.0, 1.1102230246251565e-15)\n【사례】 표준 정규확률분포함수\\(\\int_{0}^{\\infty}\\frac{1}{\\sqrt{2\\pi}}e^{- \\frac{x^{2}}{2}}dx\\) 구하시오.\n#부정적분\nfrom sympy import *\nimport numpy as np\nx=Symbol('x')\nintegrate(1/(2*np.pi)**0.5*exp(-x**2/2), x)\n\\[\n0.199471140200716 \\sqrt{2} \\sqrt{\\pi} \\, \\mathrm{erf}\\left( \\frac{\\sqrt{2}x}{2} \\right)\n\\]\nimport numpy as np\n#정적분\nfrom scipy.integrate import quad\ndef integrand(x):\n   return 1/(2*np.pi)**0.5*exp(-x**2/2)\nquad(integrand,0,np.inf)\n【결과】 (0.49999999999999983, 5.08909572547112e-09)\n\n\n(4) 표적분 tabular integral\n표 적분은 부분적분을 용이하게 한다. 미분 부분 \\(f(x)\\)는 미분하면서 차수가 용이해야 하고, 적분함수 \\(g(x)\\)는 용이하게 적분할 수 있어야 한다.\n(방법1) 미분 부분이 0이 될 때까지 미분과 적분을 반복 시행한다.\n\\[\\int_{a}^{b}udv = (1)*(a) - (2)*(b) + (3)(c)...\\rbrack_{a}^{b}\\]\n(방법2)한 번만 미분하고 \\(\\int_{a}^{b}udv = (1)*(a) - \\int_{a}^{b}(2)*(b)dx\\)\n\n\n\n\n\n【예제】 \\(\\int_{0}^{\\infty}xe^{- x}dx\\) 표 적분하시오.\n\n\n\n\n\n(방법1) \\(x( - e^{- x}) - e^{- x}\\rbrack_{0}^{\\infty} = 1\\)\n(방법2) \\(x( - e^{- x})\\rbrack_{0}^{\\infty} - \\int_{0}^{\\infty} - e^{- x} = 1\\)으로 계산한다.\n【예제】 \\(\\int_{1}^{2}ln(x)dx\\) 표 적분하시오.\n\n\n\n\n\n\\[\\int_{1}^{2}ln(x)dx = ln(x)x\\rbrack_{1}^{2} - \\int_{0}^{1}1dx = 2ln(2) - ln(1) - x\\rbrack_{0}^{1} = 0.386\\]\n\n\n\n3. 적분 응용\n연속형 확률분포의 확률밀도함수\n\n\n\n\n\n연속형 확률변수 X 의 확률밀도함수 f(x) 는 특정 구간에서 확률을 계산하는 데 사용됩니다. 이때 확률은 적분을 통해 구합니다:\n\\[P(a \\leq X \\leq b) = \\int_{a}^{b}f(x)dx\\]\nf(x) 는 음수가 아니며, 전체 구간에서의 적분값은 항상 1이 됩니다:\n\\[\\int_{- \\infty}^{\\infty}f(x)dx = 1\\]\n【예제】 정규분포 N(0, 1) 에서 \\(P( - 1 \\leq Z \\leq 1) = \\int_{- 1}^{1}\\phi(z)dz\\) 이다. 여기서 \\(\\phi(z) = \\frac{1}{\\sqrt{2\\pi}}e^{- z^{2}/2}\\)는 표준정규분포의 확률밀도함수입니다.\n누적확률분포함수 cumulative probability density fuction\n\n\n\n\n\n기대값\n  연속형 확률변수 X 의 확률밀도함수 f(x)라 하면 기대값은 \\(E(X) = \\int xf(x)dx\\) 이다.\n적분과 백분위값\n  백분위값 percentile은 확률분포에서 특정 비율의 누적 확률을 기준으로 하는 값입니다. P번째 백분위값은 확률변수 X의 값 \\(X_{P}\\)로, 확률변수가 \\(X_{P}\\)이하일 확률이 \\(\\frac{P}{100}\\)이 되는 값입니다.\n\\[F(x_{P}) = \\int_{- \\infty}^{x_{P}}f(x)dx = \\frac{P}{100}\\]\n  - \\(f(x)\\): 확률밀도함수(PDF)\n  - \\(F(x)\\): 누적분포함수(CDF)\n  - \\(x_{P}\\): \\(P\\)번째 백분위값"
  },
  {
    "objectID": "notes/math/function.html",
    "href": "notes/math/function.html",
    "title": "수학의 기초 1. 함수",
    "section": "",
    "text": "chapter 1. 기초\n\n1. 함수와 통계학\n함수는 통계학에서 데이터를 설명하고 모델링하는 수단이며, 이론적 개념을 수학적으로 표현하는 핵심 도구이다. 데이터 간의 관계를 나타내고, 확률분포, 추정, 검정 등 다양한 통계 기법에서 필수적인 역할을 수행한다.\n통계함수: 통계함수는 독립변수(\\(x\\))와 종속변수(\\(y\\)) 데이터 간 관계를 설명한다. \\(y = f(x) + e\\)로 표현되며 \\(e\\)는 오차항이다.\n확률밀도함수: 확률밀도함수 \\(p(x)\\)는 확률변수의 확률이 함수값이다.\n기대값: 확률변수의 평균적인 값이다. \\(E(X) = \\sum xp(x)\\)\n\n\n2. 함수와 시리즈\n시리즈는 복잡한 함수를 단순한 다항식으로 근사하거나, 함수의 특성을 분석하는 데 사용된다. 시리즈는 유한하거나 무한한 항들로 이루어진 수열의 합으로 정의된다.\n유한 시리즈: \\(S_{n} = {\\sum_{i = 1}^{n}}a_{k}\\)\n무한 시리즈: \\(S_{\\infty} = {\\sum_{i = 1}^{\\infty}}a_{k}\\)\n이항시리즈 binomial series\n\\[(a + b)^{n} = a^{n} + \\binom{n}{1}a^{n - 1}b + ... + \\binom{n}{n - 1}ab^{n - 1} + b^{n}\\]\n\n특수한 경우\n\n\\[\\frac{1}{(1 + x)^{2}} = - 1 + 2x - 3x^{2} + 4x^{3} - ...\\]\n\\[\\frac{1}{1 + x} = 1 - x + x^{2} - x^{3} + x^{4} - ...\\]\n지수시리즈 exponential series\n\\[e^{x} = 1 + x + \\frac{x^{2}}{2!} + \\frac{x^{3}}{3!} + ...\\]\n\\[e^{x} = lim_{n \\rightarrow}^{\\infty}(1 + \\frac{x}{n})^{n}\\]\n\\[ln(1 + x) = x - \\frac{{}^{2}}{2} + \\frac{x^{3}}{3} - \\frac{x^{4}}{4} + ..., - 1 &lt; x &lt; 1\\]\n산술시리즈 arithmetic series\n\\[S_{n} = a + (a + d) + (a + 2d) + \\cdots + \\lbrack a + (n - 1)d\\rbrack\\]\n      - \\(a\\): 첫 번째 항, \\(d\\): 공차(항 사이의 일정한 차이), \\(n\\): 항의 개수\n\\[S_{n} = \\frac{n}{2}\\lbrack 2a + (n - 1)d\\rbrack\\]\n기하시리즈 geometric series\n\\[S_{n} = a + ar + ar^{2} + \\cdots + ar^{n - 1}\\]\n     - \\(a\\): 첫 번째 항, \\(r\\): 공비(항 사이의 일정한 차이)\n\\[S_{n} = \\frac{a(1 - r^{n})}{1 - r},r \\neq 1\\]\n무한 기하시리즈: \\(S_{n} = \\frac{a}{1 - r}, - 1 &lt; r &lt; 1\\)\n\n\n3. 통계학 주요상수\n지수 exponent \\(e\\)\n  자연로그 함수의 밑으로 정의되며, 무한 급수로 표현된다.\n  \\(e \\approx 2.71828182845904\\ldots\\)(무리수)\n  통계학의 주요 확률분포함수(정규분포, 포아송분포)의 항이다.\n자연상수 \\(ln2\\)\n\\[\\ln 2 \\approx 0.69314718056\\ldots \\text{(무리수)}\\]\n  정보 이론: 1비트의 정보. 이진수 체계와 로그 연산.\n황금비 \\(\\phi \\approx 1.61803398874989\\ldots\\)\n  \\(a/b = (a + b)/a\\)를 만족하는 비율 \\(\\phi = \\frac{1 + \\sqrt{5}}{2}\\)\n오일러상수: 조화급수와 자연로그의 차이로 정의된다.\n\\[\\gamma = \\lim_{n \\rightarrow \\infty}\\left( \\overset{n}{\\sum_{k = 1}}\\frac{1}{k} - \\ln n \\right) \\approx 0.577215664901532\\ldots\\]\n\n기호\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n소문자\nα\nβ\nγ\nδ\nε\nζ\nη\nθ\n\n\n대문자\nΑ\nΒ\nΓ\nΔ\nΕ\nΖ\nΗ\nΘ\n\n\n발음\nalpha\nbeta\ngamma\ndelta\nepsilon\nzeta\neta\ntheta\n\n\n소문자\nι\nκ\nχ\nλ\nμ\nν\nξ\nο\n\n\n대문자\nΙ\nΚ\nΧ\nΛ\nΜ\nΝ\nΞ\nΟ\n\n\n발음\niota\nkappa\nchi\nlambda\nmu\nnu\nxi(ksi)\nomicron\n\n\n소문자\nπ\nρ\nσ\nτ\nυ\nϕ\nψ\nω\n\n\n대문자\nΠ\nΡ\nΣ\nΤ\nΥ\nΦ\nΨ\nΩ\n\n\n발음\npi\nrho\nsigma\ntau\nupsilon\nphi\npsi\nomega\n\n\n\n\n\n\n\n\nchapter 2. 좌표와 직선방정식\n\n1. 이차원평면과 데카르트 좌표\n\n\n\n\n\n이차원 평면에서 모든 점은 숫자 좌표로 coordinate 표현할 수 있으며, 점들의 집합으로 이루어진 선이나 곡선은 좌표방정식으로 나타낼 수 있다. 이를 위해 이차원 평면에는 두 개의 직선이 설정된다.. 수평선인 \\(x\\)-축 axis과 수직선인 \\(y\\)-축이다. 이 두 직선은 원점에서 직각으로 교차하며, 원점은 두 축의 기준점이 된다.\n원점을 기준으로 \\(x\\)-축에서 \\(a\\)만큼, \\(y\\)-축에서 \\(b\\)만큼 떨어진 점의 좌표는 \\((a,b)\\)로 표기된다. 이러한 표기 방식은 데카르트 좌표라고 한다. 여기서 \\(a\\)와 \\(b\\)는 각각 \\(x\\)-좌표와 \\(y\\)-좌표를 나타내며, 이 값들은 모두 실수 값으로 구성된다.\n데카르트 Cartesian 좌표계는 이차원 평면에서 점의 위치를 명확하고 직관적으로 나타내는 데 사용되며, 수학적 분석 및 응용의 기초가 된다. 이를 활용하면 점, 선, 곡선, 그리고 다양한 기하학적 형태를 방정식으로 표현하고, 이를 통해 여러 문제를 해결할 수 있다.\n\n\n2. 직선과 증가\n\n직선\n두 점을 가장 짧은 거리로 연결하는 선을 직선이라고 한다. 직선은 두 점 사이의 최단 경로로 정의되며, 그 위에는 무수히 많은 점이 존재한다. 좌표평면에서 직선은 중요한 기하학적 구조로, 점과 점 사이의 관계를 나타내는 기본 도구이다.\n\n\n증가량 (Increment)\n\n\n\n\n\n좌표평면에서 두 점 \\((x_{1},y_{1})\\)과 \\((x_{2},y_{2})\\)의 이동을 고려할 때, x-좌표와 y-좌표의 변화량을 각각 증가량이라고 한다.\nx-좌표의 증가량: \\(\\Delta x = x_{2} - x_{1}\\)\ny-좌표의 증가량: \\(\\Delta y = y_{2} - y_{1}\\)\n증가량의 부호와 크기는 두 점의 좌표 차이에 의해 결정되며, x-좌표나 y-좌표의 변화 방향을 나타낸다.\n\n\n기울기 slope\n증가량은 두 점을 지나는 직선의 기울기를 계산하는 데 활용된다. 기울기 m은 두 점 사이의 x-좌표의 증가량에 대한 y-좌표의 증가량의 비율로 정의되며, 다음과 같은 식으로 표현된다:\n\\[m = \\frac{\\Delta y}{\\Delta x} = \\frac{y_{2} - y_{1}}{x_{2} - x_{1}},\\Delta x \\neq 0\\]\n    - \\(m &gt; 0\\): 직선이 오른쪽으로 올라간다.\n    - \\(m &lt; 0\\): 직선이 오른쪽으로 내려간다.\n    - \\(m = 0\\): 직선이 수평이다.\n    - \\(m\\)이 정의되지 않음 (\\(\\Delta x = 0\\)): 직선이 수직이다.\n\n\n수평 parallel과 수직 perpendicular\n두 직선 \\(L_{1}\\)과 \\(L_{2}\\)의 기울기가 동일하면, 즉 \\(m_{1} = m_{2}\\)이면 두 직선은 서로 평행 하다고 한다. 이 경우, 두 직선은 교차하지 않으며, 동일한 방향으로 뻗어 있다.\n두 직선 \\(L_{1}\\)과 \\(L_{3}\\)의 기울기의 곱이 -1이면, 즉 \\(m_{1} \\cdot m_{2} = - 1\\)이면 두 직선은 서로 수직하다고 한다. 이는 두 직선이 교차할 때 \\(90^{\\circ}\\)의 각을 이루는 경우이다.\n\n\n3. 직선 방정식 linear equation\n직선 방정식은 직선 위의 모든 점의 좌표를 만족하며, 직선 이외의 점의 좌표에서는 만족하지 않는 방정식이다. 좌표평면에서 직선은 절편 intercept과 기울기 slope를 이용해 다음과 같은 일반적인 형태로 표현된다. \\(y = bx + a\\)\n\n\\(b\\): 직선의 기울기, \\(a\\): y-축과 교차하는 절편\n\n\n\n직선 구성요소\n기울기 \\(b\\)는 직선이 얼마나 가파르게 증가하거나 감소하는지를 나타내며, x-좌표의 변화량에 대한 y-좌표의 변화량의 비율로 정의된다:\n\\[b = \\frac{\\Delta y}{\\Delta x}\\]\n    - b &gt; 0: 직선이 오른쪽으로 올라간다.\n    - b &lt; 0: 직선이 오른쪽으로 내려간다.\n    - b = 0: 직선이 수평이다.\n절편 a는 직선이 y-축과 만나는 점의 y-좌표를 나타낸다. x = 0일 때, 직선 방정식에서 y = a가 된다.\n\n\n수평선 horizontal line\n기울기 b = 0인 경우, 직선은 수평선이 된다. 이러한 직선의 방정식은 \\(y = a\\)이다. 이 직선은 x-축과 평행하며, y-축 상에서 y = a를 지난다.\n\n\n수직선 vertical line\ny-축과 평행한 직선의 방정식으로 \\(x = c\\)이다. 이 직선은 x-축과 x = c에서 교차한다. 기울기가 정의되지 않으며, 수직선은 y-축과 항상 평행하다.\n\n\n\n\n\nchapter 3. 함수란?\n\n1. 함수 정의\n함수는 두 집합 사이의 특정 규칙에 따라 값을 대응시키는 관계를 나타낸다. 함수는 정의역과 치역으로 구성되며, 정의역의 각 원소에 대해 치역의 단 하나의 원소만 대응된다. 이를 통해 y가 x에 의해 결정된다고 표현하며, 수학적으로 다음과 같이 나타낸다. \\(y = f(x)\\)\n이는 ”y는 x의 함수이다”라고 읽는다.\n\n정의역 domain: 정의역은 함수에서 x가 가질 수 있는 값들의 집합을 말한다. 즉, 함수 f(x)가 유효하게 정의될 수 있는 모든 입력값의 집합이다.\n치역 range: 치역은 함수가 출력할 수 있는 값들의 집합이다. 정의역의 원소 x가 함수 f를 통해 출력되는 값 y = f(x)의 모임이 치역이다.\n대응 규칙: 함수는 정의역의 각 원소를 치역의 한 원소에 대응시키는 규칙을 가지고 있다. 각 정의역의 값 x는 치역에서 정확히 하나의 값 y에 대응해야 한다. (2)번은 동일 x-값에 대하여 2개 y-값이 대응되므로 함수가 아니고 다른 모든 것은 함수이다.\n\n\n\n\n\n\n\n2. 우함수와 기함수\n우함수 even function: 함수 \\(f(x)\\)가 다음 조건을 만족하면 우함수라 한다.\n\\[f( - x) = f(x)\\text{모든}x \\in \\text{정의역(domain)}\\]\n우함수는 y-축을 기준으로 대칭적이다. 즉, 그래프의 왼쪽 부분을 y-축을 따라 접으면 오른쪽 부분과 정확히 일치한다.\n기함수 odd function: 함수 \\(f(x)\\)가 다음 조건을 만족하면 기함수라 정의한다.\n\\[f( - x) = - f(x)\\text{모든}x \\in \\text{정의역(domain)}\\]\n기함수는 원점을 기준으로 대칭적이다. 즉, 그래프를 원점을 중심으로 180° 회전시키면 동일한 모양이 된다.\n\n\n3. 함수 종류\n\n(1) 함성함수 Composite Function\n합성함수는 두 함수 f(x)와 g(x)가 주어졌을 때, 함수 g(x)의 출력값이 함수 f(x)의 입력값으로 사용되는 새로운 함수이다. 이를 다음과 같이 나타낸다. \\((f \\circ g)(x) = f(g(x))\\)\n    - g(x): 먼저 적용되는 함수.\n    - f(x): g(x)의 출력값을 입력값으로 사용하는 함수.\n    - \\((f \\circ g)(x)\\): f(x)와 g(x)의 합성함수.\n합성함수 \\((f \\circ g)(x)\\)의 정의역은 g(x)와 f(x)가 동시에 유효하게 정의되는 입력값으로 구성된다. 즉, x는 g(x)의 정의역에 속하고, g(x)의 출력값은 f(x)의 정의역에 속해야 한다.\n\\((f \\circ g)(x)\\)는 다음 두 단계를 거친다:\n    - 먼저 x에 대해 g(x)를 계산하고 그런 다음, f(x)에 g(x)를 대입하여 f(g(x))를 계산한다.\n\\[f(x) = 2x + 1,g(x) = x^{2}\\]\n\\[f(g(x)) = f(x^{2}) = 2x^{2} + 1\\]\n\\[g(f(x)) = g(2x + 1) = (2x + 1)^{2}\\]\n\n\n(2) 절대값 함수\n숫자 x의 절대값(absolute value)은 x의 크기(거리를 나타냄)를 의미하며, 항상 0 이상의 값을 가진다. 절대값은 다음과 같이 정의된다.\n\\[|x| = \\{\\begin{matrix}\nx, & \\text{if}x \\geq 0 \\\\\n- x, & \\text{if}x &lt; 0\n\\end{matrix}\\]\n절대값은 숫자 x와 0 사이의 거리로 해석된다. 절대값의 결과는 항상 양수이거나 0이다.\n\n\n(3) 정수함수 integer function\n정수 함수는 숫자 x를 넘지 않는 최대 정수를 반환하는 함수이다. 이를 바닥함수 floor function라고도 하며, 다음과 같이 정의된다.\n\\[\\lfloor x\\rfloor = \\text{최대 정수}n\\text{such that}n \\leq x\\]\n    - \\(\\lfloor x\\rfloor\\): x를 넘지 않는 가장 큰 정수.\n    - \\(\\lfloor x\\rfloor\\)는 항상 \\(n \\leq x &lt; n + 1\\)을 만족한다.\n\n\n\n4. 함수의 사칙연산\n두 함수 f(x)와 g(x)가 주어졌을 때, 이들 함수에 대해 덧셈, 뺄셈, 곱셈, 나눗셈과 같은 사칙연산을 정의할 수 있다. 각 연산은 정의역에서 두 함수의 값에 기반하여 계산된다.\n함수의 덧셈/뺄셈: \\[(f \\pm g)(x) = f(x) \\pm g(x)\\]\n\n정의역: f(x)와 g(x)가 동시에 정의된 구간.\n결과: f(x)의 값과 g(x)의 값을 더한(뺀) 결과.\n\n함수의 곱셈: \\[(f \\cdot g)(x) = f(x) \\cdot g(x)\\]\n\n정의역: f(x)와 g(x)가 동시에 정의된 구간.\n결과: f(x)와 g(x)의 값을 곱한 결과.\n\n함수의 나눗셈: \\[\\left( \\frac{f}{g} \\right)(x) = \\frac{f(x)}{g(x)},g(x) \\neq 0\\]\n\n정의역: f(x)와 g(x)가 동시에 정의되고, \\(g(x) \\neq 0\\)인 구간.\n결과: f(x)의 값을 g(x)의 값으로 나눈 결과.\n\n\n\n\nchapter 4. 함수의 응용 및 극한\n\n1. 함수의 통계 응용\n\n(1) 확률밀도함수 \\(f(x)\\)\n연속형확률변수의 분포를 나타내는 함수로, 특정 구간 내에서 값이 나타날 확률의 상대적인 가능성을 표현한다.\n    - 확률밀도함수 정의: \\(f(x) \\geq 0,\\int_{- \\infty}^{\\infty}f(x)dx = 1\\)\n    - 정규분포의 확률밀도함수: \\(f(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^{2}}}e^{- \\frac{(x - \\mu)^{2}}{2\\sigma^{2}}}\\)\n    - 데이터의 분포, 확률 계산 \\(P(a \\leq X \\leq b) = \\int_{a}^{b}f(x)dx\\)\n\n\n(2) 누적확률밀도함수\n확률변수가 특정 값 이하일 확률을 나타내는 함수이다.\n\n누적확률밀도함수 정의: \\(F(x) = P(X \\leq x) = \\int_{- \\infty}^{x}f(t)dt\\)\n정규분포 CDF: \\(F(x) = \\frac{1}{2}\\left\\lbrack 1 + \\text{erf}\\left( \\frac{x - \\mu}{\\sqrt{2}\\sigma} \\right) \\right\\rbrack\\)\n분위수 결정: \\(P(X \\leq x_{p}) = p\\) 만족하는 \\(X_{p}\\)를 찾음.\n\n\n\n(3) 회귀모형\n함수는 독립변수와 종속변수 간의 관계를 모델링하는 데 사용된다. 회귀모형은 함수 형태로 데이터의 추세를 설명한다.\n    - 선형회귀: \\(y = \\beta_{0} + \\beta_{1}x + \\epsilon\\)\n    - 비선형 회귀: \\(y = ae^{bx} + \\epsilon\\)\n    - 변수 간 관계 분석, 예측 모델 구축\n\n\n(4) 생존분석\n생존분석에서는 생존시간 분포를 분석하는 데 함수가 사용된다.\n\n생존survival 함수: \\(S(t) = P(T &gt; t) = 1 - F(t)\\)\n위험hazard 함수: \\(h(t) = \\frac{f(t)}{S(t)}\\)\n제품 수명 분석, 의료 데이터에서 생존 확률 평가\n\n\n\n(5) 시계열분석\n함수는 시간에 따른 데이터의 변화를 모델링하고 분석하는 데 사용된다.\n\n자기회귀 모델: \\(X_{t} = \\phi_{1}X_{t - 1} + \\phi_{2}X_{t - 2} + \\cdots + \\epsilon_{t}\\)\n주식 시장 예측, 온도 변화 모델링.\n\n\n\n(6) 함수와 몬테카를로 시뮬레이션\n함수는 확률 분포로부터 난수를 생성하여 복잡한 통계 문제를 해결하는 데 사용된다.\n\n\\(\\pi\\) 값 추정: \\(f(x) = \\sqrt{1 - x^{2}},\\text{for}x \\in \\lbrack 0,1\\rbrack\\)\n\n\n\n\n2. 함수의 극한\n\n(1) 극한 정의\n임의의 \\(\\varepsilon &gt; 0\\)가 주어졌을 때, 모든 \\(x\\)가 특정 값 \\(a\\)에 충분히 가까워질 때 \\((0 &lt; |x - a| &lt; \\delta),f(x)\\)가 특정 값 \\(L\\)에 가까워진다면, 함수 \\(f(x)\\)의 극한은 존재하며 그 극한값은 \\(L\\)이라고 정의한다. 이를 수학적으로 표현하면 \\(\\lim_{x \\rightarrow a}f(x) = L\\) 이다.\n\n\\(\\varepsilon\\): \\(f(x)\\)와 \\(L\\)사이의 허용 오차.\n\\(\\delta\\): \\(x\\)와 \\(a\\) 사이의 거리 제한.\n\n엄밀한 정의 (\\(\\varepsilon - \\delta\\)정의)\\(\\forall\\varepsilon &gt; 0,\\exists\\delta &gt; 0\\text{such that}0 &lt; |x - a| &lt; \\delta \\Longrightarrow |f(x) - L| &lt; \\varepsilon\\)이 의미는 \\(x\\)와 \\(a\\)에 충분히 가까워지면 \\((|x - a| &lt; \\delta)\\) 함수 \\(f(x)\\)의 값이 \\(L\\)에 충분히 가까워짐 \\((|f(x) - L| &lt; \\varepsilon)\\)을 보장한다.\n\n\n(2) 함수값과 극한값\n함수값 \\(f(a)\\)는 함수가 특정 점 \\(x = a\\)에서 실제로 가지는 값이다. 반면, 극한값 \\(\\lim_{x \\rightarrow a}f(x)\\)는 \\(x\\)가 \\(a\\)에 가까워질 때 \\(f(x)\\)가 수렴하는 값을 나타낸다. 함수값과 극한값은 다를 수 있으며, 함수가 \\(x = a\\)에서 정의되지 않아도 극한값은 존재할 수 있다.\n함수값 \\(f(a) = k\\): 함수가 \\(x = a\\)에서 정의되어 있다면 \\(f(a)\\)는 \\(k\\), 특정 값을 가진다.\n극한값: 극한값은 좌극한(left-hand limit)과 우극한(right-hand limit)에 따라 달라질 수 있다:\n    - 좌극한 \\(L_{2}\\): \\(\\lim_{x \\rightarrow a^{-}}f(x) = L_{2}\\)\n    - 우극한 \\(L_{1}\\): \\(\\lim_{x \\rightarrow a^{+}}f(x) = L_{1}\\)\n    - 전체 극한은 좌극한과 우극한이 동일할 때 존재한다,\n\n\n(3) 연속함수 정의\n함수 \\(f(x)\\)가 \\(x = a\\)에서 연속하려면 다음 세 가지 조건을 모두 만족해야 한다:\n  1. \\(f(a)\\)가 정의되어 있어야 한다.\n  2. \\(\\lim_{x \\rightarrow a}f(x)\\)가 존재해야 한다.\n  3. 함수값과 극한값이 일치해야 한다. \\(\\lim_{x \\rightarrow a}f(x) = f(a)\\)\n\n\n(4) 극한 계산 규칙\n상수함수의 극한: \\(\\lim_{x \\rightarrow a}c = c\\), 상수 함수의 극한은 상수 자신이다.\n항등함수의 극한: \\[\\lim_{x \\rightarrow a}x = a\\]\n선형성: 극한 연산은 선형성을 가진다:\n\\[\\lim_{x \\rightarrow a}\\lbrack f(x) \\pm g(x)\\rbrack = \\lim_{x \\rightarrow a}f(x) \\pm \\lim_{x \\rightarrow a}g(x)\\]\n곱셈: 두 함수의 곱의 극한은 각 함수의 극한의 곱과 같다.\n\\[\\lim_{x \\rightarrow a}\\lbrack f(x) \\cdot g(x)\\rbrack = \\left( \\lim_{x \\rightarrow a}f(x) \\right) \\cdot \\left( \\lim_{x \\rightarrow a}g(x) \\right)\\]\n나눗셈: 두 함수의 나눗셈의 극한은 각 함수의 극한의 나눗셈과 같다 (분모가 0이 아닌 경우)\n\\[\\lim_{x \\rightarrow a}\\frac{f(x)}{g(x)} = \\frac{\\lim_{x \\rightarrow a}f(x)}{\\lim_{x \\rightarrow a}g(x)},\\lim_{x \\rightarrow a}g(x) \\neq 0\\]\n거듭제곱\n  \\(\\lim_{x \\rightarrow a}\\lbrack f(x)\\rbrack^{n} = \\left( \\lim_{x \\rightarrow a}f(x) \\right)^{n}\\), 여기서 \\(n\\)은 정수이다.\n루트\n  \\[\\lim_{x \\rightarrow a}\\sqrt[n]{f(x)} = \\sqrt[n]{\\lim_{x \\rightarrow a}f(x)},\\text{if}\\lim_{x \\rightarrow a}f(x) \\geq 0\\]\n합성함수의 극한 (연쇄법칙): 만약 \\(g(x)\\) 의 극한이 \\(a\\)로 접근할 때 \\(b\\)이고, \\(f(x)\\)가 \\(b\\)에서 연속이면\n\\(\\lim_{x \\rightarrow a}f(g(x)) = f\\left( \\lim_{x \\rightarrow a}g(x) \\right)\\) 이다.\nL’Hôpital’s Rule의 정의: 함수 f(x)와 g(x)가 x \\to a에서 각각 0/0 형태 또는 \\infty/\\infty 형태를 가지는 경우, 두 함수의 극한은 다음과 같이 계산할 수 있다:\n\\[\\lim_{x \\rightarrow a}\\frac{f(x)}{g(x)} = \\lim_{x \\rightarrow a}\\frac{f'(x)}{g'(x)},\\text{if}\\lim_{x \\rightarrow a}\\frac{f'(x)}{g'(x)}\\text{exists.}\\]\n\n형태: \\(\\frac{0}{0}\\) 또는 \\(\\frac{\\infty}{\\infty}\\)와 같은 불정형 형태를 가져야 한다.\n미분 가능성: \\(f(x)\\)와 \\(g(x)\\)는 \\(x \\rightarrow a\\)에서 미분 가능해야 한다.\n분모의 도함수가 0이 아님: \\(g'(x) \\neq 0\\)인 구간에서 적용 가능\n\\(\\frac{0}{0}\\) 형태: \\(\\lim_{x \\rightarrow 0}\\frac{\\sin(x)}{x} = \\lim_{x \\rightarrow 0}\\frac{\\cos(x)}{1} = \\cos(0) = 1\\)\n\\(\\frac{\\infty}{\\infty}\\) 형태: \\(\\lim_{x \\rightarrow \\infty}\\frac{x}{e^{x}} = \\lim_{x \\rightarrow \\infty}\\frac{1}{e^{x}} = 0\\)\n\n무한대 있는 극한: \\(x\\)가 무한대 \\(\\infty\\)혹은 \\(- \\infty\\)로 접근할 때 함수 \\(f(x)\\)의 극한을 구하는 규칙이다.\n\\[lim_{x \\rightarrow \\pm \\infty}\\frac{1}{x} = 0\\]\n\\(lim_{x \\rightarrow \\pm \\infty}c = c\\), \\(c\\)는 상수\n함수가 분수의 형태를 가지면 분모의 가장 큰 \\(x\\)차수로 나누고 위의 규칙을 이용하면 된다.\n특정 함수의 극한\n\n지수 함수: \\(\\lim_{x \\rightarrow \\infty}e^{- x} = 0\\)\n삼각 함수: \\(\\lim_{x \\rightarrow 0}\\frac{\\sin(x)}{x} = 1\\), \\(\\lim_{x \\rightarrow 0}\\frac{1 - \\cos(x)}{x^{2}} = \\frac{1}{2}\\)\n로그 함수: \\(\\lim_{x \\rightarrow \\infty}\\ln(x) = \\infty\\)\n\n\n\n\n3. 수렴 convergence\n수렴의 정의: 함수 \\(f(x)\\) 또는 수열 \\(\\{ a_{n}\\}\\)가 특정 값에 수렴한다는 것은 극한값이 존재하며, 일정 값에 점점 가까워진다는 것을 의미한다.\n수열의 수렴: 수열 \\(\\{ a_{n}\\}\\)이 \\(L\\)로 수렴한다면, 임의의 \\(\\varepsilon &gt; 0\\)에 대해 \\(n \\geq N\\) 일 때 다음 조건을 만족하는 \\(N\\)이 존재한다.\n\\(|a_{n} - L| &lt; \\varepsilon\\), 여기서 \\(L\\)은 수열의 극한값이다.\n함수의 수렴: 함수 \\(f(x)\\)가 \\(L\\)로 수렴하면, \\(x \\rightarrow a\\)에서 \\(\\lim_{x \\rightarrow a}f(x) = L\\)\n수렴의 성질\n\n수열이나 함수가 수렴하면 극한값은 유일하다.\n수렴하는 함수나 수열은 경계값을 가지며, 점점 극한값에 가까워진다.\n\n극한과 수렴의 차이\n\n극한은 특정 값에 접근하는 경향을 나타내며, 함수나 수열이 특정 점에서 어떻게 동작 하는지 설명한다.\n수렴은 극한값이 존재하고 일정 값에 점점 가까워지는 성질을 나타낸다.\n\n\n\n4. 확률수렴과 분포수렴\n\n(1) 확률수렴 (Convergence in Probability)\n확률변수의 열 \\(\\{ X_{n}\\}\\)이 확률변수 \\(X\\)에 확률수렴한다는 것은, 임의의 \\(\\varepsilon &gt; 0\\)에 대해 다음 조건을 만족하는 \\(n \\rightarrow \\infty\\)가 존재함을 의미한다. \\(\\lim_{n \\rightarrow \\infty}P(|X_{n} - X| \\geq \\varepsilon) = 0\\)\n  - 표기: \\(X_{n}\\overset{P}{\\rightarrow}X\\)\n해석: 확률적으로 \\(|X_{n} - X|\\)가 작아질 가능성이 1에 가까워짐을 나타낸다. 즉, \\(X_{n}\\)과 \\(X\\)가 점점 ”가까워진다”고 해석할 수 있다.\n성질\n\n확률수렴의 유일성: 극한값 \\(X\\)는 유일하다.\n확률수렴과 함수: \\(X_{n}\\overset{P}{\\rightarrow}X\\)이고 \\(g(x)\\)가 연속 함수라면 \\(g(X_{n})\\overset{P}{\\rightarrow}g(X)\\)\n\n통계학 응용: (추정량의 일치성) 추정량 \\({\\widehat{\\theta}}_{n}\\)이 모수 \\(\\theta\\)에 확률수렴하면 \\({\\widehat{\\theta}}_{n}\\)은 일치추정량이다. 법칙의 수렴: 큰 수의 약법칙은 확률수렴으로 표현된다:\n\\[{\\overline{X}}_{n}\\overset{P}{\\rightarrow}\\mu\\]\n\n\n(2) 분포수렴 (Convergence in Distribution)\n확률변수의 수열 \\(\\{ X_{n}\\}\\)이 확률변수 \\(X\\)에 분포수렴한다는 것은, 모든 연속점 \\(x\\)에서 누적분포함수(FDF) \\(F_{X_{n}}(x)\\)가 \\(F(x)\\)로 수렴함을 의미한다. \\(\\lim_{n \\rightarrow \\infty}F_{X_{n}}(x) = F_{X}(x),\\forall x\\) \\(F_{X}(x)\\)에서 연속함수.\n  표기: \\(X_{n}\\overset{\\mathcal{D}}{\\rightarrow}X\\)\n해석: 분포수렴은 \\(X_{n}\\)의 분포가 \\(X\\)의 분포로 점점 가까워지는 것을 의미한다. 개별적인 실현값이 아니라 분포 전체의 형태를 고려한다.\n성질\n\n연속성: 분포수렴은 누적분포함수의 연속점에서 정의된다.\n함수와 분포수렴: \\(X_{n}\\overset{\\mathcal{D}}{\\rightarrow}X\\)이고 \\(g(x)\\) 가 연속 함수라면 \\(g(X_{n})\\overset{\\mathcal{D}}{\\rightarrow}g(X)\\) 이다.\n\n응용: 중심 극한 정리: 표본 평균이 정규분포로 수렴하는 현상은 분포수렴으로 나타낸다. \\(\\sqrt{n}({\\overline{X}}_{n} - \\mu)\\overset{\\mathcal{D}}{\\rightarrow}N(0,\\sigma^{2})\\)\n\n\n(3) 확률수렴과 분포수렴의 관계\n확률수렴 → 분포수렴\n\\[X_{n}\\overset{P}{\\rightarrow}X \\Longrightarrow X_{n}\\overset{\\mathcal{D}}{\\rightarrow}X\\]\n분포수렴 ≠ 확률수렴\n  분포수렴이 확률수렴을 보장하지 않는다. 예를 들어, \\(X_{n} \\sim U( - n,n)\\)은 \\(X = 0\\)에 분포수렴하나 확률수렴하지 않는다."
  },
  {
    "objectID": "notes/linear_model/lm_diagnosis.html",
    "href": "notes/linear_model/lm_diagnosis.html",
    "title": "회귀분석 4. 회귀진단",
    "section": "",
    "text": "chapter 1. 회귀진단 개념\n\n1. 회귀진단이란?\n회귀분석에서 모델을 한 번 적합했다고 해서 그 결과를 그대로 신뢰할 수는 없다. 추정된 회귀계수는 최소제곱법에 의해 잘 정의되지만, 실제 데이터에서는 여러 가지 문제가 숨어 있을 수 있다. 이러한 문제를 찾아내고 모델이 가정에 잘 부합하는지 점검하는 과정이 회귀진단(regression diagnostics)이다.\n회귀진단의 출발점은 ”이 모델이 데이터를 제대로 설명하고 있는가?“라는 질문이다. 그 답을 찾기 위해 크게 두 가지 축에서 생각한다. 첫째, 회귀모형의 가정 점검이다. 선형성, 오차항의 등분산성, 독립성, 정규성과 같은 기본 가정들이 지켜지지 않으면 추정량의 성질이나 검정의 신뢰성이 흔들리게 된다. 예를 들어 잔차(residuals)를 살펴보면 선형관계가 어긋나거나 특정 구간에서만 분산이 커지는 등분산 위배 패턴을 발견할 수 있다.\n둘째, 이상점(outlier)과 영향점(influential point)의 탐색이다. 데이터 중 일부는 다른 점들과 지나치게 동떨어져 있거나, 회귀직선의 모양 자체를 크게 좌우하는 경우가 있다. 이러한 점들을 무시하면 추정치가 왜곡되므로, 잔차분석, 레버리지(leverage), 쿡의 거리(Cook’s distance)와 같은 진단통계량을 활용하여 확인한다.\n요컨대 회귀진단은 단순히 결정계수(R²)나 유의확률만 보는 것이 아니라, 모델이 데이터의 구조를 얼마나 충실히 반영하고 있는지, 가정 위배나 특정 관측치의 왜곡이 없는지를 살펴보는 과정이다. 이러한 진단 과정을 거쳐야 회귀분석 결과를 자신 있게 해석하고 실무적 의사결정에 활용할 수 있다.\n오차 가정 진단\n회귀분석의 기본 전제는 오차항이 일정한 가정을 충족한다는 것이다. 이러한 가정이 성립하지 않으면 추정된 회귀계수나 검정 결과를 신뢰할 수 없게 된다. 따라서 회귀모형이 가정을 만족하는지 여부를 점검하는 과정이 필요하며, 이를 회귀진단(잔차분석)이라 한다.\n선형성 가정은 회귀모형의 유의성 검정 결과와 직접적으로 연결되므로, 잔차분석에서는 주로 오차항에 대한 세 가지 가정을 점검한다. 즉, 정규성, 등분산성, 독립성이다. 이 가운데 독립성은 자료가 시계열 형태일 때 비로소 문제가 되므로, 일반 횡단면 자료에서는 정규성과 등분산성을 중점적으로 진단한다.\n요컨대 잔차분석은 회귀모형이 통계적 가정을 위배하지 않고 안정적으로 추정되었는지를 검증하는 핵심 절차이다.\n이상치, 영향치 진단\n회귀모형은 데이터 전체의 경향성을 바탕으로 추정되기 때문에 일부 관측값이 지나치게 벗어나 있거나 모형을 과도하게 지배할 경우 추정 결과가 왜곡될 수 있다. 이러한 점을 이상치 또는 영향치라 하며, 회귀분석에서는 이들을 진단하는 과정이 반드시 필요하다.\n이상치와 영향치 진단이 필요한 이유는 다음과 같다.\n첫째, 모형의 정확성 보장이다. 이상치나 영향치를 방치하면 회귀계수 추정치가 왜곡되고, 그 결과 예측력과 설명력이 저하된다.\n둘째, 모형의 신뢰성 확보이다. 극단적인 관측값은 추정계수를 편향시켜 통계적 검정 결과를 불안정하게 만들 수 있다.\n셋째, 적합성 확인이다. 이상치와 영향치는 잔차의 분포를 왜곡하여 정규성이나 등분산성 가정을 위배하게 하고, 이는 곧 회귀모형의 부적합성을 시사한다.\n넷째, 해석의 정확성 제고이다. 영향치는 회귀계수의 크기와 방향을 바꿀 수 있어, 이를 무시하면 계수 해석이 잘못될 수 있다.\n다섯째, 데이터 품질 향상이다. 이상치와 영향치 탐지는 자료 수집이나 측정 과정에서 발생한 오류를 찾아내어 수정할 기회를 제공한다.\n따라서 이상치와 영향치를 체계적으로 진단하고 적절히 처리하는 것은 회귀분석 결과의 타당성과 해석의 정확성을 높이는 핵심 과정이다.\n\n\n2. 잔차분석\n회귀모형 및 OLS 추정\n회귀모형 \\(y_{i} = \\alpha + \\overset{p}{\\sum_{k = 1}}\\beta_{k}x_{ki} + e_{i}\\) , \\(e_{i} \\sim N(0,\\sigma^{2})\\) [행렬 표현] \\(\\underset{¯}{y} = X\\underset{¯}{\\beta} + \\underset{¯}{e}\\), \\(\\underset{¯}{e} \\sim MN(\\underset{¯}{0},\\sigma^{2}I)\\)\n추정 (OLS) : \\(\\widehat{\\underset{¯}{\\beta}} = (X'X)^{- 1}X'\\underset{¯}{y}\\)\n적합치 : \\(\\widehat{\\underset{¯}{y}} = H\\underset{¯}{y}\\), \\(H = X(X'X)^{- 1}X'\\)는 멱등행렬\n잔차 정의\n\\(\\underset{¯}{r} = (I - H)\\underset{¯}{y}\\), \\(r_{i} = y_{i} - {\\widehat{y}}_{i}\\) : 잔차는 종속변수 관측치와 모형 적합치의 차이\n회귀모형 오차항(\\(e_{i}\\))의 MVUE : \\({\\widehat{e}}_{i} = r_{i}\\)\n잔차성질\n잔차의 평균은 \\(E(r_{i}) = 0\\)이고 분산은 \\(V(r_{i}) = \\sigma^{2}\\)이다.\n잔차는 서로 독립인가? 사실 오차는 독립을 가정하나 잔차는 독립이 아니다. 회귀계수 OLS추정에는 \\((x_{i},y_{i})\\)모든 관측치가 포함되어 있기 때문이다.\n\\(\\sum x_{i}r_{i} = 0\\): 예측변수와 잔차의 곱의 합은 0이다 - 설명변수와 잔차는 독립이다. 예측변수에 의해 설명되고 남은 부분(잔차)은 서로 독립이다.\n\\(\\sum{\\widehat{y}}_{i}r_{i} = 0\\) : 적합치와 잔차의 곱의 합은 0이다. 적합치는 예측변수에 의해 설명된 부분과 설명되지 않은 잔차 부분은 서로 독립이다.\n잔차분석이란\n잔차분석은 회귀분석에서 오차항을 직접 관측할 수 없기 때문에, 그 추정치인 잔차(residual)를 활용하여 모형의 가정을 점검하는 과정이다. 잔차가 회귀모형의 기본 가정을 잘 따르고 있는지를 확인함으로써 추정 결과의 신뢰성과 해석의 타당성을 확보할 수 있다.\n\n\n\n\n\n잔차분석에서 확인하는 주요 사항은 다음과 같다.\n첫째, 선형성이다. 설명변수와 종속변수의 관계가 선형이라면 잔차는 특정 패턴 없이 무작위로 분포해야 한다. 이는 회귀계수의 유의성 검정 결과와도 밀접하게 연결된다.\n둘째, 등분산성이다. 설명변수의 값이 달라짐에 따라 잔차의 분산이 달라지지 않아야 한다. 즉, 잔차가 일정한 폭을 유지하며 퍼져 있어야 한다.\n셋째, 독립성이다. 오차항이 서로 독립이라는 가정은 일반적인 횡단면 자료에서는 크게 문제되지 않지만, 시계열 자료에서는 반드시 확인해야 한다.\n넷째, 정규성이다. 오차항이 정규분포를 따른다는 가정은 회귀계수에 대한 통계적 추론(검정과 신뢰구간)의 타당성을 보장한다. 잔차의 분포를 히스토그램이나 Q-Q 플롯으로 확인하여 검토한다.\n다섯째, 이상치와 영향치의 존재 여부다. 표준화 잔차가 지나치게 크거나, 회귀선의 모양을 크게 바꾸는 영향점은 모형의 추정 결과를 왜곡시킬 수 있으므로 반드시 확인해야 한다.\n여섯째, 누락 변수 가능성이다. 잔차가 특정한 패턴을 보인다면 이는 모형에 포함되지 않은 중요한 설명변수가 존재함을 시사한다. 이 경우 현실성이 떨어지는 모형이 될 수 있다.\n결국 잔차분석은 단순히 모형의 적합도를 평가하는 단계를 넘어, 회귀모형이 기본 가정을 충족하는지 점검하고, 잠재적인 문제를 찾아내어 보완하는 핵심 절차이다.\n\n\n3. 잔차종류\n표준화 standardized 잔차\n\\(z_{i} = \\frac{r_{i} - \\overline{r}}{s(r_{i}) = \\sqrt{MSE}}\\), \\(MSE = \\widehat{\\sigma^{2}}\\)\n표준화 잔차는 추정 회귀식으로부터 관측치가 얼마나 떨어져 있나를 나타내는 것으로 \\(\\pm 2\\) (표준정규분포의 경우 \\(\\pm 1.96\\) 구간 안에는 95% 관측치가 있음) 보다 크면 이상치일 가능성이 높다.\n스튜던트 studentized 잔차\n\\(st_{i} = \\frac{r_{i}}{\\sqrt{MSE/(1 - h_{ii})}}\\), \\(h_{ii} = {\\underset{¯}{x}}_{i}'(X'X)^{- 1}{\\underset{¯}{x}}_{i}\\)\n잔차를 t-분포를 따르는 통계량으로 만든 것으로 \\(\\pm 2\\)이면 이상치로 판단한다.\n\\(h_{ii}\\) : \\(H = X(X'X)^{- 1}X'\\) 행렬의 대각 원소로 leverage 레버리지(지렛대)로 정의되며 영향치 판단에 사용한다.\n표준화/스튜던트 제외 standardized & deleted 잔차\n\\(z_{(i)} = \\frac{r_{(i)} - \\overline{r}}{\\sqrt{MSE_{(i)}}}\\), \\(st_{(i)} = \\frac{r_{(}i)}{\\sqrt{MSE_{(i)}/(1 - h_{ii})}}\\)\n\\(i -\\)번째 관측치를 제외하고 회귀모형을 추정한 후 얻은 적합치를 사용하여 얻은 잔차로 표준화/스튜던트 잔차에 비해 더 정확한 개념의 잔차이지만 현실에서는 자주 사용하지 않는다.\n\n\n4. 진단도구\n잔차와 종속변수 추정치 산점도\n잔차와 종속변수 추정치 산점도는 회귀모형의 타당성을 점검하는 가장 기본적인 도구이다. 이때 잔차를 Y축에 두고, 종속변수의 예측값(추정치)을 X축에 두어 산점도를 작성한다.\n잔차는 추정된 회귀모형이 설명하지 못한 부분에 해당하므로, 이 산점도에는 어떠한 체계적인 패턴도 나타나지 않아야 한다. 즉, 잔차가 평균 0을 중심으로 무작위로 흩어져 있는 형태가 바람직하다. 만약 특정 곡선 형태나 일정한 방향성을 띠는 패턴이 나타난다면, 이는 선형성 가정의 위배나 누락된 설명변수의 존재를 시사한다.\n또한 예측값이 커지거나 작아짐에 따라 잔차의 분산이 달라지는 모습이 관찰된다면, 이는 등분산성 가정의 위배를 의미한다. 따라서 잔차 산점도를 통해 선형성과 등분산성을 동시에 진단할 수 있다.\n아울러 잔차가 지나치게 크게 나타나는 점은 다른 점들과 동떨어진 이상치(outlier)일 가능성이 있으며, 이러한 점들은 회귀계수 추정치와 모형의 안정성을 심각하게 왜곡할 수 있다.\n결국 잔차–추정치 산점도는 회귀모형의 기본 가정(선형성, 등분산성) 진단뿐 아니라, 이상치 탐지에도 중요한 역할을 하는 핵심 도구이다.\n잔차(Y-축)와 시간(time, X-축)의 시간도표(time plot)\n오차의 독립성 검정은 시계열 데이터에만 국한된다.\n잔차 활용\n회귀분석에서 오차항 자체는 관측할 수 없으므로, 그 추정치인 잔차를 활용하여 모형의 타당성을 점검하게 된다. 잔차는 종속변수의 실제값과 회귀모형이 추정한 예측값의 차이로 정의되며, 이 값 속에는 모형이 설명하지 못한 변동이 담겨 있다. 따라서 잔차를 체계적으로 살펴보면 회귀모형이 기본적으로 전제한 오차항의 가정이 충족되는지, 그리고 개별 관측치가 모형에 과도한 영향을 주고 있지는 않은지를 확인할 수 있다.\n구체적으로는 잔차의 분포와 패턴을 분석하여 정규성, 등분산성, 독립성과 같은 오차항의 가정을 진단한다. 예를 들어 잔차가 무작위로 흩어져 있으면 선형성과 등분산성이 충족되었다고 볼 수 있고, 잔차의 분포가 종 모양을 띠면 정규성이 유지된 것으로 해석할 수 있다.\n아울러 잔차의 크기와 분포를 살펴보면 이상치나 영향치를 식별할 수 있다. 잔차가 지나치게 크거나 특정 관측치가 회귀직선의 기울기와 절편을 크게 좌우한다면, 이는 모형의 안정성과 해석의 신뢰성을 저해하는 요인으로 작용한다.\n결국 잔차분석은 단순히 회귀모형의 적합도를 평가하는 수준을 넘어, 오차항의 가정 점검과 이상치 진단을 동시에 수행하는 핵심 절차이며, 회귀분석 결과를 신뢰할 수 있도록 보장하는 중요한 단계이다.\n*) 잔차의 정규성 검정에 대해서는 상반된 견해가 존재한다. 한쪽에서는 잔차가 종속변수의 평균에 기반하여 계산되므로, 표본의 크기가 충분히 크면 중심극한정리에 따라 정규분포에 근사한다고 본다. 따라서 일반적으로 표본이 30개 이상인 경우에는 정규성 위배가 분석 결과에 큰 영향을 주지 않으며, 이때는 굳이 정규성 검정을 수행하지 않아도 된다는 입장이다.\n\n\n5. 회귀진단 절차 및 최종 추정모형\n\n(1) 회귀모형 추정\n데이터에 설정된 회귀모형이 완전하다면 (목표변수 관측값- 추정 모형에 의해 적합된 값) 차이인 잔차에는 어떤 패턴도 있어서는 안된다. 즉 백색잡음, 정규분포를 따라야 한다.\n# ==============================================\n# 최종 회귀모형 추정 : adjust 결정계수 최대\n# ==============================================\nimport itertools\nimport pandas as pd\nimport numpy as np\nimport statsmodels.api as sm\nfrom sklearn.datasets import fetch_openml\n\n# 1. 데이터 불러오기\nboston = fetch_openml(name=\"boston\", version=1, as_frame=True)\ndf = boston.frame.copy()\n\n# 종속변수 / 설명변수\ny = pd.to_numeric(df[\"MEDV\"], errors=\"coerce\")\nX = df.drop(columns=[\"MEDV\"]).copy()\n\n# 숫자형 변환\nfor c in X.columns:\n    if X[c].dtype.name in [\"object\", \"category\"]:\n        X[c] = X[c].astype(str).str.strip()\n        X[c] = pd.to_numeric(X[c], errors=\"coerce\")\n    else:\n        X[c] = X[c].astype(float)\n\n# 결측 제거\nmask = y.notna() & X.notna().all(axis=1)\ny = y[mask].reset_index(drop=True)\nX = X.loc[mask].reset_index(drop=True)\n\n# 2. Best Subset 함수\ndef best_subset(X, y, max_features=5, criterion=\"adj_r2\"):\n    results = []\n    n = len(y)\n\n    for k in range(1, max_features+1):  # 변수 개수 1개부터 max_features까지\n        for combo in itertools.combinations(X.columns, k):\n            X_combo = sm.add_constant(X[list(combo)], has_constant=\"add\")\n            model = sm.OLS(y, X_combo).fit()\n\n            if criterion == \"adj_r2\":\n                score = model.rsquared_adj\n            elif criterion == \"aic\":\n                score = -model.aic  # 최소화 기준 → 부호 바꿔서 최대화\n            elif criterion == \"bic\":\n                score = -model.bic\n            else:\n                raise ValueError(\"criterion은 'adj_r2', 'aic', 'bic' 중 선택\")\n\n            results.append({\n                \"num_features\": k,\n                \"features\": combo,\n                \"criterion\": score\n            })\n\n    # 최적 모형 선택\n    best_model = max(results, key=lambda x: x[\"criterion\"])\n    return pd.DataFrame(results), best_model\n\n# 3. 실행 (예: 최대 10개 변수까지 탐색, Adjusted R² 기준)\nall_results, best = best_subset(X, y, max_features=7, criterion=\"adj_r2\")\n\nprint(\"Best Model (Adjusted R² 기준):\")\nprint(\"변수:\", best[\"features\"])\nprint(\"Adj R²:\", best[\"criterion\"])\nBest Model (Adjusted R² 기준):\n변수: ('CHAS', 'NOX', 'RM', 'DIS', 'PTRATIO', 'B', 'LSTAT')\nAdj R²: 0.7182560407158507\n예측변수 DIS 상관계수 부호와 회귀계수 부호가 상이하여 제외하였다.\n# --- 4) 최종 모형 적합 ---\nselected = ['CHAS', 'NOX', 'RM', 'PTRATIO', 'B', 'LSTAT']  # 선택된 변수 리스트\nX_final = sm.add_constant(X[selected], has_constant='add')        # 정제된 X 사용\nfinal_model = sm.OLS(y, X_final).fit()\n\nprint(final_model.summary())\n\n\n\n\n\n6개 예측변수로 하여 모형을 추정한 출력결과 하단에 다중공선성 경고는 있으나 VIF 값 진단 결과 모두 3미만이므로 본 모형을 최종 추정모형으로 사용하였다.\n[2] VIF (모든 변수)\nvariable VIF  5 LSTAT 2.453  2 RM 1.698  1 NOX 1.664  4 B 1.242  3 PTRATIO 1.216  0 CHAS 1.045\n\n\n(2) 잔차 vs 예측값 산점도\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# 1. 잔차 계산\nresiduals = final_model.resid_pearson\nfitted = final_model.fittedvalues\n\n# 2. 잔차, 예측 산점도\nsns.residplot(x=fitted, y=residuals, lowess=True, line_kws={'color': 'red', 'lw': 1})\nplt.title('scatter plot of (yhat vs standard_res)')\nplt.axhline(2)\nplt.axhline(-2)\nplt.show()\n\n\n\n\n\n곡선 패턴 존재\n잔차가 단순히 무작위로 흩어진 것이 아니라, 예측값이 작을 때와 클 때 잔차가 양수 방향으로 커지고, 중간값에서는 음수 쪽으로 몰려 있으므로 빨간 곡선(스무딩 선)이 U자형을 그리는 모습이 선형성 가정이 무너졌음을 보여준다. → 선형성 가정 위배\n잔차 분산의 불균일\n예측값이 커질수록 잔차의 분산이 조금 더 커지는 경향이 보이므로 이는 등분산성 위배(이분산성)의 가능성을 시사한다.\n극단적인 점들\ny축 ±2를 넘는 점들이 상당히 보이고, 특히 위쪽으로 튀는 점들이 다수 있으므로 이는 이상치 또는 영향치일 가능성이 크다.\n\n(3) 선형성 검정\n회귀분석에서 가장 먼저 확인해야 할 사항은 선형성 가정이다. 즉, 종속변수와 설명변수 간의 함수관계가 선형(직선)으로 표현될 수 있어야 한다는 것이다. 이는 선형 회귀모형의 기본 전제 조건이다.\n실제 분석에서는 회귀모형을 추정하기 전에 종속변수와 각 설명변수 간의 산점도를 통해 선형성 여부를 사전 점검하고, 필요하다면 적절한 변환을 통해 문제를 해결한다. 이후 회귀모형의 유의성 검정을 거치면, 일반적으로 선형성 가정은 만족하는 것으로 판단할 수 있다. 따라서 기본적인 회귀분석에서는 별도의 선형성 검정 절차가 필수적이지 않다.\n그러나 설명변수 간의 관계가 복잡하게 얽혀 있거나, 함수형태의 잘못된 특정화 가능성이 우려될 경우에는 보다 엄밀한 통합적 선형성 검정 방법을 적용할 필요가 있다. 대표적으로 Harvey와 Collier(1977)가 제안한 Functional Form Misspecification Test가 있으며, Johnston(1984)에서도 이 방법이 소개되어 있다.\nHarvey A. and Collier P. (1977); Testing for Functional Misspecification in Regression Analysis, Journal of Econometrics 6, 103--119. Johnston, J. (1984); Econometric Methods, Third Edition, McGraw Hill Inc.\n\n귀무가설 : 목표변수와 예측변수들 간에는 선형성이 존재한다.\n대립가설 : 목표변수와 예측변수들 간에는 선형성이 존재하지 않는다.\n\n본 연구에서는 목표변수와 설명변수 간의 상관계수가 매우 유의하게 나타났으며, 이는 곧 선형 함수관계가 존재함을 의미한다. 따라서 선형성 가정은 충족되었고, 앞서 언급한 것처럼 굳이 별도의 검정을 수행할 필요는 없다는 결론에 도달한다.\n# 선형성 검정\nfrom statsmodels.stats.diagnostic import linear_harvey_collier\n\nt_stat, p_val = linear_harvey_collier(final_model)\nprint(f\"[Harvey–Collier] t = {t_stat:.3f}, p-value = {p_val:.4g}\")\n# 해석: p-value &lt; 0.05 이면 선형성 가정이 유의하게 깨졌다고 본다.\nlinear_harvey_collier 오류는 초기 부분표본이 특이(singular) 해서 생긴다. 해결은 두 가지다:\n1. 초기 표본 크기 skip를 충분히 크게 잡고,\n2. 초기에 변동이 충분히 들어오도록 표본 순서를 order_by로 재배열한다.\nfrom statsmodels.stats.diagnostic import linear_reset\n\n# Ramsey RESET (비선형성/함수형 잘못특정)\nreset_res = linear_reset(final_model, power=2, use_f=True)\n\nprint(\"[RESET test]\")\nprint(f\"F-statistic = {reset_res.fvalue:.3f}\")\nprint(f\"p-value     = {reset_res.pvalue:.4g}\")\n[RESET test] F-statistic = 183.651 p-value = 7.741e-36\np-value 사실상 0에 가까우므로 귀무가설(회귀모형은 선형으로 잘 특정되었다)**는 기각된다. 즉, 현재 추정한 선형회귀모형은 종속변수와 예측변수 간의 관계를 제대로 설명하지 못하고 있으며, 비선형성이나 누락된 변수가 존재할 가능성이 매우 높다는 의미한다.\n\n비선형항(예: RM², LSTAT²)이나 변수 변환(로그, 제곱근 등)을 도입해 모델을 개선해야 한다.\n또는 상호작용항 추가, 더 나아가 비선형 모형(GAM, 트리기반 모형)을 고려할 필요가 있다.\n\n\n\n(4) 정규성\n회귀분석에서 오차항이 정규분포를 따른다는 가정은 매우 중요한 전제이다. 이 가정이 충족되어야만 모형 전체의 유의성을 검정하는 분산분석 F-검정과 개별 예측변수의 유의성을 검정하는 t-검정이 타당하게 적용될 수 있다.\n오차항은 직접 관측할 수 없으므로, 그 추정치인 잔차를 이용하여 정규성을 검정한다. 따라서 잔차의 정규성 검정은 곧 오차항의 정규성 가정을 점검하는 절차이다.\n\n귀무가설 : 데이터는 정규분포를 따른다.\n대립가설 : 정규분포를 따르지 않는다.\n\n잔차가 정규성을 만족한다면 회귀분석에서의 추론이 통계적으로 타당하다고 볼 수 있으며, 정규성이 위배될 경우에는 비모수적 방법, 변수변환, 또는 강건추정 방법을 고려해야 한다.\nfrom scipy.stats import shapiro, jarque_bera, kstest\n\n# Shapiro–Wilk\nstat, p = shapiro(residuals)\nprint(f\"Shapiro–Wilk W={stat:.3f}, p={p:.4g}\")\n\n# Jarque–Bera\njb_stat, jb_p = jarque_bera(residuals)\nprint(f\"Jarque–Bera JB={jb_stat:.3f}, p={jb_p:.4g}\")\nShapiro–Wilk W=0.873, p=6.84e-20  Jarque–Bera JB=1187.536, p=1.348e-258\n\n\n(5) 등분산 가정\n잔차와 예측치 산점도가 다음과 같이 나팔 fan 모양일 때 등분산 가정이 무너진다. 종속변수의 값에 의존하여 분산이 커지거나 작아짐 - 등분산 가정이 무너지면 분산이 큰 부분에서 종속변수의 값이 적합선을 많이 벗어난 것이 적합 정도가 떨어진다고 결론 내릴 수 없음, 이는 분산이 다르므로 이상치가 발생할 수 있기 때문이다.\n\n\n\n\n\n\n귀무가설 : 잔차는 등분산성을 갖는다.\n대립가설 : 잔차는 등분산성을 갖지 않는다.\n\nBreush-Pagan test\nGujarati, Damodar N.; Porter, Dawn C. (2009). Basic Econometrics (Fifth ed.). New York: McGraw-Hill Irwin. pp. 385–86.\nGoldfeld-Quandt test\nGoldfeld, Stephen M.; Quandt, R. E. (June 1965). \"Some Tests for Homoscedasticity\". Journal of the American Statistical Association. 60 (310): 539–547.\n# 등분산 가정 검정 패키지\nfrom statsmodels.stats.diagnostic import het_breuschpagan, het_white, het_goldfeldquandt\nfrom statsmodels.nonparametric.smoothers_lowess import lowess\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# ---------------------------------------\n# 0) 준비: 잔차(피어슨), 적합값\n# ---------------------------------------\nresid = final_model.resid_pearson      # 표준화 잔차(피어슨)\nfitted = final_model.fittedvalues\nexog = final_model.model.exog          # 상수 포함 X 행렬\nendog = final_model.model.endog        # y\n\n# ---------------------------------------\n# 1) Breusch–Pagan test\n#   귀무: 등분산 (Var(ε) = σ²)\n# ---------------------------------------\nbp_lm, bp_lm_p, bp_f, bp_f_p = het_breuschpagan(final_model.resid, exog)\nprint(\"[Breusch–Pagan]\")\nprint(f\"LM stat = {bp_lm:.3f},  LM p = {bp_lm_p:.4g}\")\nprint(f\"F  stat = {bp_f:.3f},   F  p = {bp_f_p:.4g}\\n\")\n\n# ---------------------------------------\n# 2) White test (이분산 + 일부 비선형성 민감)\n#   귀무: 등분산\n# ---------------------------------------\nw_lm, w_lm_p, w_f, w_f_p = het_white(final_model.resid, exog)\nprint(\"[White]\")\nprint(f\"LM stat = {w_lm:.3f},  LM p = {w_lm_p:.4g}\")\nprint(f\"F  stat = {w_f:.3f},   F  p = {w_f_p:.4g}\\n\")\n\n# ---------------------------------------\n# 3) Goldfeld–Quandt test (양쪽/단측 가능)\n#   데이터 순서를 어떤 기준으로 정렬해 부분집단 분산 비교\n#   보통 fitted(적합값)나 특정 설명변수에 따라 정렬합니다.\n#   귀무: 등분산\n# ---------------------------------------\n# 적합값 기준으로 정렬 인덱스 생성\norder_idx = np.argsort(fitted.values)\n\n# GQ: two-sided(양측) 권장. 단측은 'increasing' 또는 'decreasing'\ngq_stat, gq_p, gq_df = het_goldfeldquandt(endog[order_idx], exog[order_idx, :], alternative='two-sided')\nprint(\"[Goldfeld–Quandt]\")\nprint(f\"F stat = {gq_stat:.3f}, df = {gq_df}, p = {gq_p:.4g}\\n\")\n\n# ---------------------------------------\n# 4) 잔차 진단 플롯 (패턴/부채꼴 확인)\n# ---------------------------------------\nplt.figure(figsize=(6,4))\nsns.scatterplot(x=fitted, y=resid, s=25, edgecolor=None)\n# LOWESS 추세선(잔차-적합값 관계 시각화)\nlo = lowess(resid, fitted, frac=0.6, return_sorted=True)\nplt.plot(lo[:,0], lo[:,1], linewidth=2)\nplt.axhline(0, linestyle=\"--\")\nplt.xlabel(\"Fitted values\")\nplt.ylabel(\"Pearson residuals\")\nplt.title(\"Residuals vs Fitted\")\nplt.tight_layout()\nplt.show()\n\n# Scale-Location(=Spread-Location) plot: sqrt(|standardized residuals|) vs fitted\nplt.figure(figsize=(6,4))\nsr = np.sqrt(np.abs(resid))\nsns.scatterplot(x=fitted, y=sr, s=25, edgecolor=None)\nlo2 = lowess(sr, fitted, frac=0.6, return_sorted=True)\nplt.plot(lo2[:,0], lo2[:,1], linewidth=2)\nplt.xlabel(\"Fitted values\")\nplt.ylabel(\"sqrt(|Pearson residuals|)\")\nplt.title(\"Scale-Location (Spread vs Fitted)\")\nplt.tight_layout()\nplt.show()\n\n# ---------------------------------------\n# 5) 이분산 존재 시: 강건표준오차(HC3) 요약\n# ---------------------------------------\nrobust = final_model.get_robustcov_results(cov_type=\"HC3\")\nprint(robust.summary())\n[Breusch–Pagan]  LM stat = 27.371, LM p = 0.0001234  F stat = 4.756, F p = 9.984e-05\n[White]  LM stat = 182.805, LM p = 1.644e-25  F stat = 10.420, F p = 1.309e-32\n[Goldfeld–Quandt]  F stat = 1.229, df = two-sided, p = 0.1068\nBP, White는 ”이분산 있음” 쪽으로 강한 신호이나 GQ는 ”등분산 vs 이분산”에서 결정적 증거를 주지 못하고 있다.\n보통 White가 가장 포괄적이라, 결과적으로는 이 모형의 잔차는 등분산 가정을 만족하지 않는다라고 보는 게 타당합니다.\n1. 강건표준오차(robust standard errors, HC3)로 계수 검정 보정 → 추론 왜곡 방지.\n2. WLS(가중최소제곱): 분산구조를 모형화(예: fitted에 비례, 특정 변수의 역수).\n3. 변수변환: y나 문제되는 X에 로그/제곱근 변환 고려.\n4. 모형 재특정: 누락된 변수·비선형 항이 없는지 점검.\nOLS대신 WLS 가중최소자승법 사용\n\\(min_{\\alpha,\\beta_{1},...,\\beta_{p}}\\sum w_{i}(y_{i} - \\alpha - \\overset{p}{\\sum_{k = 1}}\\beta_{k}x_{ki})^{2}\\)을 최소화 하는 추정치를 가중최소추정량이라 한다. 가중치 \\(w_{i} = \\frac{1}{{\\widehat{y}}_{i}^{2}}\\)을 사용한다.\nimport statsmodels.api as sm\nimport numpy as np\n\n# --- 1) 적합값 기반 가중치 ---\n# 잔차 크기와 fitted 관계를 보고, 분산이 fitted^2에 비례한다고 가정\nweights = 1 / (final_model.fittedvalues**2)\n\nX_final = sm.add_constant(X[selected], has_constant='add')\nwls_model = sm.WLS(y, X_final, weights=weights).fit()\nprint(\"\\n[WLS: fitted값 기반 가중치]\")\nprint(wls_model.summary())\n\n\n\n\n\n잔차는 등분산 가정이 무너져 WLS 추정 결과 결정계수 55%로 떨어졌고 예측변수 RM는 유의하지 않고 회귀계수 부호와 상관계수 부호가 일치하지 않는다. 등분산 검정 결과 등분산성이 여전히 만족되지 않는다.\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom statsmodels.nonparametric.smoothers_lowess import lowess\nimport numpy as np\n\n# --- WLS 잔차 vs 적합값 ---\nresid_wls  = wls_model.resid      # WLS 잔차\nfitted_wls = wls_model.fittedvalues  # WLS 적합값\n\nplt.figure(figsize=(7,5))\nsns.scatterplot(x=fitted_wls, y=resid_wls, s=28, alpha=0.8, edgecolor=None)\n\n# LOWESS 추세선\nlo = lowess(resid_wls, fitted_wls, frac=0.6, return_sorted=True)\nplt.plot(lo[:,0], lo[:,1], color=\"red\", linewidth=2)\n\nplt.axhline(0, ls=\"--\", lw=1, color=\"black\")\nplt.xlabel(\"WLS(Fitted)\")\nplt.ylabel(\"WLS (Residuals)\")\nplt.title(\"WLS Residuals vs Fitted\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n(6) 독립성\n개념\n시계열 데이터의 경우 오차항이 전 차항의 오차들에 의해 영향을 받게 되면 오차의 독립성이 파괴된다. 오차항 독립이 아니면 종속변수에 설정된 설명변수가 설명하지 못하는 일정의 패턴이 존재하므로 회귀추정이 불완전하게 된다.\n진단도구\nDurbin Watson 퉁계량 \\(d=\\frac{\\sum_{T}^{t=2} (e_t - e_{t-1})^2}{\\sum_{t=1}^{T} e_t^2}\\)\nDW 검정통계량의 값은 \\(2(1 - r)\\)에 근사한다. 상관계수 \\(r\\)은 \\((e_{t},e_{t - 1})\\)의 상관계수 (&lt;=&gt; 오차의 1차 자기상관계수)이다.\n오차가 독립(자기상관이 존재하지 않음)이면 \\(r = 0\\)이고 \\(DW = 2\\)에 근사한다.\n\n\n\n\n\npositive autocorrelation (양의 상관관계)\n\nIf \\(DW &lt; D_{L}\\) , 양의 상관관계가 존재한다.\nIf \\(DW &gt; D_{U}\\), 자기상관이 존재하지 않는다. 독립이다.\nIf \\(D_{L} &lt; DW &lt; D_{U}\\), 결론 내릴 수 없음\n\nnegative autocorrelation (음의 상관관계) the test statistic (4 − d)\n\nIf \\(DW &gt; 4 - D_{L}\\), 음의 상관관계가 존재한다.\nIf \\(DW &lt; 4 - D_{U}\\), 자기상관이 존재하지 않는다. 독립이다.\nIf \\(4 - d_{u} &lt; DW &lt; 4 - d_{L}\\), 결론 내릴 수 없음\n\n해결책\n목표변수의 1차 전기 항\\((y_{t - 1})\\)을 예측변수로 사용하거나 종속변수의 차분항(\\(\\bigtriangledown Y_{t} = (Y_{t} - Y_{t - 1})\\))을 목표변수로 사용한다.\nsm.OLS, sm.WLS 실행하면 DW(Durbin Watson) 자동 출력된다.\n\n\n\n\n\n\n\n(7) 오차 가정 만족하지 않은 회귀모형 이유 파악\n#잔차분석 관련 모든 그래프\nimport matplotlib.pyplot as plt\n\n# 잔차\nresiduals = final_model.resid\n\n# 선택된 변수들\nselected = ['CHAS', 'NOX', 'RM', 'PTRATIO', 'B', 'LSTAT']\n\n# 그리기\nfig, axes = plt.subplots(2, 3, figsize=(15, 8))\naxes = axes.flatten()\n\nfor i, var in enumerate(selected):\n    axes[i].scatter(X[var], residuals, alpha=0.7)\n    axes[i].axhline(y=0, color=\"red\", linestyle=\"--\", linewidth=1)\n    axes[i].set_xlabel(var)\n    axes[i].set_ylabel(\"Residuals\")\n    axes[i].set_title(f\"{var} vs Residuals\")\n\nplt.suptitle(\"scatter plot of (yhat vs standard_res)\", fontsize=14)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\nRM, LSTA 예측변수가 이분산 형태를 가지므로 잔차의 등분산 가정이 무너졌다.\n\n\n(8) 이상치 & 영향치\n\n\n\n\n\n이상치와 영향치는 회귀분석에서 반드시 구분해야 하는 개념이다.\n이상치(outlier)는 설명변수의 관측값은 정상 범위 내에 있으나 반응변수 값이 회귀선에서 크게 벗어난 점이다. 이상치는 잔차가 크므로 회귀모형의 적합 정도를 떨어뜨리고, 결정계수를 낮추는 원인이 된다. 데이터 입력 오류로 발생한 이상치는 제거하는 것이 옳고, 실제 값인 경우에는 robust 회귀와 같은 방법으로 영향을 줄일 수 있다.\n영향치(influential point)는 설명변수 값 자체가 다른 관측치와 동떨어져 분포 범위를 벗어난 점이다. 이러한 점은 레버리지가 높아 회귀선을 강하게 끌어당긴다. 표면적으로는 회귀선에 잘 붙어 있어 잔차가 크지 않을 수 있으나, 실제로는 회귀계수와 결정계수를 왜곡시킨다. 그 결과 모형이 잘 맞는 것처럼 보이게 만들며, 회귀계수의 유의성이 과대평가되는 문제가 발생한다. 영향치는 주변에 표본을 추가로 수집하거나 해당 관측치를 제외한 후 모형을 다시 적합하는 방식으로 대처할 수 있다.\n따라서 이상치는 반응변수의 문제이고, 영향치는 설명변수의 문제이다. 이 둘을 구분하여 진단하고 적절히 처리하는 것이 회귀모형 분석의 핵심이다.\n이상치 진단도구\n\n원 변수 산점도, 잔차-적합치 산점도에서 시각적 판단\n표준화 잔차: \\(r_{i} = \\frac{e_{i}}{\\widehat{\\sigma}\\sqrt{1 - h_{ii}}}\\), 여기서 \\(e_{i} = y_{i} - {\\widehat{y}}_{i}\\)는 잔차, \\({\\widehat{\\sigma}}^{2} = MSE\\), \\(h_{ii}\\)는 hat matrix \\(H = X(X'X)^{- 1}X'\\)의 대각원소, 레버리지이다. 단순 잔차를 표준오차로 나눈 값으로, 서로 다른 관측치 간 비교가 가능하다. \\(\\pm 2\\)이상이면 이상치이다.\n스튜던트 잔차: \\(t_{i} = \\frac{e_{i}}{\\widehat{\\sigma}(i)\\sqrt{1 - hii}}\\), \\(\\widehat{\\sigma}{(i)}^{2} = \\frac{\\sum j \\neq ie_{j}^{2}}{n - p - 1}\\)는 \\(i\\)번째 관측치를 제외하고 추정한 오차분산이다.표준화 잔차와 비슷하지만, 특정 관측치를 제거하고 분산을 다시 계산하기 때문에 해당 점의 영향력을 더 정확히 반영한다. \\(t_{i}\\)는 근사적으로 자유도 \\(n - p - 1\\)인 \\(t\\)-분포를 따른다.\n\n영향치 판단도구\n\n레버러지 \\(h_{ii} = x_{i}'(X'X)^{- 1}x_{i}\\): \\(i\\)번째 예측변수 벡터(관측값)가 전체 회귀모형 적합에 얼마나 영향을 주는지 나타내는 값이다. \\(0 \\leq h_{ii} \\leq 1\\), 평균값은 \\(\\overline{h} = \\frac{p + 1}{n}\\)( p: 예측변수 개수, n: 표본 수 )이고 \\(h_{ii} &gt; 2\\overline{h}\\)이면 높은 레버리지 점으로 본다.\nDFFITS(Difference in Fits) \\(DFBETAS_{ij} = \\frac{{\\widehat{\\beta}}_{j} - {\\widehat{\\beta}}_{j(i)}}{SE({\\widehat{\\beta}}_{j})}\\): 관측치 i를 제거했을 때 회귀계수 \\(\\beta_{j}\\)가 얼마나 변하는지 표준오차 단위로 측정한다. \\(|DFBETAS_{ij}| &gt; \\frac{2}{\\sqrt{n}}\\)이면 변수 j의 추정치에 큰 영향을 미친다고 판단한다.\nCook’s 거리 \\(D_{i} = \\frac{e_{i}^{2}}{p{\\widehat{\\sigma}}^{2}} \\cdot \\frac{h_{ii}}{(1 - h_{ii})^{2}}\\), 관측치 i를 제거했을 때 회귀계수 전체가 얼마나 변하는지를 나타내는 척도이다. 값이 클수록 그 점이 회귀선 전체에 큰 영향을 미친다는 의미하며 \\(D_{i} &gt; 1\\)이면 영향치로 의심할 수 있다.\nDFFITS \\(DFFITS_{i} = \\frac{\\widehat{y}i - \\widehat{y}i(i)}{\\widehat{\\sigma}(i)\\sqrt{hii}}\\): 관측치 i를 제거했을 때 예측값 \\({\\widehat{y}}_{i}\\)가 얼마나 변하는지 나타낸다. \\(|DFFITS_{i}| &gt; 2\\sqrt{\\frac{p}{n}}\\)이면 영향력이 크다고 본다.\n\n해결책\n\n이상치 - 삭제 : 회귀모형의 적합성 높아짐 &lt;=&gt; 결정계수 높아짐\n영향치 : 결정계수를 커지게 하는 경향이 있음 &lt;=&gt; 제외하고 추정 모형을 예측하는 것이 적절하다.\n\n# -----------------------------------------\n# 1) 진단 그래프 2종: Studentized Residuals vs Fitted, Influence plot\n# -----------------------------------------\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom statsmodels.nonparametric.smoothers_lowess import lowess\nfrom statsmodels.graphics.regressionplots import influence_plot\n\n# --- 최종 모형 적합 ---\nselected = ['CHAS', 'NOX', 'RM', 'PTRATIO', 'B', 'LSTAT']  # 선택된 변수 리스트\nX_final = sm.add_constant(X[selected], has_constant='add')        # 정제된 X 사용\nfinal_model = sm.OLS(y, X_final).fit()\n\nprint(final_model.summary())\n\nsns.set_style(\"whitegrid\")\n\n# ✅ studentized residuals는 get_influence()에서 가져온다.\ninfl   = final_model.get_influence()\nresid  = infl.resid_studentized_internal   # 또는 infl.resid_studentized_external\nfitted = final_model.fittedvalues\n\n# (a) Studentized Residuals vs Fitted\nplt.figure(figsize=(7,5))\nsns.scatterplot(x=fitted, y=resid, s=28, edgecolor=None)\nlo = lowess(resid, fitted, frac=0.6, return_sorted=True)\nplt.plot(lo[:,0], lo[:,1], linewidth=2)\nplt.axhline(0, ls=\"--\", lw=1)\nplt.xlabel(\"Fitted values\")\nplt.ylabel(\"Studentized residuals\")\nplt.title(\"Studentized Residuals vs Fitted (Final OLS)\")\nplt.tight_layout()\nplt.show()\n\n# (b) Influence plot (Studentized residual vs Leverage, 원 크기=Cook's D)\nfig = influence_plot(final_model, criterion=\"cooks\", size=8)\nplt.title(\"Influence Plot (Final OLS)\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nfrom statsmodels.stats.outliers_influence import OLSInfluence\n\n# --- 1. 기존 최종 모형 적합 ---\nselected = ['CHAS', 'NOX', 'RM', 'PTRATIO', 'B', 'LSTAT']\nX_final = sm.add_constant(X[selected], has_constant='add')\nfinal_model = sm.OLS(y, X_final).fit()\n\nprint(\"[원래 최종 모형]\")\nprint(final_model.summary())\n\n# --- 2. 영향치/이상치 진단 ---\ninfl = OLSInfluence(final_model)\nstd_resid = infl.resid_studentized_internal   # studentized residuals\ncooks_d   = infl.cooks_distance[0]            # Cook's D\nlev       = infl.hat_matrix_diag              # leverage\n\nn = int(final_model.nobs)\np = int(final_model.df_model) + 1\n\n# 기준값\nthr_resid = 2.0\nthr_cook  = 4/n\nthr_lev   = 2*p/n\n\n# 이상치 또는 영향치 플래그\nmask_bad = (np.abs(std_resid) &gt; thr_resid) | (cooks_d &gt; thr_cook) | (lev &gt; thr_lev)\nprint(f\"제거 대상 관측치 수: {mask_bad.sum()} / {n}\")\n\n# --- 3. 이상치/영향치 제거 후 모형 재적합 ---\nX_clean = X_final.loc[~mask_bad].copy()\ny_clean = y.loc[~mask_bad].copy()\n\nrefit_model = sm.OLS(y_clean, X_clean).fit()\n\nprint(\"\\n[이상치/영향치 제거 후 모형]\")\nprint(refit_model.summary())\nCHAS의 회귀계수 부호가 상관계수 부호와 상이하여 제거 후 최종 회귀모형을 추정하였다.\n\n\n\n\n\n예측값, 신뢰구간, 예측구간\n회귀모형 \\(\\underset{¯}{y} = X\\underset{¯}{\\beta} + \\underset{¯}{e}\\)\nOLS 추정치 : \\(\\widehat{\\underset{¯}{\\beta}} = (X'X)^{- 1}X'\\underset{¯}{y}\\)\n주어진 예측변수 값 벡터 : \\({\\underset{¯}{x}}_{0}\\) 예측구간\n주어진 예측변수 \\({\\underset{¯}{x}}_{0}\\) 목표변수 적합치 : \\(\\widehat{{\\underset{¯}{y}}_{0}}|{\\underset{¯}{x}}_{0} = {\\underset{¯}{x}}_{0}\\widehat{\\underset{¯}{\\beta}} + {\\underset{¯}{e}}_{0}\\)\n\\({\\underset{¯}{e}}_{0}\\)는 오차이고 평균이 0이므로 추정량은 \\(\\widehat{{\\underset{¯}{y}}_{0}}|{\\underset{¯}{x}}_{0} = {\\underset{¯}{x}}_{0}\\widehat{\\underset{¯}{\\beta}}\\)으로 기대값 추정량과 동일하다.\n추정량 평균 : \\(E(\\widehat{{\\underset{¯}{y}}_{0}}|{\\underset{¯}{x}}_{0}) = {\\underset{¯}{x}}_{0}\\widehat{\\underset{¯}{\\beta}}\\)\n추정량 분산 : \\(V(\\widehat{{\\underset{¯}{y}}_{0}}|{\\underset{¯}{x}}_{0}) = \\sigma^{2}(I + {\\underset{¯}{x}}_{0}'(X'X)^{- 1}{\\underset{¯}{x}}_{0})\\)\n추정량은 목표변수의 선형결합이므로 추정량의 샘플링분포는 정규분포를 따른다.\n주어진 예측변수 값 벡터 : \\({\\underset{¯}{x}}_{0}\\) 신뢰구간\n주어진 예측변수의 목표변수 기대값 추정량 : \\(E(\\widehat{{\\underset{¯}{y}}_{0}}|{\\underset{¯}{x}}_{0}) = {\\underset{¯}{x}}_{0}\\widehat{\\underset{¯}{\\beta}}\\)\n추정량 평균 : \\(E(\\widehat{{\\underset{¯}{y}}_{0}}|{\\underset{¯}{x}}_{0}) = {\\underset{¯}{x}}_{0}\\widehat{\\underset{¯}{\\beta}}\\)\n추정량 분산 : \\(V(\\widehat{{\\underset{¯}{y}}_{0}}|{\\underset{¯}{x}}_{0}) = \\sigma^{2}I\\)\n추정량은 목표변수의 선형결합이므로 추정량의 샘플링분포는 정규분포를 따른다.\n예측구간, 신뢰구간 어느 것을 사용하나? 신뢰구간이 예측구간보다 작으나 예측변수의 개별 관측값이 주어진 경우 목표변수 관측값을 예측하는 것이므로 예측구간을 사용하는 것이 적절하다.\n# refit_model에 적합했던 행렬을 그대로 사용\npred_int = refit_model.get_prediction(X_clean)\nprint(pred_int.summary_frame(alpha=0.05))\nmean mean_se mean_ci_lower mean_ci_upper obs_ci_lower obs_ci_upper  0 29.474740 0.399196 28.690070 30.259410 22.918274 36.031206  1 25.281274 0.195985 24.896039 25.666508 18.760542 31.802006\nimport pandas as pd\n\n# 예: 새 관측치 (변수는 최종 모형과 동일하게 입력)\nnew_X = pd.DataFrame({\n    \"NOX\": [0.5],\n    \"RM\": [6.5],\n    \"PTRATIO\": [18],\n    \"B\": [390],\n    \"LSTAT\": [10]\n})\n\n# 상수항 추가\nX_new = sm.add_constant(new_X, has_constant=\"add\")\n\n# 최종 모형 기준 예측\npred = refit_model.get_prediction(X_new)\n\n# 신뢰구간(평균 응답), 예측구간(새 관측치) 모두 포함\nprint(pred.summary_frame(alpha=0.05))\nmean mean_se mean_ci_lower mean_ci_upper obs_ci_lower obs_ci_upper  25.022419 0.178576 24.671405 25.373433 18.503619 31.541219"
  },
  {
    "objectID": "notes/linear_model/index.html",
    "href": "notes/linear_model/index.html",
    "title": "회귀분석",
    "section": "",
    "text": "회귀분석은 설명변수와 반응변수 간의 관계를 모형화하는 가장 기본적이면서도 중요한 통계 방법이다.\n본 장에서는 선형 회귀모형을 중심으로 추정, 진단, 확장 모형까지를 단계적으로 다룬다.\n왼쪽 메뉴에서 주제를 선택하시오."
  },
  {
    "objectID": "notes/linear_model/lm_multicolin.html",
    "href": "notes/linear_model/lm_multicolin.html",
    "title": "회귀분석 3. 다중공선성",
    "section": "",
    "text": "chapter 1. 다중공선성 개념\n다중공선성이란 독립변수들 간에 강한 선형관계가 존재하는 경우를 말한다. 회귀모형은 각 독립변수가 종속변수에 미치는 ”순수한 영향”을 추정하는 것을 목표로 한다. 그러나 독립변수끼리 강하게 상관되어 있으면 각 계수 추정치가 불안정해지고, 통계적 유의성이 왜곡될 수 있다.\n\n1. 다중공선성이란?\n다중 회귀모형에서 분석자가 가장 먼저 확인하는 것은 모형 전체가 목표변수를 잘 설명하는지에 대한 F-검정과, 각각의 예측변수가 독립적으로 의미 있는 역할을 하는지를 살펴보는 t-검정이다. 이러한 과정에서 자연스럽게 두 가지 관심사가 생긴다. 하나는 여러 예측변수 중에서 어떤 변수가 상대적으로 더 중요한가 하는 문제이고, 다른 하나는 각 변수가 목표변수에 구체적으로 어느 정도 영향을 미치는가 하는 문제이다.\n상대적 중요성은 표준화 회귀계수를 통해 비교할 수 있으며, 목표변수에 대한 구체적 설명력은 OLS로 추정된 회귀계수를 통해 확인할 수 있다. 회귀계수의 의미는 단순하다. 다른 조건이 모두 동일할 때, 특정 예측변수가 한 단위 증가하면 목표변수는 그 계수 값만큼 변동한다는 것이다. 그러나 이런 해석은 다른 예측변수들이 변하지 않고 고정되어 있다는 전제가 필요하다. 만약 예측변수들 사이에 상관관계가 전혀 없다면, 각 회귀계수는 그 변수의 순수한 효과, 즉 주변효과를 그대로 보여주게 된다.\n이처럼 회귀모형의 해석은 예측변수들 사이의 관계 구조에 크게 의존하며, 특히 예측변수들이 서로 강하게 연관되어 있을 때는 해석이 훨씬 복잡해진다.\n현실의 데이터에서 예측변수들이 완벽하게 서로 독립인 경우는 거의 없다. 다만 예측변수들 간의 상관계수가 낮아 통계적으로 유의하지 않다면, 회귀계수의 해석은 여전히 타당하게 이루어질 수 있다. 반대로 예측변수들 간의 상관관계가 높아 통계적으로 유의하다면, 그때는 OLS로 얻은 회귀계수의 추정치와 분산 추정이 불안정해지고 검정 결과 역시 신뢰할 수 없게 된다. 이러한 상황을 다중공선성(multicollinearity) 문제라고 부른다.\n예측변수들 사이에 높은 상관관계가 존재한다는 것은 곧 두 변수가 유사한 특성을 공유하고 있다는 뜻이며, 따라서 목표변수를 설명하는 역할에서도 상당 부분이 겹치게 된다. 원래 회귀분석에서 예측변수가 목표변수를 잘 설명한다는 것은 목표변수의 변동 방향을 정확하게 예측할 수 있음을 의미한다. 회귀계수가 양수라면 같은 방향으로, 음수라면 반대 방향으로 움직임을 포착한다는 것이다. 그런데 여러 예측변수가 동시에 목표변수와 강한 상관을 보인다면, 이들이 설명하는 변동 부분이 서로 중복되어 구분이 어려워진다. 이 때문에 다중공선성은 계수 해석을 혼란스럽게 만들고, 때로는 분석자가 의도한 해석을 무너뜨리기도 한다.\n발생 원인\n첫째는 구조적 다중공선성이다. 이는 연구자가 설정한 회귀모형 자체에서 비롯된다. 예를 들어 예측변수의 제곱항이나 곱항 같은 다항식 항을 포함하면, 원래 변수와 이들 변환 변수 사이에 강한 상관이 자연스럽게 발생한다. 이 경우 다중공선성은 데이터의 특성이 아니라 모형의 구조 때문에 나타난다.\n둘째는 데이터 기반 다중공선성이다. 이는 실제 관측된 자료에서 예측변수들 간의 상관관계가 매우 높을 때 발생한다. 예컨대 소득과 소비, 교육수준과 소득처럼 현실적으로 밀접히 연관된 변수들을 동시에 포함하면 이들 간에 중복된 설명력이 생겨 다중공선성이 나타난다. 즉, 구조적 다중공선성은 모형 설계의 결과, 데이터 다중공선성은 자료의 속성에서 비롯된다고 정리할 수 있다.\n\n\n2. 다중공선성 문제는 무엇인가?\n예측변수(\\(X_{k}\\)) 간의 상관관계가 높으면(예: 두 설명변수의 상관관계가 높으면 (데이터 행렬의 한 열(변수)이 다른 열(변수와 상관계수가 큰 변수)로 표현되므로 \\(det|X'X| \\approx 0\\) 되므로 \\((X'X)^{- 1} = \\frac{adj(X'X)}{det|X'X|}\\)의 값이 매우 커진다. 이로 인하여 회귀계수 OLS 추정치 \\(\\widehat{\\underset{¯}{b}} = (X'X)^{- 1}X'\\underset{¯}{y}\\) 값이 불안정해진다.\n다중공선성 문제를 일으키지 않는 예측변수의 회귀계수에 비해 변동이 커진다. - 회귀모형의 작은 변화에도 회귀계수의 추정치는 매우 민감하게 반응한다.\n그리고 OLS 추정의 추정분산(\\(s^{2}(\\widehat{\\underset{¯}{b}}) = MSE(X'X)^{- 1}\\)) 또한 매우 커져 회귀계수의 부호까지 바뀌는 문제가 발생한다.\n회귀계수 추정 오차가 커지면 추정 정확도가 떨어지고 더 이상 회귀계수 유의성을 판단하는 유의확률(p-값)에 대한 신뢰성도 낮아진다. - 즉, 회귀계수 유의성 검정의 신뢰성 저하되어 추정 회귀모형 신뢰성 낮아진다.\n\n\n3. 다중공선성 맛보기\n보스톤 주택 가격(medv)의 예측변수 (lstat, indus, rm) 3개를 이용 다중공선성 문제를 살펴보자.\n# ==============================================\n# Boston house 데이터\n# ==============================================\nimport pandas as pd\nfrom sklearn.datasets import fetch_openml\n\n# 1. 데이터 불러오기\nboston = fetch_openml(name=\"boston\", version=1, as_frame=True).frame\n\n#피어슨 상관계수\nboston[['MEDV','LSTAT','RM','PTRATIO']].corr(method ='pearson')\n\n\n\n\n\n\\(medv = 34.6 - 0.95*lstat,R^{2} = 54.4\\%\\)\n먼저 lstat만 포함한 단순회귀모형에서는 회귀계수가 -0.95로 추정된다. 이는 저소득층 비율이 한 단위 증가할 때 주택가격이 평균적으로 약 0.95 단위 감소한다는 의미이며, 결정계수는 약 54.4%로 나타난다.\n\\(medv = - 1.36 - 0.64*lstat + 5.09*rm,R^{2} = 63.9\\%\\)\n여기에 rm을 함께 넣으면 상황이 달라진다. lstat과 rm은 강한 상관관계를 갖는 변수인데, 두 변수를 동시에 포함하면 lstat의 회귀계수가 -0.64로 크게 줄어든다. 그 대신 rm의 회귀계수는 5.09로 추정되고, 결정계수는 63.9%로 상승한다. 즉, 두 변수가 설명하는 부분이 겹치면서 계수 값이 불안정해지고, 그 결과 단일변수 모형과 다변수 모형에서 해석이 달라지는 다중공선성의 전형적인 현상이 발생한다.\n\\(medv = 34.9 - 0.903*lstat - 0.08*indus,R^{2} = 54.6\\%\\)\n반면 indus처럼 lstat과의 상관관계가 낮은 변수를 추가하면 다른 결과가 나온다. indus를 포함한 모형에서 lstat의 계수는 여전히 -0.9 수준으로 거의 변하지 않고, 결정계수 역시 54.6%로 단 0.2% 정도만 증가한다. 즉, 상관성이 낮은 변수를 추가하면 기존 변수의 계수에 큰 영향을 주지 않으며, 설명력도 크게 개선되지 않는다.\n이 사례는 다중공선성이 주로 서로 상관관계가 큰 예측변수를 함께 투입할 때 발생한다는 점을 잘 보여준다. 목표변수를 설명하는 부분이 겹치기 때문에 회귀계수 값이 불안정해지고, 해석이 왜곡되거나 혼란스러워진다. 반대로 상관성이 낮은 변수를 포함할 때는 계수 해석이 비교적 안정적이다.\n\n\n4. 다중공선성 문재 반드시 해결 해야 하나?\n다중공선성 문제가 발생했다고 해서 반드시 그것을 제거하거나 해결해야 하는 것은 아니다. 다중공선성이 존재해도 회귀모형의 전체 예측력이나 적합도에는 큰 문제가 없기 때문이다. 실제로 목표변수를 예측하는 데 초점을 맞춘다면, 다중공선성이 있더라도 모형의 예측 성능은 크게 저하되지 않는다.\n문제는 주로 해석 단계에서 나타난다. 다중공선성이 심할 경우, 회귀계수의 표준오차가 커져서 유의성이 불명확해지고, 계수의 크기나 부호가 직관에 어긋나게 추정될 수 있다. 따라서 ”어떤 예측변수가 목표변수에 더 큰 영향을 미치는가?“와 같은 변수별 효과 해석이 중요한 연구에서는 다중공선성을 무시하기 어렵다.\n다중공선성이 반드시 해결해야 할 문제는 아니다. 특히 회귀모형에 통제변수가 포함된 경우, 다중공선성이 통제변수들에서만 발생하고 연구자가 관심을 두는 실험변수(주효과 변수)에서는 발생하지 않는다면, 굳이 제거하거나 수정할 필요가 없다.\n실제로 Applied Linear Statistical Models (4판, p.289)에서도 언급하듯이, 예측변수들 사이에 일정 수준의 상관이 존재한다고 해서 모형의 적합 자체가 저해되거나 새로운 관측값에 대한 예측이 무의미해지는 것은 아니다. 다중공선성은 주로 개별 회귀계수의 해석과 검정에 영향을 미칠 뿐, 평균 반응(mean response)이나 예측값(prediction)의 정확성에는 큰 타격을 주지 않는다.\n따라서 연구 목적이 ”주요 실험변수의 효과 검정”이라면, 통제변수들 사이의 다중공선성은 크게 문제 되지 않는다. 오히려 통제변수의 역할은 실험변수의 효과를 명확히 드러내는 데 있으므로, 약간의 공선성을 이유로 불필요하게 제거한다면 모형 해석이 왜곡될 수 있다.\n이는 Baron & Kenny(1986)가 조절변수(moderator)와 매개변수(mediator) 모형을 제시할 때도 중요한 함의로 다루어졌다. 통제변수나 매개·조절변수는 본래 주요 변수의 관계를 보다 정교하게 설명하기 위해 포함되는 것이며, 이 과정에서 일부 상관관계가 존재하더라도 반드시 제거해야 하는 것은 아니라는 것이다.\n즉, 실험변수에서 다중공선성이 없다면 문제 삼지 않아도 되며, 통제변수 사이의 공선성은 분석 목적상 자연스럽게 허용될 수 있다는 점이 핵심이다.\n매개효과 Mediator\n\n\n\n\n\n(순서1) \\(y = a + b_{11}X + e\\) : \\(b_{11}\\) 유의해야 한다.\n(순서2) \\(Me = a + b_{21}X + e\\) : \\(b_{21}\\) 유의해야 한다.\n(순서3) \\(y = a + b_{31}X + b_{32}Me + e\\) : \\(b_{32}\\) (매개효과) 유의하고 \\(b_{31}\\)은 \\(b_{11}\\)보다 절대값이 작아지거나 유의하지 않을 수 있음\n조절효과 Moderate\n\n\n\n\n\n\\(y = a + b_{1}X + b_{2}MO + b_{3}X*MO + e\\)\n\\(b_{3}\\)는 반드시 유의해야 한다.\n\n\n\nchapter 2. 다중공선성 진단\n\n1. 진단 도구\n\n(1) 산점도_상관계수\n다중공선성을 진단하는 가장 기본적인 출발점은 산점도 행렬과 상관계수 행렬이다. 회귀분석을 시작할 때, 먼저 산점도 행렬을 통해 종속변수와 각 설명변수의 관계를 시각적으로 살펴볼 수 있으며, 동시에 설명변수들 사이의 관계도 확인할 수 있다.\n상관계수 행렬은 보다 정량적으로 설명변수들 간의 상관관계를 파악하게 해준다. 이를 통해 종속변수와 밀접한 관련이 있는 설명변수를 미리 선별할 수 있을 뿐만 아니라, 설명변수들 사이에 높은 상관이 존재한다면 다중공선성이 발생할 가능성을 사전에 진단할 수 있다.\n이 방법은 직관적이고 계산이 간단하다는 장점이 있지만, 몇 가지 한계가 있다. 무엇보다 상관계수 행렬은 설명변수들 간의 쌍체(pairwise) 관계만 보여줄 뿐, 여러 변수들이 함께 작용하는 경우의 다중공선성을 직접 검정하지는 못한다. 또한 단순히 상관계수가 높다고 해서 반드시 심각한 다중공선성이 발생한다고 단정할 수 없고, 반대로 상관계수가 낮더라도 다중공선성이 존재할 수 있다.\n따라서 산점도와 상관계수는 다중공선성을 ”예상하고 탐색하는 도구”일 뿐, 유의성 검정을 제공하지는 못한다는 점을 명확히 이해할 필요가 있다..\n\n\n(2) 분산팽창지수 Variance Inflation Index\n\\(VIF_{k} = \\frac{1}{1 - R_{k}^{2}}\\)\n특정 예측변수를 목표변수로 하고 나머지 예측변수를 설명변수로 하여 회귀분석 한 후 결정계수를 구한다. 결정계수 \\(R_{k}^{2}\\) 값이 크다는 것은 \\(X_{k}\\)가 다른 예측변수들에 의해 충분히 설명되고 있다는 것이고 이는 바로 다중공선성 문제가 발생할 수 있다는 증거이다. 만약 결정계수 \\(R^{2} = 0.9\\)이면 분산팽창지수 \\(VIF = 10\\)이다.\n다수의 예측변수들에 의해 설명되는 정도를 나타내지만 상관계수와 같이 두 개의(쌍체) 예측변수에 의한 진단에는 적절하지 못하다. VIF 진단 기준값은 다음과 같이 상이하다.\n\n10 이상(심각한 수준): (Hair, J. F. Jr., Anderson, R. E., Tatham, R. L. & Black, W. C. (1995). Multivariate Data Analysis (3rd ed). New York: Macmillan.)\n5이상(문제로 간즈): Ringle, Christian M., Wende, Sven, & Becker, Jan-Michael. (2015).\n빅데이터에서는 보수적 기준으로 2.5를 사용한다.\n\n\n\n(3) 상태지수 Condition Index \\(CI = \\sqrt{\\frac{\\lambda_{max}}{\\lambda_{k}}}\\)\n예측변수의 공분산(상관계수)행렬을 이용하여 고유값 eigen value(\\(\\lambda_{k}\\)) 를 구한다. 이에 대응하는 Norm 1인 고유벡터가 주성분 부하 loading 이다. 고유값은 원변수(설명변수)의 선 형결합에 의해 만들어진 주성분 변수의 원변수 변동에 대한 설명력이다. 그러므로 고유값가 크다는 것(상태지수 값이 큰 값) 주성분의 원 변수 변동에 설명력이 크다는 것을 의미한다.\n고유벡터 내의 부하크기가 상대적으로 큰 변수들 간에는 상관관계가 높다는 것을 의미한다. 상태지수가 10이상(일반적으로 30이상)인 부하벡터에서 부하계수가 상대적으로 큰 변수들에 의해 다중공선성 문제가 발생한다고 진단할 수 있다.\n\n\n(4) 권장 진단 방법\n다중공선성을 진단할 때는 우선 분산팽창지수(VIF)를 활용하는 것이 일반적이다. 실무적으로는 VIF \\geq 3을 기준으로 주의가 필요하다고 본다. 이때 단순히 모든 변수를 대상으로 일괄 판단하기보다는, 최종적으로 유의하다고 확인된 설명변수를 중심으로 검토하는 것이 바람직하다.\n특히 최종 모형에서 추정된 회귀계수의 부호와, 해당 설명변수와 종속변수 간의 피어슨 상관계수 부호를 비교하는 것이 중요하다. 두 부호가 동일하다면, 비록 다중공선성이 존재한다고 하더라도 모형의 해석에 심각한 왜곡을 주지 않으므로 굳이 다중공선성을 해결할 필요가 없다.\n\n\n\n\n\n\n\n\n구분\n반드시 해결해야 하는 경우\n해결하지 않아도 되는 경우\n\n\n연구 목적\n각 설명변수의 개별 효과 해석이 중요할 때 (정책·이론적 함의)\n예측력 확보가 주목적일 때\n\n\n통계 지표\nVIF 값이 매우 큼 (예: 10 이상) / Condition Index ≥ 30 이상\nVIF 값이 다소 높더라도 (3~5 수준)\n\n\n회귀계수\n추정된 회귀계수의 부호가 종속변수와 설명변수 간 피어슨 상관계수 부호와 불일치할 때\n회귀계수 부호와 피어슨 상관계수 부호가 동일할 때\n\n\n모형 안정성\n표본을 바꿀 때 회귀계수 추정치가 크게 요동할 때\n표본 변화에도 회귀계수가 크게 변하지 않을 때\n\n\n\n# ==============================================\n# Boston Housing: Multicollinearity Diagnostics (All predictors - \"ZN\" 제외 + Correlation)\n# ==============================================\nimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nfrom sklearn.datasets import fetch_openml\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\n# ------------------------------------------------\n# 1) 데이터 로드\n# ------------------------------------------------\nboston = fetch_openml(name=\"boston\", version=1, as_frame=True)\ndf = boston.frame.copy()\n\n# 숫자형으로 변환\nfor c in df.columns:\n    if df[c].dtype.name in [\"object\", \"category\"]:\n        df[c] = pd.to_numeric(df[c].astype(str).str.strip(), errors=\"coerce\")\ndf = df.dropna().reset_index(drop=True)\n\ny = df[\"MEDV\"]\nX = df.drop(columns=[\"MEDV\",\"ZN\"])   # 모든 예측변수\n\nprint(\"전체 예측변수 목록:\", list(X.columns))\n\n# ------------------------------------------------\n# 2) 상관계수 행렬\n# ------------------------------------------------\nprint(\"\\n[1] 상관계수 행렬 (medv 포함)\")\ncorr = df.corr()\nprint(corr.round(3))\n\n# ------------------------------------------------\n# 3) VIF 계산\n# ------------------------------------------------\ndef compute_vif(Xdf):\n    X_ = sm.add_constant(Xdf)\n    vif_list = []\n    for i, col in enumerate(X_.columns):\n        if col == \"const\":\n            continue\n        vif = variance_inflation_factor(X_.values, i)\n        vif_list.append({\"variable\": col, \"VIF\": vif})\n    return pd.DataFrame(vif_list).sort_values(\"VIF\", ascending=False)\n\nprint(\"\\n[2] VIF (모든 변수)\")\nvif_tbl = compute_vif(X)\nprint(vif_tbl.round(3))\n\n# ------------------------------------------------\n# 4) Condition Index (Belsley)\n# ------------------------------------------------\ndef condition_index(Xdf):\n    Z = (Xdf - Xdf.mean(axis=0)) / Xdf.std(axis=0, ddof=1)\n    C = np.corrcoef(Z.values, rowvar=False)\n    eigvals, eigvecs = np.linalg.eigh(C)\n    lam_max = eigvals.max()\n    ci = np.sqrt(lam_max / eigvals)\n    Phi = eigvecs / np.sqrt(eigvals)[None, :]\n    phi2 = Phi**2\n    prop = phi2 / phi2.sum(axis=0, keepdims=True)\n    ci_tbl = pd.DataFrame({\n        \"eigenvalue\": eigvals,\n        \"condition_index\": ci\n    })\n    prop_tbl = pd.DataFrame(\n        prop,\n        index=Xdf.columns,\n        columns=[f\"eig{k+1}\" for k in range(len(eigvals))]\n    )\n    return ci_tbl.sort_values(\"condition_index\", ascending=True), prop_tbl\n\nci_tbl, prop_tbl = condition_index(X)\n\nprint(\"\\n[3] Condition Index\")\nprint(ci_tbl.round(3))\nprint(\"\\n[3-1] Variance-Decomposition Proportions\")\nprint(prop_tbl.round(3))\n\n# ------------------------------------------------\n# 5) 전체 변수 포함 OLS 회귀 적합\n# ------------------------------------------------\nX_const = sm.add_constant(X)\nols_full = sm.OLS(y, X_const).fit()\n\nprint(\"\\n[4] 전체 회귀 결과 요약\")\nprint(ols_full.summary())\n\n# ------------------------------------------------\n# 6) 결론 요약\n# ------------------------------------------------\nprint(\"\\n[결론]\")\nprint(\"- 상관계수 행렬로 변수 간 높은 pairwise 관계 확인.\")\nprint(\"- VIF ≥ 5 (또는 10) → 다중공선성 문제 의심.\")\nprint(\"- Condition Index ≥ 30 → 다중공선성 가능, ≥ 100 → 심각.\")\nprint(\"- 회귀계수 부호와 단순 상관 부호가 다를 경우 해석 주의.\")\n전체 예측변수 목록: ['CRIM', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT']\n[1] 상관계수 행렬 (medv 포함)\nCRIM ZN INDUS CHAS NOX RM AGE DIS RAD TAX PTRATIO B LSTAT MEDV  CRIM 1.000 -0.200 0.407 -0.056 0.421 -0.219 0.353 -0.380 0.626 0.583 0.290 -0.385 0.456 -0.388  ZN -0.200 1.000 -0.534 -0.043 -0.517 0.312 -0.570 0.664 -0.312 -0.315 -0.392 0.176 -0.413 0.360  (중략)  MEDV -0.388 0.360 -0.484 0.175 -0.427 0.695 -0.377 0.250 -0.382 -0.469 -0.508 0.333 -0.738 1.000\n[2] VIF (모든 변수)\nvariable VIF  8 TAX 8.560  7 RAD 7.401  3 NOX 4.388  1 INDUS 3.946  6 DIS 3.314  5 AGE 3.056  11 LSTAT 2.933  4 RM 1.887  0 CRIM 1.779  9 PTRATIO 1.625  10 B 1.348  2 CHAS 1.074\n[3] Condition Index\neigenvalue condition_index  11 5.769 1.000  10 1.367 2.054  9 1.143 2.246  8 0.855 2.598  7 0.772 2.734  6 0.602 3.094  5 0.490 3.432  4 0.330 4.179  3 0.239 4.915  2 0.197 5.408  1 0.170 5.827  0 0.066 9.362\n[3-1] Variance-Decomposition Proportions\neig1 eig2 eig3 eig4 eig5 eig6 eig7 eig8 eig9 eig10 eig11 eig12  CRIM 0.002 0.006 0.018 0.013 0.099 0.206 0.407 0.031 0.002 0.109 0.035 0.072  INDUS 0.064 0.024 0.038 0.468 0.136 0.085 0.016 0.013 0.000 0.013 0.015 0.127  (중략)  LSTAT 0.000 0.004 0.240 0.138 0.265 0.024 0.010 0.079 0.004 0.115 0.016 0.103\n\n\n\n\n\n1. 상관계수 행렬\n\nINDUS–NOX (0.764), RAD–TAX (0.910), NOX–DIS (-0.769) 같이 변수 간 높은 상관이 존재합니다.\nLSTAT는 MEDV와 -0.738, RM은 MEDV와 0.695로 강한 상관 → 주택가격의 핵심 설명변수임을 확인.\n따라서, 일부 예측변수는 서로 매우 비슷한 정보를 담고 있음(중복 설명력).\n\n2. VIF\n\nTAX(8.56), RAD(7.40) → 공선성 높은 변수 후보.\nNOX(4.39), INDUS(3.95), DIS(3.31), AGE(3.06)도 다소 높은 값.\n”RAD–TAX”가 대표적 다중공선성 원인 쌍임을 알 수 있음.\n\n3. Condition Index\n\n최대 Condition Index = 9.36, 기준선(30 이상)에는 미치지 않음.\n즉, 심각한 다중공선성은 아님. 다만 분산분해비(Proportions)를 보면 특정 고유성분에서 RAD, TAX, NOX 등이 같이 큰 값을 가짐 → 이 변수들이 공선성에 기여.\nOLS 결과의 condition number = 1.51e+04는 수치적으로 큰 편이라, 다중공선성 가능성을 경고. (표준화/스케일 차이의 영향도 일부 있음.)\n\n4. OLS 결과\n\n모형 설명력: 결정계수 73.5%로 모든 예측변수는 주택가격 변동의 73.5% 설명 → 상당히 높은 수준이다.\n주요 유의 변수: RM(방 개수, 양수 효과), LSTAT(저소득층 비율, 음수 효과), PTRATIO(학생-교사 비율, 음수 효과), NOX(대기오염도, 음수 효과), DIS(도심 접근성, 음수 효과), RAD, TAX, B, CHAS.\n비유의 변수: INDUS, AGE → 높은 상관은 있지만 독립적으로 설명력 없음. 이는 다른 변수들과 중복된 영향(공선성) 때문.\nRAD 예측변수는 목표변수와 음의 상관관계가 있는데 회귀계수는 양이므로 RAD–TAX”가 대표적 다중공선성 원인 쌍임으로 보여주므로 TAX보다 목표변수와 상관관계가 낮은 RAD 변수는 제외하는 것이 적절하다.\n\n5. 종합 결론\n\n다중공선성 신호: RAD–TAX, INDUS–NOX–DIS–AGE. → 이들 변수는 서로 강하게 연결되어 있어, 개별 해석은 조심해야 함.\n해석 안정성: 주요 변수 RM(+), LSTAT(-), PTRATIO(-)는 상관계수 부호와 회귀계수 부호가 일치 → 해석 신뢰 가능.\nRAD 변수를 제외하고, 예측변수의 유의성을 검증한 후 회귀진단을 실시하면 된다.\n예측 vs 해석: 전체 모형은 예측에는 강력하지만, 각 설명변수의 순수 효과 해석은 일부 왜곡 가능성이 있음. 정책적 함의를 뽑아내려면 변수 선택/차원축소(예: Ridge, PCA 회귀)가 바람직.\n\n6. 최종 회귀모형\n\n(RM, “DIS”,“AGE”)은 변수선택에서 제외\nRAD는 다중공선성 문제 발생으로 제외\n\n# ==============================================\n# 최종 회귀모형 선택\n# ==============================================\n\nimport pandas as pd\nimport numpy as np\nimport statsmodels.api as sm\nfrom sklearn.datasets import fetch_openml\n\n# --- 0) 데이터 로드 ---\nboston = fetch_openml(name=\"boston\", version=1, as_frame=True)\ndf = boston.frame.copy()\n\n# 종속변수 / 설명변수\ny = pd.to_numeric(df[\"MEDV\"], errors=\"coerce\")\nX = df.drop(columns=[\"MEDV\", \"RM\", \"RAD\",\"DIS\",\"AGE\"]).copy()\n단계선택 방법: 결정계수 67.1%\n\n\n\n\n\n\nDIS: 목표변수와 상관계수 부호와 회귀계수 부호 상이 -&gt; 경고문에 다중 공선성 문제 발생 출력되었다.\n다시 단계선택 방법을 적용한 결과 AGE 변수도 동일한 문제 발셍 -&gt; 경고문에 다중 공선성 문제 발생 출력되었다.\n최종 모형: LSTAT, PTRATIO, CHAS 3개 예측변수만 사용되었다.\n\n\n\n\n\n\n\n\n\n\nchapter 3. 다중공선성 해결방안\n구조적 다중공선성 문제 해결 - 예측변수 centering (중심화) - 다항식 모형(예측변수의 제곱항, 세제곱항)에 적합\n\n표준화는 데이터의 평균과 표준편차를 활용하여 단위를 없앤 개념이다. \\(z_{i} = \\frac{x_{i} - {\\overline{x}}_{i}}{s(x_{i})}\\)\n중심화는 데이터의 평균을 0으로 이동한 개념임, 산포는 동일하게 유지된다. \\(c_{i} = x_{i} - {\\overline{x}}_{i}\\)\n\n문제변수 제거\n\n다중공선성 문제가 되는 설명변수를 제외한다.\n여러개가 존재하는 경우 목표변수와 상관관계가 낮은 예측변수를 제외한다.\n\n\n1. 주성분분석\n주성분분석(PCA)은 여러 개의 상관된 변수들을, 서로 상관이 없는 새로운 변수들(주성분, principal components)로 변환하는 차원 축소 방법이다. 각 주성분은 원래 변수들의 선형결합으로 만들어지며, 첫 번째 주성분은 데이터 변동성을 가장 크게 설명하고, 두 번째 주성분은 그 다음으로 큰 변동성을 설명하되 첫 번째 주성분과 직교(orthogonal)하도록 구성된다.\n즉, PCA의 목적은 복잡하게 얽힌 변수들의 중복된 정보를 제거하고, 데이터의 주요한 구조를 단순화하는 데 있다. 주성분분석은 단순히 데이터 요약 방법이 아니라, 다중공선성 문제를 해결할 수 있는 강력한 도구이다. 원래 변수들 간의 상관관계를 제거하고 새로운 직교 좌표축을 제공하기 때문에, 예측력을 유지하면서 회귀계수의 안정성을 높여준다. 다만, 주성분의 해석이 직관적이지 않다는 점을 감안하여, 예측이 목적일 때 특히 효과적이고, 해석이 목적일 때는 신중하게 적용해야 한다.\n다중공선성은 예측변수들 간의 높은 상관관계에서 발생한다. 여러 변수가 서로 비슷한 정보를 담고 있기 때문에, 회귀분석에서 회귀계수가 불안정해지고 표준오차가 커지는 문제가 생긴다.\nPCA를 적용하면 원래 상관관계가 높던 변수들을 서로 독립적인 주성분으로 변환할 수 있다. 이렇게 하면 회귀모형에 투입되는 설명변수들 사이의 공선성이 사라지고, 안정된 계수 추정이 가능해진다.\n해결 방안으로서 PCA의 활용\n\nPCA를 통해 얻은 주성분들을 새로운 설명변수로 사용하여 회귀분석을 수행한다.\n주성분들은 서로 독립적이므로 다중공선성 문제가 해소된다.\n다만, 주성분은 원래 변수의 해석이 쉽지 않다는 단점이 있다.\n\n차원 축소 효과\n\n설명력이 낮은 주성분을 제외하고 상위 몇 개의 주성분만 사용하면, 변동성을 유지하면서 불필요한 잡음과 다중공선성을 동시에 줄일 수 있다.\n예를 들어, 원래 10개 변수로 구성된 데이터라도 상위 34개의 주성분으로 전체 변동성의 8090%를 설명할 수 있다면, 훨씬 단순하고 안정적인 모형을 얻을 수 있다.\n\n# ==============================================\n# PCA (주성분분석) + Principal Component Regression\n# ==============================================\nimport numpy as np\nimport pandas as pd\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nimport statsmodels.api as sm\n\n# ------------------------------------------------\n# 1) 데이터 로드\n# ------------------------------------------------\nboston = fetch_openml(name=\"boston\", version=1, as_frame=True)\ndf = boston.frame.copy()\n\n# 숫자형 변환\nfor c in df.columns:\n    if df[c].dtype.name in [\"object\", \"category\"]:\n        df[c] = pd.to_numeric(df[c].astype(str).str.strip(), errors=\"coerce\")\ndf = df.dropna().reset_index(drop=True)\n\ny = df[\"MEDV\"]\nX = df.drop(columns=[\"MEDV\",\"ZN\"])\n\nprint(\"예측변수 목록:\", list(X.columns))\n\n# ------------------------------------------------\n# 2) 표준화\n# ------------------------------------------------\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# ------------------------------------------------\n# 3) PCA 적합\n# ------------------------------------------------\npca = PCA()\nX_pca = pca.fit_transform(X_scaled)\n\n# 설명된 분산 비율 (각 주성분 기여도)\nexplained_var = pca.explained_variance_ratio_\nprint(\"\\n[1] 각 주성분 설명 분산 비율\")\nprint(np.round(explained_var, 3))\n\nprint(\"\\n[2] 누적 설명력\")\nprint(np.round(np.cumsum(explained_var), 3))\n\n# ------------------------------------------------\n# 4) 주성분 부하량 (loadings)\n# ------------------------------------------------\nloadings = pd.DataFrame(\n    pca.components_.T,\n    index=X.columns,\n    columns=[f\"PC{i+1}\" for i in range(len(X.columns))]\n)\nprint(\"\\n[3] 주성분 부하량 (원 변수 → 주성분 기여도)\")\nprint(loadings.round(3))\n\n# ------------------------------------------------\n# 5) 주성분 점수 DataFrame\n# ------------------------------------------------\npc_scores = pd.DataFrame(X_pca, columns=[f\"PC{i+1}\" for i in range(len(X.columns))])\nprint(\"\\n[4] 주성분 점수 DataFrame (앞부분)\")\nprint(pc_scores.head())\n\n# ------------------------------------------------\n# 6) PCR (주성분 회귀 예시: 상위 k개 주성분만 사용)\n# ------------------------------------------------\nk = 5   # 상위 5개 주성분만 사용\nX_pc = sm.add_constant(pc_scores.iloc[:, :k])\npcr_model = sm.OLS(y, X_pc).fit()\n\nprint(f\"\\n[5] PCR 결과 (상위 {k}개 주성분 사용)\")\nprint(pcr_model.summary())\n예측변수 목록: ['CRIM', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT']\n[1] 각 주성분 설명 분산 비율\n[0.481 0.114 0.095 0.071 0.064 0.05 0.041 0.028 0.02 0.016 0.014 0.005]\n[2] 누적 설명력\n[0.481 0.595 0.69 0.761 0.825 0.876 0.916 0.944 0.964 0.98 0.995 1. ]\n[3] 주성분 부하량 (원 변수 → 주성분 기여도)\nPC1 PC2 PC3 PC4 PC5 PC6 PC7 PC8 PC9 PC10 PC11 PC12  CRIM 0.268 -0.186 0.330 0.046 -0.177 0.638 0.454 -0.315 0.114 0.135 0.075 0.041  INDUS 0.356 0.124 -0.114 0.009 0.112 -0.128 -0.292 -0.369 0.684 0.196 -0.154 -0.254  (중략)  LSTAT 0.321 -0.127 -0.339 -0.062 -0.281 0.102 0.155 0.515 0.371 -0.490 0.067 0.018\n[4] 주성분 점수 DataFrame (앞부분)\nPC1 PC2 PC3 PC4 PC5 PC6 PC7 PC8 PC9 PC10 PC11 PC12  0 -2.100508 0.794845 0.098831 -0.978169 -0.095970 0.507272 -0.023877 -0.043068 -0.750781 0.223956 0.029284 0.362596  1 -1.649298 0.094635 -0.483831 -0.397519 0.141078 -0.028798 0.605383 0.183476 -0.022345 0.529996 -0.219936 -0.128753  2 -2.280518 0.345673 0.555412 -0.384408 0.592043 -0.231237 0.497210 -0.166574 0.223905 0.313149 0.045882 -0.085665  3 -2.826744 -0.361504 0.637358 0.061903 0.436344 -0.235396 0.502932 -0.070508 -0.213217 0.249970 0.429724 -0.080312  4 -2.669692 -0.242109 0.591630 -0.035176 0.478284 -0.226862 0.723477 0.269469 -0.043381 0.212839 0.408998 -0.076439\n\n\n\n\n\n1. PC1 (분산 비율 48.1%)\n\nCRIM(+), INDUS(+), NOX(+), AGE(+), TAX(+), LSTAT(+) vs RM(-), DIS(-), B(-)\n해석: 도시집중·노후·환경악화 축 → 범죄율, 공업비율, 공해, 노후주택, 세금, 저소득층 비율이 높고, 방 개수·쾌적성은 낮은 방향\n이름 제안: ”도시 낙후·환경 악화 요인”\n\n2. PC2 (11.4%)\n\nCHAS(+), RM(+), AGE(+), NOX(+), vs PTRATIO(-), DIS(-)\n해석: 강변·주거환경·교육조건 축 → 찰스강 인접, 방 개수와 노후도, 환경이 관련되며, 학급당 학생수(PTRATIO)와 도심거리(DIS)는 반대\n이름 제안: ”강변 주거환경·교육 조건 요인”\n\n3. PC3 (9.5%)\n\nRM(+), RAD(+), TAX(+), CRIM(+), vs B(-), LSTAT(-)\n해석: 주거규모·교통·재정 축 → 방이 많고 교통 접근성(RAD, TAX)과 범죄율이 높을수록, 흑인비율(B)과 저소득층 비율(LSTAT)은 반대\n이름 제안: ”주거규모·교통 재정 요인”\n\n4. PC4 (7.1%)\n\nCHAS(+), PTRATIO(+), B(+), INDUS(+), 일부 환경 변수와 약한 관련\n해석: 특수 입지·교육 인프라 축 → 찰스강 인접, 교육 환경, 일부 산업 비율 등이 섞여 있는 보조 요인\n이름 제안: ”입지·교육 보조 요인”\n\n5. PC5 (6.4%)\n\nB(+), RM(+), PTRATIO(+), 일부 LSTAT(-)\n해석: 주택 품질·사회구성 축 → 흑인비율, 방 개수, 학급당 학생 수와 연관 → 주거 품질과 사회적 요인이 혼합\n이름 제안: ”주택 품질·사회구성 요인”\n\n\n\n2. 능형 회귀분석 Ridge Regression\n다중공선성은 회귀계수의 추정오차를 증가시키므로 불편성(OLS는 불편 추정량이다)을 포기하는 대신 MSE(Mean Square of Error 평균오차자승합)를 최소화 하는 편기(biased) 추정량을 구하는 추정 방법을 사용함으로써 다중공성선 문제를 해결하는데 이를 능형 회귀분석((Ridge Regression)이라 한다.\n\n편기 추정량 \\(\\widehat{{\\underset{¯}{b}}_{*}} = (X'X + \\lambda I)X'\\underset{¯}{y}\\)의 \\(MSE(\\widehat{{\\underset{¯}{b}}_{*}}) = E(\\widehat{{\\underset{¯}{b}}_{*}} - \\underset{¯}{b})^{2}\\)를 최소화 하는 \\(0 &lt; \\lambda\\)를 구하면 이를 능형 추정량이라 한다. \\(c = 0\\)이면 OLS 추정값이다.\n상수 \\(c\\)와 VIF 산점도를 이용하여 각 예측변수의 추정 회귀계수 값이 안정화 되는 \\(c\\)값을 최적값으로 설정한다. 다소 주관적이다.\nRidge 회귀는회귀계수 해석”보다는 ”예측 안정성 향상에 초점을 두므로 OLS처럼 ”이 변수가 유의하다/아니다”를 말하기 위해 쓰는 기법은 아니다.\n\n# ==============================================\n# 능형 Regression\n# ==============================================\nimport numpy as np\nimport pandas as pd\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import RidgeCV\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import KFold\n\n# 데이터\nboston = fetch_openml(name=\"boston\", version=1, as_frame=True)\ndf = boston.frame.dropna().reset_index(drop=True)\ny = df[\"MEDV\"]\nX = df.drop(columns=[\"MEDV\",\"ZN\"])\n\n# 후보 λ (alpha)들\nalphas = np.logspace(-3, 3, 100)\n\n# 파이프라인: 표준화 + RidgeCV(K-fold)\npipe = Pipeline([\n    (\"scaler\", StandardScaler()),\n    (\"ridge\", RidgeCV(alphas=alphas, cv=KFold(n_splits=10, shuffle=True, random_state=42),\n                      scoring=\"neg_mean_squared_error\"))\n])\n\npipe.fit(X, y)\nbest_alpha = pipe.named_steps[\"ridge\"].alpha_\ncoefs = pipe.named_steps[\"ridge\"].coef_\n\nprint(\"✅ 최적 λ(alpha):\", best_alpha)\nprint(\"✅ 회귀계수:\", pd.Series(coefs, index=X.columns).sort_values(key=abs, ascending=False))\n최적 λ(alpha): 4.9770235643321135\n추정회귀 계수:  LSTAT -3.625057  RM 2.852304  DIS -2.451308  PTRATIO -2.298852  RAD 2.188506  NOX -1.981206  TAX -1.381737  B 0.855040  CRIM -0.815508  CHAS 0.704650  AGE -0.152481  INDUS -0.094937"
  },
  {
    "objectID": "notes/math_stat/multi_variate.html",
    "href": "notes/math_stat/multi_variate.html",
    "title": "수리 통계 4. 이변량 확률변수",
    "section": "",
    "text": "chapter 1. 확률변수와 확률분포함수\n두 개 이상의 확률변수를 포함하는 확률 모형을 다루는데, 이는 자연스럽게 다변량 모형이라고 불린다.\n실험적 상황에서 단 하나의 확률변수 값만을 관측하는 것은 매우 이례적일 것이다. 즉, 모든 수집된 데이터가 단 하나의 숫자로 이루어진 실험은 매우 드문 경우이다. 은행 고객이 창구에서 기다리는 시간과 실제로 서비스를 받는 시간은 서로 연관될 수 있다. 대기 시간이 길어지면 서비스 시간이 짧아질 수도 있고, 반대로 복잡한 업무로 인해 서비스 시간이 길어질 경우 대기 시간이 더욱 증가할 수도 있다. 이러한 두 확률변수를 함께 분석하면 은행의 서비스 효율성을 평가하는 데 유용하다.\n특정 시간에 바이러스에 감염된 환자 수와 24시간이 지난 후 치료된 환자 수는 상관관계를 가질 수 있다. 예를 들어, 감염 환자가 많을수록 치료된 환자 수도 많을 가능성이 크지만, 치료 속도나 의료 시스템의 효율성에 따라 그 관계는 다르게 나타날 수 있다.\n동일한 개체에서 여러 물리적 특성을 측정함으로써 다변량 데이터가 생성될 수도 있다. 예를 들어, 체중뿐만 아니라 체온, 키, 혈압 등의 다양한 건강 지표가 함께 측정될 수 있다. 이와 같이 서로 다른 특성에 대한 측정값들은 각각 개별적인 확률변수로 모델링될 수 있다. 따라서 우리는 두 개 이상의 확률변수를 동시에 다루는 확률 모형을 기술하고 활용하는 방법을 알아야 한다.\n이변량 확률변수는 \\((X,Y)\\)등 알파벳 대문자를 사용하며 다변량 확률변수는 \\((X_{1},X_{2},\\ldots,X_{n})\\) 알파벳 대문자와 아래 첨자를 사용하여 표현한다.\n\n1. 결합 확률밀도함수\n\\(n\\)차원 확률벡터는 표본공간 \\(S\\)에서 \\(\\mathbb{R}^{n}\\)(즉, \\(n\\)차원 유클리드 공간)으로 가는 함수이다.\n【정의】 이차원 확률벡터는 \\((x,y) \\in \\mathbb{R}^{2}\\)(이차원 평면) 된다. 두 개의 주사위를 던지는 실험을 고려하자. 이 실험의 표본공간은 36개의 동일한 확률을 가지는 결과들로 이루어진다.\n【예제 ①】 확률변수 \\(X\\)을 두 주사위 눈금의 합, 확률변수 \\(Y\\)을 두 주사위 눈금의 차이의 절대값으로 정의하자.\n표본점 (4,1)과 (1,4)는 주사위 두 개의 눈이 각각 (4,1) 또는 (1,4)로 나오는 경우이므로 \\(P(\\{(4,1)\\}) = P(\\{(1,4)\\}) = 1/36\\)이다.\n\\[P(X = 5,Y = 3) = P(\\{ 1,4\\},\\{ 4,1\\}) = 1/18\\]\n위에서 정의된 확률벡터 \\((X,Y)\\)는 이산확률벡터라고 불린다. 이는 가능한 값들의 개수가 셀 수 있으며(이 경우 유한한 개수) 존재하기 때문이다. 이산확률벡터의 경우 확률밀도함수는 다음과 같다.\n\\(f(x,y) = P(X = x,Y = y)\\).\n\\((X,Y)\\)를 이산형 이변량 확률벡터라고 하자. 이때, 함수 \\(f(x,y)\\)는 \\(\\mathbb{R}^{2}\\)에서 \\(\\mathbb{R}\\)로 정의되며, 다음과 같이 표현된다.\n【정의 이산형】 \\(f_{X,Y}(x,y) = P(X = x,Y = y)\\) . 이 함수는 이산형 결합확률질량함수(joint probability mass function)라고 불린다.\n함수 \\(f(x,y)\\)가 \\(\\mathbb{R}^{2}\\)에서 \\(\\mathbb{R}\\)로 가는 함수일 때,\n【정의 연속형】 이 함수가 연속 이변량 확률벡터 \\((X,Y)\\)의 연속형 결합확률밀도함수라고 불리려면, 모든 \\(A \\subset \\mathbb{R}^{2}\\)에 대해 다음 조건을 만족해야 한다.\n\\[P((X,Y) \\in A) = \\int_{A}\\int f(x,y)dxdy\\]\n【예제 ① 계속】 위의 주사위 두 개를 굴리는 경우로 정의된 \\((X,Y)\\)가 가질 수 있는 21가지 가능한 값이 있다. 이 21가지 가능한 값에 대한 확률, \\(f(x,y)\\) 값을 정의하는 함수가 결합확률밀도함수이다.\n결합확률밀도함수를 이용하면 \\((X,Y)\\)에 대한 임의의 사건 \\(A \\subseteq \\mathbb{R}^{2}\\)에 대한 확률은 \\(P((X,Y) \\in A) = \\sum_{(x,y) \\in A}f(x,y)\\)이다. 예를 들어, \\(A = \\{(x,y):x = 7\\text{and}y \\leq 4\\}\\)애 대한 확률을 계산해 보자.\n\\[P(X = 7,Y \\leq 4) = P((X,Y) \\in A) = f(7,1) + f(7,3) = \\frac{1}{18} + \\frac{1}{18} = \\frac{1}{9}\\]\n【예제 ① 계속】 \\(E(XY)\\)을 구하시오.\n\\(E\\lbrack XY\\rbrack = (2)(0) \\cdot \\frac{1}{36} + (4)(0) \\cdot \\frac{1}{36} + \\cdots + (8)(4) \\cdot \\frac{1}{18} + (7)(5) \\cdot \\frac{1}{18} = 13\\frac{11}{18}\\)\n\\(g_{1}(x,y)\\)와 \\(g_{2}(x,y)\\)가 두 확률밀도함수이고, \\(a,b,c\\)가 상수일 때 다음의 기대값 합의 연산자가 성립한다.\n\\[\\mathbb{E}\\left\\lbrack ag_{1}(X,Y) + bg_{2}(X,Y) + c \\right\\rbrack = a\\mathbb{E}\\lbrack g_{1}(X,Y)\\rbrack + b\\mathbb{E}\\lbrack g_{2}(X,Y)\\rbrack + c\\]\n【예제 ②】 다음은 두 확률변수 \\((X,Y)\\)의 결합확률밀도함수, \\(f(x,y)\\) 이다. \\(f(0,0) = f(0,1) = \\frac{1}{6},f(1,0) = f(1,1) = \\frac{1}{3}.\\)\n확률계산은 \\(P(X = Y) = f(0,0) + f(1,1) = \\frac{1}{2}\\) 바로 가능하다.\n【연속형】 \\(\\mathbb{E}\\lbrack g(X,Y)\\rbrack = \\int_{- \\infty}^{\\infty}\\int_{- \\infty}^{\\infty}g(x,y)f(x,y)dxdy\\).\n\n\n2. 주변 확률밀도함수\n\n(1) 이산형\n【정리】 이산 이변량 확률벡터 \\((X,Y)\\)가 결합 확률질량함수 \\(f_{X,Y}(x,y)\\)를 가진다고 하자. 그러면, \\(X\\)와 \\(Y\\)의 주변 확률질량함수(marginal pmf)는 각각 다음과 같이 주어진다.\n\\[f_{X}(x) = \\sum_{y \\in \\mathbb{R}}f_{X,Y}(x,y),f_{Y}(y) = \\sum_{x \\in \\mathbb{R}}f_{X,Y}(x,y)\\]\n【예제 ① 계속】 확률변수 \\(Y\\)의 주변 확률밀도함수는 다음과 같다.\n\\[f_{Y}(0) = f_{X,Y}(2,0) + f_{X,Y}(4,0) + f_{X,Y}(6,0) + f_{X,Y}(8,0) + f_{X,Y}(10,0) + f_{X,Y}(12,0) = \\frac{1}{6}\\]\n\\[f_{Y}(1) = \\frac{5}{18},f_{Y}(2) = \\frac{2}{9},f_{Y}(3) = \\frac{1}{6},f_{Y}(4) = \\frac{1}{9},f_{Y}(5) = \\frac{1}{18}\\]\n【예제 ① 계속】 \\(P(Y &lt; 3)\\)과 \\(E(Y^{3})\\)을 구하시오.\n\\[P(Y &lt; 3) = f_{Y}(0) + f_{Y}(1) + f_{Y}(2) = \\frac{1}{6} + \\frac{5}{18} + \\frac{2}{9} = \\frac{2}{3}\\]\n\\[\\mathbb{E}\\lbrack Y^{3}\\rbrack = 0^{3}f_{Y}(0) + \\cdots + 5^{3}f_{Y}(5) = 20 \\cdot \\frac{11}{18}\\]\n\n\n(2) 연속형\n\\(X\\)와 \\(Y\\)의 주변 확률밀도함수는 다음과 같이 주어진다.\n\\[f_{X}(x) = \\int_{- \\infty}^{\\infty}f(x,y)dy, - \\infty &lt; x &lt; \\infty,\\]\n\\[f_{Y}(y) = \\int_{- \\infty}^{\\infty}f(x,y)dx, - \\infty &lt; y &lt; \\infty.\\]\n【예제 ③】 이변량 확률변수 \\((X,Y)\\)의 연속형 이변량 확률밀도함수는\n\\(f(x,y) = \\{\\begin{matrix}\n6xy^{2} & \\text{if}0 &lt; x &lt; 1\\text{and}0 &lt; y &lt; 1 \\\\\n0 & \\text{otherwise}\n\\end{matrix}\\)이다.\n\\(P(X + Y \\geq 1)\\) 확률 값은?\n\\[\\begin{matrix}\nA & = \\{(x,y):x + y \\geq 1,0 &lt; x &lt; 1,0 &lt; y &lt; 1\\} \\\\\n& = \\{(x,y):x \\geq 1 - y,0 &lt; x &lt; 1,0 &lt; y &lt; 1\\} \\\\\n& = \\{(x,y):1 - y \\leq x &lt; 1,0 &lt; y &lt; 1\\}\n\\end{matrix}\\]\n\\[P(X + Y \\geq 1) = \\int_{A}f(x,y)dxdy = \\int_{0}^{1}\\int_{1 - y}^{1}6xy^{2}dxdy = \\frac{9}{10}\\]\n【예제 ④】 이변량 확률변수 \\((X,Y)\\)의 연속형 이변량 확률밀도함수는 \\(f(x,y) = e^{- y},0 &lt; x &lt; y &lt; \\infty\\)이다. \\(P(X + Y \\geq 1)\\)?\n\\[\\begin{matrix}\nP(X + Y \\geq 1) & = 1 - P(X + Y &lt; 1) \\\\\n& = 1 - \\int_{0}^{\\frac{1}{2}}\\int_{x}^{1 - x}e^{- y}dydx \\\\\n& = 1 - \\int_{0}^{\\frac{1}{2}}\\left( e^{- x} - e^{- (1 - x)} \\right)dx \\\\\n& = 2e^{- 1/2} - e^{- 1}\n\\end{matrix}\\]\n\n\n\n3. 조건부 확률밀도함수와 독립\nGPA와 SAT 점수가 어느 정도 관련이 있다고 생각할 수 있다. 예를 들어, 어떤 학생이 GPA가 3.9라고 하면, 그의 SAT 점수가 1400 이상일 확률이 높다고 예상할 수 있다. 반면 GPA가 2.0인 학생이라면, SAT 점수가 1400 이상일 가능성은 상대적으로 낮을 수 있다.\n즉, GPA(X)를 알고 나면 SAT 점수(Y)의 분포에 대한 정보를 얻을 수 있다는 것인데 이는 조건부 확률 \\(P(Y = y \\mid X = x)\\)으로 표현된다.\n【정의 이산형】 \\((X,Y)\\)가 이산 이변량 확률벡터이고, 결합 확률질량함수가 \\(p(x,y)\\), \\(X\\) 주변 확률질량함수가 각각 \\(p_{X}(x)\\)라고 하자.\n\\(P(X = x) = p_{X}(x) &gt; 0\\)인 임의의 \\(x\\)에 대해, \\(X = x\\)라는 정보가 주어졌을 때의 \\(Y\\)의 조건부 확률질량함수는 다음과 같이 정의된다:\n\\[p(y|x) = P(Y = y \\mid X = x) = \\frac{p(x,y)}{p_{X}(x)}\\]\n【예제 ①】 \\((X,Y)\\)가 이산 이변량 확률벡터이고, 결합 확률질량함수가 \\(p(x,y)\\)는 다음과 같다.\n\\[\\begin{matrix}\np(0,10) & = p(0,20) = \\frac{2}{18},p(1,10) = p(1,30) = \\frac{3}{18}, \\\\\np(1,20) & = \\frac{4}{18},p(2,30) = \\frac{4}{18}.\n\\end{matrix}\\]\n\\[p_{X}(0) = \\frac{4}{18},p_{X}(1) = \\frac{10}{18},p_{X}(2) = \\frac{4}{18}\\]\n\\(p(10 \\mid 0) = \\frac{p(0,10)}{p_{X}(0)} = \\frac{\\frac{2}{18}}{\\frac{4}{18}} = \\frac{1}{2}\\)\\(p(20 \\mid 0) = \\frac{p(0,20)}{p_{X}(0)} = \\frac{\\frac{2}{18}}{\\frac{4}{18}} = \\frac{1}{2}\\)이다.\n\\[p(10 \\mid 1) = p(20 \\mid 1) = 3/10,p(30 \\mid 1) = 3/10\\]\n\\[p(30|2) = 1\\]\n【예제 ②】 이산형 결합 확률밀도함수는 \\(p(x,y) = \\frac{x + y}{21},x = 1,2,3,y = 1,2\\) 이다. \\(Y = y\\)가 주어졌을 때 확률변수 \\(X\\) 조건부 확률밀도함수를 구하고 \\(P(X = 2|Y = 1)\\) 확률을 구하라.\n\\(Y\\) 주변 확률밀도함수 : \\(p_{Y}(y) = \\sum_{x}^{}{\\frac{x + y}{21} = \\frac{3y + 6}{21}},y = 1,2\\).\n\\(Y = y\\)가 주어졌을 때 확률변수 \\(X\\) 조건부 확률밀도함수 : \\(p\\left( x \\middle| y \\right) = \\frac{x + y}{3y + 6},x = 1,2,3wheny = 1,2\\).\n\\(P\\left( X = 2 \\middle| Y = 1 \\right) = \\frac{3}{9} = \\frac{1}{3}\\).\n【정의 연속형】 \\((X,Y)\\)가 연속 이변량 확률벡터이고, 결합 확률질량함수가 \\(f(x,y)\\), \\(X\\) 주변 확률질량함수가 각각 \\(f_{X}(x)\\)라고 하자.\n\\(f_{X}(x) &gt; 0\\)인 임의의 \\(x\\)에 대해, \\(X = x\\)라는 정보가 주어졌을 때의 \\(Y\\)의 조건부 확률질량함수는 다음과 같이 정의된다:\n\\[f(y|x) = = \\frac{f(x,y)}{f_{X}(x)}\\]\n【예제 ③】 결합 확률밀도함수는 \\(f(x,y) = \\frac{1}{2},0 \\leq x \\leq y \\leq 2\\)이다. \\(P(X \\leq 1|Y = 2)\\) 확률을 구하라.\n확률변수 \\(Y\\) 주변 확률밀도함수 : \\(f(y) = \\int_{0}^{y}{\\frac{1}{2}dx = \\frac{x}{2}}\\left. \\  \\right|_{0}^{y} = \\frac{y}{2},0 \\leq y \\leq 2\\).\n조건부 확률밀도함수 : \\(f\\left( x \\middle| y \\right) = \\frac{f(x,y)}{f(y)} = \\frac{1}{y},,0 \\leq x \\leq y \\leq 2\\).\n\\[P\\left( X \\leq 1 \\middle| Y = 2 \\right) = \\int_{0}^{1}{f\\left( x \\middle| y \\right)dx =}\\int_{0}^{1}{\\frac{1}{2}dx = \\frac{x}{2}\\left. \\  \\right|_{0}^{1} = \\frac{1}{2}}\\]\n\n(1) 조건부 기대값 및 분산\n【정의 조건부 기대값】 만약 \\(g(Y)\\)가 Y에 대한 함수라면, \\(X = x\\)라는 조건 하에서의 \\(g(Y)\\)의 조건부 기대값은 다음과 같이 정의된다.\n\\[\\mathbb{E}(g(Y) \\mid x) = \\sum_{y}g(y)f(y \\mid x)\\text{(이산형 경우)}\\]\n\\[\\mathbb{E}(g(Y) \\mid x) = \\int_{- \\infty}^{\\infty}g(y)f(y \\mid x)dy\\text{(연속형 경우)}\\]\n조건부 분산: \\(Var(g(Y) \\mid x) = \\mathbb{E}((g(Y))^{2} \\mid x) - \\left( \\mathbb{E}(g(Y) \\mid x) \\right)^{2}\\)\n【예제 ④】 결합밀도함수: \\(f(x,y) = e^{- y},0 &lt; x &lt; y &lt; \\infty\\)\n\\(X\\)의 주변 확률밀도함수: \\(f_{X}(x) = \\int_{- \\infty}^{\\infty}f(x,y)dy = \\int_{x}^{\\infty}e^{- y}dy = e^{- x}\\)\n\\(Y\\)의 조건부 확률밀도함수 : \\(f(y \\mid x) = \\frac{f(x,y)}{f_{X}(x)} = \\frac{e^{- y}}{e^{- x}} = e^{- (y - x)},y &gt; x\\)\n조건부 확률밀도함수는 위치모수 \\(\\alpha = x\\), 척도모수 \\(\\beta = 1\\)인 감마분포이므로 \\(\\mathbb{E}(g(Y) \\mid x) = x\\)이다.\n\n\n(2) 독립\n【정의 독립】 \\((X,Y)\\)를 결합 확률밀도함수 \\(f(x,y)\\)와 주변 확률함수 \\(f_{X}(x),f_{Y}(y)\\)를 가지는 이변량 확률벡터라고 하자. 다음을 만족하는 \\(X\\)와 \\(Y\\)는 독립 확률변수라고 정의한다.\n모든 \\(x \\in \\mathbb{R},y \\in \\mathbb{R}\\)에 대해 \\(f(x,y) = f_{X}(x)f_{Y}(y)\\)이다.\n즉, 결합 확률함수가 두 변수의 주변 확률함수의 곱으로 표현될 수 있으면, 두 변수는 서로 독립이다.\n【정리】 \\(F_{X,Y}(x,y) = F_{X}\\left( x)F_{Y}(y \\right)\\)이면 두 확률변수 \\((X,Y)\\)는 서로 독립이고 역도 성립한다.\n【정리】 \\((X,Y)\\) 독립이면 조건부 확률밀도함수는 주변 확률밀도함수와 동일하다. \\(p_{X|Y}\\left( x|y \\right) = p_{X}(x)\\), \\(f_{X|Y}\\left( x \\middle| y \\right) = f_{X}(x)\\).\n【정리】 확률변수 \\(X\\)의 확률 밀도함수는 \\(f(x)\\), 확률변수 \\(Y\\)의 확률밀도함수는 \\(f(y),y \\in S_{Y}\\) 이다. 결합 확률밀도함수 \\(f(x,y))\\)가 두 확률변수의 함수의 곱(\\(g(x)h(y)\\))으로 표현되고 \\(x \\in S_{X}\\)이고 \\(y \\in S_{Y}\\) (영역이 서로 의존하지 않으면) 두 확률변수 \\((X,Y)\\)는 서로 독립이고 역도 성립한다. 【필요 충분 조건】\n【정리】 \\(P(a &lt; X \\leq b,c &lt; Y \\leq d) = P(a &lt; X \\leq b)P(c &lt; X \\leq d)\\)이면 두 확률변수 \\((X,Y)\\)는 서로 독립이고 역도 성립한다.\n【예제 ①】 결합 확률밀도함수: \\(p(x,y) = \\frac{x + y}{21},x = 1,2,3,y = 1,2\\)두 확률변수는 서로 독립인가?\n\\(P(X = 1,Y = 1) = \\frac{2}{21} \\neq P(X = 1)P(Y = 1) = \\frac{5}{21}\\frac{9}{21} = \\frac{45}{21^{2}}\\) 독립 아니다.\n\\(X\\) 주변 확률밀도함수 : \\(p_{X}(x) = \\sum_{y}^{}{\\frac{x + y}{21} = \\frac{x + 1}{21} + \\frac{x + 2}{21}} = \\frac{2x + 3}{21},x = 1,2,3\\)\n\\(Y\\) 주변 확률밀도함수 : \\(p_{Y}(y) = \\sum_{x}^{}{\\frac{x + y}{21} = \\frac{3y + 6}{21}},y = 1,2\\)\n\\(p(x,y) \\neq p_{X}(x)p_{Y}(y)\\)이므로 독립이 아니다.\n【예제 ②】 이변량 결합 확률밀도함수 \\(f(x,y) = e^{- (x + y)},0 &lt; x,y\\)을 갖는 (\\(X,Y\\))는 서로 독립인가?\n결합 확률밀도함수가 \\(g(x) = e^{- x}\\), \\(h(y) = e^{- y}\\)의 곱으로 표현되고 각 영역도 \\(S_{X} = \\{ x;0 &lt; x\\}\\)와 \\(S_{Y} = \\{ y;0 &lt; y\\}\\) 독립이므로 (\\(X,Y\\))는 서로 독립이다.\n\\(X\\) 주변 확률밀도함수 : \\(f(x) = \\int_{0}^{\\infty}{e^{- (x + y)}dy} = e^{- x},0 &lt; x\\).\n\\(Y\\) 주변 확률밀도함수 : \\(f(y) = \\int_{0}^{\\infty}{e^{- (x + y)}dx} = e^{- y},0 &lt; y\\).\n\\(f(x,y) = f(x)f(y)\\)이므로 서로 독립이다.\n【예제 ③】 이변량 결합 확률밀도함수 \\(f(x,y) = x + y,0 &lt; x,y &lt; 1\\)을 갖는 (\\(X,Y\\))는 서로 독립인가?\n결합 확률밀도함수를 \\(x\\)의 함수와 \\(y\\)의 함수 곱으로 표현되지 않으므로 (\\(X,Y\\))는 서로 독립이 아니다.\n\\(f(x) = \\int_{0}^{1}{x + y}dy = x + \\frac{1}{2},0 &lt; x &lt; 1\\).\n\\(f(y) = \\int_{0}^{1}{x + y}dx = y + \\frac{1}{2},0 &lt; y &lt; 1\\).\n\\(f(x,y) \\neq f(x)f(y)\\)이므로 (\\(X,Y\\))는 서로 독립이 아니다.\n\n\n\n\nchapter 2. 변수변환\n이변량 확률변수 \\((X,Y)\\)의 함수, \\(U = g(X,Y),V = h(X,Y)\\)의 확률밀도함수를 구하는 방법에 대하여 살펴보기로 하자.\n\n1. 이산형 이변량 변수변환\n이변량 이산형 확률변수 \\((X,Y)\\)의 결합 확률밀도함수를 \\(p_{X,Y}(x,y),(x,y) \\in \\mathcal{S}\\)라 하자. \\(u = g(x,y),v = h(x,y)\\) 일대일 변환(\\(\\mathcal{S} \\rightarrow \\mathcal{T}\\))이라 정의하면 이변량 확률변수 \\((U,V)\\)의 결합밀도함수는 다음과 같다.\n\\[p_{U,V}(u,v) = p_{X,Y}\\left( x = w(u,v),y = z(u,v) \\right),(u,v) \\in \\mathcal{T}where\\]\n\\(x = w(u,v),y = z(u,v)\\)은 \\(u = g(x,y),v = h(x,y)\\)의 일대일 변환 함수이다.\n【예제 ①】 이산형 이변량 확률변수 \\((X,Y)\\)의 결합밀도함수는 \\(p(x,y) = \\frac{\\lambda_{1}^{x}\\lambda_{2}^{y}e^{- x}e^{- y}}{x!y!},x,y = 0,1,2,\\ldots\\)이다. \\(U = X + Y\\)의 확률밀도함수를 구하라.\n\n이변량 확률변수의 변수 변환이므로 \\(U\\)변환 이외 관심 변환은 아니지만 간단한 변환변환을 고려하여 \\((X,Y) \\rightarrow (U,V)\\) 이변량 변수 변환을 실시한다.\n\\(U = X + Y,V = Y\\)변환을 고려하면 \\(X = U - V,Y = V\\)이다.\n\\(\\mathcal{S} = \\left\\{ (x,y);x = 0,1,2,\\ldots,y = 0,1,2,\\ldots \\right\\}\\) ▷ \\(\\mathcal{T} = \\left\\{ (u,v);v = 0,1,\\ldots,u,v = 0,1,2,\\ldots \\right\\}\\)이다.\n\\((U,V)\\)의 결합 확률밀도함수는 \\(p_{U,V}(u,v) = \\frac{\\lambda_{1}^{(u - v)}\\lambda_{2}^{v}e^{- (\\lambda_{1} + \\lambda_{2})}}{v!(u - v)!},(u,v) \\in \\mathcal{T}\\)이다.\n확률변수 \\(U\\)에 대한 주변 확률밀도함수를 구하면 된다.\\(p_{U}(u) = \\sum_{v = 0}^{u}\\frac{\\lambda_{1}^{(u - v)}\\lambda_{2}^{v}e^{- \\left( \\lambda_{1} + \\lambda_{2} \\right)}}{v!(u - v)!} = \\frac{e^{- \\left( \\lambda_{1} + \\lambda_{2} \\right)}}{u!}\\sum_{v = 0}^{u}{\\frac{u!}{v!(u - v)!}\\lambda_{1}^{u - v}\\lambda_{2}^{v}}\\) \\(= \\frac{e^{- \\left( \\lambda_{1} + \\lambda_{2} \\right)}}{u!}\\left( \\lambda_{1} + \\lambda_{2} \\right)^{u} \\sim Poisson(\\lambda_{1} + \\lambda_{2})\\).\n\n\n\n2. 연속형 이변량 변수변환\n\n\n\n\n\n【예제 ②】 이변량 연속형 확률변수 \\((X,Y)\\)의 결합 확률밀도함수를 \\(f(x,y) = 1,0 &lt; x,y &lt; 1\\)라 하자. \\(U = X + Y\\)의 확률밀도함수를 구하라.\n\n확률변수 영역 : \\(0 &lt; x,y &lt; 1\\)으로부터 \\(0 &lt; u &lt; 2\\)이다.\n만약 \\(0 &lt; u \\leq 1\\), \\(F_{U}(u) = P(U \\leq u) = P(X + Y \\leq u)\\)\\(= \\int_{0}^{u}{\\int_{0}^{u - x}{1dydx = \\int_{0}^{u}{y\\left. \\  \\right|_{0}^{u - x}dx =}\\int_{0}^{u}{(u - x)dx = \\frac{u^{2}}{2}}}}\\).\n만약 \\(1 &lt; u &lt; 2\\), \\(F_{U}(u) = P(U \\leq u) = P(X + Y \\leq u)\\) \\(= 1 - \\int_{u - 1}^{1}{\\int_{u - x}^{1}{1dydx = 1 - \\frac{(2 - u)^{2}}{2}}}\\).\n확률변수 \\(U\\)의 확률밀도함수는 다음과 같다. \\(f_{U}(u) = \\left\\{ \\begin{array}{r}\nu,0 &lt; u \\leq 1 \\\\\n(2 - u),1 &lt; u &lt; 2\n\\end{array} \\right.\\ \\)\n\n\n(2) 연속형 이변량 확률변수 Jacobian 방법 이용하기\n이변량 연속형 확률변수 \\((X,Y)\\)의 결합 확률밀도함수를 \\(f(x,y),(x,y) \\in \\mathcal{S}\\)라 하자. 이변량 변환 확률변수 \\(U = g(X,Y),V = h(X,Y)\\)의 결합 확률밀도함수를 \\(f(u,v),(u,v) \\in \\mathcal{T}\\)라 정의하고 \\(A \\in \\mathcal{S}\\)에 대하여 \\(u = g(x,y),v = h(x,y)\\) 일대일 변환 함수에 의해 \\(B \\in \\mathcal{T}\\)에 대응된다고 하자.\n\n일대일 변환 함수 : \\(u = g(x,y),v = h(x,y) \\Longleftrightarrow x = w(u,v),y = z(u,v)\\)\n\\(J = \\left| \\begin{matrix}\n\\frac{\\partial x}{\\partial u} & \\frac{\\partial x}{\\partial v} \\\\\n\\frac{\\partial y}{\\partial u} & \\frac{\\partial y}{\\partial v}\n\\end{matrix} \\right|\\).\n\\(P\\left\\lbrack (U,V) \\in B \\right\\rbrack = P\\left\\lbrack (X,Y) \\in A \\right\\rbrack = \\int\\int f_{X,Y}(x,y)dxdy\\).\n\\(f_{U,V}(u,v) = f_{X,Y}\\left( w(x,y),z(x,y) \\right),(u,v) \\in \\mathcal{T}\\).\n\n【예제 ③】 이변량 연속형 확률변수 \\((X,Y)\\)의 결합 확률밀도함수를 \\(f(x,y) = 1,0 &lt; x,y &lt; 1\\)라 하자. \\(U = X + Y,V = X - Y\\)의 이변량 결합 확률밀도함수를 구하라.\n\n확률변수 \\((X,Y)\\)의 영역 : \\(\\mathcal{S} = \\{(x,y);0 &lt; x &lt; 1,0 &lt; y &lt; 1\\}\\)\n\\(U = X + Y,V = X - Y\\) 변환 : \\(X = \\frac{1}{2}(U + V),Y = \\frac{1}{2}(U - V)\\)\n\\(\\mathcal{S} = \\{(x,y);0 &lt; x &lt; 1,0 &lt; y &lt; 1\\}\\) 의 경계 값 확률변수 \\((U,V)\\)의 영역 \\(\\mathcal{T} = \\{(u,v)\\}\\) 매핑\n\\(x = 0into0 = \\frac{1}{2}(u + v)\\) \\(x = 1into1 = \\frac{1}{2}(u + v)\\) \\(y = 0into0 = \\frac{1}{2}(u - v)\\) \\(y = 1into0 = \\frac{1}{2}(u - v)\\)\n\n\n\n\n\n\n\nJacobian 구하기 : \\(J = \\left| \\begin{matrix}\n\\frac{\\partial x}{\\partial u} & \\frac{\\partial x}{\\partial v} \\\\\n\\frac{\\partial y}{\\partial u} & \\frac{\\partial y}{\\partial v}\n\\end{matrix} \\right| = \\left| \\begin{matrix}\n\\frac{1}{2} & \\frac{1}{2} \\\\\n\\frac{1}{2} & - \\frac{1}{2}\n\\end{matrix} \\right| = \\left( \\frac{1}{2} \\right)\\left( - \\frac{1}{2} \\right) - \\left( \\frac{1}{2} \\right)\\left( \\frac{1}{2} \\right) = - \\frac{1}{2}\\)\n\\(0 &lt; x &lt; 1,0 &lt; y &lt; 1 \\leftrightarrow 0 &lt; \\frac{1}{2}(u + v) &lt; 1,0 &lt; \\frac{1}{2}(u - v) &lt; 1 \\leftrightarrow - u &lt; v,v &lt; 2 - u,v &lt; u,u - 2 &lt; v\\). 그러므로 \\(\\mathcal{T} = \\{(u,v);u &lt; v,v &lt; 2 - u,v &lt; u,u - 2 &lt; v\\}\\)\n\\[f_{U,V}(u,v) = f_{X,Y}\\left( \\frac{1}{2}(u + v),\\frac{1}{2}(u - v) \\right)|J| = \\frac{1}{2},(u,v) \\in \\mathcal{T}\\]\n\n\n\n(3) convolution 방법\n이변량 결합 확률밀도함수 \\(f_{X,Y}(x,y),\\mathcal{S} = \\{(x,y); - \\infty &lt; x,y &lt; \\infty\\}\\)이고 \\(U = X + Y,V = Y\\) 변환에서 \\(U = X + Y\\)의 확률밀도함수는 다음과 같다.\n\\(f_{U}(u) = \\int_{- \\infty}^{\\infty}{f_{X,Y}(u - v,v)dv}\\).\n【예제 ④】 이변량 결합 확률밀도함수 \\(f(x,y) = 2,0 &lt; x &lt; y &lt; 1\\)인 경우 \\(U = \\frac{X}{Y},V = Y\\) 이변량 결합 확률밀도함수를 구하시오.\n\n\\(U = \\frac{X}{Y},V = Y\\) 변환 : \\(X = UV,Y = V\\)\n\\[\\mathcal{S} = \\left\\{ (x,y);0 &lt; x,y \\right\\}\\mathcal{} \\rightarrow \\mathcal{T} = \\{(u,v);0 &lt; v &lt; 1,0 &lt; v &lt; 1\\}\\]\n\\(J = \\left| \\begin{matrix}\nV & U \\\\\n0 & 1\n\\end{matrix} \\right| = UV\\)\n\\(f_{U,V}(u,v) = f_{X,Y}(uv,v)|J| = 2v,(u,v) \\in \\mathcal{T}\\)\n\n\n\n\n\n\n\n\n\n3. 변수변환에 의한 분포함수의 관계\n코쉬 분포\n서로 독립인 표준정규분포의 비는 표준 코쉬분포를 따른다.\n카이제곱 분포\n서로 독립인 정규분포 \\(Z_{i} \\sim \\mathcal{N}(0,1)\\)에 대해 \\(X = \\overset{k}{\\sum_{i = 1}}Z_{i}^{2} \\sim \\chi_{k}^{2}\\).\n감마 분포\n지수분포 \\(X_{i} \\sim \\text{Exp}(\\lambda)\\)인 n개의 독립 변수 합 \\(Y = \\overset{n}{\\sum_{i = 1}}X_{i} \\sim \\text{Gamma}(n,\\lambda)\\).\n베타분포\n독립인 두 감마분포 변수 \\(X \\sim \\text{Gamma}(\\alpha,1),Y \\sim \\text{Gamma}(\\beta,1)\\)일 때, \\(\\frac{X}{X + Y} \\sim \\text{Beta}(\\alpha,\\beta)\\).\nF 분포\n독립인 두 카이제곱 변수 \\(U \\sim \\chi_{m}^{2},V \\sim \\chi_{n}^{2}\\)에 대해 \\(F = \\frac{\\frac{U}{m}}{\\frac{V}{n}} \\sim F_{m,n}\\).\nt-분포\n정규분포 \\(Z \\sim \\mathcal{N}(0,1)\\)와 독립된 카이제곱 분포 \\(V \\sim \\chi_{n}^{2}\\)를 이용하여 \\(T = \\frac{Z}{\\sqrt{V/n}} \\sim t_{n}\\).\n\n\n4. 계층적 모형\n어떤 확률변수는 하나의 분포를 갖고 있으며, 그 분포는 어떤 모수에 의존할 수 있다. 일반적으로, 확률변수는 하나의 분포만을 가진다고 보지만, 상황을 계층적으로 바라보는 것이 오히려 더 쉽게 모델링을 가능하게 하는 경우가 많다.\n가장 고전적인 계층모형(hierarchical model) 중 하나는 다음과 같다. 한 곤충이 매우 많은 수의 알을 낳고, 각 알은 확률 \\(p\\)로 생존한다고 하자. 그렇다면 평균적으로 몇 개의 알이 생존하는가? 이때 ”많은 수”의 알은 실제로는 확률변수이며, 흔히 포아송 분포 \\(\\text{Poisson}(\\lambda)\\)로 모델링한다. 또한, 각 알의 생존 여부가 독립적이라고 가정하면, 이는 베르누이 시행의 반복이고, 이항분포를 따른다.\n낳은 알의 수를 \\(Y\\), 생존한 알의 수를 \\(X\\) 라고 하면,\n\\(X \\mid Y \\sim \\text{Binomial}(Y,p),Y \\sim \\text{Poisson}(\\lambda)\\)이다.\n\\[P(X = x) = \\overset{\\infty}{\\sum_{y = 0}}P(X = x,Y = y) = \\overset{\\infty}{\\sum_{y = 0}}P(X = x \\mid Y = y)P(Y = y)\\]\n\\[= \\overset{\\infty}{\\sum_{y = x}}\\left\\lbrack \\binom{y}{x}p^{x}(1 - p)^{y - x} \\right\\rbrack\\left\\lbrack \\frac{e^{- \\lambda}\\lambda^{y}}{y!} \\right\\rbrack\\]\n이제 이 표현을 단순화하면, \\(\\frac{\\lambda^{x}}{\\lambda^{x}}\\)로 약분하고 정리하면,\n\\(P(X = x) = \\frac{(\\lambda p)^{x}e^{- \\lambda}}{x!}\\overset{\\infty}{\\sum_{y = x}}\\frac{((1 - p)\\lambda)^{y - x}}{(y - x)!}\\).\n\\(t = y - x\\) 적용하면, \\(P(X = x) = \\frac{(\\lambda p)^{x}}{x!}e^{- \\lambda p} \\sim \\text{Poisson}(\\lambda p)\\).\n그러므로 \\(E(X) = \\lambda p\\) 생존하는 알의 수이다.\n【정리】\n\\(\\mathbb{E}\\lbrack X\\rbrack = \\mathbb{E}\\left( \\mathbb{E}\\lbrack X \\mid Y\\rbrack \\right)\\)\n\\(\\mathbb{E}\\left( \\mathbb{E}\\lbrack X \\mid Y\\rbrack \\right) = \\mathbb{E}(pY)\\) (왜냐하면 \\(X|Y \\sim B(Y,p)\\))\n\\(\\mathbb{E}\\left( \\mathbb{E}\\lbrack X \\mid Y\\rbrack \\right) = \\lambda p = \\mathbb{E}(X)\\) (왜냐하면 \\(Y \\sim Poisson(\\lambda)\\))\n\n\n\nchapter 3. 기대값\n【정의】 이변량 확률변수 \\((X,Y)\\)의 함수 \\(g(X,Y)\\) 기대값은 \\(E\\left( g(X,Y) \\right) = \\sum_{x}^{}{\\sum_{y}^{}{g(X,Y)p(x,y)}}\\).\\(E\\left( g(X,Y) \\right) = \\int_{x}^{}{\\int_{y}^{}{g(X,Y)f(x,y)dydx}}\\).\n【정리】 이변량 확률변수 \\((X,Y)\\)가 독립이면\\(E(XY) = E(X)E(Y)\\)이다.\n\\(E(XY) = \\int\\int xyf(x,y)dxdy = (independence)\\int\\int xyf\\left( x)f(y \\right)dxdy\\)\n\\(= \\int xf(x)dx\\int yf(y)dy\\) \\(= E(X)E(Y)\\)\n【정리】 이변량 확률변수 \\((X,Y)\\)가 독립이면 각각의 함수 \\(g(X),h(Y)\\)에 대해서도 다음이 성립한다.\n\\(E\\left( g(X)h(Y) \\right) = E(g(X))E(h(Y))\\)이다.\n【예제 ①】 결합 확률밀도함수는 \\(f(x,y) = 2x,0 \\leq x,y \\leq 1\\) 일 경우 \\(E(XY),E(X),V(X)\\)을 구하시오.\n\\[E(XY) = \\int\\int xyf(x,y)dxdy = \\int_{0}^{1}{\\int_{0}^{1}{xy(2x)dydx =}}2\\int_{0}^{1}{x^{2}\\int_{0}^{1}{ydydx =}}2\\int_{0}^{1}{x^{2}(\\frac{1}{2})dx = \\frac{1}{3}}x^{3}\\left. \\  \\right|_{0}^{1} = \\frac{1}{3}\\]\n\\[E(X) = \\int_{0}^{1}{\\int_{0}^{1}{x(2x)dydx =}}2\\int_{0}^{1}{x^{2}\\int_{0}^{1}{1dydx =}}2\\int_{0}^{1}{x^{2}dx = \\frac{2}{3}}x^{3}\\left. \\  \\right|_{0}^{1} = \\frac{2}{3}\\]\n\\[E(Y) = \\int_{0}^{1}{\\int_{0}^{1}{y(2x)dydx =}}2\\int_{0}^{1}{x\\int_{0}^{1}{ydydx =}}2\\int_{0}^{1}{x(\\frac{1}{2})dx =}\\frac{1}{2}x^{2}\\left. \\  \\right|_{0}^{1} = \\frac{1}{2}\\]\n확률변수 \\((X,Y)\\)가 독립이므로 \\(E(XY) = \\frac{1}{3} = E(X)E(Y) = \\frac{2}{3}\\frac{1}{2} = \\frac{1}{3}\\)\n\\[E\\left( X^{2} \\right) = \\int_{0}^{1}{\\int_{0}^{1}{x^{2}(2x)dydx =}}2\\int_{0}^{1}{x^{3}\\int_{0}^{1}{1dydx =}}2\\int_{0}^{1}{x^{3}dx = \\frac{2}{4}}x^{4}\\left. \\  \\right|_{0}^{1} = \\frac{1}{2}\\]\n\\[V(X) = E(X^{2}) - E(X)^{2} = \\frac{1}{2} - \\left( \\frac{2}{3} \\right)^{2} = \\frac{1}{18}\\]\n【예제 ②】 결합 확률밀도함수는 \\(f(x,y) = 8xy,0 &lt; x &lt; y &lt; 1\\) 일 경우 \\(E\\left( XY^{2} \\right),E(Y),E(\\frac{X}{Y})\\)을 구하시오.\n\\(E\\left( X^{2}Y \\right) = \\int_{0}^{1}{\\int_{0}^{y}{xy^{2}(8xy)dxdy =}}\\int_{0}^{1}{\\frac{8}{3}y^{6}dy =}\\frac{8}{21}\\).\n\\(E(Y) = \\int_{0}^{1}{\\int_{0}^{y}{y(8xy)dxdy =}}\\int_{0}^{1}{y\\left( 4y^{3} \\right)dy =}\\frac{4}{5}\\).\n\\(U = \\frac{X}{Y}\\)라 놓자. \\(U\\)의 확률분포함수 \\(F_{U}(u) = P(U \\leq u) = P(X \\leq uY) = \\int_{0}^{1}{\\int_{0}^{uy}{8xydxdy = \\int_{0}^{1}{4u^{2}y^{3}dy = u^{2},0 &lt; u &lt; 1}}}\\)이다. 그러므로 \\(f(u) = F_{U}'(u) = 2u,0 &lt; u &lt; 1\\)이다.\n\\(E\\left( U = \\frac{X}{Y} \\right) = \\int_{0}^{1}udu = \\frac{2}{3}\\).\n【예제 ③】 결합 확률밀도함수는 \\(f(x,y) = 3x,0 \\leq y \\leq x \\leq 1\\) 일 경우 \\(E(X - Y)\\)을 구하시오.\n\\(E(X) = \\int_{0}^{1}{\\int_{0}^{x}{x(3x)dydx = \\int_{0}^{1}{3x^{2}(x)dx = \\frac{3}{4}x^{4}\\left. \\  \\right|_{0}^{1} = \\frac{3}{4}}}}\\).\n\\[E(Y) = \\int_{0}^{1}{\\int_{0}^{x}{y(3x)dydx = \\int_{0}^{1}{3x\\left( \\frac{1}{2}x^{2} \\right)dx = \\frac{3}{2}(\\frac{1}{4}x^{4})\\left. \\  \\right|_{0}^{1} = \\frac{3}{8}}}}\\]\n\\(E(X - Y) = E(X) - E(Y) = \\frac{3}{4} - \\frac{3}{8} = \\frac{3}{8}\\).\n\\[E(X - Y) = \\int_{0}^{1}{\\int_{0}^{x}{(x - y)3xdydx =}}\\int_{0}^{1}{\\int_{0}^{x}{(x - y)3xdydx =}}\\int_{0}^{1}{\\int_{0}^{x}{x3xdydx - \\int_{0}^{1}{\\int_{0}^{x}{yxdydx =}\\frac{3}{4} - \\frac{3}{8} = \\frac{3}{8}}}}\\]\n【정의】 이변량 확률변수 벡터 \\(\\overline{X} = (X,Y)'\\)의 기대값 벡터는 \\(E\\left( \\overline{X} \\right) = \\lbrack\\begin{array}{r}\nE(X) \\\\ E(Y)\n\\end{array}\\rbrack\\)이다.\n【예제 ④】 결합 확률밀도함수는 \\(f(x,y) = 3x,0 \\leq y \\leq x \\leq 1\\) 일 경우 \\(E\\)(\\(\\lbrack\\begin{array}{r}\nX \\\\\nY\n\\end{array}\\rbrack\\))을 구하라.\n\\(E(X) = \\int_{0}^{1}{\\int_{0}^{x}{x(3x)dydx = \\int_{0}^{1}{3x^{2}(x)dx = \\frac{3}{4}x^{4}\\left. \\  \\right|_{0}^{1} = \\frac{3}{4}}}}\\).\n\\(E(Y) = \\int_{0}^{1}{\\int_{0}^{x}{y(3x)dydx = \\int_{0}^{1}{3x\\left( \\frac{1}{2}x^{2} \\right)dx = \\frac{3}{2}(\\frac{1}{4}x^{4})\\left. \\  \\right|_{0}^{1} = \\frac{3}{8}}}}\\).\n\\(E\\)(\\(\\lbrack\\begin{array}{r}\nX \\\\\nY\n\\end{array}\\rbrack\\))= \\(\\lbrack\\begin{array}{r}\n3/4 \\\\\n3/8\n\\end{array}\\rbrack\\) (차수가 2인 열 벡터임)\n【정의】 이변량 확률변수 \\((X,Y)\\)에서 \\(X = x\\)가 주어졌을 때 \\(Y\\)의 조건부 기대값 \\(E\\left( Y \\middle| x \\right) = \\int yf_{Y|X}\\left( y \\middle| x \\right)dy = \\frac{\\int yf(x,y)dy}{f_{X}(x)},y \\in \\mathcal{S}_{Y}\\)\n【예제 ⑤】 결합 확률밀도함수는 \\(f(x,y) = 2,0 &lt; x &lt; y &lt; 1\\)이다. \\(E\\left( Y \\middle| x \\right),V(Y|x)\\)을 구하라.\n\\(f(x) = \\int_{y}^{1}{2dy} = 2(1 - x),0 &lt; x &lt; 1\\)\n\\[f\\left( y \\middle| x \\right) = \\frac{f(x,y)}{f(x)} = \\frac{2}{2(1 - x)} = \\frac{1}{1 - x},0 &lt; x &lt; y &lt; 1\\]\n\\[E\\left( Y \\middle| x \\right) = \\int_{x}^{1}{\\frac{1}{1 - x}ydy = \\frac{1}{2(1 - x)}}y^{2}\\left. \\  \\right|_{x}^{1} = \\frac{1 + x}{2},0 &lt; x &lt; 1\\]\n\\[E\\left( Y^{2} \\middle| x \\right) = \\int_{x}^{1}{\\frac{1}{1 - x}y^{2}dy = \\frac{1}{3(1 - x)}}y^{3}\\left. \\  \\right|_{x}^{1} = \\frac{x^{2} + x + 1}{3},0 &lt; x &lt; 1\\]\n\\(V\\left( Y^{} \\middle| x \\right) = \\frac{x^{2} + x + 1}{3} - \\left( \\frac{1 + x}{2} \\right)^{2}\\)\n【정리】 이변량 확률변수 \\((X,Y)\\)에 대하여 다음이 성립한다.\n\\((a)E(E(Y|X)) = E(Y)\\)\n\\((b)V(E(Y|X)) = V(Y)\\)\n【예제 ⑥】 결합 확률밀도함수는 \\(f(x,y) = 6y,0 &lt; y &lt; x &lt; 1\\)이다. 위의 정리를 보여라.\n확률변수 \\(Y\\)주변 확률밀도함수 : \\(f(y) = \\int_{y}^{1}{6ydx} = (6y)x\\left. \\  \\right|_{y}^{1} = 6y(1 - y),0 &lt; y &lt; 1\\)\n\\[E(Y) = \\int y(6y(1 - y))dy = 6\\left( \\frac{1}{3}y^{3} - \\frac{1}{4}y^{4} \\right)\\left. \\  \\right|_{0}^{1} = \\frac{1}{2}\\]\n확률변수 \\(X\\)주변 확률밀도함수 : \\(f(x) = \\int_{0}^{x}{6ydx} = 3x^{2},0 &lt; y &lt; 1\\)\n\\(X = x\\)가 주어졌을 때 \\(Y\\)의 조건부 확률밀도함수 : \\(f\\left( y \\middle| x \\right) = \\frac{6y}{3x^{2}} = \\frac{2y}{x^{2}},0 &lt; y &lt; x &lt; 1\\)\n\\(X = x\\)가 주어졌을 때 \\(Y\\)의 조건부 기대값 : \\(E\\left( Y \\middle| x \\right) = \\int_{0}^{x}{y\\left( \\frac{2y}{x^{2}} \\right)dy = \\frac{2}{3}x,0 &lt; x &lt; 1}\\).\n\\(E\\left( Y \\middle| x \\right) = \\frac{2}{3}X\\)는 확률변수이므로 \\(U = g(X) = \\frac{2}{3}X\\)의 확률밀도함수를 구해야 \\(E(E\\left( Y|x \\right))\\) 기대값을 구할 수 있다. \\(0 &lt; x &lt; 1 \\rightarrow 0 &lt; u &lt; \\frac{2}{3}\\) 이고\\(X = \\frac{3}{2}U\\)이다.\n\\(\\frac{2}{3}X\\)는 증가 함수이므로 \\(f_{X}(u) = f\\left( x = \\frac{3}{2}u \\right)|J| = 3\\left( \\frac{3}{2}u \\right)^{2}\\left| \\frac{3}{2} \\right| = \\frac{81}{8}u^{2},0 &lt; u &lt; \\frac{2}{3}\\)\n그러므로 \\(E\\left( E\\left( Y \\middle| x \\right) \\right) = \\int_{0}^{\\frac{2}{3}}{u\\left( \\frac{81}{8}u^{2} \\right)du = \\frac{81}{8*4}u^{4}\\left. \\  \\right|_{0}^{\\frac{2}{3}} = \\frac{1}{2}}\\).\n\n\nchapter 4. 상관계수\n두 확률변수 간의 직선적(선형) 관계의 정도를 측정하는 지표로 공분산(covariance) 이 사용된다. 하지만 공분산은 각 변수의 단위(scale)에 영향을 받기 때문에, 이를 비교 가능하도록 변수의 표준편차로 나눈 값을 상관계수(correlation coefficient) 라 한다.\n상관계수는 단위에 무관하며, 두 변수 사이의 선형 관계의 강도와 방향을 -1에서 1 사이의 값으로 나타낸다.\n선형(직선) 관계\n상관계수는 두 변수 사이의 선형 관계를 측정하는 지표로, 두 변수가 일정한 비율로 함께 증가하거나 감소할 때 높은 값을 갖는다. 그러나 두 변수 간에 비선형적인 관계가 존재하는 경우, 상관계수는 이러한 관계를 적절히 반영하지 못할 수 있다. 따라서 상관계수는 선형성 가정 하에서만 해석하는 것이 적절하며, 비선형 관계를 탐색할 때는 다른 분석 도구가 필요하다.\n범위\n상관계수의 값은 -1에서 1 사이의 범위를 가지며, 이 값은 두 변수 사이의 선형 관계의 방향과 강도를 나타낸다. 상관계수가 1에 가까울수록 두 변수는 같은 방향으로 강하게 함께 움직이며, 이는 완전한 양의 선형 관계를 의미한다. 반대로 상관계수가 -1에 가까울수록 두 변수는 서로 반대 방향으로 강하게 움직이며, 이는 완전한 음의 선형 관계를 뜻한다. 상관계수가 0이면 두 변수 사이에 선형적인 관련성은 없다고 해석할 수 있다. 다만, 상관계수가 0이라 하더라도 비선형적인 형태의 관계가 존재할 수 있으므로, 상관계수만으로 변수 간 관계를 전부 설명할 수 있는 것은 아니다.\n부호\n상관계수의 부호는 두 변수 간 관계의 방향을 나타낸다. 상관계수가 양수일 경우, 한 변수가 증가할 때 다른 변수도 함께 증가하는 양의 관계가 존재함을 의미한다. 반대로 상관계수가 음수이면, 한 변수가 증가할 때 다른 변수는 감소하는 음의 관계가 있음을 뜻한다. 이처럼 상관계수의 부호는 변수 간의 변화 방향이 서로 같은지 혹은 반대인지를 판단하는 데 중요한 정보를 제공한다.\n무작위성\n상관계수는 두 변수 간의 선형적인 관련성만을 측정할 뿐, 인과관계를 입증하는 지표는 아니다. 즉, 두 변수 사이에 높은 상관관계가 나타난다고 해서 한 변수가 다른 변수에 영향을 준다고 단정할 수는 없다. 상관관계는 단지 두 변수 간에 일정한 방향성과 강도로 변화가 함께 나타나는 경향을 보여줄 뿐이며, 이러한 관계가 우연인지, 제3의 변수에 의한 것인지, 혹은 실제 인과관계인지는 별도의 분석과 검증을 통해 확인해야 한다.\n【정의】 확률변수 \\(X\\)의 기대값과 분산을 \\(E(X),V(X)\\), 확률변수 \\(Y\\)의 기대값과 분산을 \\(E(Y),V(Y)\\)라 하자. 이변량 확률변수 \\((X,Y)\\)의 공변량 covariance는 다음과 같이 정의한다. \\(COV(X,Y) = E\\left( \\left( X - E(X) \\right)\\left( Y - E(Y) \\right) \\right)\\).\n【정의】 이변량 확률변수 \\((X,Y)\\)의 상관계수correlation coefficient는 다음과 같이 정의한다.\n\\(\\rho = \\frac{COV(X,Y)}{\\sqrt{V(X)}\\sqrt{V(Y)}}\\).\n상관계수 해석\n\n\n\n\n\n공분산 간편식\n\\(COV(X,Y) = E\\left( \\left( X - E(X) \\right)\\left( Y - E(Y) \\right) \\right) = E(XY) - E(X)E(Y)\\)만약 두 확률변수가 서로 독립이면 공분산은 0이다. 그러나 역은 성립하지 않는다. 즉, 공분산이 0이어도 서로 독립이 아닐 수 있다.\n\\(E(XY) = \\int\\int xyf(x,y)dxdy = \\left( \\because indepentf(x,y) = f(x)f(y) \\right)\\)\n\\(= \\int xf(x)dx\\int yf(y)dy = E(X)E(Y)\\) 이므로 \\(COV(X,Y) = 0\\).\n【예제 ①】 결합 확률밀도함수는 \\(f(x,y) = 3x,0 \\leq y \\leq x \\leq 1\\) 일 경우 공분산 \\(COV(X,Y)\\)을 구하라.\n\\[E(XY) = \\int_{0}^{1}{\\int_{0}^{x}{xy(3x)dydx = \\int_{0}^{1}{3x^{2}\\left( \\frac{1}{2}x^{2} \\right)dx = \\frac{3}{10}5\\left. \\  \\right|_{0}^{1} = \\frac{3}{10}}}}\\]\n\\[E(X) = \\int_{0}^{1}{\\int_{0}^{x}{x(3x)dydx = \\int_{0}^{1}{3x^{2}(x)dx = \\frac{3}{4}x^{4}\\left. \\  \\right|_{0}^{1} = \\frac{3}{4}}}}\\]\n\\[E(Y) = \\int_{0}^{1}{\\int_{0}^{x}{y(3x)dydx = \\int_{0}^{1}{3x\\left( \\frac{1}{2}x^{2} \\right)dx = \\frac{3}{2}(\\frac{1}{4}x^{4})\\left. \\  \\right|_{0}^{1} = \\frac{3}{8}}}}\\]\n\\(COV(X,Y) = \\frac{3}{10} - \\left( \\frac{3}{4} \\right)\\left( \\frac{3}{8} \\right) = \\frac{3}{160}\\).\n【예제 ②】 결합 확률밀도함수는 \\(f(x,y) = x + y,0 \\leq x,y \\leq 1\\) 일 경우 상관계수 \\(corr(X,Y)\\)을 구하라.\n\\[E(XY) = \\int_{0}^{1}{\\int_{0}^{1}{xy(x + y)dxdy = \\int_{0}^{1}{\\frac{1}{3}y + \\frac{1}{2}y^{2}dy = \\frac{1}{6}y^{2} + \\frac{1}{6}y^{3}\\left. \\  \\right|_{0}^{1} = \\frac{1}{3}}}}\\]\n\\[E(X) = \\int_{0}^{1}{\\int_{0}^{1}{x(x + y)dxdy = \\int_{0}^{1}{\\frac{1}{3} + \\frac{1}{2}ydy = \\frac{1}{3}y + \\frac{1}{4}y^{2}\\left. \\  \\right|_{0}^{1} = \\frac{7}{12}}}}\\]\n\\[E\\left( X^{2} \\right) = \\int_{0}^{1}{\\int_{0}^{1}{x^{2}(x + y)dxdy = \\int_{0}^{1}{\\frac{1}{4} + \\frac{1}{3}ydy = \\frac{1}{4}y + \\frac{1}{6}y\\left. \\  \\right|_{0}^{1} = \\frac{5}{12}}}}\\]\n\\[V(X) = E\\left( X^{2} \\right) - E(X)^{2} = \\frac{5}{12} - \\left( \\frac{7}{12} \\right)^{2} = \\frac{11}{144}\\]\n동일하게 \\(E(Y) = \\frac{7}{12}\\), \\(V(Y) = \\frac{11}{144}\\)\n\\[COV(X,Y) = \\frac{7}{12}\\frac{7}{12} - \\frac{48}{144} = \\frac{1}{144}\\]\\[corr(X,Y) = \\rho = \\frac{COV(X,Y)}{\\sqrt{}V(X)\\sqrt{}V(Y)} = \\frac{\\frac{1}{144}}{\\frac{11}{144}} = 1/11\\]\n【예제 ③】 다음은 공분산은 0이지만 서로 독립은 아니다.\n\n\n\n\n\n\n\n\n\n\n\\(X\\) \\(Y\\)\n0\n1\n2\n주변 합\n\n\n0\n1/16\n3/16\n1/16\n5/16\n\n\n1\n3/16\n0\n3/16\n6/16\n\n\n2\n1/16\n3/16\n1/16\n5/16\n\n\n주변 합\n5/16\n6/16\n5/16\n1\n\n\n\n\\(E(X) = 0*\\frac{5}{16} + 1*\\frac{6}{16} + 2*2*\\frac{5}{16} = 1\\), \\(E(Y) = 1\\)\n\\(E(XY) = (0)(0)\\frac{1}{16} + (0)(1)\\frac{1}{16} + \\ldots + (1)(2)\\frac{1}{16} = 1\\).\n그러므로 \\(COV(X,Y) = E(XY) - E(X)E(Y) = 1 - 1 = 0\\)\\(P(X = 0,Y = 0) = \\frac{1}{16} \\neq P(X = 0)P(Y = 0) = \\frac{5}{16}\\frac{5}{16}\\) 독립이 아니다.\n【정리】 이변량 확률변수 \\((X,Y)\\)의 평균과 분산을 각각 \\(\\left( \\mu_{X},\\mu_{Y} \\right),(\\sigma_{X}^{2},\\sigma_{Y}^{2})\\)라 하고 상관계수를 \\(\\rho\\)라 하자. \\(E\\left( Y \\middle| x \\right)\\)는 확률변수 \\(X\\)의 선형함수이다. \\(E\\left( Y \\middle| X \\right) = \\mu_{Y} + \\rho\\frac{\\sigma_{Y}}{\\sigma_{X}}(X - \\mu_{X})\\).\n그리고 \\(E\\left( V(Y) \\middle| X \\right) = \\sigma_{Y}^{2}(1 - \\rho^{2})\\).\n【예제 ④】 결합 확률밀도함수는 \\(f(x,y) = 2,0 \\leq x \\leq y \\leq 1\\)이다. \\(E(Y|x)\\)을 구하라.\n\\(\\int_{x}^{1}{y(2)dy = y^{2}\\left. \\  \\right|_{x}^{1} = 1 - x^{2} = (1 - x)(1 + x)}\\).\n확률변수 \\(X\\) 주변 확률밀도함수 : \\(f(x) = \\int_{x}^{1}{2dy = 2y}\\left. \\  \\right|_{x}^{1} = 2(1 - x),0 \\leq x \\leq 1\\).\n\\(E\\left( Y \\middle| x \\right) = \\frac{(1 - x)(1 + x)}{2(1 - x)} = \\frac{1 + x}{2}\\).\n\n\nchapter 5. 다변량 확률변수\n\n1. 개념\n【정의】 확률실험의 표본공간을 \\(\\mathcal{S}\\)에서 정의된 \\(n\\)개의 확률변수 \\(X_{i}(w) = x_{i},i = 1,2,\\ldots,n,w \\in \\mathcal{S}\\)에 대하여 다음을 n-차 랜덤 벡터라 정의한다.\n랜덤 벡터: \\(\\overline{X} = (X_{1},X_{2},\\ldots,X_{n})'\\)\n영역:\\(\\mathcal{D} = \\{\\left( x_{1},x_{2},\\ldots,x_{n} \\right);x_{1} = X_{1}(w),X_{2}(w),\\ldots,x_{n} = X_{n}(w),w \\in \\mathcal{S}\\}\\)\n다변량 확률 분포함수 \\(F_{\\overline{X}}\\left( \\overline{x} \\right) = P(X_{1} \\leq x_{1},X_{2} \\leq x_{2},\\ldots,X_{n} \\leq x_{n})\\)\n다변량 결합 확률밀도함수\n(연속형) \\(f_{\\overline{x}}\\left( \\overline{x} \\right) = \\frac{\\partial^{n}}{\\partial x_{1}\\partial x_{2}\\ldots\\partial x_{n}}F_{\\overline{x}}\\left( \\overline{x} \\right) = f(x_{1},x_{2},\\ldots,x_{n})\\)\n(이산형) \\(p_{\\overline{x}}\\left( \\overline{x} \\right) = p(x_{1},x_{2},\\ldots,x_{n})\\)\n【정의】 확률변수 \\((X_{1},X_{2},\\ldots,X_{n})\\)의 함수 \\(U = g(X_{1},X_{2},\\ldots,X_{n})\\)(확률변수) 기대값은 다음과 같이 정의된다.\n(연속형) \\(E(U) = \\int\\int\\cdots\\int uf\\left( x_{1},x_{2},\\ldots,x_{n} \\right)dx_{1}dx_{2}\\ldots dx_{n}\\)\n(이산형) \\(E(U) = \\sum_{x_{1}}^{}{\\sum_{x_{2}\\cdots}^{}{\\sum_{x_{n}}^{}{p(x_{1},x_{2},\\ldots,x_{n})}}}\\)\n【정의】 동일한 모집단 분포 \\(f(x)\\)으로부터 서로 독립적으로 추출한 크기 \\(n\\)인 표본 \\((X_{1},X_{2},\\ldots,X_{n})\\)을 확률표본이라 한다.\n확률변수 \\(X_{i}\\)의 확률 밀도함수 :\\(f\\left( x_{i} \\right) = f(x)\\) (동일한 모집단 분포)\n\\((X_{1},X_{2},\\ldots,X_{n})\\) 결합 확률밀도함수 :\n\\(f\\left( x_{1},x_{2},\\ldots,x_{n} \\right) = (iid)f\\left( x_{1} \\right)f\\left( x_{2} \\right)\\ldots f\\left( x_{n} \\right) = f(x)^{n}\\)\n【정의】 확률표본 \\((X_{1},X_{2},\\ldots,X_{n})\\)의 평균은 \\((\\mu_{1},\\mu_{2},\\ldots,\\mu_{n})\\), 분산은 \\((\\sigma_{1}^{2},\\sigma_{2}^{2},\\ldots,\\sigma_{n}^{2})\\)인 경우 확률표본의 선형함수 \\(U = \\sum_{i}^{n}{a_{i}X}_{i}\\)의 평균은 \\(\\sum_{i}^{n}{a_{i}\\mu_{i}}\\)이고 분산은 \\(\\sum_{i}^{n}{a_{i}^{2}\\sigma_{i}^{2}}\\)이다. 단, \\(a_{i}\\)는 상수이다.\n【정리】 확률표본 \\((X_{1},X_{2},\\ldots,X_{n})\\)의 선형함수 \\(U = \\sum_{i}^{n}{a_{i}X}_{i}\\)의 적률생성함수는 다음과 같다. \\(M_{\\sum_{i}^{}{a_{i}X_{i}}}(t) = \\prod_{i}^{n}{M_{X_{i}}(a_{i}t)}\\).\n\n\n2. 공분산 행렬\n랜덤 확률 벡터 \\(\\overline{X} = (X_{1},X_{2},\\ldots,X_{n})'\\)의 평균 벡터와 공분산 행렬은 다음과 같이 정의된다.\n기호 : \\(E\\left( X_{i} \\right) = \\mu_{i},\\)\n\\(COV\\left( X_{i},Y_{j} \\right) = \\sigma_{ij} = E\\left( X_{i} - \\mu_{i} \\right)\\left( X_{j} - \\mu_{j} \\right) = \\sigma_{ji}fori \\neq j,\\sigma_{ii} = \\sigma_{i}^{2}fori = j\\)\n랜덤 확률변수 벡터 : \\(\\overline{X} = \\left\\lbrack \\begin{array}{r}\nX_{1} \\\\\nX_{2} \\\\\n\\vdots \\\\\nX_{n}\n\\end{array} \\right\\rbrack\\)\n평균 벡터 : \\(E(\\overline{X}) = \\left\\lbrack \\begin{array}{r}\n\\mu_{1} \\\\\n\\mu_{2} \\\\\n\\vdots \\\\\n\\mu_{n}\n\\end{array} \\right\\rbrack\\)\n공분산 행렬 : \\(COV\\left( \\overline{X} \\right) = \\Sigma = \\begin{pmatrix}\n\\sigma_{ii} & \\cdots & \\sigma_{in} \\\\\n\\vdots & \\ddots & \\vdots \\\\\n\\sigma_{ni} & \\cdots & \\sigma_{nn}\n\\end{pmatrix}\\)\n【정리】 만약 \\(X_{i}\\)가 서로 독립이면 공분산이 0이므로 공분산 행렬은 대각원소가 분산인 대각 행렬이다.\n\\(COV(\\overline{X}) = \\begin{pmatrix}\n\\sigma_{ii} & \\cdots & 0 \\\\\n\\vdots & \\ddots & \\vdots \\\\\n0 & \\cdots & \\sigma_{nn}\n\\end{pmatrix}\\)\n【정의】 \\(\\overline{X} \\sim (E\\left( \\overline{X} \\right) = \\overline{\\mu},COV\\left( \\overline{X} \\right) = \\Sigma)\\)이고 \\(\\overline{a} = (a_{1}a_{2}\\ldots a_{n})'\\)이 상수 벡터인 경우 다음이 성립하다.\n\\(E\\left( {\\overline{a}}'\\overline{X} \\right) = {\\overline{a}}'\\overline{\\mu} = \\sum_{i}^{n}{a_{i}\\mu_{i}}\\).\n\\(COV\\left( {\\overline{a}}'\\overline{X} \\right) = {\\overline{a}}'COV(\\overline{X})\\overline{a} = {\\overline{a}}'\\Sigma\\overline{a}\\).\n【정리】 \\(X_{i} \\sim iidN(\\mu_{i},\\sigma_{i}^{2})\\)이면\\(\\sum_{i}^{n}{a_{i}X_{i} \\sim N(\\sum_{i}^{n}{a_{i}\\mu_{i}},\\sum_{i}^{n}{a_{i}^{2}\\sigma_{i}^{2}})}\\)\n\\(X_{i} \\sim N(\\mu_{i},\\sigma_{i}^{2})\\) 적률생성함수 : \\(M_{X_{i}}(t) = exp(\\mu_{i}t + \\frac{\\sigma_{i}^{2}t^{2}}{2})\\)\n\\[{M_{\\sum_{i}^{n}a_{i}X}}_{i}(t) = \\prod_{i}^{n}{M_{X_{i}}(a_{i}t)} = \\prod_{n}^{n}{\\exp\\left( \\mu_{i}(a_{i}t) + \\frac{\\sigma_{i}^{2}{(a_{i}t)}^{2}}{2} \\right)} = \\exp\\left( \\sum_{i}^{n}{{(\\mu}_{i}a_{i}})t + \\frac{\\sum_{i}^{n}{{(\\sigma}_{i}^{2}{a_{i})t}^{2}}}{2} \\right)\\]\n그러므로 \\(\\sum_{i}^{n}{a_{i}X_{i} \\sim N(\\sum_{i}^{n}{a_{i}\\mu_{i}},\\sum_{i}^{n}{a_{i}^{2}\\sigma_{i}^{2}})}\\)\n【정리】 \\(X \\sim iidN(\\mu,\\sigma^{2})\\)의 확률표본\\((X_{1},X_{2},\\ldots,X_{n})\\)의 평균 \\(\\frac{\\sum_{i}^{n}X_{i}}{n} \\sim N(\\mu,\\frac{\\sigma^{2}}{n})\\)이다.\n【정리】 \\(X \\sim iidN(\\mu,\\sigma^{2})\\)의 확률표본\\((X_{1},X_{2},\\ldots,X_{n})\\)의 분산 \\(S^{2} = \\frac{\\sum_{i}^{n}\\left( X_{i} - \\overline{X} \\right)^{2}}{n - 1}\\)이면 \\(\\frac{(n - 1)S^{2}}{\\sigma^{2}} \\sim \\chi^{2}(n - 1)\\).\n【정리】 위의 2 정리를 통합하면 다음과 같다. \\(X \\sim iidN(\\mu,\\sigma^{2})\\)의 확률표본\\((X_{1},X_{2},\\ldots,X_{n})\\)의 평균 \\(\\overset{¯}{X} = \\frac{\\sum_{i}^{n}X_{i}}{n}\\), 분산 \\(S^{2} = \\frac{\\sum_{i}^{n}\\left( X_{i} - \\overline{X} \\right)^{2}}{n - 1}\\)에 대하여 다음이 성립한다.\n표본평균 \\(\\overline{X}\\), 표본분산 \\(S^{2}\\) 서로 독립이다.\n\\(\\overset{¯}{X} \\sim N(\\mu,\\frac{\\sigma^{2}}{n})\\).\n\\(\\frac{(n - 1)S^{2}}{\\sigma^{2}} \\sim \\chi^{2}(n - 1)\\)."
  },
  {
    "objectID": "notes/math_stat/probability.html",
    "href": "notes/math_stat/probability.html",
    "title": "수리 통계 1. 확률",
    "section": "",
    "text": "chapter 1. 집합론\n\n1. 확률론이란?\n파스칼의 삼각형은 숫자를 삼각형 모양으로 배열하여 이항 계수를 나타내는 도형이다. 각 행의 숫자는 조합의 개수를 의미하며, 위에서부터 차례로 0행, 1행, 2행 등으로 번호를 매긴다. 특정 행의 각 항은 바로 위 행의 두 항을 더한 값으로 계산된다.\n이 삼각형은 프랑스의 수학자 블레즈 파스칼의 이름을 따서 불리지만, 그 기원은 훨씬 이전으로 거슬러 올라간다. 인도, 중국, 페르시아 등 여러 고대 문명에서도 이미 유사한 형태의 삼각형이 사용되었으며, 특히 중국에서는 ’양휘삼각형’이라는 이름으로 알려져 있다.\n오늘날 파스칼의 삼각형은 조합론, 확률론, 수열의 성질 분석 등 다양한 수학 분야에서 핵심적인 도구로 활용된다.\n\n\n\n\n\n이항전개 계수 : \\((a + b)^{n} = \\overset{n}{\\sum_{k = 0}}\\binom{n}{k}a^{n - k}b^{k}\\)\n\\[(a + b)^{4} = a^{4} + 4a^{3}b + 6a^{2}b^{2} + 4ab^{3} + b^{4}\\]\n\\(11^{n}\\)의 값: \\(11^{3} = 1331\\)\n7전 4선승제로 진행되는 경기에서 두 선수의 실력이 동등하다고 가정할 때, 현재 A는 1승, B는 2승을 거둔 상태에서 경기가 종료되었다고 하자. 이 경우, 남은 경기에서 A는 3번, B는 2번을 추가로 이겨야 승리할 수 있었던 상황이다.\n이러한 조건에서 상금을 공정하게 배분하려면, 이후 가능한 모든 경기 결과의 경우의 수를 고려해야 한다. 이는 파스칼의 삼각형에서 5번째 행(즉, 5개의 경기가 남은 경우)에 해당하며, 그 합은 16이다.\n이 중 A가 3승을 먼저 달성하는 경우는 앞쪽 2개의 항(즉, A가 이기는 모든 경우)에 해당하며, 그 합은 5이다. 반대로 B가 2승을 추가하여 먼저 4승을 달성하는 경우는 뒤쪽 3개의 항의 합이며, 이는 11이다.\n따라서 A는 상금의 5/16, B는 11/16을 받는 것이 확률적으로 공정한 분배가 된다.\n\n\n2. 표본공간과 사건\n통계분석의 주요 목표 중 하나는 실험이나 관찰을 통해 특정 집단에 대한 합리적인 결론을 도출하는 데 있다. 이를 위해서는 먼저 실험이나 조사를 통해 얻을 수 있는 모든 가능한 결과의 집합, 즉 표본공간을 명확히 정의하는 것이 출발점이다. 표본공간을 식별함으로써 분석의 범위와 확률적 판단의 기반이 설정되며, 이후의 데이터 해석과 추론이 체계적으로 이루어질 수 있다.\n\n(1) 표본공간 정의\n【표본공간 정의】 표본공간(sample space)은 통계 실험 또는 관찰에서 발생할 수 있는 모든 가능한 결과의 집합을 의미하며, 일반적으로 \\(S\\)로 표기한다.\n\n실험이 동전을 던지는 것으로 구성되어 있다면, 표본공간은 앞면과 뒷면의 두 가지 결과를 포함한다. 따라서, \\(S = \\{ H,T\\}\\)\n실험이 특정 대학에서 무작위로 선택된 학생들의 수능 점수를 관찰하는 것이라면, 표본공간은 \\(S = \\{ 100,110,...,505\\}\\)(10점 단위)이다.\n특정 자극에 대한 반응 시간을 관찰하는 실험에서는 \\(S = (0,\\infty)\\)가 표본공간이다.\n\n\n\n(2) 표본공간의 유형\n표본공간은 그 안에 포함된 원소의 개수에 따라 가산 표본공간(countable sample space)과 비가산 표본공간(uncountable sample space)으로 구분된다.\n\n표본공간의 원소가 정수 집합의 부분집합과 1:1 대응을 이룰 수 있으면, 그 표본공간은 가산이다. 예를 들어, 동전 던지기나 SAT 점수의 표본공간은 가산 표본공간이다(사실, 유한한 집합이다)\n반응 시간 표본공간은 비가산 표본공간이다. 이는 양의 실수들이 정수와 1:1 대응을 이룰 수 없기 때문이다. 하지만 반응 시간을 가장 가까운 초 단위로 측정한다면, 표본공간은 다음과 같이 표현될 수 있다. \\(S = \\{ 0,1,2,3,\\ldots\\}\\)\n이러한 구분은 확률의 정의 방식과 수학적 모델링에 중요한 영향을 미친다.\n\n\n\n(3) 사건\n【사건 정의】 사건(event)이란 실험에서 발생 가능한 결과들의 모임, 즉 표본공간 \\(S\\)의 임의의 부분집합을 말한다. 표본공간, S 자체도 하나의 사건으로 포함된다.\n어떤 사건 \\(A\\)가 표본공간 \\(S\\)의 부분집합이라고 하자. 이때 실험의 결과가 \\(A\\)에 속하면, 사건 \\(A\\)가 발생했다고 말한다. 확률을 논의할 때는 개별 결과보다는 사건 전체에 대한 확률을 다루는 경우가 많으며, 이때 사건과 그에 대응하는 집합은 상호 교환적으로 사용된다. 즉, 사건의 확률을 말하는 것은 해당 부분집합에 속하는 결과가 나올 확률을 의미한다.\n\n포함 관계, 부분집합: \\(A \\subseteq B \\Longleftrightarrow x \\in A \\Longrightarrow x \\in B\\)\n동일성: \\(A = B \\Longleftrightarrow A \\subseteq B\\text{and}B \\subseteq A\\)\n\n\n\n\n3. 집합연산\n\n집합 \\(A\\)의 모든 원소를 셀 수 있다면 가산 countable 집합이다.\n만약 집합 \\(A\\)의 모든 원소가 집합 \\(B\\)에 포함되어 있다면 \\(A\\)는 \\(B\\)의 부분 subset 집합이다.\n\\(\\phi\\)는 공집합(null or empty set)을 의미하며 원소가 하나도 없는 집합이다.\n\\(A \\subset B\\) : 집합 A의 모든 원소가 집합 B에 있는 경우 집합 A는 집합 B의 부분집합 subset\n\\(A^{C}or\\overline{A}\\) : 집합 A의 여집합 complement로 집합 A에 있는 원소를 제외한 모든 표본공간 원소의 모임 ⇔ NOT\n\n\n\n\n\n\n어떤 두 사건 A와 B가 주어졌을 때, 다음과 같은 기본적인 집합 연산이 성립한다:\n\n\\(A \\cup B\\) 합집합 union: A 또는 B 또는 둘 다에 속하는 원소들로 구성된 집합이다. \\(A \\cup B = \\{ x:x \\in A\\text{또는}x \\in B\\}\\)\n\\(A \\cap B\\) 교집합 intersection: A와 B 모두에 속하는 원소들로 구성된 집합이다. \\(A \\cap B = \\{ x:x \\in A\\text{그리고}x \\in B\\}\\)\n\n표본공간 \\(S\\)에서 정의된 임의의 세 사건 \\(A,B,C\\)에 대해 다음이 성립합니다.\n정리\n\n교환법칙 commutativity: \\(A \\cup B = B \\cup A\\), \\(A \\cap B = B \\cap A\\)\n결합법칙 associativity: \\(A \\cup (B \\cup C) = (A \\cup B) \\cup C\\),\\(A \\cap (B \\cap C) = (A \\cap B) \\cap C\\)\n분배법칙 distributive Laws: \\(A \\cap (B \\cup C) = (A \\cap B) \\cup (A \\cap C)\\), \\(A \\cup (B \\cap C) = (A \\cup B) \\cap (A \\cup C)\\)\n드모르간 법칙 DeMorgan’s Laws: \\((A \\cup B)^{c} = A^{c} \\cap B^{c}\\), \\((A \\cap B)^{c} = A^{c} \\cup B^{c}\\)\n\n\n\n\n\n\n만약 \\(A_{1},A_{2},A_{3},\\ldots\\)가 표본공간 \\(S\\)위에 정의된 집합이라면,\n\\(\\overset{\\infty}{\\bigcup_{i = 1}}A_{i} = \\{ x \\in S:x \\in A_{i}\\text{for some}i\\}\\): 무한 합집합을 나타내며 \\(x\\)는 \\(A_{i}\\) 중 적어도 하나의 집합에 포함되면 합집합에 포함된다.\n\\(\\overset{\\infty}{\\bigcap_{i = 1}}A_{i} = \\{ x \\in S:x \\in A_{i}\\text{for all}i\\}\\): 무한 교집합을 나타내며 \\(x\\)는 모든 \\(A_{i}\\) 포함되어야 교집합에 포함된다.\n【예제】 주사위를 한번씩 두 번 던져 나타난 결과를 적는다. 모든 가능한 쌍(pair)을 표본공간이라 정의하자. 집합 A={두 번째 주사위 눈금이 짝수}, B={두 주사위 눈금의 합이 짝수}, C={두 주사위 눈금 중 적어도 하나가 홀수}라 정의할 때 다음을 구하라.\n\\(B^{C},A \\cup B,A \\cap B^{C},A^{C} \\cap C\\)\n\\(A = \\{ 3,456,7,8,9,10,11\\}\\), \\(B = \\{ 2,4,6,8,10,12\\}\\), \\(C = \\{ 23,4,5,6,7,8,9,10,11\\}\\)\n【풀이】 \\(S = \\{ 2,3,4,5,6,7,8,9,10,11,12\\}\\), \\(A^{c} = \\{ 2,12\\}\\), \\(B^{c} = \\{ 3,5,7,9,11\\}\\), \\(A \\cup B = \\left\\{ 2,3,4,5,6,7,8,9,10,11,12 \\right\\} = S\\), \\(A \\cap B^{c} = \\{ 3,5,7,9,11\\}\\), \\(A^{C} \\cap C = \\{ 2\\}\\).\n\n\n4. 상호 배타적 사건\n만약 \\(A \\cap B = \\varnothing\\)이면, 두 사건 A와 B는 서로소 disjoint, 또는 상호 배타적 mutually exclusive 라고 한다.\n【상호배타 정의】 만약 \\(A_{i} \\cap A_{j} = \\varnothing foralli \\neq j\\)이면 사건 \\(A_{1},A_{2},\\ldots\\)는 쌍별로 서로소 또는 상호 배타적라고 한다.\n만약 \\(A_{1},A_{2},\\ldots\\)가 쌍별로 서로소이고 \\(\\overset{\\infty}{\\bigcup_{i = 1}}A_{i} = S\\)라면, 이 집합 모음 \\(A_{1},A_{2},\\ldots\\)는 \\(S\\)의 분할 partition을 이룬다.\n【유용한 사례】 집합 \\(A_{i} = \\lbrack i,i + 1)\\)는 \\(\\lbrack 0,\\infty)\\)의 분할을 이룬다.\n\n\n\nchapter 2. 확률론 기초\n확률실험이 수행되면, 그 결과는 표본공간에 속하는 하나의 결과로 나타난다. 동일한 실험을 여러 번 반복하면 다양한 결과가 나타날 수 있으며, 특정 결과는 여러 번 반복되어 관찰될 수도 있다. 이때 결과의 발생 빈도는 확률의 개념으로 이해할 수 있다.\n일반적으로 확률이 높은 결과는 더 자주 나타나는 경향이 있으며, 이러한 반복적 관찰을 통해 실험 결과를 확률적으로 설명할 수 있다면, 이는 곧 통계적 분석의 출발점이 된다. 이러한 과정은 실험 데이터로부터 의미 있는 결론을 도출하는 데 중요한 기초를 이룬다.\n확률 실험(probability experiment)이란 특정 조건 하에서 수행되며, 그 결과가 미리 확정되어 있지 않은 실험을 의미한다. 이러한 실험은 불확정성, 반복 가능성, 그리고 확률적 설명 가능성이라는 세 가지 특징을 갖는다.\n확률 실험은 동일한 조건에서 여러 번 반복 수행될 수 있으며, 각 실험의 결과는 표본공간의 원소로 나타난다. 예를 들어, 주사위를 던지거나 동전을 던지는 실험은 확률 실험의 대표적인 예로, 결과는 사전에 예측할 수 없지만 반복을 통해 통계적 규칙성을 관찰할 수 있다.\n확률이란 미래에 발생할 수 있는 사건에 대한 믿음 또는 기대 정도를 수치로 나타낸 측정값이다. 이는 물리, 화학, 생물학, 사회과학 등 다양한 분야에서 관찰되는 관심 현상이 불확실성으로 인해 정확하게 예측될 수 없는 상황에서 활용된다.\n예를 들어, 향후 1분 동안의 심박수, 다리가 무너지기 직전의 최대 하중, 또는 주식 시장의 등락과 같은 현상은 정확한 값을 사전에 알 수 없다. 이러한 상황을 랜덤 상황(random situation)이라고 하며, 결과가 우연에 의해 결정되는 특성을 가진다. 그러나 비록 개별 결과는 불확실하더라도, 충분히 많은 반복 관찰을 통해 얻은 상대 빈도 정보가 있다면 사건 발생의 가능성을 확률로 추정할 수 있다.\n확률은 관심 사건이 일어날 가능성(chance or likelihood)을 0과 1 사이의 숫자로 표현한다. 확률이 0이면 해당 사건은 절대로 발생하지 않음을 의미하고,확률이 1이면 항상 발생함을 뜻한다. 일반적으로 확률은 0에서 1 사이의 값으로 주어지며, 0%에서 100%까지의 비율로 해석할 수도 있다.\n결국 확률은 불확실성을 수치화하여 미래를 예측하고 의사결정을 도울 수 있도록 하는 도구라 할 수 있다.\n\n1. 확률 측정\n확률은 관심 사건이 일어날 가능성 chance or likelihood을 숫자로 표현한 것으로 확률의 0 (일어날 가능성 없음)과 1(항상 일어남) 사이의 값으로 나타낸다.\n확률 정의\n\n\n\n\n\n\n상대 빈도 relative frequency\n동전을 던지는 경우 {앞 면이 나올 사건}에 관심이 있어 실험을 한다고 하자. 10번을 던지니 6번이 앞 면이었다면 상대빈도는 0.6이다. 계속 100번 던지니 52번이 앞 면이 나왔다면 상대 빈도는 0.52이다. 1000번을 던지니 515번이 앞면이었다면 상대 빈도는 0.515이다.\n이처럼 실험 횟수가 증가할수록 상대빈도는 점차 일정한 값에 수렴해 가는 경향을 보이며, 이 수렴하는 값이 바로 해당 사건의 확률이라고 할 수 있다. 이러한 현상은 법칙적 경향성 또는 큰 수의 법칙(Law of Large Numbers)을 통해 설명된다.\n\\(P(A) = \\lim_{n \\rightarrow \\infty}\\frac{f}{n}\\), \\(n\\)=실험 횟수, \\(f\\)=사건 A가 발생한 횟수, 무한히 많은 시행 후에는 관심 사건의 나타날 가능성을 예상-확률은 무한 실험 후에 관심사건이 발생한 횟수(상대 빈도)를 확률이라 정의한다.\n\nCount Buffon (1707-1788): 4040번 동전 던지기 실험 앞면 2048회, P(앞면)= 0.5069\nKarl Pearson (1900): 24, 000 던지기 앞면 12,012, P(앞면)=0.5005\nJohn Kerrich : 10,000 던지기, 앞면 5067 heads, P(앞면)=0.5067.\n\n예 : 공정 생산 제품의 불량률(확률)에 대한 모형을 위해서는 제품 검사(확률실험)를 통하여 검사 제품 개수 중 불량품의 개수(상대 빈도)를 계산하면 된다.\nLaplace 확률\n표본공간의 각 원소들이 일어날 가능성이 같다고 가정하고 확률을 정의하는 방식을 라플라스 확률(Laplace probability) 또는 고전적 확률의 정의라고 한다.\n예를 들어, 주사위를 던지는 실험에서 짝수가 나올 확률은 다음과 같이 계산된다. 표본공간의 원소 개수는 총 6개이며, 그 중 짝수(2, 4, 6)에 해당하는 원소는 3개이다. 따라서 짝수가 나올 확률은 \\(\\frac{3}{6} = 0.5\\) 이다.\n이 정의는 각 원소(주사위 눈금 1~6)가 동일한 확률, 즉 \\(\\frac{1}{6}\\) 의 확률로 발생한다는 균등가능성(equally likely)을 전제로 한다.\n고전적 정의는 단순하고 직관적이며, 다음에서 소개할 공리적 기반의 확률 정의와 동일한 수학적 구조를 갖는다. 다만 고전적 정의는 모든 결과가 동등하게 가능하다는 이상적인 가정에 의존한다는 점에서, 현실의 복잡한 확률 현상을 설명하는 데에는 한계가 있다.\n공리적기반\n표본공간 \\(S\\) 내의 사건 \\(A\\)에 대해, \\(A\\)에 0과 1 사이의 숫자를 대응시키고 이를 \\(A\\)의 확률(probability)이라 하며, 보통 \\(P(A)\\)로 표기한다. 이때 \\(P\\)는 사건에 확률을 부여하는 함수로 이해된다.\n자연스럽게, \\(P\\) 함수의 정의역은 \\(S\\)의 모든 부분집합으로 확장하는 것이 바람직해 보인다. 즉, \\(A \\subseteq S\\)인 모든 사건에 대해 \\(P(A)\\)를 정의할 수 있어야 한다.\n그러나 수학적으로 엄밀한 확률 이론을 구축하기 위해서는, \\(S\\)의 모든 부분집합에 대해 확률을 정의하는 것이 항상 가능하지 않다. 따라서 확률이 정의되는 사건의 집합, 즉 \\(P\\)의 정의역으로 적절한 부분집합들의 모음 \\(\\mathcal{B}\\)를 선정해야 하며, 이 \\(\\mathcal{B}\\)가 다음 세 가지 조건을 만족하면 이를 시그마 대수(sigma algebra) 또는 보렐 필드(Borel field)라고 한다.\n시그마 필드\n\n\\(\\varnothing \\in \\mathcal{B}\\) (공집합은 \\(\\mathcal{B}\\)에 포함된다).\n\\(A \\in \\mathcal{B}\\)라면 \\(A^{c} \\in \\mathcal{B}\\) (보수 연산에 대해 닫혀 있다).\n\\(A_{1},A_{2},\\ldots \\in \\mathcal{B}\\)라면 \\(\\overset{\\infty}{\\bigcup_{i = 1}}A_{i} \\in \\mathcal{B}\\) (가산 합집합에 대해 닫혀 있다).\n\n가장 간단한 시그마 대수는 \\(\\{\\varnothing,S\\}\\) 이다.\n표본공간 \\(S\\)와 이에 연관된 시그마 대수 \\(\\mathcal{B}\\)가 주어졌을 때, 확률함수 \\(P\\)는 다음 세 가지 조건을 만족하는 \\(\\mathcal{B}\\) 상의 함수이다.\n확률함수 정의\n\n비음성 조건: \\(P(A) \\geq 0\\text{for all}A \\in \\mathcal{B}\\). (모든 사건 A 에 대해 확률은 0 이상이다.)\n정규화 조건: \\(P(S) = 1\\). (전체 표본 공간 S 의 확률은 항상 1이다.)\n가산 가법성 조건: 만약 \\(A_{1},A_{2},\\ldots \\in \\mathcal{B}\\)가 쌍별로 상호배타적이라면, \\(P\\left( \\bigcup_{i = 1}^{\\infty}A_{i} \\right) = \\overset{\\infty}{\\sum_{i = 1}}P(A_{i})\\). (가산 합집합의 확률은 개별 사건들의 확률 합과 같습니다.)\n\n위 정의는 Kolmogorov 공리로 잘 알려져 있습니다. 이 공리적 정의는 확률을 직관적으로 정의하려는 시도(예: 빈도적 정의)와 달리, 수학적으로 엄격하고 보편적인 방식으로 확률을 설명합니다. 이 공리를 만족하는 함수는 모두 확률 함수로 간주된다.\n\n확률 공간 probability space: 확률함수는 \\((S,\\mathcal{B},P)\\)로 정의되는 확률 공간의 일부입니다. 여기서 \\(S\\)는 표본 공간, \\(\\mathcal{B}\\)는 사건의 집합(시그마 대수), \\(P\\)는 확률 함수입니다.\n확률 함수의 다양성: 동일한 표본 공간에서도 여러 가지 확률 함수가 정의될 수 있습니다. 예를 들어, 공정한 동전과 편향된 동전은 서로 다른 확률 함수를 가질 수 있습니다.\n\n\n\n\n2. 확률정의 방법\n\n(1) 확률정의 방법\n\\[S = \\{\\text{앞면, 뒷면}\\},\\mathcal{B} = \\{ S,\\varnothing,\\{\\text{앞면}\\},\\{\\text{뒷면}\\}\\},P(\\{\\text{앞면}\\}) = 0.5\\]\n\n표본공간: 공정한 동전을 던지는 실험 \\(S = \\{\\text{H},\\text{T}\\}\\)\n공정한 동전의 정의: 공정한 동전은 앞면과 뒷면이 나올 확률이 동일한 동전을 의미합니다. 따라서 \\(P(\\{\\text{H}\\}) = P(\\{\\text{T}\\})\\).\n\n이 관계는 직관에 의해 설정된 것이며 Kolmogorov 공리에서 직접적으로 도출된 것은 아니다.\n【Kolmogorov 공리 적용】 Kolmogorov 공리에 따라, 표본 공간 S 의 확률은 \\(P(S) = 1\\).\n\n\\(S = \\{\\text{H}\\} \\cup \\{\\text{T}\\}\\)이고, 앞면과 뒷면은 상호배타적이므로, 가산 가법성에 의해 \\(P(\\{\\text{H}\\} \\cup \\{\\text{T}\\}) = P(\\{\\text{H}\\}) + P(\\{\\text{T}\\})\\).\n이를 통해 \\(P(\\{\\text{H}\\}) + P(\\{\\text{T}\\}) = 1\\)이 성립합니다.\n\n【결론】 앞면과 뒷면의 확률이 동일하므로 \\(P(\\{\\text{H}\\}) = P(\\{\\text{T}\\}) = 0.5\\).\n다음 정리는 확률함수 \\(P\\)를 정의하는 구체적인 방법을 설명합니다. 이 정리는 유한 또는 가산 표본 공간에서 Kolmogorov 공리를 만족하는 확률함수를 체계적으로 정의하는 데 사용됩니다.\n【정리】 주어진 조건:\n\n\n표본공간 \\(S = \\{ s_{1},s_{2},\\ldots,s_{n}\\}\\)는 유한 집합이다.\n\n\n\n\n\\(\\mathcal{B}\\)는 \\(S\\)의 부분집합들로 이루어진 시그마 대수이다.\n\n\n\n\n\\(p_{1},p_{2},\\ldots,p_{n}\\)은 0 이상의 숫자로 이들의 합은 1이다 \\(\\overset{n}{\\sum_{i = 1}}p_{i} = 1\\).\n\n\n\\(A \\in \\mathcal{B}\\)에 대해 확률 함수 P(A) 는 다음과 같이 정의된다. \\[P(A) = \\sum_{\\{ i:s_{i} \\in A\\}}p_{i}\\]\n\n공집합에 대한 합은 0으로 정의됩니다. \\(P(\\varnothing) = 0\\).\n가산 집합 확장: \\(S = \\{ s_{1},s_{2},\\ldots\\}\\)가 가산 집합인 경우에도 위 방식으로 \\(P\\)를 정의할 수 있습니다. 이 경우에도 Kolmogorov 공리를 만족합니다.\n\n\n\n(2) 확률정의 방법\n다트 게임을 통해 확률을 정의하는 방법으로 표본공간에서 영역의 크기에 비례하여 확률을 정의하는 방법을 보여준다.\n【문제 설정】\n\n다트 게임에서는 다트를 던져 특정 영역에 맞추었을 때, 그 영역에 해당하는 점수를 얻는다.\n초보 플레이어를 대상으로, 다트가 특정 영역에 맞을 확률이 영역의 면적에 비례한다고 가정한다.\n따라서, 더 큰 영역은 맞을 확률이 더 높다.\n\n【가정】\n\n다트 보드의 반지름은 \\(r\\)이다.\n다트 보드에는 여러 개의 원형 영역이 있으며, 각 영역의 반지름은 일정 간격으로 나뉜다.\n다트가 반드시 보드에 맞는다고 가정한다.\n\n【특정점수 \\(i\\)를 얻을 확률】\n\\(P(\\text{scoring}i\\text{points}) = \\frac{\\text{Area of region}i}{\\text{Area of dart board}}\\).\n【계산 과정】\n\n점수 1을 얻을 확률: 영역 1은 중심 영역으로, 면적은 보드 전체 면적에서 두 번째 원까지의 영역을 뺀 값이다.\n\n\\[P(\\text{scoring 1 point}) = \\frac{\\pi r^{2} - \\pi(4r/5)^{2}}{\\pi r^{2}} = 1 - \\left( \\frac{4}{5} \\right)^{2}\\]\n\n영역 i 의 면적은 두 원 사이의 차이로 계산된다.\n\n\\[P(\\text{scoring}i\\text{points}) = \\frac{(6 - i)^{2} - (5 - i)^{2}}{5^{2}},i = 1,2,\\ldots,5\\]\n【특징】\n\n\\(\\pi\\)와 \\(r\\)은 최종 확률 계산에서 상쇄되므로, 확률은 반지름과 독립적이다.\n각 영역의 면적 합이 전체 보드 면적과 같으므로, 확률의 총합은 1이 된다.\n이 정의는 위 정리에 의해 올바른 확률 함수로 간주된다.\n\n\n\n\n\nchapter 3. 확률 계산\n\n1. 확률계산 관련 정리\n확률의 공리적 정의를 바탕으로 확률 함수의 다양한 성질을 유도할 수 있으며, 이러한 성질들은 복잡한 확률 계산에 효과적으로 활용된다. 특히, 이 성질들은 단일 사건뿐만 아니라 여러 사건의 결합 또는 조건부 관계에서 발생하는 확률을 계산하는 데 핵심적인 역할을 한다.\n\\(P\\)가 확률 함수이고 \\(A\\)가 \\(\\mathcal{B}\\)에 속하는 임의의 집합일 때, 다음이 성립한다.\n【정리 ①】\n\n\\(P(\\varnothing) = 0\\): 공집합의 확률은 항상 0입니다. 이는 직관적으로 공집합에는 발생할 수 있는 사건이 없기 때문입니다.\n\\(P(A) \\leq 1\\): 임의 사건 \\(A\\)의 확률은 1을 초과할 수 없습니다. 이는 확률이 0에서 1 사이에 있어야 한다는 공리에서 유도됩니다.\n\\(P(A^{c}) = 1 - P(A)\\): 여집합의 확률은 A 의 확률을 전체 확률1에서 뺀 값입니다.\n\n\\(P\\)가 확률 함수이고, A 와 B가 \\(\\mathcal{B}\\)에 속하는 임의의 집합일 때, 다음이 성립합니다.\n【정리 ②】\n\n\\(P(B \\cap A^{c}) = P(B) - P(A \\cap B)\\).\n\\(P(A \\cup B) = P(A) + P(B) - P(A \\cap B)\\).\n\\(A \\subseteq B\\)라면 \\(P(A) \\leq P(B)\\).\n\n\\(P(A \\cup B) \\leq 1\\) 이므로 정리 ②-2를 다음과 쓸 수 있다.\n\nBonferroni’s Inequality: \\(P(A \\cap B) \\geq P(A) + P(B) - 1\\)\n\n\\(P\\)가 확률 함수이고, A 와 B가 \\(\\mathcal{B}\\)에 속하는 임의의 집합일 때, 다음이 성립합니다.\n【정리】\n\n\\(P(A) = \\overset{\\infty}{\\sum_{i = 1}}P(A \\cap C_{i})\\), 각 \\(C_{i}\\)는 상호배타적이고 \\(\\overset{\\infty}{\\bigcup_{i = 1}}C_{i} = S\\).\n\n\n\n(Boole’s Inequality) \\(P\\left( \\bigcup_{i = 1}^{\\infty}A_{i} \\right) \\leq \\overset{\\infty}{\\sum_{i = 1}}P(A_{i})\\), 모든 \\(A_{i}\\)는 임의의 집합입니다.\n\n\n\n2. 확률계산 방법\nsample-point 방법\n이산형(표본공간의 원소의 수가 한정적일 경우, 헤아릴 수 있는 경우) 확률실험에서 사건 A에 대한 확률 계산 방법이다.\n\n실험을 정의하고 표본공간을 정의한다. 표본공간의 원소 개수(\\(N\\))를 카운트한다.\n표본공간의 각 원소에 확률 공리를 만족하도록 하여 적절한 확률 값을 할당한다. 라플라스 동등성 가정에 의해 각 원소의 활당 확률은 \\(1/N\\) 이다.\n사건A에 대한 원소를 정의하고 원소의 개수(n)를 카운트 한다.\n사건 A의 원소들의 확률을 더해 사건 A의 확률로 정의한다. \\(P(A)=n/N\\)\n\n곱의 법칙\nr번의 실험, 각 실험의 결과 수를 \\(n_1, n_2, ..., n_r\\) 이라 하면 실험 전체 결과 수는 \\(n_1 \\times n_2 \\times ... \\times n_r\\) 이다.\n【예제】 컴퓨터 비번을 만들려고 한다. 총 7자리 중 첫 2자리는 소문자, 3번째는 대문자, 남은 4자리는 0~9까지 숫자로 나열하여 만들 때 총 비번 개수는? \\(26 \\times 26 \\times 26 \\times 10^{4}\\)\n반복있고 순서 고려\n실험의 결과 수가 n이고 실험을 r번 반복할 때 나타날 수 있는 결과(경우) 수는 \\(n^r\\) 이다.\n4자리로 숫자로 구성된 비밀번호을 잊었다. 반복이 가능하다면, 최악의 경우 몇 번이나 시도해야 맞출 수 있나? \\(10^{4}\\)\n반복없이 나열\n실험의 결과 수가 n인 경우 이를 나열하여 만들 수 있는 총 결과 수는 \\(n! = n \\times (n - 1) \\times ... \\times 1\\) 이다.\n【예제】 4자리로 숫자로 구성된 비밀번호을 잊었다. 첫 번호에 0을 사용할 수 없고 반복이 불가능하다면, 최악의 경우 몇 번이나 시도해야 맞출 수 있나? \\(9 \\times 9 \\times 8 \\times 7\\)\n반복없고 순서 고려\nn개의 서로 다른 원소들 중 r개를 뽑아 순서대로 배열할 경우 발생하는 총 결과 수는 \\(nP_{r} = \\frac{n!}{(n - r)!}\\) 이다.\n【예제】 4자리로 숫자로 구성된 비밀번호을 잊었다. 반복이 불가능하다면, 최악의 경우 몇 번이나 시도해야 맞출 수 있나? \\({}_{10}P_{4} = 10 \\times 9 \\times 8 \\times 7\\)\n【예제】 주머니에 6개의 칩이 있거 칩에는 E, E, P, P, P, R이 각각 적혀있다. (1) 복원 추출(with replacement) (2) 비복원 추출(without replacement)로 하나씩 차례로 6개를 뽑아 영어 단어 PEPPER를 만들 확률을 구하시오. \\((1)P(\\text{“PEPPER”}) = \\frac{1}{2} \\times \\frac{1}{3} \\times \\frac{1}{2} \\times \\frac{1}{2} \\times \\frac{1}{3} \\times \\frac{1}{6}\\)\n\\((2)\\frac{6!}{3! \\times 2! \\times 1!} = \\frac{720}{6 \\times 2 \\times 1} = 60\\) /720=1/12\n반복, 순서 모두 없는 경우\nn개의 서로 다른 원소들 중 r개를 뽑아 순서없이 배열할 경우 발생하는 총 결과 수 \\({}_{n}C_{r} = \\frac{n!}{(n - r)!r!}\\) 이다.\n【예제】 5명의 대학원생과 3명의 학부생이 공무원 시험에 응시하였다. 여기서 4명을 선발할 경우 대학원 생이 3명 포함되어 있을 확률을 구하시오. \\(\\frac{{}_{5}C_{3} \\times_{3}C_{1}}{{}_{8}C_{4}}\\)\nEnumerating Outcomes\n유한 표본공간에서 가능한 결과를 나열하고, 이를 통해 확률을 계산하는 방법이다.\n\n유한 표본공간 \\(S = \\{ s_{1},s_{2},\\ldots,s_{N}\\}\\)을 정의한다.\n각 결과 \\(s_{j}\\)의 발생 가능성은 동일하다. \\(P(s_{j}) = \\frac{1}{N}\\)\n사건 A 의 확률 계산: \\(P(A) = \\sum_{s_{i} \\in A}P(\\{ s_{i}\\}) = \\sum_{s_{i} \\in A}\\frac{1}{N}\\) 이는 사건 A 에 속한 원소의 개수를 전체 표본 공간의 원소 수로 나눈 값과 같습니다. \\(P(A) = \\frac{\\text{no. of elements in}A}{\\text{no. of elements in}S}\\).\n\n\n\n\nchapter 4. 조건부 확률과 독립\n\n1. 조건부 확률\n만약 사건 A 와 B는 표본공간 \\(S\\)에서 정의되고 \\(P(B) &gt; 0\\)이라면, B가 발생했을 때 A가 발생할 조건부 확률은 다음과 같이 정의된다. \\(P(A|B) = \\frac{P(A \\cap B)}{P(B)}\\)\n조건부 확률 정의\n【재표현】 \\(P(A \\cap B) = P(A|B)P(B)\\)\n조건부 확률은 사건 B가 발생했다고 가정하므로 B가 새로운 표본 공간 역할을 합니다. 상호 배타적인 사건 A와 B는 공통된 원소를 가지지 않으므로 조건부 확률은 0입니다.\n【예제】 잘 섞인 52장의 카드 중에서 4장을 뽑을 때, 뽑은 4장이 모두 에이스일 확률을 계산 하시오.\n\n(방법1) \\(P(\\text{4 aces}) = 1/\\binom{52}{4}\\)\n(방법2) \\(P(\\text{4 aces}) = \\frac{4}{52} \\times \\frac{3}{51} \\times \\frac{2}{50} \\times \\frac{1}{49} = \\frac{1}{270,725}\\)\n\n【예제】 뽑힌 \\(i\\) 장 카드 중 에이스가 \\(i\\) 개 있다면, 뽑은 4장이 모두 에이스일 확률을 계산 하시오.\n\\[P(\\text{4 aces in 4 cards | i aces in i cards}) = \\frac{P(\\text{4 aces in 4 cards})}{P(\\text{i aces in i cards})}\\]\n\\[P(\\text{4 aces in 4 cards}) = 1/\\binom{52}{4}\\]\n\\[P(\\text{i aces in i cards}) = \\frac{\\binom{4}{i}}{\\binom{52}{i}}\\]\n\\[P(\\text{4 aces in 4 cards | i aces in i cards}) = \\frac{\\binom{52}{i} \\cdot \\binom{4}{4}}{\\binom{52}{4} \\cdot \\binom{4}{i}} = \\frac{1}{\\binom{52 - i}{4 - i}}\\]\n【Monte Hall Show】 몬티 홀 딜레마는 미국 TV 게임쇼 Make a Deal”에서 유래한 퀴즈 게임으로, 진행자 몬티 홀의 이름을 따왔습니다. 이 게임에서 참가자는 세 개의 문 중 하나를 선택해, 문 뒤에 숨겨진 선물을 받을 기회를 갖습니다. 세 문 중 하나 뒤에는 자동차가, 나머지 두 문 뒤에는 염소가 있습니다.\n예를 들어, 한 참가자가 1번 문을 선택했다고 가정해봅시다. 이때 진행자는 3번 문을 열어 염소가 있음을 보여준 후, 참가자에게 처음 선택한 1번 문 대신 2번 문으로 바꿀 기회를 제안합니다. 이 상황에서, 참가자가 자동차를 받을 확률을 최대화하려면 처음 선택을 유지하는 것이 좋을까요, 아니면 선택을 바꾸는 것이 유리할까요? 몬티 홀 문제는 이러한 질문을 통해 선택의 전략과 확률을 탐구하는 흥미로운 사례로, 단순한 직관에 반하는 결과를 보여줍니다.\n\n\n\n\n\n선택을 바꾸지 않을 경우 자동차에 당첨될 확률은 \\(\\frac{1}{3}\\)이지만, 선택을 바꿀 경우 확률은 \\(\\frac{2}{3}\\)로 증가합니다. 그럼에도 불구하고, 대부분의 참가자는 처음 선택을 고수하는 경향을 보입니다. 이는 인간이 항상 합리적 선택을 한다는 전통 경제학의 가정에 반하는 사례로, 몬티 홀 딜레마는 이러한 점에서 매우 유명합니다. 전통 경제학에 따르면 인간은 합리적이고 이성적인 존재로, 언제나 자신의 이익을 극대화하기 위해 행동한다고 가정합니다. 따라서 전통 경제학의 관점에서 보면, 몬티 홀 문제를 푸는 합리적인 인간은 선택을 바꾸는 것이 당연한 전략이 될 것입니다.\n【죄수 3인의 딜레마】 세 명의 수감자 A, B, C는 각각 별도의 감방에 수감되어 사형을 선고받았고, 주지사는 이들 중 무작위로 한 명을 사면하기로 결정하였습니다. 소장은 누가 사면 대상인지 알고 있지만, 이를 직접적으로 밝힐 수는 없습니다.\n이 상황에서, 수감자 A는 교도관에게 처형될 두 사람 중 한 사람의 이름을 알려달라고 요청합니다. A는 다음과 같은 제안을 합니다: ”B가 사면 대상이라면 C의 이름을, C가 사면 대상이라면 B의 이름을 알려주세요. 만약 제가 사면 대상이라면 동전을 던져 B 또는 C 중 하나의 이름을 알려주세요.” 이 요청에 따라 교도관은 A에게 B가 처형될 것이라고 말합니다.\n이 말을 들은 A는 자신이 사면될 확률이 원래의 \\(\\frac{1}{3}\\)에서 \\(\\frac{1}{2}\\)로 높아졌다고 믿고 기뻐합니다. 반면, 수감자 C는 A와 다르게 자신이 사면될 확률이 \\(\\frac{2}{3}\\)로 증가했다고 생각하며 안도합니다. 이 상황에서 누가 옳은 것일까요?\n\\[P(A|P) = \\frac{P(AP)}{P(P)} = \\frac{P(A)P(P|A)}{P(A)P(P|A) + P(B)P(P|B) + P(C)P(P|C)}\\]\n\\(= \\frac{1/3 \\times 1/2}{1/3 \\times 1/2 + 1/3 \\times 0 + 1/3 \\times 1} = 1/3\\).\n\\[P(C|P) = \\frac{P(CP)}{P(P)} = \\frac{P(C)P(P|C)}{P(A)P(P|A) + P(B)P(P|B) + P(C)P(P|C)}\\]\n\\(= \\frac{1/3 \\times 1}{1/3 \\times 1/2 + 1/3 \\times 0 + 1/3 \\times 1} = 2/3\\).\n\n\n2. 독립\n두 사건 A와 B가 독립적이면 아래와 같은 조건을 만족한다. \\(P(A \\cap B) = P(A)P(B\\)\n독립 정의\n사건 \\(E_{1},E_{2},{\\ldots,E}_{k}\\)가 다음 조건을 만족하면 서로 독립 mutually independent 이라 한다.\n서로독립 정의\n\\[P(E_{1}E_{2}...E_{k}) = P(E_{1})P(E_{2})...P(E_{k})\\]\n【사례】 도박사 Chevalier de Meré가 던진 질문을 다룬다. 그는 주사위를 4번 던졌을 때, 최소한 한 번 ’6’이 나올 확률에 관심을 가졌다.\n\\[P(\\text{at leat 1 six in 4 rolls}) = 1 - P(\\text{no six in 4 rolls}) = 1 - (5/6)^{4}\\]\n【사례】 \\(P(A) = 0.5,P(B) = 0.3,P(AB) = 0.1\\)이다. 다음을 구하라.\n\n\\[P(A|B) = P(AB)/P(B) = 0.1/0.3 = 1/3\\]\n\\[P(A|A \\cup B) = \\frac{P(A \\cap (A \\cup B))}{P(A \\cup B)} = \\frac{0.5}{0.5 + 0.3 - 0.1} = 5/7\\]\n\\[P(A|A \\cap B) = \\frac{P(A \\cap (A \\cap B))}{P(A \\cap B)} = \\frac{0.1}{0.1} = 1\\]\n\n【사례】 \\(P(A) &gt; 0,P(B) &gt; 0\\)이고 사건 A, B는 상호 배반이다. 서로 독립인가? 증명하라.\n【풀이】 상호 배반이면 \\(A \\cap B = \\phi\\)이므로 \\(P(A \\cap B) = 0\\)이므로 독립\\((P(AB) \\neq P(A)P(B))\\)은 아니다.\n【사례】 만약 \\(P(A) &gt; 0,P(B) &gt; 0\\), \\(P(A) &lt; P(A|B)\\)이면 \\(P(B) &lt; P(B|A)\\)임을 증명하라.\n【풀이】 \\(P(A) &lt; \\frac{P(AB)}{P(B)}\\)이므로 \\(P(A)P(B) &lt; P(AB)\\)이다. 그러므로 \\(P(B) &lt; P(AB)/P(A) = P(B|A)\\)이다.\n【사례】 두 사건 A, B는 서로 독립이다. 사건 \\(A,B^{c}\\)는 서로 독립인가?\n【풀이】 두 사건 A, B는 서로 독립이면 \\(P(AB) = P(A)P(B)\\), \\(P(AB^{C}) = P(A) - P(AB) = P(A) - P(A)P(B) = P(A)(1 - P(B)) = P(A)P(B^{C})\\)이므로 독립이다.\n동일한 방법으로 \\((A^{c},B^{c}),(A^{c},B)\\)도 독립임을 보일 수 있다.\n【사례】 동전을 세 번 던지는 실험의 표본공간은 다음과 같다.\\(S = \\{ HHH,HHT,HTH,THH,TTH,THT,HTT,TTT\\}\\)사건 \\(H_{i},i = 1,2,3\\)를 “i번째 던짐에서 앞면이 나오는 사건”으로 정의한다면 \\(H_{i}\\)는 서로 독립이다.\n\n\n\nchapter 5. 베이즈 확률\n베이즈 규칙(Bayes’ rule)은 영국의 수학자이자 장로교 목사였던 토머스 베이즈(Thomas Bayes, 1702–1761)가 발견한 확률 이론의 핵심 정리이다. 그는 생전에 이를 발표하지 않았지만, 사후에 동료였던 리처드 프라이스(Richard Price)가 베이즈의 미완성 원고를 정리하여, “An Essay Towards Solving a Problem in the Doctrine of Chances” 라는 제목으로 1763년에 왕립학회에 발표하였다.\n이 논문에서 베이즈는 관측된 증거에 근거하여 어떤 사건의 확률을 갱신하는 방법, 즉 사전 확률(prior probability)과 조건부 확률을 통해 사후 확률(posterior probability)을 계산하는 공식을 처음 제시하였다. 이후 이 정리는 베이즈 정리(Bayes’ theorem)로 불리며, 통계학, 기계 학습, 의학, 법학 등 다양한 분야에서 불확실한 정보를 체계적으로 업데이트하는 도구로 널리 활용되고 있다.\n토머스 베이즈는 확률 이론을 활용하여, 어떤 결과가 관측되었을 때 그것이 어떤 원인에 의해 발생했는지를 추론하는 문제를 해결하고자 했다. 그의 연구는 두 가지 핵심적인 방향으로 전개되었으며, 이는 오늘날 베이즈 통계학의 이론적 기초를 형성한다.\n역추론 문제 (Inverse Inference): 베이즈는 특정 결과가 관측되었을 때, 그 결과를 유발한 원인을 역으로 추정하는 방법을 연구했다. 이는 단순히 결과에서 확률을 계산하는 것이 아니라, 주어진 데이터를 바탕으로 사건의 사전 확률(prior probability)을 사후 확률(posterior probability)로 업데이트하는 과정이다. 이 접근은 오늘날 베이즈 추론의 중심 개념으로 자리잡았다.\n불확실한 상황에서의 의사결정: 베이즈는 불확실한 조건하에서 사건이 발생할 가능성을 계산하고, 추가적인 정보가 주어졌을 때 확률을 어떻게 갱신할 수 있는지에 관심을 가졌다. 이러한 사고 방식은 정보가 축적됨에 따라 확률을 동적으로 갱신하며, 더 나은 의사결정을 가능하게 한다. 이는 베이즈 정리가 단순한 수학 공식을 넘어서, 학습과 판단의 원리로 기능하게 되는 이유이기도 하다.\n\n1. 전확률 법칙\n\\(S = \\bigcup_{i = 1}^{k}B_{i},B_{i}B_{j} = \\phi fori \\neq j,P\\left( B_{i} \\right) &gt; 0\\), 즉 \\(B_{1},B_{2},\\ldots,B_{k}\\) 상호 배타적 합이 표본공간인 mutually exclusive and collectively exhausted 사건이라 하자. 임의의 사건 \\(A\\)에 대하여 다음이 성립하는데 이를 전확률 법칙이라 한다.\n\\[P(A) = \\sum_{i = 1}^{k}{P(AB_{i})} = \\sum_{i = 1}^{k}{P(B_{i})P({A|B}_{i})}\\]\n\n\n\n\n\n\n\n2. 베이지 규칙 Bayes Rule\n영국 철학자 Thomas Bayes의 이름에서 유래된 것으로 사건 H(가설)의 확률을 새로운 사건 D(데이터)에 의해 조건부 확률을 계산하는 개념이다. \\(P\\left( H \\middle| D \\right) = \\frac{P(D|H)P(H)}{P(D)}\\).\n\\(B_{1},B_{2},\\ldots,B_{k}\\) 상호 배타적 합이 표본공간인 mutually exclusive and collectively exhausted 사건인 경우 다음이 성립하는데 이를 베이지 규칙이라 한다.\n\\[P\\left( B_{j} \\middle| A \\right) = \\frac{P(B_{j}A)}{P(A)} = \\frac{P\\left( B_{j} \\right)P(A|B_{j})}{\\sum_{i = 1}^{k}{P\\left( B_{i} \\right)P(A|B_{i})}}\\]\n【예제】 노랑 주머니에는 3개 빨강 구슬, 7개 파랑 구슬이 있고 초록 주머니에는 8개 빨강 구슬, 2개 파랑 구슬이 있다. 주사위를 굴려 5 이상 수가 나오면 노랑 주머니에서 4이하 수가 나오면 초록 주머니에서 구슬을 뽑는다고 하자. 뽑은 구슬이 빨강이었다면 그 구슬이 노랑 주머니에서 나왔을 확률은? (답) 1/3\n\n\n\n\n\n\n\n3. 특이도와 민감도\n\n\n\n\n\n\n민감도 sensitivity : 양성(감염)을 양성으로 진단할 확률, 실제 양성 중에서 모델이 양성으로 올바르게 예측한 비율.\n특이도 specificity : 음성(정상)을 음성으로 진단할 확률\n양성 예측율 positive predicted Value, 정밀도 precision: 양성 진단자 중 참 양성 확률, 실제 음성 중에서 모델이 음성으로 올바르게 예측한 비율.\n음성 예측율 negative predicted Value : 음성 진단자 중 참 음성 확률\n감염율 prevalence : 전체 검사자 중 양성 비율\n정확도 accuracy : 전체 예측 중에서 올바르게 예측한 비율\n\\(\\text{F1 Score} = 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}\\): 정밀도와 민감도의 조화 평균. 불균형 데이터셋에서 정밀도와 민감도를 함께 고려한 척도"
  },
  {
    "objectID": "notes/math_stat/hypothesis_testing.html",
    "href": "notes/math_stat/hypothesis_testing.html",
    "title": "수리 통계 7. 가설검정과 신뢰구간",
    "section": "",
    "text": "chapter 1. 가설검정\n\n1. 기초\n\n(1) 개념\n추론 통계학 inference statistics 은 모집단의 특성치인 모수 \\(\\theta\\)에 대한 추정치(점 추정치, 구간 추정치)를 구하는 추정 과 모수 값에 대한 가설의 진위 여부(통계적 유의성)를 알아보는 가설 검정 hypothesis testing이 있다.\n가설 검정은 과학적 연구와 유사하다. ⑴자연 현상을 관찰하여 ⑵이론을 정립하고 (임의의 모수 값) ⑶관측치를 통하여 이론 진위 여부를 테스트 한다. 이론의 옳고 그름은 표본 관측치에 의해 판단된다.\n가설검정은 무죄 추정의 원칙으로 범인의 유죄여부를 배심원의 결정과정과 동일하다. ’범인은 무죄이다’를 시작으로 ’검사가 제시한 여러 증거’(표본 데이터)를 기반하여 범인의 유죄, 무죄를 판결한다. 가설검정은 연역적 추론 과정과 유사하여 1) 자연현상, 사회현상을 관찰하여 이론을 도출하고 2)이를 통계적 가설로 만든 후 3)적절한 (확률표본) 데이터를 수집 분석하여 4)가설의 진위를 검증한다.\n\n\n(2) 통계적 가설\n통계적 가설은 모수에 관한 가정이다. \\(X \\sim f(x;\\theta,\\theta \\in \\Omega)\\)의 모수 \\(\\theta\\)가 가지는 값에 대한 가정이다. 설정한 통계적 가설은 통계적 방법으로 테스트 할 수 있도록 한 것으로 모수의 값에 관한 것이다.\n\n불량률이 5%인 제조 공정을 개선하기 위한 새로운 공정이 제안되었다. 모수는 모집단 비율 \\(\\theta = p\\)\n기존의 약에 비해 두통에 효과적인 새로운 약을 개발하였다고 하자. 두 모집단 평균 차이 \\(\\theta = \\mu_{old} - \\mu_{new}\\)\n수능성적과 GPA의 관계? \\(GPA = a + b \\times SAT\\), \\(b = 0\\)?\n\n【정의】 모수 \\((\\theta \\in \\Omega = \\left\\{ \\theta;\\omega_{0} \\cup \\omega_{1} \\right\\},\\omega_{0} \\cap \\omega_{1} = \\phi)\\)에 관한 통계적 가설은 다음과 같이 2개의 가설로 나뉜다.\n\n귀무가설 null hypothesis : \\(H_{0}:\\theta \\in \\omega_{0}\\), 집합 \\(\\omega_{0}\\)에는 하나의 값 \\(\\omega_{0}\\)만 있다.\n대립가설 alternative hypothesis : \\(H_{1}:\\theta \\in \\omega_{1}\\), 집합 \\(\\omega_{1} = \\{\\theta;\\theta \\neq \\theta_{0}\\}\\)이다.\n양측 대립가설 : \\(\\omega_{1} = \\{\\theta;\\theta \\neq \\theta_{0}\\}\\)\n단측 대립가설 : \\(\\omega_{1} = \\{\\theta;\\theta &lt; \\theta_{0}\\}\\), \\(\\omega_{1} = \\{\\theta;\\theta &gt; \\theta_{0}\\}\\)\n\n우리가 지지하는 가설은 연구가설 research hypothesis 혹은 대립가설이라 한다. 대립가설과 반대되는 가설을 귀무가설라 한다. 어떤 가설이 지지되느냐는 관측된 표본 데이터에 의존한다. ”귀무”(null)의 의미는 아무 것도 없다는 것이다.\n\n\n(3) 검정통계량\n검정통계량은 통계적 가설 검정에서 사용되는 중요한 개념으로 확률표본 데이터를 기반으로 계산되며 귀무가설을 평가하는데 사용된다. 검정통계량은 일반적으로 확률표본 데이터의 요약된 값으로 귀무가설이 참일 때 얼마나 불일치하는지를 나타내는 역할을 한다. 검정통계량의 값은 귀무가설의 가정과 확률표본 데이터 간의 차이를 측정하며, 이 값을 기반으로 가설 검정의 결과를 판단한다.\n예를 들어, 평균값의 차이에 대한 가설 검정을 수행한다고 가정해보자. 두 그룹의 평균을 비교하려면 먼저 평균의 차이를 계산하고, 이 차이를 그룹 간의 변동을 나타내는 표준 오차로 나누어 표준화된 차이를 얻을 수 있는데. 이 표준화된 차이가 검정통계량이 된다.\n검정통계량의 값은 주어진 데이터에 따라 달라지며 이 값을 귀무가설에 대한 기대값과 비교하여 가설 검정의 결과를 결정한다. 일반적으로 검정통계량이 특정한 임계값보다 크거나 작으면 귀무가설을 기각하고 대립가설을 지지하는 결론을 내린다. 이 때 임계값은 유의수준에 따라 결정되며, 유의수준은 연구자가 허용하는 오류의 수준을 나타낸다.\n【검정통계량】\n\\(f(x;\\theta)\\)에서 확률표본 \\((X_{1},X_{2},\\ldots,X_{n})\\)의 함수인 통계량 \\(t(x_{1},x_{2},\\ldots,x_{n})\\)이 가설 검정에 사용되면 이를 검정 통계량이라 한다.\n【가설 검정 방법】\n\n채택 영역 acceptance region: 어떤 확률표본 값들에 대하여 귀무가설을 참으로 받아들일지 결정\n기각 영역 rejection region : 어떤 확률표본 값들에 대하여 귀무가설을 기각하고 대립가설을 참으로 받아들일지 결정\n\n\"귀무가설 기각\"과 \"대립가설 수용\" 사이의 차이를 이해하는 것은 쉽지 않다. 첫 번째 경우에는 실험자가 어떤 상태를 수용하는지에 대한 내용은 함축되지 않으며, 오직 \\(H_{0}\\)에 의해 정의된 상태가 기각된다는 것만을 나타낸다. 마찬가지로 \"귀무가설 수용\"과 \"귀무가설 기각하지 않음\" 사이에도 차이가 있을 수 있다. 첫 번째 구문은 실험자가 \\(H_{0}\\)로 지정된 상태(단일 모수 값)를 주장하려고 하는 것을 의미하며, 두 번째 구문은 실험자가 실제로 \\(H_{0}\\)를 믿지 않지만 기각할 증거가 없다는 것을 의미한다.\n\n\n(4) 기각역\n확률표본 \\((X_{1},X_{2},\\ldots,X_{n})\\)의 영역을 \\(\\mathcal{D}\\)라 하자. \\(\\mathcal{D}\\)의 부분 집합 \\(RR\\) 에 확률표본 값이 포함되면 귀무가설을 기각한다면 \\(RR\\) 영역을 기각역 critical region 이라 한다.\n\n귀무가설: \\(H_{0}:\\theta \\in \\omega_{0} = \\{\\theta;\\theta_{0}\\}\\) – 단일 simple 가설\n대립가설: \\(H_{1},H_{a}:\\theta \\in \\omega_{1} = \\{\\theta;{\\theta \\neq \\theta}_{0}\\}\\) 복합 composite 가설\n\n만약 \\(t\\left( x_{1},x_{2},\\ldots,x_{n} \\right) \\in RR\\), 귀무가설 기각한다. ⬄ 대립가설 채택한다. 만약 \\(t\\left( x_{1},x_{2},\\ldots,x_{n} \\right) \\in {RR}^{c}\\), 귀무가설 채택한다. ⬄ 대립가설을 기각한다.\n귀무가설을 기각하는 검정통계량의 값이 포함된 영역을 기각역 이라 한다. 만약 확률표본으로부터 계산된 검정통계량의 값이 기각역에 포함되면 귀무가설을 기각(대립가설 채택)하고 그렇지 않으면 귀무가설을 채택한다. 빨간 영역 부분이 기각역이다. 영역이 시작되는 값을 기각값, 임계값 critical value 이라 한다.\n\n\n(5) 1종오류, 2종 오류\n【1종 오류, 2종 오류】\n\n1종 오류 : 귀무가설이 사실일 때 귀무가설을 기각할 확률이다. type I error, \\(\\alpha\\) 라고 표기한다.\n2종 오류 : 대립가설이 사실일 때 귀무가설을 채택하는 확률이다. type II error, \\(\\beta\\)라 표기한다.\n\n즉 \\(\\alpha,\\beta\\)는 통계적 가설 검정의 정확도를 측정하는데 매우 유용하다.\n\n\n\n\n\n\n\n\n실제 모집단\n가설 판단\n귀무가설(\\(H_{0}\\)) 진실\n대립가설(\\(H_{1}\\)) 진실\n\n\n귀무가설 기각\n1종 오류 \\(\\alpha\\)\n옳은 판단\n\n\n귀무가설 채택\n옳은 판단\n2종 오류 \\(\\beta\\)\n\n\n\n가설 검정의 목표는 모든 가능한 기각 영역 중에서 이러한 오류의 확률을 최소화하는 영역을 선택하는 것이지만 일반적으로 이것은 불가능한다. 이러한 오류의 확률은 종종 풍선 효과를 가지고 있는데 이것은 극단적인 경우에 즉시 볼 수 있다.\n단순히 \\(RR = \\phi\\)라 가정해 보자. 이 기각 영역에서는 결코 귀무가설을 기각하지 않을 것이므로 제1종 오류의 확률은 0이 될 것이지만, 제2종 오류의 확률은 1이다. 종종 우리는 두 오류 중에서 제1종 오류를 더 나쁜 오류로 간주한다. 따라서 우리는 제1종 오류의 확률을 제한하는 기각 영역을 선택한 다음 이러한 기각 영역 중에서 제2종 오류의 확률을 최소화하는 하나를 선택하려고 노력하게 된다. 검정 시 설정한 1종 오류(검정 크기)을 유의수준 significance level이라 한다.\n【검정 크기】\n만약 \\(\\alpha = \\max_{\\theta \\in \\omega_{0}}{P_{\\theta}(t\\left( X_{1},X_{2},\\ldots,X_{n} \\right) \\in RR)}\\) 이면 기각역의 크기 size는 \\(\\alpha\\)라 한다. 크기 \\(\\alpha\\)의 모든 기각 영역을 고려할 때, 우리는 제2종 오류의 확률이 더 낮은 영역을 고려한다. 또한 제2종 오류의 여집합을 살펴볼 수 있는데 즉 귀무가설이 참일 때 귀무가설을 기각하는 것이며 이것은 옳은 결정이다. 우리는 이후의 결정의 확률을 최대화하려고 하므로 이 확률이 최대한 크도록 하려고 합니다. 즉, \\(\\theta \\in \\omega_{1}\\)인 경우 우리는 다음을 최대화하고자 한다.\n\\[1 - P_{\\theta}(typeIIerror) = P_{\\theta}\\left( \\left( X_{1},X_{2},\\ldots,X_{n} \\right) \\in RR \\right)\\]\n【1종 오류】\n1종 오류는 유의수준, 검정 크기와 동일하다. 기각역 \\(C\\)에 검정통계량 값이 속할 확률이다. \\(P_{\\theta_{0}}(t\\left( X_{1},X_{2},\\ldots,X_{n} \\right) \\in RR)\\)\n【2종 오류 계산】\n귀무가설 \\(H_{0}:\\theta = \\theta_{0}\\), 대립가설 \\(H_{1}:\\theta = \\theta_{1} &lt; \\theta_{0}\\)의 경우 2종 오류을 계산하는 방법을 살펴 보자. 2종 오류를 계산하려면 대립가설에 속한 모수의 하나의 값이 필요하다. 대립가설의 모수 값은 구간이기 때문이다. 대립가설의 하나의 값을 \\(\\theta_{1}\\)라 하자.\n\n2종 오류: \\(P_{\\theta_{1}}(t\\left( X_{1},X_{2},\\ldots,X_{n} \\right) \\in CR)\\) 검정 통계량이 기각역(\\(CR\\))에 속하지 않을 확률\n대립가설의 \\(\\theta_{1}\\)이 \\(\\theta_{0}\\)에 가깝다면 대립가설이 사실임에도 불구하고 귀무가설 받아들일 가능성이 높다. 즉 2종 오류는 커진다.\n\n\n\n(6) 검정력\n【검정력 함수】\n다음은 기각역의 검정력 함수 power function로 정의한다. 검정력 함수는 모수 함수이다.\n\\(\\gamma_{C}(\\theta) = P_{\\theta}(t\\left( X_{1},X_{2},\\ldots,X_{n} \\right) \\in RR;\\theta \\in \\omega_{1})\\)\n귀무가설 하의 모수 값에서 검정력은 1 종 오류(유의수준)이다.\n【예제】 \\(f(x;\\theta) = \\frac{1}{\\theta},0 &lt; x &lt; \\theta \\sim U(0,\\theta)\\)에서 모수 \\(\\theta\\)에 대한 다음 가설을 검정하기 위하여 확률표본을 추출하였다.\n\n귀무가설 : \\(H_{0}:\\theta = \\theta_{0}\\)\n대립가설 : \\(H_{1}:\\theta = \\theta_{1} &lt; \\theta_{0}\\) 단측 대립가설\n검정통계량 : MVUE의 함수이며 확률밀도함수가 알려진 통계량을 이용한다. *) 모집단 비율의 MVUE는 \\(\\sum X_{i}/n\\)이지만 \\(\\sum X_{i} \\sim B(n,p)\\) 이므로 \\(T = \\sum X_{i}\\)을 검정 통계량으로 사용한다.\n기각역 및 임계값 : 만약 \\(RR = \\{ t;T \\leq k\\}\\)이면 귀무가설 기각한다. 임계값 \\(\\alpha = P_{\\theta_{0}}(T \\leq k)\\)에 의해 결정된다.\n검정력 함수: \\(\\gamma_{C}(\\theta) = P_{\\theta}(T \\leq k),\\theta \\leq \\theta_{0}\\), 대립가설의 참 모수 값이 귀무가설 \\(\\theta_{0}\\)에서 멀어질수록 검정력은 커진다.\n\n【예제】 A 교수는 수리통계학 시험 점수가 평균 70점 수준으로 출제했다고 했다. 수강생이 시험을 보고 나와 어려워 70점 미만이라고 주장했다. 25명 수강생 점수를 확인한 결과 평균 69점, 표준편차 9점이었다. 수강생들의 주장을 유의수준 5%에서 검정하라. 수리통계학 점수는 정규분포를 따른다.\n\n귀무가설: \\(H_{0}:\\mu_{0} = 70\\)\n대립가설: \\(H_{1}:\\mu_{1} &lt; 70\\) 단측 대립가설\n검정통계량: 모집단 평균 MVUE는 \\(\\overline{X}\\)이고 \\(\\frac{\\overline{X} - \\mu_{0}}{\\frac{S}{\\sqrt{n}}} = \\frac{69 - 70}{\\frac{9}{\\sqrt{25}}} = - 1.67 \\sim t(n - 1 = 24)\\)이다.\n기각역 및 임계값: \\(0.05 = P_{\\mu_{0}}(t(24) \\leq k)\\) 식에서 의해 결정된다. \\(k = - 1.71\\)\n검정력 함수: \\(\\gamma_{C}(\\theta) = P_{\\mu}\\left( \\frac{69 - \\mu}{\\frac{9}{\\sqrt{25}}} \\leq - 1.71 \\right)\\)\n\n\n\n(7) 유의확률\n1종 오류, 2종 오류, 검정력을 계산하는 방법에 대해 다루었다. 가설검정 절차를 보면 ① 가설 설정과 유의수준 \\(\\alpha\\)을 설정하고 ② 적절한 검정통계량 계산 ③ 기각역(유의수준과 검정통계량의 분포에 따라 계산된다)을 얻고 이에 따라 귀무가설 채택 여부를 결정한다.\n설정된 1종 오류를 유의수준 significant level, test level 이라 하고 \\((1 - \\beta)\\)을 검정력 power of test, test power 이라 한다. 유의수준은 일반적으로 1%, 5%, 10% (물론 5%을 가장 많이 사용하지만)을 사용하게 되므로 기각역이 달라진다. 만약 어떤 사람이 유의수준 5%에서 귀무가설을 기각하지 못했다고 발표했다. 그럼? 10%일 때는… 우리는 다시 가설 검정을 해야 한다. why? 기각역이 달라지므로…\n이런 문제를 해결할 방법이 없을까? p-값 p-value, 유의확률 significant probability 개념을 도입하자. 검정통계량이 계산되면 귀무가설을 더 기각할 영역의 확률을 p-값이라 한다. 다음은 대립가설이 \\(H_{1}:\\theta &gt; \\theta_{0}\\)일 경우 유의확률을 계산 방법이다.\n【유의확률】\n대립가설은 \\(H_{1}:\\theta = \\theta_{1} &gt; \\theta_{0}\\)이고 확률표본에 의해 계산된 검정통계량 값을 \\(t^{*}\\) 하자.\n\n\\(RR = \\{{T &gt; t}^{*}\\}\\).\n유의확률: \\({p - value = P}_{\\theta_{0}}(t\\left( X_{1},X_{2},\\ldots,X_{n} \\right) \\in RR)\\)\n\n계산된 검정통계량이 귀무가설이 진실이라는 가정 하에 귀무가설을 기각할 최소의 확률을 유의확률 혹은 p-값이라 한다. 그러므로 유의확률을 계산된 유의수준이다. 계산된 검정통계량이 귀무가설을 기각할 방향(대립가설을 채택할 방향)의 영역 확률을 유의확률이라 한다.\n유의확률이 주어지면 귀무가설 채택 여부를 알 수 있다. 유의확률이 유의수준보다 크다면 검정통계량의 값은 기각역에 속한 것이 아니므로 귀무가설이 채택되고 유의확률이 유의수준보다 적다면 검정통계량의 값이 기각역에 속하므로 귀무가설을 기각한다.\n【예제】 모집단 \\(f(x;\\theta) \\sim N(\\mu,\\sigma^{2})\\)에서 표본 크기 \\(n\\)인 확률표본을 추출하였다. 모분산 \\(\\sigma^{2}\\)을 알지 못할 때 nuisance parameter 귀무가설 \\(H_{0}:\\mu = \\mu_{0}\\)와 대립가설 \\(H_{1}:\\mu = \\mu_{1} &gt; \\mu_{0}\\)을 유의수준 \\(\\alpha\\) 에서 우도비 검정방법은 \\(T(\\overline{x}) = \\frac{\\sqrt{n}(\\overset{¯}{x} - \\mu_{0})}{S} \\sim t(n - 1)\\)이다.(자세한 내용은 다음 2절. 우도비 검정을 참고하기 바란다.)\n\n\\({p - value = P}_{\\theta_{0}}\\left( t\\left( X_{1},X_{2},\\ldots,X_{n} \\right) \\in RR \\right) = P(t(n - 1) \\geq T(\\overline{x}))\\)\n만약 대립가설 \\(H_{1}:\\mu = \\mu_{1} \\neq \\mu_{0}\\) (양측 대립가설)이면 \\(p - value = 2P(t(n - 1) \\geq |T\\left( \\overline{x} \\right)|)\\).\n\n【유의확률과 대립가설 형태】\n\n\n\n\n\n유의확률을 계산할 때 계산된 검정 통계량 값이 귀무가설에 설정한 모수 값(확률밀도함수의 중앙)으로부터 멀어지는 구간에 속할 확률을 구하면 된다. 단측 대립가설이면 구한 유의확률을 그대로 사용하면 되지만 양측 대립가설일 경우에는 계산된 유의확률을 2배 하여 유의확률로 사용해야 한다. 그러므로 양측 대립가설이 채택되면 단측 대립가설도 항상 성립한다.\n\n\n(8) 가설검정 관련 코멘트\n\n양측검정이냐? 단측검정이냐? 우리가 무엇에 관심을 갖느냐 하는데 있다. 불량률과 같이 ~값 미만에 관심이 있다면 왼쪽이 단측 검정, 수익과 같이 ~값 이상에 관심이 있다면 우측이 대립가설이 단측 검정을 실시한다. ”차이가 없다” 같이 양쪽 모두에 관심이 있다면 양측검정을 실시한다.\n양측 대립가설이 기각되면 단측 대립가설은 항상 기각된다. 그러므로 “차이가 없다”가 기각되면 “차이가 있다”는 물론 “크다” 혹은 “적다”로 검정 결과를 내려도 된다.\n1종 오류는 기각역이 정해지면 계산 가능하지만 2종 오류는 대립 가설의 값 중 임의의 값이 설정되어야 가능하다. 검정력은 (1-2종 오류) 정의된다. 또한 우리의 관심이 대립가설에 있으므로 1종 오류를 설정하고(이를 유의수준이라 한다) 2종 오류를 최소화 하는 검정 방법을 구하게 된다. (1종, 2종 오류 모두 줄이는 방법은 없다.)\n단측 검정일 때 귀무가설에 대하여 대립가설이 \\(H_{1}:\\theta &lt; \\theta_{0}\\)라면 귀무가설이 \\(H_{0}:\\theta \\geq \\theta_{0}\\)은 아닌가? 1종 오류는 \\(\\alpha = \\max_{\\theta \\in \\omega_{0}}{P_{\\theta}(t\\left( X_{1},X_{2},\\ldots,X_{n} \\right) \\in CR)}\\)에 해당된다. 그러므로 Boundary 값이 \\(\\theta_{0}\\)에서 최대화되므로 \\(H_{0}:\\theta = \\theta_{0}\\)이 적절하다. 그리고 우리의 관심은 귀무가설이 아니므로 귀무가설이 채택된다면 우리는 헛수고 한 것이다.\n\n\n\n(9) 가설검정과 신뢰수준\n\n\n\n\n\n유의수준 \\(\\alpha\\)의 양측 가설 검정과 \\(100(1 - \\alpha)\\%\\)신뢰구간은 일대일 대응관계가 있다. 모집단 \\(f(x;\\theta = \\mu)\\), 확률표본, 추정량 \\(\\overset{\\hat{}}{\\theta} = \\overset{¯}{X}\\), 표본분산 \\(S^{2}\\), 귀무가설 \\(H_{0}:\\theta = \\theta_{0}\\)인 경우 대표본 이론에 의해 \\(T = \\frac{\\overset{\\hat{}}{\\theta} - \\theta_{0}}{\\frac{S}{\\sqrt{n}}}\\)는 \\(N(0,1)\\)을 따르므로 다음이 성립한다.\n\n\\(100(1 - \\alpha)\\%\\) 신뢰구간 : \\(\\theta \\pm z_{\\left( 1 - \\frac{\\alpha}{2} \\right)}\\frac{s}{\\sqrt{n}}\\)\n유의수준 \\(\\alpha\\)인 경우 기각역 : \\(C = \\{|\\frac{\\overset{\\hat{}}{\\theta} - \\theta}{\\frac{s}{\\sqrt{n}}}| &gt; z_{\\left( 1 - \\frac{\\alpha}{2} \\right)}\\}\\)\n유의수준 \\(\\alpha\\)인 경우 채택역 : \\(C^{c} = \\{\\left| \\frac{\\overset{\\hat{}}{\\theta} - \\theta}{\\frac{s}{\\sqrt{n}}} \\right| \\leq z_{\\left( 1 - \\frac{\\alpha}{2} \\right)}\\}\\)\n\n\n\n\n\n\n즉 신뢰구간과 채택역은 동일함을 알 수 있다. 신뢰구간에 속한 모수 값이 귀무가설에 설정되면 그 귀무가설은 기각되지 않는다. 표본으로부터 통계량이 계산되면 신뢰구간의 식에의해 모수에 대한 구간이 결정된다. 그 부분이 빨간 영역 부분이다.귀무가설에 빨간 영역 안에 있는 모수 값이 설정되면 귀무가설을 기각하지 못한다.\n단측 가설 검정과 상한(혹은 하한) 신뢰구간도 일대일 관계가 있다. \\(100(1 - \\alpha)\\%\\) 하한 신뢰구간은 유의수준 \\(\\alpha\\), 귀무가설 \\(H_{0}:\\theta = \\theta_{0}\\), 대립가설: \\(H_{0}:\\theta_{1} &gt; \\theta_{0}\\)의 채택역과 동일하다. 또한 상한신뢰구간 대립가설 \\(H_{0}:\\theta_{1} &lt; \\theta_{0}\\)의 채택역과 동일하다.\n【예제】 정부는 PC 구입 가격이 900$이라고 발표했다. 이를 알아보기 위하여 소비자 55명을 임의 조사하였더니 평균 885$, 표준편차는 50$이었다.\n\n모집단 평균에 대한 99%(양측)신뢰구간을 구하시오. (\\(\\overset{¯}{X} \\pm Z_{0.995}\\frac{S}{\\sqrt{n}}) \\rightarrow (885 \\pm 2.58\\frac{50}{\\sqrt{25}}) \\rightarrow (859.2,910.8)\\)\n정부의 발표가 진실이라고 할 수 있나(유의수준 1%)? 유의수준 1% 검정 통계량 : \\(\\frac{\\overset{¯}{X} - \\mu_{0}}{\\frac{S}{\\sqrt{n}}} = \\frac{885 - 900}{\\frac{50}{\\sqrt{25}}} = - 0.5\\)\n기각역 : \\(C = \\{ t;|t| &lt; 2.58\\}\\) 검정 통계량 값이 기각역에 속하지 않으므로 귀무가설을 채택한다. 유의수준 1%에서 귀무가설 값이 99% 신뢰구간에 속하므로 귀무가설 채택하게 되므로 신뢰구간을 계산한 경우에는 가설검정을 할 필요가 없다.\n\n\n\n(10) 랜덤화 검정\n가설 검정에서 설정한 유의수준에 의해 기각역이 설정되는데 이산형 확률분포의 경우 설정된 유의수준 정확한 값에 대응하는 기각역을 설정하지 못하는 경우가 발생하는 경우 랜덤화 검정을 실시한다. 예제로 이 개념을 설명하기로 한다.\n【예제】 \\(f(x;\\theta = \\lambda) \\sim Poisson(\\theta)\\)의 모수 \\(\\theta\\)에 대한 가설 검정을 위하여 표본크기 \\(n = 10\\)인 확률표본 \\((X_{1},X_{2},\\ldots,X_{10})\\)을 추출하였다. 귀무가설 \\(\\theta = 0.1\\), 귀무가설 \\(\\theta &gt; 0.1\\)을 유의수준 5%에서 검정하려고 할 때 기각역을 구하라\n\n검정 통계량: \\(T = \\sum_{i}^{10}X_{i} \\sim Poisson(\\lambda = 10\\theta)\\)\n기각역: \\(P_{\\theta_{0} = 0.1}(T \\geq 3) = 0.08\\), \\(P_{\\theta_{0} = 0.1}(T \\geq 4) = 0.019\\) 이처럼 유의수준 5%에 해당하는 기각역 얻을 수 없다. 보간법에 의해 \\(\\frac{0.05 - 0.019}{0.08 - 0.019} = \\frac{31}{61}\\) 이다. 그러므로 유의수준 5% 기각역은 다음과 같다. 만약 \\((T \\geq 4)\\) 귀무가설을 기각하고 \\((T = 3)\\)이면 \\(\\frac{31}{61}\\) 확률로 기각한다.\n\n\n\n\n2. 가설검정 방법\n\n(1) 최강 검정법\n검정방법의 정도 goodness는 1종 오류와 2종 오류로 판단하는데 가설검정 시작 시 1종 오류는 유의수준으로 설정되므로 좋은 검정 방법이란 2종 오류를 최소화 하는 것이다. 즉 (1-2종 오류), 검정력 최대화 하는 것이다. 이를 최강 검정법 most powerful test 이라 한다.\n모집단 확률변수 \\(X \\sim f(x;\\theta),\\theta \\in \\Omega = \\{\\omega_{0} \\cup \\omega_{1}\\}\\)이고 다음 2개의 단일 귀무가설과 대립가설을 가정하자.\n\n귀무가설 : \\(H_{0}:\\theta \\in \\omega_{o}\\)\n대립가설 : \\(H_{1}:\\theta \\in \\omega_{1}\\)\n\n모집단 확률변수 \\(X \\sim f(x;\\theta)\\) 에서 확률표본 \\({\\overline{X}}' = (X_{1},X_{2},\\ldots,X_{n})\\)이라 하고 \\(\\mathcal{S}\\)을 확률표본 영역이라 하자.\n【유의수준】 확률표본의 영역을 \\(\\mathcal{S}\\), 확률표본 영역의 부분 집합 \\(RR \\in \\mathcal{S}\\)이라 하자. 다음 \\(\\alpha\\)을 1종 오류 type I error, 유의수준 significant level 이라 한다. \\(\\alpha = \\max_{\\theta \\in \\omega_{0}}{P_{\\theta}\\left( T\\left( X_{1},X_{2},\\ldots,X_{n} \\right) \\in RR \\right)}\\).\n【검정력】 다음을 검정력 power function 이라 정의한다.\n\\({{P(\\theta) = \\gamma}_{RR}(\\theta) = P}_{\\theta}\\left( T\\left( X_{1},X_{2},\\ldots,X_{n} \\right) \\in RR \\right);\\theta \\in \\omega_{1}\\)\n【예제】 표본크기 5인 확률표본 \\(\\left( X_{1},X_{2},\\ldots,X_{5} \\right) \\sim B(p)\\)을 이용하여 \\(H_{0}:p = 0.5vs.H_{1}:p &gt; 0.5\\) 가설을 검정한다고 하자.\n(1) 기각역을 \\(RR = \\{\\sum x_{i} = 5\\}\\)\n\n1종 오류: \\(\\alpha = P(\\sum x_{i} = 5,\\sum x_{i} \\sim B(5,0.5)) = 0.031\\)\n2종 오류 : \\(\\beta(p) = P(\\sum x_{i} \\leq 4,\\sum x_{i} \\sim B(5,p &gt; 0.5))\\)\n검정력 : \\(Power(p) = 1 - \\beta(p)\\)이므로 \\(p = 0.5\\)에서 멀어질수록 검정력은 커진다. \\(p = 1\\)일 때 검정력은 0이다.\n\n(2) 기각역을 \\(RR = \\{\\sum x_{i} \\geq 3\\}\\)\n\n1종 오류 : \\(\\alpha = P(\\sum x_{i} \\geq 5,\\sum x_{i} \\sim B(5,0.5)) = 0.5\\)\n2종 오류 : \\(\\beta(p) = P(\\sum x_{i} \\leq 4,\\sum x_{i} \\sim B(5,p &gt; 0.5))\\)\n\\(RR = \\{\\sum x_{i} = 5\\}\\)에 비해 1종 오류는 늘었으나 2종 오류는 줄었다. 검정력은 커졌다. 즉, 1종 오류와 2종 오류를 동시에 줄이는 검정방법은 존재하지 않는다.\n\n【최강 검정력】 확률표본의 영역을 \\(\\mathcal{S}\\), 확률표본 영역의 부분 집합 \\(RR \\in \\mathcal{S}\\)이라 하자. 다음 조건을 만족하는 \\(RR\\)을 검정 크기 \\(\\alpha\\)에서 귀무가설 \\(H_{0}:\\theta = \\theta_{0}\\), 대립가설 \\(H_{1}:\\theta = \\theta_{1}\\)을 검정하는 최량 기각역 best critical region이다. \\(\\max_{\\theta \\in \\omega_{0}}{P_{\\theta}\\left( T\\left( X_{1},X_{2},\\ldots,X_{n} \\right) \\in RR \\right)} = \\alpha\\)\n임의의 부분집합 \\(A \\in \\mathcal{D}\\)에 대하여 다음이 성립한다.\n\\(P_{\\theta_{0}}\\left( T\\left( X_{1},X_{2},\\ldots,X_{n} \\right) \\in A \\right) = &gt; P_{\\theta_{1}}\\left( T\\left( X_{1},X_{2},\\ldots,X_{n} \\right) \\in RR \\right) \\geq P_{\\theta_{1}}\\left( T\\left( X_{1},X_{2},\\ldots,X_{n} \\right) \\in A \\right)\\)\n【예제】 모집단 \\(X \\sim B(p = \\theta)\\) 에서 귀무가설 \\(H_{0}:p = 0.1\\), 대립가설 \\(H_{0}:p = 0.2\\) 가설을 검정하기 위하여 표본크기3인 확률표본을 추출한 결과는 \\((1,0,0)\\) 이다.\n\n기각역을 \\(RR = \\{ T(\\overline{x});\\sum X_{i} \\geq 1\\}\\)이라 하자. \\(\\sum X_{i} \\sim B(n = 3,p)\\)분포를 따른다.\n유의수준: \\({P_{p = 0.1}\\left( T\\left( X_{1},X_{2},\\ldots,X_{n} \\right) \\in RR \\right)}{= 0.243 + 0.027 + 0.001 = 0.271}\\)\n2종 오류: \\({P_{p = 0.2}\\left( T\\left( X_{1},X_{2},\\ldots,X_{n} \\right) \\in RR^{C} \\right)}{= 0.5121}\\)\n검정력 : \\(1 - 0.5121 = 0.4879\\)\n\n\n\n\n\n\n\n\n\n\n\\[\\sum x\\]\n0\n1\n2\n3\n\n\n\\[f(x;p = 0.1)\\]\n0.729\n0.243\n0.027\n0.001\n\n\n\\[f(x;p = 0.2)\\]\n0.512\n0.384\n0.096\n0.008\n\n\n\\[\\frac{f(x;p = 0.1)}{f(x;p = 0.2)}\\]\n1.424\n0.633\n0.281\n0.125\n\n\n\n\n만약 대립가설의 모수 하나의 값(대립가설은 귀무가설과는 달리 모수의 영역이다)을 \\(\\theta_{1}\\)라 한다면 검정력은 \\({P(\\theta) = P}_{\\theta}\\left( T\\left( X_{1},X_{2},\\ldots,X_{n} \\right) \\in RR^{C}|\\theta = \\theta_{1} \\right)\\)이고 2종오류는 \\(\\beta(\\theta) = 1 - P(\\theta)\\) 이다. \\(H_{0}:\\theta = \\theta_{0}vs.H_{1}:\\theta = \\theta_{1} \\neq \\theta_{0}\\) 가설검정의 경우 전형적인 검정력 함수는 다음과 같다.\n\n\n\n\n\n검정력은 귀무가설 단일 설정 값 \\(\\theta_{0}\\)에서 최저였다가 멀어질수록 증가한다. 그러므로 유의수준 \\(\\alpha\\)을 설정하고 2종 오류를 최소화할 수 있는 검정 방법(기각역 찾음)을 찾는다.\n【most powerful test 최강력 검정법】\n가설에 설정된 모수 값이 하나인 경우 이를 단순가설이라 하고 설정된 모수가 영역인 경우 이를 복합가설이라 한다. 단순 귀무가설 \\(H_{0}:\\theta = \\theta_{0}\\)와 단순 대립가설 \\(H_{1}:\\theta = \\theta_{1}\\)을 검정한다고 하자. 모수 영역은 \\(\\Omega = \\{\\theta;\\theta_{0},\\theta_{1}\\}\\)이고 \\(P(\\theta) = \\alpha\\)인 기각역을 설정하고 \\(\\beta(\\theta) = 1 - p(\\theta)\\) 을 최소화 하는 기각역을 구한다. 이는 \\(P(\\theta_{1})\\)을 최대화 하는 최강 검정력 검정 most powerful test을 찾는 것과 동일하다.\n【Neyman-Pearson Lemma】 단순 귀무가설 \\(H_{0}:\\theta = \\theta_{0}\\)와 단순 대립가설 \\(H_{1}:\\theta = \\theta_{1}\\)을 검정한다고 하자. 가설 검정을 확률표본 \\(\\overline{X} = \\left( X_{1},X_{2},\\ldots,X_{n} \\right)\\)으로부터 구한 통계량을 이용한다. 이 통계량의 확률밀도함수(샘플링 분포)는 모수 \\(\\theta\\)의 함수이며 이를 우도 함수 \\(L(\\overline{x};\\theta)\\)라 한다. 유의수준 \\(\\alpha\\) 하에서 검정력을 최대화 하는 기각역 \\(RR\\)은 다음에 의해 정해진다. 이렇게 얻은 검정 방법(기각역)을 최강력 검정법이라 한다.\n\\(\\frac{L({\\overline{x};\\theta}_{0})}{L({\\overline{x};\\theta}_{1})} \\leq k,for\\overline{x} \\in RR\\).\n\\(\\frac{L({\\overline{x};\\theta}_{0})}{L({\\overline{x};\\theta}_{1})} \\geq k,for\\overline{x} \\in RR^{C}\\).\n\\[{{\\alpha = P}_{H}}_{0}(\\overline{x} \\in RR)\\]\n【불편 검정방법】 모집단 \\(X \\sim f(x;\\theta),\\theta \\in \\Omega\\) 에서 확률표본 \\(\\overline{X} = \\left( X_{1},X_{2},\\ldots,X_{n} \\right)\\), 기각역은 \\({{\\alpha = P}_{H}}_{0}(\\overline{x} \\in RR)\\)이라 하면 다음을 불편 unbiased 검정방법이다. \\(P_{\\theta}\\left( \\overline{X} \\in RR \\right) \\geq \\alpha,forall\\theta \\in \\omega_{1}\\)\n【예제】 확률표본 \\(\\overline{X} = \\left( X_{1},X_{2},\\ldots,X_{n} \\right) \\sim f(x;\\theta = \\mu)\\sim N(\\mu,1)\\)이고 귀무가설 \\(H_{0}:\\mu = 0\\), 대립가설 \\(H_{0}:\\mu = 1\\) 검정하는 최강 기각역을 구하라.\nN-P Lemma 의해 최강 기각역 : \\(\\frac{L({\\overline{x};\\theta}_{0} = 0)}{L({\\overline{x};\\theta}_{1} = 1)} = \\frac{N(0,1)}{N(1,1)} = \\exp\\left( - \\sum x_{i} + \\frac{n}{2} \\right) \\leq k\\) \\[\\Leftrightarrow - \\sum x_{i} + \\frac{n}{2} \\leq \\ln(k) \\Leftrightarrow \\sum x_{i} &gt; - \\frac{n}{2} - \\ln{(k) \\Leftrightarrow}\\frac{\\sum x_{i}}{n} &gt; c\\]\n그러므로 최강 기각역은 \\(RR = \\left\\{ \\left( x_{1},x_{2},\\ldots,x_{n} \\right);\\overline{x} \\geq c \\right\\},\\overline{x} \\sim N(0,\\frac{1}{n})\\).\n유의수준 \\(\\alpha\\) 임계값 critical value, \\(c_{1}\\) 결정 : \\({P_{H}}_{0}\\left( \\overline{x} \\geq c \\middle| \\mu = 0 \\right) = \\int_{c}^{\\infty}{\\frac{1}{\\sqrt{2\\pi}\\sqrt{1/n}}\\exp\\left( - \\frac{\\left( \\overset{¯}{x} - 0 \\right)^{2}}{2(1/n)} \\right)d\\overset{¯}{x}} = \\alpha\\)을 만족하는 \\(c\\) 값을 기각역의 시작 값인 임계값 \\(c_{1}\\)이다.\n검정력: \\({P_{H}}_{1}\\left( \\overline{x} \\geq c_{1} \\middle| \\mu = 1 \\right) = \\int_{c_{1}}^{\\infty}{\\frac{1}{\\sqrt{2\\pi}\\sqrt{1/n}}\\exp\\left( - \\frac{\\left( \\overset{¯}{x} - 1 \\right)^{2}}{2(1/n)} \\right)d\\overset{¯}{x}}\\)\n【예제】 확률표본 \\(\\overline{X} = \\left( X_{1},X_{2},\\ldots,X_{n} \\right) \\sim f(x)\\)이고 귀무가설, 대립가설이 아래와 같이 확률분포함수인 경우 최강 기각역을 구하라.\n\n귀무가설: \\(H_{0}:f_{0}(x) = \\frac{e^{- 1}}{x!}\\sim Poisson(\\lambda = 1),x = 0,1,2,\\ldots\\)\n대립가설: \\(H_{1}:f_{1}(x) = \\left( \\frac{1}{2} \\right)^{x + 1},x = 0,1,2,\\ldots\\)\n\n우도비 : \\(\\frac{f_{0}(x)}{f_{1}(x)} = \\frac{\\left( 2e^{- 1} \\right)^{n}2^{\\sum x_{i}}}{\\Pi x_{i}!}\\)\n최강 기각역 : \\(\\frac{\\left( 2e^{- 1} \\right)^{n}2^{\\sum x_{i}}}{\\Pi x_{i}!} \\leq k \\Leftrightarrow \\left( \\sum x_{i} \\right)\\ln(2) - \\ln\\left( \\Pi x_{i}! \\right) \\leq ln)k) - nln(2e^{- 1}) = c\\)\n【예제】 모집단 \\(f(x;\\theta) \\sim B(p),x = 0,1\\)에서 모수 \\(\\theta = p\\)에 대한 가설 검정을 위하여 표본 크기 \\(n\\)인 확률표본 \\(\\overline{X} = \\left( X_{1},X_{2},\\ldots,X_{n} \\right)\\) 을 추출하였다. 귀무가설 \\(H_{0}:p_{0} = 0.5\\), 대립가설 \\(H_{1}:p_{1} = 0.1\\)을 유의수준 0.05에서 검정하는 최강 검정방법을 찾아라.\nN-P Lemma에 의해 \\(\\frac{L\\left( \\overline{x}{;\\theta}_{0} \\right)}{L\\left( \\overline{x}{;\\theta}_{1} \\right)} = \\frac{{0.5}^{\\sum x_{i}}{0.5}^{n - \\sum x_{i}}}{{0.2}^{\\sum x_{i}}{0.8}^{n - \\sum x_{i}}} \\leq k \\Leftrightarrow \\sum x_{i} \\leq c\\) 이 일양 최강 기각역이다.\n\\(\\sum x_{i} \\sim B(n,p)\\)이므로 \\(0.05 = P(\\sum x_{i} \\leq c|\\sum x_{i} \\sim B(n,0.5))\\)을 만족하는 정수 \\(c\\)를 찾을 수 없다. 앞에서 살펴본 랜덤화 검정방법을 이용하여 검정하면 된다.\n\n\n(2) 일양 최강 검정\n【일양 최강 검정법】 귀무가설 \\(H_{0}:\\theta = \\theta_{0}\\), 단순 대립가설 \\(H_{1}:\\theta = \\theta_{1} &lt; \\theta_{0}\\)의 최강 검정 기각역 \\(RR\\)이 모든 복합 대립가설 \\(H_{1}:\\theta = \\theta_{1} &lt; \\theta_{0}\\)의 최강 기각역이면 \\(RR\\)에 의한 검정방법을 일양 최강 검정법 uniformly most powerful test 이라 한다.\n【예제】 모집단 \\(f(x;\\theta) = \\frac{1}{\\theta}e^{- x/\\theta},x &gt; 0\\)에서 모수 \\(\\theta\\)에 대한 가설 검정을 위하여 표본 크기 \\(n\\)인 확률표본 \\(\\overline{X} = \\left( X_{1},X_{2},\\ldots,X_{n} \\right)\\) 을 추출하였다. 귀무가설 \\(H_{0}:{\\theta = \\theta}_{0}\\) 대립가설 \\(H_{1}:{\\theta = \\theta}_{1} &lt; \\theta_{0}\\)을 유의수준 \\(\\alpha\\)에서 검정하는 일양 최강 검정방법을 찾아라.\nN-P Lemma에 의해 \\(\\frac{L\\left( \\overline{x}{;\\theta}_{0} \\right)}{L\\left( \\overline{x}{;\\theta}_{1} \\right)} = \\frac{\\frac{1}{\\theta_{0}^{n}}e^{- \\frac{\\sum x}{\\theta_{0}}}}{\\frac{1}{\\theta_{1}^{n}}e^{- \\frac{\\sum x}{\\theta_{1}}}} \\leq k \\Leftrightarrow \\sum x_{i} \\leq c\\) 이 일양 최강 기각역이다. \\(RR = \\{\\sum x_{i} \\leq c\\}\\) 이 UMPT 기각역이다. 항상 기각역에 사용되는 통계량의 분포는 알려져 있어야 가능하다. \\(\\sum x_{i} \\sim Gamma(n,\\theta)\\). 임계값은 \\(0.05 = P(\\sum x_{i} \\geq c|\\sum x_{i} \\sim Gamma(n,\\theta_{0}))\\)을 만족하는 \\(c\\)값이다.\n동일한 방법으로 귀무가설 \\(H_{0}:{\\theta = \\theta}_{0}\\) 대립가설 \\(H_{1}:{\\theta = \\theta}_{1} &gt; \\theta_{0}\\)을 유의수준 \\(\\alpha\\)에서 검정하는 일양 최강 검정력은 \\(RR = \\{\\sum x_{i} \\geq c|\\sum x_{i} \\sim Gamma(n,\\theta)\\}\\).\n그러나 대립가설 \\(H_{0}:{\\theta = \\theta}_{1} \\neq \\theta_{0}\\)이면 Uniformly Most Powerful Test는 존재하지 않는다.\n【예제】 모집단 \\(f(x;\\theta) \\sim N(0,\\theta)\\)에서 모수 \\(\\theta\\)(분산)에 대한 가설 검정을 위하여 표본 크기 \\(n\\)인 확률표본 \\(\\overline{X} = \\left( X_{1},X_{2},\\ldots,X_{n} \\right)\\) 을 추출하였다. 귀무가설 \\(H_{0}:{\\theta = \\theta}_{0}\\) 대립가설 \\(H_{1}:{\\theta = \\theta}_{1} &gt; \\theta_{0}\\)을 유의수준 \\(\\alpha\\)에서 검정하는 일양 최강 검정방법을 찾아라.\nN-P Lemma에 의해 \\(\\frac{L\\left( \\overline{x}{;\\theta}_{0} \\right)}{L\\left( \\overline{x}{;\\theta}_{1} \\right)} = \\frac{\\left( \\frac{1}{2\\pi\\theta_{0}} \\right)^{\\frac{n}{2}}exp( - \\frac{1}{2\\theta_{0}}\\sum x_{i}^{2})}{\\left( \\frac{1}{2\\pi\\theta_{1}} \\right)^{\\frac{n}{2}}exp( - \\frac{1}{2\\theta_{1}}\\sum x_{i}^{2})} \\leq k \\Leftrightarrow \\sum x_{i}^{2} \\geq c\\)이 일양 최강 기각역이다.\n그러나 \\(\\sum x_{i}^{2}\\)의 확률밀도함수는 알 수 없으므로 기각역에 사용되는 검정 통계량은 \\(\\frac{\\sum x_{i}^{2}}{\\theta_{0}} \\sim \\chi^{2}(n - 1)\\) 분포를 따르므로 \\(\\frac{\\sum x_{i}^{2}}{\\theta_{0}}\\)을 이용하는데 이 통계량을 주축 pivotal 검정 통계량이라 한다.\n【단조우도비】 만약 우도비 \\(\\frac{L\\left( \\overline{x}{;\\theta}_{0} \\right)}{L\\left( \\overline{x}{;\\theta}_{1} \\right)},for\\theta_{0} &lt; \\theta_{1}\\)가 \\(y = u(x)\\)의 단조 함수이면 우도함수 \\(L\\left( \\overline{x};\\theta \\right)\\)은 통계량 \\(y = u(x)\\)의 단조 우도비 monotone likelihood ratio(mlr)을 갖는다.\n\\(\\frac{L\\left( \\overline{x}{;\\theta}_{0} \\right)}{L\\left( \\overline{x}{;\\theta}_{1} \\right)} \\leq k \\Leftrightarrow t\\left( x_{1},x_{2},\\ldots,x_{n} \\right) \\in RR\\). \\(\\alpha = P_{\\theta_{0}}(t\\left( x_{1},x_{2},\\ldots,x_{n} \\right) \\in RR)\\)을 만족하는 \\(k\\)가 임계값이다.\n\\(\\frac{L\\left( \\overline{x}{;\\theta}_{0} \\right)}{L\\left( \\overline{x}{;\\theta}_{1} \\right)} = g(Y) \\leq k \\Leftrightarrow Y \\geq g^{- 1}(k)\\). \\(g^{- 1}(k)\\)는 \\(\\alpha = P_{\\theta_{0}}(Y \\geq g^{- 1}(k))\\)을 만족한다.\n【예제】 모집단 \\(f(x;\\theta) \\sim B(\\theta = p),0 &lt; \\theta &lt; 1\\)에서 모수 \\(\\theta\\)에 대한 가설 검정을 위하여 표본 크기 \\(n\\)인 확률표본 \\(\\overline{X} = \\left( X_{1},X_{2},\\ldots,X_{n} \\right)\\) 을 추출하였다. 귀무가설 \\(H_{0}:{\\theta = \\theta}_{0}\\) 대립가설 \\(H_{1}:{\\theta = \\theta}_{1} &gt; \\theta_{0}\\)을 유의수준 \\(\\alpha\\)에서 검정하는 일양 최강 검정방법을 찾아라.\n\\(\\frac{L\\left( \\overline{x}{;\\theta}_{0} \\right)}{L\\left( \\overline{x}{;\\theta}_{1} \\right)} = \\frac{{\\theta_{0}}^{\\sum x_{i}}{\\theta_{0}}^{n - \\sum x_{i}}}{{\\theta_{1}}^{\\sum x_{i}}{\\theta_{1}}^{n - \\sum x_{i}}} = \\left( \\frac{\\theta_{0}(1 - \\theta_{1})}{\\theta_{1}(1 - \\theta_{0})} \\right)^{\\sum x_{i}}\\left( \\frac{1 - \\theta_{0}}{1 - \\theta_{1}} \\right)^{n}\\)이므로 우도비는\\(y = \\sum x_{i}\\)의 단조 함수이다. 그러므로 UMPT 기각역은 \\(RR = \\{\\sum x_{i} \\geq c\\}\\)이다.\n【지수족 UMPT】\n모집단 확률밀도함수가 지수족이면 다음 가설의 일양 최강 검정 기각역은 \\(RR = \\{ y = \\sum K\\left( x_{i} \\right) \\geq c\\}\\)이다.\n귀무가설 : \\(H_{0}:\\theta = \\theta_{0}\\) vs. 대립가설 : \\(H_{1}:\\theta = \\theta_{1} &gt; \\theta_{0}\\)\n만약 대립가설 : \\(H_{1}:\\theta = \\theta_{1} &lt; \\theta_{0}\\) 이면 일양 최강 검정 기각역은 \\(RR = \\{ y = \\sum K\\left( x_{i} \\right) \\leq c\\}\\)이다.\n\n\n(3) 우도비 검정\n단순 귀무가설과 단순 대립가설에 대한 Most powerful test을 얻으려면 Neyman-Pearson 정리를 이용하면 된다. 물론 대립가설이 단측 가설(한 쪽 방향)에 Most powerful 검정을 얻을 수 있지만… 양측 검정이나 Nuisance parameter가 있는 경우에는 N-P 정리를 사용할 수 없다.\n모수 θ 영역을 \\(\\Omega\\)라 하자. 귀무가설에 설정된 모수 값은 단일 값으로 \\(\\Omega_{0} = \\{\\theta_{0}\\}\\)이고 대립가설의 모수 공간은 \\(\\Omega_{1} = \\Omega_{0}^{C}\\)이다. 전체 모수 공간 \\(\\Omega\\)에서 우도함수를 \\(L\\left( \\overline{x};\\Omega \\right)\\)라 정의하고 모수에 대한 MLE(최대우도추정량)을 추정치로 사용한 우도함수를 \\(L\\left( \\overline{x};\\overset{\\hat{}}{\\Omega} \\right)\\)라 하자.\n【우도비 검정】 귀무가설 \\(H_{0}:\\theta \\in \\Omega_{0}\\), 대립가설 \\(H_{1}:\\theta \\in \\Omega_{0}^{C}\\) 검정하는 우도 통계량은 \\(\\lambda\\left( \\overline{x} \\right) = \\frac{\\max_{\\Omega_{0}}{L\\left( \\overline{x};\\Omega_{0} \\right)}}{\\max_{\\Omega}{L\\left( \\overline{x};\\Omega \\right)}}\\)이다. 우도비 검정 likelihood ratio test 기각역은 \\(\\left\\{ \\lambda\\left( \\overline{x} \\right) \\leq c \\right\\}\\)이다. \\(c\\)는 \\(0 &lt; c &lt; 1\\)의 값을 갖는다.\n만약 \\(\\lambda \\rightarrow 0\\)이면 귀무가설 하의 우도함수 값이 매우 작다는 것을 의미하므로 대립가설에 비해 귀무가설을 기각하는 것이 적절하다. 만약 \\(\\lambda \\rightarrow 1\\)이면 귀무가설이 진실일 가능성이 높다. 그러므로 기각역이 \\(\\lambda \\leq c\\) 이다.\n【예제】 모집단 \\(f(x;\\theta) \\sim N(\\mu,\\sigma^{2})\\)에서 표본 크기 \\(n\\)인 확률표본 \\(\\overline{X} = \\left( X_{1},X_{2},\\ldots,X_{n} \\right)\\) 을 추출하였다. 모분산 \\(\\sigma^{2}\\)을 알지 못할 때 nuisance parameter 귀무가설 \\(H_{0}:\\mu = \\mu_{0}\\)와 대립가설 \\(H_{1}:\\mu = \\mu_{1} &gt; \\mu_{0}\\)을 유의수준 \\(\\alpha\\)에서 우도비 검정방법을 찾으시오.\n귀무가설 하 모수 공간은 \\(\\Omega_{0} = \\{\\mu,\\sigma_{0};\\mu = \\mu_{0},\\sigma^{2} &gt; 0\\}\\), 대립가설 하에서 모수 공간은 \\(\\Omega_{1} = \\{\\mu,\\sigma_{1};\\mu &gt; \\mu_{0},\\sigma_{1}^{2} &gt; 0\\}\\)이므로 모수 전체 공간은 \\(\\Omega = \\{\\mu,\\sigma;\\mu \\geq \\mu_{0},\\sigma^{2} &gt; 0\\}\\)이다.\n\\(L\\left( \\overline{x};\\Omega_{0} \\right) = \\Pi\\frac{1}{\\sqrt{2\\pi}\\sigma}exp( - \\frac{\\left( x_{i} - \\mu_{0} \\right)^{2}}{2\\sigma^{2}})\\)이고 귀무가설 모평균은 \\(\\mu_{0}\\)로 주어져 있으니 모분산에 대한 MLE 추정치를 구하면 \\(\\overset{\\hat{}}{\\sigma_{0}^{2}} = \\frac{1}{n}\\sum\\left( x_{i} - \\mu_{0} \\right)^{2}\\)이다.\n\\(L\\left( \\overline{x};\\Omega \\right)\\)에서 \\(\\mu\\)의 MLE는 \\(\\overset{\\hat{}}{\\mu} = max(\\overset{¯}{x},\\mu_{0})\\)이다. 왜냐하면 모수 공간 \\(\\Omega\\)의 \\(\\mu \\geq \\mu_{0}\\)이기 때문이다. 모분산에 대한 MLE 추정치는 \\(\\overset{\\hat{}}{\\sigma} = \\frac{1}{n}\\sum\\left( x_{i} - \\overset{\\hat{}}{\\mu} \\right)^{2}\\)이다.\n\\(\\lambda = \\frac{L(\\overset{\\hat{}}{\\Omega_{0}})}{L(\\overset{\\hat{}}{\\Omega})} = \\left( \\frac{\\overset{\\hat{}}{\\sigma_{0}^{2}}}{\\overset{\\hat{}}{\\sigma^{2}}} \\right)^{\\frac{n}{2}} = \\left\\{ \\begin{array}{r}\n\\left( \\frac{\\sum\\left( x_{i} - \\overset{¯}{x} \\right)^{2}}{\\sum\\left( x_{i} - \\mu_{0} \\right)^{2}} \\right)^{\\frac{n}{2}} \\\\\n1,if\\overline{x} \\leq \\mu_{0}\n\\end{array},if\\overline{x} &gt; \\mu_{0} \\right.\\ \\) .\n\\(\\sum\\left( x_{i} - \\mu_{0} \\right)^{2} = \\sum\\left( x_{i} - \\overset{¯}{x} \\right)^{2} + \\sum\\left( \\overset{¯}{x} - \\mu_{0} \\right)^{2}\\) 이므로 정리하면\n\\[\\lambda = \\left( \\frac{\\sum\\left( x_{i} - \\overset{¯}{x} \\right)^{2}}{\\sum\\left( x_{i} - \\mu_{0} \\right)^{2}} \\right)^{\\frac{n}{2}} \\leq k \\Leftrightarrow \\frac{\\sum\\left( x_{i} - \\overset{¯}{x} \\right)^{2}}{\\sum\\left( x_{i} - \\mu_{0} \\right)^{2}} \\leq k^{\\frac{2}{n}} \\Leftrightarrow \\frac{\\sum\\left( x_{i} - \\overset{¯}{x} \\right)^{2}}{\\sum\\left( x_{i} - \\overset{¯}{x} \\right)^{2} + \\sum\\left( \\overset{¯}{x} - \\mu_{0} \\right)^{2}} \\leq k^{\\frac{2}{n}}\\]\n\\[\\Rightarrow \\frac{\\sqrt{n}(\\overset{¯}{x} - \\mu_{0})}{S} &gt; c,whereS^{2} = \\sum\\left( x_{i} - \\overset{¯}{x} \\right)^{2}/(n - 1)\\]\n\\(\\frac{\\sqrt{n}(\\overset{¯}{x} - \\mu_{0})}{S}\\)은 t-분포를 따르므로 우도비 검정은 t-검정과 일치한다.\n【예제】 모집단 \\(f(x;\\theta) = e^{- (x - \\theta)},x \\geq 0\\)에서 표본 크기 \\(n\\)인 확률표본 \\(\\overline{X} = \\left( X_{1},X_{2},\\ldots,X_{n} \\right)\\)을 추출하였다. 귀무가설 \\(H_{0}:\\theta = \\theta_{0}\\)와 대립가설 \\(H_{1}:\\theta &gt; \\theta_{0}\\)을 유의수준 \\(\\alpha\\)에서 우도비 검정방법을 찾으시오.\n우도 함수는 \\(L\\left( \\overline{x};\\theta \\right) = \\Pi e^{- (x - \\theta)} = e^{- \\sum(x_{i} - \\theta)},\\theta \\leq x_{(1)}\\)이므로 우도 함수는 모수 \\(\\theta\\)의 증가함수이다.\n\\(\\lambda = \\frac{L(\\overset{\\hat{}}{\\Omega_{0}})}{L(\\overset{\\hat{}}{\\Omega})} = \\frac{e^{- \\sum(x_{i} - \\theta_{0})}}{e^{- \\sum(x_{i} - x_{(1)})}} = \\left\\{ \\begin{array}{r}\ne^{- n\\left( x_{(1)} - \\theta_{0} \\right)},ifx_{(1)} &gt; \\theta_{0} \\\\\n1,ifx_{(1)} \\leq \\theta_{0}\n\\end{array} \\right.\\ \\)\n기각역 \\(\\{\\lambda \\leq k\\} \\Leftrightarrow \\{\\overline{x};x_{(1)} \\geq \\theta_{0} - \\frac{ln(c)}{n}\\}\\)\n일반적으로 우도비 검정의 경우 검정통계량의 분포를 아는 것은 쉽지 않다. 이런 경우 다음 정리를 이용하여 검정하게 된다.\n【정리】 \\(\\lambda = \\frac{L(\\overset{\\hat{}}{\\Omega_{0}})}{L(\\overset{\\hat{}}{\\Omega})}\\)라 정의하자. 표본의 크기 \\(n\\)이 충분히 크다면 귀무가설 \\(H_{0}:\\theta \\in \\Omega_{0}\\), 대립가설 \\(H_{1}:\\theta \\in \\Omega_{1}\\)에 대한 우도비(LTR) 검정은 검정통계량 \\(\\lambda\\)에 대해 다음이 성립한다.\n\\(- 2\\ln(\\lambda) \\sim (app)\\chi^{2}(r_{0} - r)\\), \\(r\\)은 모수 공간 \\(\\Omega\\)하에서 설정된 모수의 개수이고 \\(r_{0}\\)은 모수 공간 \\(\\Omega_{0}\\) 하에서 설정된 모수의 개수이다.\n【예제】 작업 라인이 2개 있다. 각 작업을 일주일 단위로 생산된 제품에 대한 불량 개수를 조사하였더니 작업라인1은 평균 20, 작업라인 2는 22이다(표본의 개수는 각각 100이다). 불량 개수는 포아송 분포를 따르고 작업 라인 1의 평균을 \\(\\lambda_{1}\\), 작업 라인 2의 평균을 \\(\\lambda_{2}\\)라 하자. 귀무가설 \\(H_{0}:\\lambda_{1} = \\lambda_{2}\\)와 대립가설 \\(H_{0}:\\lambda_{1} \\neq \\lambda_{2}\\)을 검정하는 우도비 검정을 유의수준 0.01에서 실시하자.\n작업라인 1의 확률표본을 \\((X_{1},X_{2},...,X_{n})\\), 작업라인 2의 확률표본을 \\((Y_{1},Y_{2},...,y_{n})\\)라 하자.\n귀무가설 모수 공간은 \\(\\Omega_{0} = \\{\\lambda_{1} = \\lambda_{2} = \\lambda\\}\\)이고 전체 모수 공간은 \\(\\Omega = \\{\\lambda_{1} = \\lambda_{2} &gt; 0\\}\\)이다.\n\n\n\n\n\n\n\n\n\n\n귀무가설에서 설정된 귀무가설의 수는 1이고 전체 모수의 수는 2이다. 그러므로 \\(- 2ln\\lambda \\sim \\chi^{2}(df = 2 - 1 = 1)\\)이 성립한다. 자유도 1이고 유의수준이 0.01인 경우 기각치(임계치)는 \\(\\chi^{2}\\)-분포표에 의해 6.635이다. 9.53이 기각역에 속하므로 귀무가설은 기각된다.\n\n\n\n\nchapter 2. 구간 추정\n앞에서는 모수 \\(\\theta\\)의 점 추정에 대해 논의했는데 \\(\\theta\\)의 값을 하나의 추정치로 추측하는 것입니다. 여기서는 구간 추정 및 더 일반적으로 집합 추정에 대해 논의한다. 집합 추정 문제에서의 추론은 \\(\\theta\\)이 구간 \\(C(\\overline{x})\\)에 속한다.\"라는 추정됩니다. 즉, \\(P\\left( \\overset{\\hat{}}{\\theta} = \\theta \\right) = 0\\)(점 추정차와 모수가 같을 확률은 0이지만 \\(P(\\theta \\in C(\\overline{x}))\\)는 0보다 큰 확률을 가지게 되는데 이를 신뢰수준이라 한다.\n\n1. 구간추정\n【구간 추정량】\n모집단 \\(f(x;\\theta),\\overline{x} \\in \\mathcal{S}\\)에서 모수 \\(\\theta\\) 추론을 위하여 표본 크기 \\(n\\)인 확률표본 \\(\\overline{X} = \\left( X_{1},X_{2},\\ldots,X_{n} \\right)\\)을 추출하였다. 확률표본의 함수, 통계량 \\(L\\left( \\overline{x} \\right) \\leq U\\left( \\overline{x} \\right)\\)을 정의하자. \\((L\\left( \\overline{x} \\right),U\\left( \\overline{x} \\right))\\)을 모수 \\(\\theta\\) 구간 추정량 interval estimator 이라 한다.\n그리고 \\(\\min_{\\theta}{P_{\\theta}(\\theta \\in (L\\left( \\overline{x} \\right),U\\left( \\overline{x} \\right)))}\\)을 신뢰수준이라 한다. 좌우 대칭인 분포의 경우에는 양쪽 꼬리 부분에 동일한 확률 값을 배분해야 동일 신뢰수준 하에서 신뢰구간 폭이 가장 작아진다(정도는 높아짐).\n【예제】 모집단 \\(f(x;\\theta) \\sim N(\\mu,1)\\)에서 모수 \\(\\theta\\) 추론을 위하여 표본 크기 2인 확률표본 \\(\\overline{X} = \\left( X_{1},X_{2} \\right)\\) 추출하였다. 모수 \\(\\theta\\)에 대한 신뢰구간을 구하라.\n모수 \\(\\theta\\)의 MVUE가 \\(\\overset{¯}{X}\\)이므로 \\((\\overset{¯}{X} - 1,\\overset{¯}{X} + 2)\\) 신뢰구간 중 하나이다. 신뢰수준 : 91.9% \\(P\\left( \\overset{¯}{X} - 1 \\leq \\mu \\leq \\overset{¯}{X} + 2 \\middle| \\overset{¯}{X} \\sim N\\left( \\mu,\\frac{1}{2} \\right) \\right) = P\\left( - \\frac{2}{\\sqrt{1/2}} \\leq z = \\frac{\\overset{¯}{X} - \\mu}{\\sqrt{1/2}} \\leq \\frac{1}{\\sqrt{1/2}} \\right) = P\\left( - 2\\sqrt{2} \\leq z \\leq \\sqrt{2} \\right) = 0.921 - 0.002 = 0.919\\)\n【comment】\n\n유의수준 \\(\\alpha\\)의 양측 대립가설 검정방법과 신뢰수준 \\(100(1 - \\alpha)\\%\\) 구간 추정은 동일하다.\n앞에서 설명하였듯이 신뢰수준 \\(100(1 - \\alpha)\\%\\) 구간의 의미는 신뢰수준 \\(100(1 - \\alpha)\\%\\) 구간 내에 모수 \\(\\theta\\)가 있을 확률이 \\(100(1 - \\alpha)\\%\\)이 아니라 100번의 신뢰구간을 구했을 때 \\(100(1 - \\alpha)\\%\\)개 신뢰구간이 모수를 포함하고 있다는 것으로 신뢰구간의 모수 커버리지 coverage 확률이다.\n\n【예제】 모집단 \\(f(x;\\theta) \\sim U(0,\\theta)\\)에서 표본 크기 \\(n\\)인 확률표본 \\(\\overline{X} = \\left( X_{1},X_{2},\\ldots,X_{n} \\right)\\) 추출하였다. 모수 \\(\\theta\\)에 대한 신뢰구간으로 다음 2개를 생각해 보자. 모수 \\(\\theta\\)의 MLE는 \\(Y = x_{(n)}\\) 이다.\n\\(Y = x_{(n)}\\) 확률밀도함수는 \\(f(y) = ny^{n - 1}\\left( \\frac{1}{\\theta} \\right)^{n},0 &lt; y &lt; \\theta\\) 이므로 변수변환 \\(T = \\frac{Y}{\\theta}\\) 확률밀도함수는 \\(f(t) = nt^{n - 1},0 &lt; y &lt; 1\\) 이다.\n(1) 신뢰구간 \\((aY,bY)\\)의 신뢰수준 : \\(P_{\\theta}(aY \\leq \\theta \\leq bY) = P_{\\theta}\\left( \\frac{1}{a} \\leq T = \\frac{Y}{\\theta} \\leq \\frac{1}{b} \\right) = \\int_{\\frac{1}{a}}^{\\frac{1}{b}}{nt^{n - 1}dt} = \\left( \\frac{1}{a} \\right)^{n} - \\left( \\frac{1}{b} \\right)^{n}\\) 신뢰구간은 모수에 의존하지 않는다.\n(2) 신뢰구간 \\((Y + c,Y + d)\\)의 신뢰수준 : \\(P_{\\theta}(Y + c \\leq \\theta \\leq Y + d) = P_{\\theta}\\left( 1 - \\frac{d}{\\theta} \\leq T = \\frac{Y}{\\theta} \\leq 1 - \\frac{c}{\\theta} \\right) = \\left( 1 - \\frac{c}{\\theta} \\right)^{n} - \\left( 1 - \\frac{d}{\\theta} \\right)^{n}\\) 신뢰구간은 모수에 의존한다.\n【유의수준 \\(\\alpha\\) 가설검정과 \\(100(1 - \\alpha)\\%\\) 신뢰구간은 동일하다.】\n모집단 \\(f(x;\\theta) \\sim N(\\mu,\\sigma^{2})\\)에서 표본 크기 \\(n\\)인 확률표본 \\(\\overline{X} = \\left( X_{1},X_{2},\\ldots,X_{n} \\right)\\) 추출하였다. 귀무가설 \\(H_{0}:\\mu = \\mu_{0}\\), 대립가설 \\(H_{0}:\\mu \\neq \\mu_{0}\\) UMPT 기각역은 \\(RR = \\{\\overline{x};\\left| \\overline{x} - \\mu_{0} \\right| &gt; z_{1 - \\frac{\\alpha}{2}}\\frac{\\sigma}{\\sqrt{n}}\\}\\)(유의수준 \\(\\alpha\\))이다. 그러므로 채택역은 \\(\\left| \\overline{x} - \\mu_{0} \\right| \\leq z_{1 - \\alpha/2}\\frac{\\sigma}{\\sqrt{n}}\\)이다. 이것은 모수 \\(\\theta\\)에 대한 \\(100(1 - \\alpha)\\%\\) 신뢰구간과 동일하다.\n\\[P\\left( \\overline{x} - z_{1 - \\frac{\\alpha}{2}}\\frac{\\sigma}{\\sqrt{n}} \\leq \\mu \\leq \\overline{x} + z_{1 - \\frac{\\alpha}{2}}\\frac{\\sigma}{\\sqrt{n}} \\right) = 1 - \\alpha\\]\n결론적으로 \\(100(1 - \\alpha)\\%\\) 신뢰구간 포함된 모수 값을 (유의수준 \\(\\alpha\\)) 설정한 귀무가설은 채택하게 된다. 신뢰구간 밖의 모수를 설정한 귀무가설은 기각된다.\n【신뢰구간】\n귀무가설 \\(H_{0}:\\theta = \\theta_{0}\\)에 대한 유의수준 \\(\\alpha\\)의 검정의 채택역을 \\(A(\\theta_{0})\\) 이라 하자. 모수 공간 \\(C\\left( \\overline{x} \\right) = \\{\\theta_{0}:\\overline{x} \\in A(\\theta_{0})\\}\\)은 \\((1 - \\alpha)\\) 신뢰구간이다.\n【예제】 모집단 \\(f(x;\\theta) \\sim exponetial(\\theta = \\lambda)\\)에서 표본 크기 \\(n\\)인 확률표본 \\(\\overline{X} = \\left( X_{1},X_{2},\\ldots,X_{n} \\right)\\) 추출하였다. 모수 \\(\\lambda\\)(평균)에 대한 \\(100(1 - \\alpha)\\%\\) 신뢰구간을 구하라.\n귀무가설 : \\(H_{0}:\\lambda = \\lambda_{0}\\) vs. 대립가설 : \\(H_{0}:\\lambda \\neq \\lambda_{0}\\)\n우도함수 : \\(L\\left( \\overline{x};\\Omega \\right) = \\Pi\\frac{1}{\\lambda}\\exp\\left( - \\frac{x_{i}}{\\lambda} \\right) = \\frac{1}{\\lambda^{n}}exp( - \\frac{\\sum x_{i}}{\\lambda})\\)\n우도비 : \\(\\lambda = \\frac{L(\\overset{\\hat{}}{\\Omega_{0}})}{L(\\overset{\\hat{}}{\\Omega})} = \\frac{\\frac{1}{\\lambda_{0}^{n}}exp( - \\frac{\\sum x_{i}}{\\lambda_{0}})}{\\frac{1}{(\\sum x_{i}/n)^{n}}exp( - n)} = \\left( \\frac{\\sum x_{i}}{n\\lambda_{0}} \\right)^{n}e^{n}e^{- \\sum x_{i}/\\lambda_{0}}\\) (모수 \\(\\lambda\\)의 MLE는 \\(\\overset{¯}{x}\\) 이기 때문이다)\n기각역 : \\({\\lambda = \\left( \\frac{\\sum x_{i}}{n\\lambda_{0}} \\right)}^{n}e^{n}e^{- \\sum x_{i}/\\lambda_{0}} \\leq k \\Leftrightarrow {\\lambda = \\left( \\frac{\\sum x_{i}}{\\lambda_{0}} \\right)}^{n}e^{- \\frac{\\sum x_{i}}{\\lambda_{0}}} \\leq k^{*}\\), \\(RR(\\overline{x}) = \\{\\overline{x};\\left( \\frac{\\sum x_{i}}{\\lambda_{0}} \\right)^{n}e^{- \\frac{\\sum x_{i}}{\\lambda_{0}}} \\leq c\\}\\)\n그러므로 채택역은 \\(A(\\theta) = \\{\\overline{x};\\left( \\frac{\\sum x_{i}}{\\lambda} \\right)^{n}e^{- \\frac{\\sum x_{i}}{\\lambda}} \\geq c\\}\\) 이고 충분통계량 \\(\\sum x_{i}\\)의 함수이므로 신뢰구간은 다음과 같다. \\(C\\left( \\sum x_{i} \\right) = \\{\\lambda;L\\left( \\sum x_{i} \\right) &lt; \\lambda &lt; U\\left( \\sum x_{i} \\right)\\}\\)\n\\(\\left( \\frac{\\sum x_{i}}{L\\left( \\sum x_{i} \\right)} \\right)^{n}e^{- \\frac{\\sum x_{i}}{L\\left( \\sum x_{i} \\right)}}\\), \\(\\left( \\frac{\\sum x_{i}}{U\\left( \\sum x_{i} \\right)} \\right)^{n}e^{- \\frac{\\sum x_{i}}{U\\left( \\sum x_{i} \\right)}}\\) =&gt; 만약 \\(a = \\frac{\\sum x_{i}}{L\\left( \\sum x_{i} \\right)},b = \\frac{\\sum x_{i}}{U\\left( \\sum x_{i} \\right)}\\)라 놓으면 \\(a^{n}e^{- a}\\), \\(be^{- b}\\)\n\\(\\sum x_{i} \\sim Gamma(n,\\lambda)\\) 이므로 \\(\\frac{\\sum x_{i}}{\\lambda} \\sim Gamma(n,1)\\) 다음을 만족하는 \\((a,b)\\)를 찾으면 \\(100(1 - \\alpha)\\%\\) 신뢰구간이다.\n\\[P_{\\lambda}\\left( \\frac{\\sum x_{i}}{a} \\leq \\lambda \\leq \\frac{\\sum x_{i}}{b} \\right) = P_{\\lambda}\\left( b \\leq \\frac{\\sum x_{i}}{\\lambda} \\leq a \\right) = 1 - \\alpha\\]\n【주축통계량】\n통계량 \\(Q\\left( \\overline{x},\\theta \\right)\\)의 분포가 모수와 독립이면 이를 주축 pivotal 통계량이라 한다. 즉, \\(X \\sim F(x)\\)이면 \\(Q\\left( \\overline{x},\\theta \\right)\\) 확률밀도함수는 모수 \\(\\theta\\)에 상관없이 동일하다. 일반적으로 주축 통계량은 충분 통계량의 함수이다.\n\n\n\n\n\n\n\n\n확률분포 형식\n타입\n주축 통계량\n\n\n\\[f(x - \\mu)\\]\n위치\n\\[\\overline{x} - \\mu\\]\n\n\n\\[\\frac{1}{\\sigma}f(\\frac{x}{\\sigma})\\]\n크기\n\\[\\frac{\\overline{x}}{S}\\]\n\n\n\\[\\frac{1}{\\sigma}f(\\frac{x - \\mu}{\\sigma})\\]\n위치-크기\n\\[\\frac{\\overline{x} - \\mu}{S}\\]\n\n\n\n【예제】 모집단 \\(f(x;\\theta) \\sim exponetial(\\theta = \\lambda)\\)에서 표본 크기 \\(n\\)인 확률표본 \\(\\overline{X} = \\left( X_{1},X_{2},\\ldots,X_{n} \\right)\\) 추출하였다. 모수 \\(\\lambda\\)(평균)에 대한 \\(100(1 - \\alpha)\\%\\) 신뢰구간을 구하라.\n지수분포는 지수족이므로 \\(T = \\sum x_{i}\\)는 충분 통계량이고 \\(T = \\sum x_{i} \\sim Gamma(n,\\lambda))\\)이다.\\(\\sum x_{i}/\\lambda \\sim Gamma(1,\\lambda))\\)이 주축 통계량이다. 그러므로 \\(100(1 - \\alpha)\\%\\) 신뢰구간은 다음을 만족하는 \\((a,b)\\)이다. \\(P_{\\lambda}\\left( a \\leq \\frac{\\sum x_{i}}{\\lambda} \\leq b \\right) = 1 - \\alpha\\).\n【예제】 \\(f(x;\\theta) \\sim N\\left( \\theta = \\mu,\\sigma^{2} \\right),where\\sigma^{2}isknown\\)에서 표본 크기 \\(n\\)인 확률표본 \\(\\left( X_{1},X_{2},\\ldots,X_{n} \\right)\\) 추출하였다. 모수 \\(\\mu\\)(평균)에 대한 \\(100(1 - \\alpha)\\%\\) 신뢰구간을 구하라.\n정규분포는 위치-크기 모수 타입이므로 주축 통계량은 \\(\\frac{\\overline{x} - \\mu}{S}\\) 형태이다. 만약 분산 \\(\\sigma^{2}\\)을 알고 있다면 \\(P\\left( a \\leq \\frac{\\overline{x} - \\mu}{\\frac{\\sigma}{\\sqrt{n}}} \\sim N(0,1) \\leq b \\right) = 1 - \\alpha\\) =&gt; \\((\\overline{x} - z_{1 - \\frac{\\alpha}{2}}\\frac{\\sigma}{\\sqrt{n}},\\overline{x} + z_{1 - \\frac{\\alpha}{2}}\\frac{\\sigma}{\\sqrt{n}})\\)\n만약 분산 \\(\\sigma^{2}\\)을 모른다면 \\(P\\left( a \\leq \\frac{\\overline{x} - \\mu}{\\frac{s}{\\sqrt{n}}} \\sim t(n - 1) \\leq b \\right) = 1 - \\alpha\\)\n=&gt; \\((\\overline{x} - t_{1 - \\frac{\\alpha}{2},n - 1}\\frac{s}{\\sqrt{n}},\\overline{x} + t_{1 - \\frac{\\alpha}{2},n - 1}\\frac{s}{\\sqrt{n}})\\)\n\n\n2. 정규분포 가정 모형\n모집단 \\(f(x;\\theta)\\)에서 확률표본 \\(\\overline{X} = \\left( X_{1},X_{2},\\ldots,X_{n} \\right)\\)의 제곱 변환 \\(X_{i}^{2}\\)의 확률분포함수에 대하여 살펴보자. 모집단 \\(f(x;\\theta) \\sim N(0,1)\\)이면 \\(X^{2}\\)의 분포는 \\(\\chi^{2}(1)\\)을 따른다.\n【cochran theorem】\n모집단 \\(f(x;\\theta) \\sim N(\\mu,\\sigma^{2})\\)에서의 확률표본 \\(\\overline{X} = \\left( X_{1},X_{2},\\ldots,X_{n} \\right)\\) 의 제곱 형태 (이차형식, quadratic form) \\(Q_{i}^{2},fori = 1,2,\\ldots,k\\) 확률변수와 \\({Q = \\sum Q}_{i}^{2}\\) 확률변수에 대하여 다음이 성립한다.\n\n\\(Q_{1},Q_{2},\\ldots,Q_{k}\\)는 서로 독립이다.\n\\(\\frac{Q_{k}}{\\sigma^{2}} \\sim \\chi^{2}(r_{k})\\)을 갖는다.\n\\(\\frac{Q}{\\sigma^{2}} \\sim \\chi^{2}(\\sum r_{i})\\)을 갖는다.\n\n【예제】 모집단 \\(f(x;\\theta) \\sim N\\left( \\mu,\\sigma^{2} \\right)\\)에서 확률표본 \\(\\left( X_{1},X_{2},\\ldots,X_{n} \\right)\\)의 분산 \\(\\sigma^{2}\\)의 MVUE 표본 분산 \\(S^{2} = \\frac{\\sum\\left( x_{i} - \\overset{¯}{x} \\right)^{2}}{n - 1}\\) 은 이차형식이다. \\((n - 1)S^{2} = \\sum\\left( x_{i} - \\overset{¯}{x} \\right)^{2} \\Leftrightarrow \\frac{(n - 1)S^{2}}{\\sigma^{2}} = \\frac{\\sum\\left( x_{i} - \\overset{¯}{x} \\right)^{2}}{\\sigma^{2}} = \\frac{\\sum\\left( x_{i} - \\mu \\right)^{2}}{\\sigma^{2}} - \\frac{\\sum\\left( \\overset{¯}{x} - \\mu \\right)^{2}}{\\sigma^{2}}\\)\n\\(X_{i} \\sim N\\left( \\mu,\\sigma^{2} \\right)\\) 이므로 \\(Q_{i} = \\frac{\\left( X_{i} - \\mu \\right)^{2}}{\\sigma^{2}} \\sim \\chi^{2}(1) = &gt; \\sum Q_{i} = \\frac{\\sum\\left( X_{i} - \\mu \\right)^{2}}{\\sigma^{2}} \\sim \\chi^{2}(n)\\) 이다.\n\\(\\overset{¯}{X} \\sim N\\left( \\mu,\\frac{\\sigma^{2}}{n} \\right)\\) 이므로 \\(Q_{1} = \\frac{{n\\left( \\overset{¯}{X} - \\mu \\right)}^{2}}{\\sigma^{2}} \\sim \\chi^{2}(1\\)) 이다. 그러므로 코크란 정리에 의해 \\(\\frac{(n - 1)S^{2}}{\\sigma^{2}} \\sim \\chi^{2}(n - 1)\\)이다.\n\n(1) 분산분석\n\n모델: \\(X_{ij} = \\mu_{i} + e_{ij};i = 1,2,\\ldots,a,j = 1,2,\\ldots,n\\)\n가정: \\(e_{ij} \\sim (iid)N(0,\\sigma^{2})\\)\n모수: \\(\\Omega = \\{\\left( \\mu_{1},\\mu_{2},\\ldots,\\mu_{k},\\sigma^{2} \\right); - \\infty &lt; \\mu_{j} &lt; \\infty,0 &lt; \\sigma^{2} &lt; \\infty\\)\n귀무가설 \\(H_{0}:\\mu_{1} = \\mu_{2} = \\ldots = \\mu_{k} = \\mu\\)\n귀무가설 모수: \\(\\Omega_{0} = \\{\\left( \\mu_{1},\\mu_{2},\\ldots,\\mu_{k},\\sigma^{2} \\right); - \\infty &lt; \\mu_{1} = \\mu_{2} = \\ldots = \\mu_{k} = \\mu &lt; \\infty,0 &lt; \\sigma^{2} &lt; \\infty\\)\n\n【표본분산 분할】\n분산분석 모형에서 표본분산 \\(S^{2} = \\frac{1}{(an - 1)}\\sum_{i}^{}{\\sum_{j}^{}\\left( x_{ij} - {\\overset{¯}{x}}_{..} \\right)^{2}}\\)을 이차형식으로 분할하라. 단, \\({\\overset{¯}{x}}_{..} = \\frac{\\sum_{i}^{}{\\sum_{j}^{}x_{ij}}}{an},{\\overset{¯}{x}}_{i.} = \\frac{\\sum_{j}^{}x_{ij}}{n}\\), \\({\\overset{¯}{x}}_{.j} = \\frac{\\sum_{i}^{}x_{ij}}{a}\\)이다.\n\\({(an - 1)S}^{2} = \\sum_{i}^{}{\\sum_{j}^{}\\left( x_{ij} - {\\overset{¯}{x}}_{..} \\right)^{2}} = \\sum_{i}^{}{\\sum_{j}^{}\\left( x_{ij} - {\\overset{¯}{x}}_{i.} \\right)^{2}} + \\sum_{i}^{}{\\sum_{j}^{}\\left( {\\overset{¯}{x}}_{i.} - {\\overset{¯}{x}}_{..} \\right)^{2}}\\)\n\\(= \\sum_{i}^{}{\\sum_{j}^{}\\left( x_{ij} - {\\overset{¯}{x}}_{i.} \\right)^{2}} + n\\sum_{i}^{}{\\left( {\\overset{¯}{x}}_{i.} - {\\overset{¯}{x}}_{..} \\right)^{2} = Q_{1} + Q_{2}}\\)\n\\({(an - 1)S}^{2} = \\sum_{i}^{}{\\sum_{j}^{}\\left( x_{ij} - {\\overset{¯}{x}}_{..} \\right)^{2}} = \\sum_{i}^{}{\\sum_{j}^{}\\left( x_{ij} - {\\overset{¯}{x}}_{.j} \\right)^{2}} + \\sum_{i}^{}{\\sum_{j}^{}\\left( {\\overset{¯}{x}}_{.j} - {\\overset{¯}{x}}_{..} \\right)^{2}}\\)\n\\(= \\sum_{i}^{}{\\sum_{j}^{}\\left( x_{ij} - {\\overset{¯}{x}}_{.j} \\right)^{2}} + a\\sum_{j}^{}{\\left( {\\overset{¯}{x}}_{.j} - {\\overset{¯}{x}}_{..} \\right)^{2} = Q_{3} + Q_{4}}\\)\n\\[{(an - 1)S}^{2} = a\\sum_{j}^{}{\\left( {\\overset{¯}{x}}_{.j} - {\\overset{¯}{x}}_{..} \\right)^{2} +}n\\sum_{i}^{}{\\left( {\\overset{¯}{x}}_{i.} - {\\overset{¯}{x}}_{..} \\right)^{2} +}\\sum_{i}^{}{\\sum_{j}^{}\\left( x_{ij} - {\\overset{¯}{x}}_{i.} - {\\overset{¯}{x}}_{.j} + {\\overset{¯}{x}}_{..} \\right)^{2}} = Q_{2} + Q_{3} + Q_{4}\\]\n【우도비 검정】\n\n분자 우도함수 : \\(L\\left( \\Omega_{0} \\right) = \\left( \\frac{1}{2\\pi\\sigma} \\right)^{an/2}exp( - \\frac{1}{2\\sigma^{2}}\\sum_{i}^{}{\\sum_{j}^{}\\left( x_{ij} - \\mu \\right)^{2}})\\)\n\\(\\mu\\)  MLE : \\(\\frac{\\partial L\\left( \\Omega_{0} \\right)}{\\partial\\mu} = 0 \\Rightarrow \\overset{\\hat{}}{\\mu} = \\frac{1}{an}\\sum_{i}^{}{\\sum_{j}^{}x_{ij} = {\\overset{¯}{x}}_{..}}\\)\n\\(\\sigma^{2}\\)  MLE : \\(\\frac{\\partial L\\left( \\Omega_{0} \\right)}{\\partial\\sigma^{2}} = 0 \\Rightarrow \\overset{\\hat{}}{\\sigma^{2}} = \\frac{1}{an}\\sum_{i}^{}{\\sum_{j}^{}{{(x}_{ij} - {\\overset{¯}{x}}_{..})^{2}} = v}\\)\n분모 우도함수 : \\(L(\\Omega) = \\left( \\frac{1}{2\\pi\\sigma} \\right)^{an/2}exp( - \\frac{1}{2\\sigma^{2}}\\sum_{i}^{}{\\sum_{j}^{}\\left( x_{ij} - \\mu_{i} \\right)^{2}})\\)\n\\(\\mu_{i}\\)  MLE : \\(\\frac{\\partial L(\\Omega)}{\\partial\\mu_{i}} = 0 \\Rightarrow \\overset{\\hat{}}{\\mu_{i}} = \\frac{1}{n}\\sum_{j}^{}{x_{ij} = {\\overset{¯}{x}}_{i.}}\\)\n\\(\\sigma^{2}\\)  MLE : \\(\\frac{\\partial L(\\Omega)}{\\partial\\sigma^{2}} = 0 \\Rightarrow \\overset{\\hat{}}{\\sigma^{2}} = \\frac{1}{an}\\sum_{i}^{}{\\sum_{j}^{}{{(x}_{ij} - {\\overset{¯}{x}}_{..})^{2}} = w}\\)\n우도비 : \\(\\lambda = \\frac{L\\left( {\\overset{\\hat{}}{\\Omega}}_{0} \\right)}{L\\left( \\overset{\\hat{}}{\\Omega} \\right)} = \\left( \\frac{\\sum_{i}^{}{\\sum_{j}^{}\\left( x_{ij} - {\\overset{¯}{x}}_{i.} \\right)^{2}}}{\\sum_{i}^{}{\\sum_{j}^{}\\left( x_{ij} - {\\overset{¯}{x}}_{..} \\right)^{2}}} \\right)^{an/2}\\).\n가정: \\(e_{ij} \\sim (iid)N(0,\\sigma^{2})\\)에 의해 \\(X_{ij} \\sim (iid)N(\\mu_{i},\\sigma^{2})\\) 이다.\n\n그러므로 \\(V = \\frac{1}{an}\\sum_{i}^{}{\\sum_{j}^{}{{(x}_{ij} - {\\overset{¯}{x}}_{..})^{2}} = \\frac{Q}{an}}\\) 이고 \\(W = \\frac{1}{an}\\sum_{i}^{}{\\sum_{j}^{}{{(x}_{ij} - {\\overset{¯}{x}}_{i.})^{2}} = \\frac{Q_{3}}{an}}\\) 이다.\n우도비: \\(\\lambda^{2/an} = \\frac{Q_{3}}{Q_{3} + Q_{4}} = \\frac{1}{1 + Q_{4}/Q_{3}}\\)\n귀무가설 검정 유의수준: \\(\\alpha = P_{H_{0}}\\left( \\frac{1}{1 + \\frac{Q_{4}}{Q_{3}}} \\leq \\lambda^{\\frac{2}{an}} \\right) = P_{H_{0}}\\left( \\frac{Q_{4}/(a - 1)}{Q_{3}/a(n - 1)} \\leq c \\right)\\)\n\\(\\frac{Q_{4}/(\\sigma^{2}(a - 1))}{Q_{3}/(\\sigma^{2}a(n - 1))} \\sim F(a - 1,a(n - 1))\\)이다.\n【ANOVA table】\n\n\n\n\n\n\n\n(2) 회귀분석\n\n모델 : \\(Y_{i} = a + bx_{i} + e_{i};i = 1,2,\\ldots,n\\)\n가정 : \\(e_{i} \\sim (iid)N(0,\\sigma^{2})\\)\n귀무가설 \\(H_{0}:\\alpha = \\beta = 0\\)\n대립가설 \\(H_{1}:\\alpha \\ne \\beta = 0\\)\n\n【OLS 최소자승추정법】\n\n\n\n\n\n관측점들을 가장 대표하는 직선 (best fit)을 어떻게 구할 것인가? 데이터 \\((x_{i},y_{i})\\)를 활용하여 점들에 가장 적합한 직선의 회귀계수 \\((a,b)\\)를 추정하고 이를 이용하여 목표변수 추정값을 \\(\\overset{\\hat{}}{y_{i}} = \\overset{\\hat{}}{a} + \\overset{\\hat{}}{b}x_{i}\\) fitted value 구한다. 오차항에 대한 추정값으로  \\({\\overset{\\hat{}}{e}}_{i} = y_{i} - \\overset{\\hat{}}{y_{i}}\\)을 사용하여 직선의 유의성을 검증한다.\n\\(\\min_{(a,b)}{Q = \\sum_{i}^{}\\left( y_{i} - a - bx_{i} \\right)}^{2}\\)$$\\(\\frac{\\partial Q}{\\partial a} = - 2\\sum\\left( y_{i} - a - bx_{i} \\right) = 0,\\frac{\\partial Q}{\\partial b} = - 2\\sum x_{i}\\left( y_{i} - a - bx_{i} \\right) = 0\\)\n\\(\\overset{\\hat{}}{b} = \\frac{\\sum\\left( x_{i} - \\overset{¯}{x} \\right)\\left( y_{i} - \\overset{¯}{y} \\right)}{\\sum\\left( x_{i} - \\overset{¯}{x} \\right)^{2}} = \\frac{S_{XY}}{S_{XX}}\\), \\(\\overset{\\hat{}}{a} = \\overset{¯}{y} - \\overset{\\hat{}}{b}\\overset{¯}{x}\\)\n【MLE】\n\\(e_{i} \\sim (iid)N(0,\\sigma^{2})\\) 이므로 \\(y_{i} \\sim (iid)N(a + bx_{i},\\sigma^{2})\\) 이다.\n우도함수: \\(L\\left( \\overline{x};a,b \\right) = \\left( \\frac{1}{2\\pi\\sigma} \\right)^{\\frac{n}{2}}exp( - \\frac{{\\sum_{i}^{}\\left( y_{i} - a - bx_{i} \\right)}^{2}}{2\\sigma^{2}}) \\propto {\\sum_{i}^{}\\left( y_{i} - a - bx_{i} \\right)}^{2}\\)우도함수를 최대화 하는 것이나 \\({\\sum_{i}^{}\\left( y_{i} - a - bx_{i} \\right)}^{2}\\)을 최소화 하는 OLS와 동일하다.\n【OLS is blue】 markov theorem\n회귀계수에 대한 OLS 추정치는 BLUE(Best Linear Unbiased Estimator)이다. 즉 모든 선형, 불편 추정량 중 최소 분산(minimum variance)를 갖는다. 분포함수가 지수족이므로 MLE는 CSS이고 불편추정량이므로 Rao-Blackwell 정리에 의해 MVUE이다.\n【오차 분산 MVUE 추정량】\n\\(\\overset{\\hat{}}{\\sigma^{2}} = \\frac{1}{n - 2}\\sum\\left( y_{i} - \\overset{\\hat{}}{a} - \\overset{\\hat{}}{b}x_{i} \\right)^{2} = MSE\\).\n【\\(\\overset{\\hat{}}{b}\\) 샘플링 분포】\n\\(\\overset{\\hat{}}{b} = \\frac{\\sum\\left( x_{i} - \\overset{¯}{x} \\right)\\left( y_{i} - \\overset{¯}{y} \\right)}{\\sum\\left( x_{i} - \\overset{¯}{x} \\right)^{2}}\\) 이므로 \\(k_{i} = \\frac{\\left( y_{i} - \\overset{¯}{y} \\right)}{\\sum\\left( x_{i} - \\overset{¯}{x} \\right)^{2}}\\) 이라 하면 \\(\\overset{\\hat{}}{b} = \\sum k_{i}y_{i}\\) 이다. 그리고 \\(\\sum k_{i} = 0,\\sum k_{i}x_{i} = 1,\\sum k_{i}^{2} = \\frac{1}{\\sum\\left( x_{i} - \\overset{¯}{x} \\right)^{2}}\\)이다.\n\\(y_{i} \\sim (iid)N(a + bx_{i},\\sigma^{2})\\) 이므로 선형 결합 \\(\\overset{\\hat{}}{b} = \\sum k_{i}y_{i}\\)도 정규분포를 따른다.\n\\[E\\left( \\overset{\\hat{}}{b} \\right) = E\\left( \\sum k_{i}y_{i} \\right) = \\sum k_{i}{E(y}_{i}) = \\sum k_{i}(a + bx_{i}) = b\\]\n\\[V\\left( \\overset{\\hat{}}{b} \\right) = V\\left( \\sum k_{i}y_{i} \\right) = \\sum k_{i}^{2}{V(y}_{i}) = \\sum k_{i}^{2}(\\sigma^{2}) = \\frac{\\sigma^{2}}{\\sum\\left( x_{i} - \\overset{¯}{x} \\right)^{2}}\\]\n\\(\\overset{\\hat{}}{b} \\sim N(b,\\frac{\\sigma^{2}}{\\sum\\left( x_{i} - \\overset{¯}{x} \\right)^{2}})\\). 만약 \\(\\sigma^{2}\\)을 MVUE 추정량 \\(\\frac{(n - 2)\\overset{\\hat{}}{\\sigma^{2}}}{\\sigma^{2}} = \\frac{\\sum\\left( y_{i} - \\overset{\\hat{}}{a} - \\overset{\\hat{}}{b}x_{i} \\right)^{2}}{\\sigma^{2}} \\sim \\chi^{2}(n - 2)\\) 성립한다.\n그러므로 \\(\\frac{\\overset{\\hat{}}{b} - b}{s\\left( \\overset{\\hat{}}{b} \\right)} \\sim t(n - 2)\\), \\(s^{2}\\left( \\overset{\\hat{}}{b} \\right) = \\frac{MSE}{\\sum\\left( x_{i} - \\overset{¯}{x} \\right)^{2}}\\)이다.\n【기울기 \\(b\\) 에 대한 \\(100(1 - \\alpha)\\%\\) 신뢰구간】\n\\((\\overset{\\hat{}}{b} - t_{1 - \\frac{\\alpha}{2},(n - 1)}s\\left( \\overset{\\hat{}}{b} \\right),\\overset{\\hat{}}{b} + t_{1 - \\frac{\\alpha}{2},(n - 1)}s\\left( \\overset{\\hat{}}{b} \\right)\\), \\(s^{2}\\left( \\overset{\\hat{}}{b} \\right) = \\frac{MSE}{\\sum\\left( x_{i} - \\overset{¯}{x} \\right)^{2}}\\).\n【절편 \\(a\\) 에 대한 \\(100(1 - \\alpha)\\%\\) 신뢰구간】\n\\((\\overset{\\hat{}}{a} - t_{1 - \\frac{\\alpha}{2},(n - 1)}s\\left( \\overset{\\hat{}}{a} \\right),\\overset{\\hat{}}{a} + t_{1 - \\frac{\\alpha}{2},(n - 1)}s\\left( \\overset{\\hat{}}{a} \\right)\\), \\(s^{2}\\left( \\overset{\\hat{}}{a} \\right) = MSE(\\frac{1}{n} + \\frac{{\\overset{¯}{x}}^{2}}{\\sum\\left( x_{i} - \\overset{¯}{x} \\right)^{2}}\\))"
  },
  {
    "objectID": "notes/math_stat/random_variable.html",
    "href": "notes/math_stat/random_variable.html",
    "title": "수리 통계 2. 확률변수와 확률분포함수",
    "section": "",
    "text": "chapter 1. 확률변수와 확률분포함수\n많은 확률 실험에서 원래의 복잡한 표본공간을 직접 다루기보다는, 관심 있는 정보를 요약한 확률변수를 정의하여 문제를 단순화하는 것이 보다 효율적이다.\n예를 들어, 하나의 주사위를 10번 던지는 실험을 생각해보자. 이 경우 표본공간은 \\(S = \\{(x_1, x_2, \\ldots, x_{10}) \\mid x_i \\in \\{1,2,3,4,5,6\\} \\}\\) 로 구성되며, 가능한 결과의 수는 총 6^{10}개에 이른다. 각각의 원소는 1부터 6까지의 숫자로 이루어진 길이 10의 순서쌍이므로, 이 방대한 표본공간을 그대로 분석하는 것은 매우 비현실적이다.\n그러나 만약 이 실험에서 우리가 관심 갖는 양이 단지 “숫자 6이 나온 횟수”라면, 이 정보를 요약하는 확률변수 X를 다음과 같이 정의할 수 있다. X = 이렇게 정의하면, 확률변수 X는 0부터 10까지의 정수 값을 가질 수 있으며, 그 표본공간은 \\(\\{0, 1, 2, \\ldots, 10\\}\\) 로 단순화된다.\n이처럼 확률변수는 원래의 복잡한 표본공간으로부터 관심 있는 특성만을 추출하여 새로운 공간(보통 실수 집합 또는 이산 집합)으로의 매핑을 형성한다. 이를 통해 문제를 보다 단순하게 구성할 수 있으며, 복잡한 현상에 대한 확률적 분석과 계산이 훨씬 용이해진다.\n결국 확률변수의 정의는 복잡한 현실 세계의 정보를 수학적으로 구조화하고 요약하는 핵심적인 도구라 할 수 있다.\n\n1. 확률변수\n\n(1) 확률변수 정의\n【확률변수 정의】 확률변수는 표본공간 S에서 실수로의 함수이다.\n\n\n\n\n\n확률변수란 확률실험의 결과, 즉 표본공간의 원소에 실수 값을 대응시키는 함수이다. 다시 말해, 확률실험에서 관측된 각 결과에 실수 값을 부여하는 규칙을 정의한 것이며, 이를 확률변수(random variable)라고 한다. 보통 확률변수는 X, Y, Z 등의 알파벳 기호로 나타낸다.\n예를 들어, 표본공간이 \\(S = \\{ s_1, s_2, \\ldots, s_n \\}\\) 이고, 각 원소에 대해 확률함수 P가 정의되어 있다고 하자. 이때 확률변수 X는 S의 각 원소에 대해 어떤 실수 값을 대응시키는 함수로 정의된다. 그 결과로 X가 취할 수 있는 값들의 집합, 즉 확률변수의 범위(range)를 \\(\\mathcal{X} = \\{ x_1, x_2, \\ldots, x_m \\}\\) 이라 하자.\n이러한 설정 하에서 확률변수 X에 대해 새로운 확률 함수 \\(P_X(x_i) = P(X = x_i)\\) 를 정의할 수 있다. 이는 표본공간의 원소 중에서 확률변수 X가 x_i 값을 갖는 경우들의 확률을 모두 합산한 값으로 해석된다.\n즉, 실험 결과가 \\(s_j \\in S\\) 일 때, \\(X(s_j) = x_i\\) 이면 우리는 확률변수 X가 값 \\(x_i\\) 를 관찰한 것으로 간주한다. 이러한 방식으로 복잡한 표본공간을 실수 값 중심의 보다 간결한 구조로 변환하고 분석할 수 있다.\n\\(P_{X}(X = x_{i}) = P(\\{ s_{j} \\in S:X(s_{j}) = x_{i}\\})\\).\n【예제】 공정한 동전을 세 번 던질 때 얻는 앞면의 수를 나타내는 확률 변수 \\(X\\)의 값(범위)을 구해 봅시다.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n확률실험\nHHH\nHHT\nHTH\nHTT\nTHH\nTHT\nTTH\nTTT\n\n\n확률변수 X\n3\n2\n2\n1\n2\n1\n1\n0\n\n\n\n확률변수 \\(X\\)의 범위는 \\(\\mathcal{X} = \\{ 0,1,2,3\\}\\)이다. 표본공간 \\(S\\)의 모든 8개의 점이 확률 \\(\\frac{1}{8}\\)을 가진다고 가정하면,\n\n\n\n\n\n\n\n\n\n\n확률변수\n0\n1\n2\n3\n\n\n\\[P_{X}(X = x)\\]\n1/8\n3/8\n3/8\n1/8\n\n\n\n\n\n(2) 확률변수 종류\n확률변수는 확률실험의 결과를 수치로 표현하는 변수로, 불확실한 현상을 수리적으로 분석하기 위한 핵심 개념이다. 이러한 확률변수는 바라보는 관점에 따라 다르게 분류될 수 있다.\n먼저, 수리 통계적 관점에서는 확률변수를 크게 두 가지 유형으로 나눈다. 하나는 이산형 확률변수로, 주사위의 눈처럼 셀 수 있는 유한하거나 가산무한한 값을 가지며, 개별적인 결과들이 명확히 구분된다. 다른 하나는 연속형 확률변수로, 키나 체온처럼 어떤 구간 내에서 무한히 많은 실수 값을 가질 수 있는 변수를 말한다.\n반면, 데이터 분석의 관점에서는 확률변수를 정량적 변수와 정성적 변수로 구분한다. 정량적 변수는 숫자로 표현되며 크기 비교나 사칙 연산이 가능한 변수로, 연속형 변수뿐만 아니라 숫자 값을 갖는 이산형 변수도 포함된다. 정성적 변수는 문자나 범주로 나타나는 변수로, 성별, 지역, 혈액형처럼 수치적 연산이 아닌 분류와 구분에 초점을 둔다.\n이러한 분류 기준은 서로 다르지만, 통계 분석에서는 유기적으로 연결되어 있다. 예를 들어 수리 통계적으로는 숫자 값을 가지는 변수는 모두 연속형 변수의 분석 범주에 포함되며, 범주형 변수는 일반적으로 이산형 확률변수로 간주된다. 따라서 분석 목적에 따라 확률변수를 어떻게 정의하고 해석할 것인지를 명확히 하는 것이 중요하다.\n1. 이산형 확률변수 (Discrete Random Variable)\n이산형 확률변수란 가능한 값들의 집합이 유한하거나 셀 수 있는 경우를 의미한다. 이러한 변수는 특정한 고유한 값들만을 취할 수 있으며, 연속적인 값들 사이의 중간값은 존재하지 않는다. 즉, 이산형 변수는 결과가 불연속적(discrete)인 특성을 가진다.\n이산형 확률변수의 핵심 특징은 다음과 같다.\n\n셀 수 있는 값만 가능하다: 이산형 변수는 개별적인 값들로 이루어진 집합을 가지며, 각 값은 분리되어 있다.\n유한하거나 가산무한한 경우를 포함한다: 변수의 값이 유한한 경우뿐만 아니라, 이론적으로는 무한하더라도 하나씩 차례로 셀 수 있는 경우(예: 자연수 집합)도 이산형 변수에 해당한다.\n\n이산형 확률변수는 확률질량함수(Probability Mass Function, PMF)를 통해 각 가능한 값에 대해 확률을 정의한다. 대표적인 이산형 분포로는 베르누이(Bernoulli), 이항(Binomial), 포아송(Poisson) 분포 등이 있다.\n【유한한 이산형 확률변수】\n\n두 개의 주사위를 던졌을 때 눈금의 합: \\(\\mathcal{X} = \\{2, 3, 4, \\ldots, 12\\}\\)\n한 반 학생들의 성별 (예: 남성=1, 여성=0)\n한 회사의 월별 결함 발생 횟수 (예: 월별 고장 수 0, 1, 2,… 최대 10건 등)\n\n【무한한 이산형 확률변수】\n\n특정 지역의 하루 교통사고 발생 건수: \\(\\mathcal{X} = \\{0, 1, 2, 3, \\ldots\\}\\)\n대기행렬에서 단위 시간 동안 도착하는 고객 수\n이메일이나 콜센터로 하루에 들어오는 문의 건수\n\n이산형 확률변수는 개별적인 사건을 다루는 데 적합하며, 실제 문제에서는 횟수, 개수, 성별, 성공/실패 여부 등과 같은 데이터를 수리적으로 다루기 위해 자주 사용된다.\n2. 연속형 확률변수 (Continuous Random Variable)\n연속형 확률변수는 가능한 값의 범위가 무한히 많은 실수값을 포함하는 확률변수로, 정의된 임의의 구간 내에서 모든 값을 가질 수 있는 것이 특징이다. 즉, 두 값 사이에는 무수히 많은 값들이 존재하며, 측정 단위의 정밀도를 높일수록 더욱 세밀한 값을 구분할 수 있다.\n【주요 특성】\n\n연속형 확률변수는 임의의 실수 구간 내에서 값을 취할 수 있으므로, 개별적인 특정 값이 아니라 값의 범위에 대해 확률이 정의된다.\n확률은 확률밀도함수(PDF, Probability Density Function)를 통해 정의되며, 특정 값의 확률은 0이고 구간에 대한 확률은 누적분포함수(CDF)를 통해 계산된다.\n대표적인 연속형 분포로는 정규분포, 지수분포, 감마분포 등이 있다.\n\n【사례】\n\n자동차의 연비(\\(km/L\\)): 정확한 연비는 소수점 단위의 연속적인 값으로 표현된다.\n수능 시험 점수: 0에서 100점 사이의 연속적인 실수값으로 나타나며, 일반적으로 정규분포를 가정할 수 있다.\n항공기의 비행 시간: 실제 비행 시간은 2.31시간, 5.82시간처럼 임의의 실수값을 가질 수 있으므로 연속형 변수로 다룬다.\n\n연속형 확률변수는 측정값, 비율, 시간, 거리, 온도 등 연속적인 양을 나타내는 현상을 수리적으로 모델링할 때 사용되며, 확률 이론과 통계학에서 핵심적인 개념이다.\n3. 데이터 분석에서의 확률변수 구분\n데이터 분석 관점에서 확률변수는 정량적 변수와 정성적 변수로 구분되며, 이 중 모든 정성적(범주형) 변수는 이산형 확률변수로 간주된다. 이러한 분류는 데이터를 해석하고 분석 방법을 선택하는 데 중요한 기준이 된다.\n(1) 정량적(Quantitative) 변수\n정량적 변수는 수치적인 값을 가지며 산술 연산이 가능한 변수이다. 이 범주에는 이산형 확률변수와 연속형 확률변수가 모두 포함된다.\n\n이산형 예시: 생산량, 자녀 수, 결함 개수\n연속형 예시: 연봉, 키, 몸무게, 점수, 소득\n\n정량적 변수는 평균, 분산, 표준편차 등 수리 통계적 계산이 가능하며, 시각화나 예측 분석에 자주 활용된다.\n(2) 정성적(Qualitative) 변수\n정성적 변수는 수치적 의미가 없는 범주(categorical)로 구성된 변수이며, 일반적으로 이산형 확률변수로 분류된다. 정성적 변수는 명목형(nominal)과 순서형(ordinal)으로 다시 나눌 수 있다.\n\n명목형 예시: 성별(남/여), 혈액형(A/B/O/AB), 지역, 부서명\n순서형 예시: 선호도(매우 좋음/보통/나쁨), 고객 만족도(1~5점 척도), 학력 수준\n\n정성적 변수는 수치 연산보다는 범주 간의 구분이나 순서 관계를 중심으로 해석되며, 교차표나 범주형 시각화(막대그래프 등)에서 주로 사용된다.\n이처럼 확률변수를 정량적/정성적으로 나누는 분류 방식은 데이터의 속성을 이해하고 분석 도구를 적절히 선택하는 데 핵심적인 역할을 한다.\n\n\n\n2. 확률분포함수\n\n(1) 분포함수 정의\n무작위 확률변수 X 의 누적분포함수 cumulative distribution function 또는 cdf는 \\(F_{X}(x)\\)로 나타내며, 다음과 같이 정의된다.\n확률변수가 연속형 확률변수라고 불리려면 누적분포함수는 연속 함수이여야 한다. 반면, 확률변수가 이산형 확률변수라고 불리려면 누적분포함수는 계단 함수이어야 한다.\n동일 분포 정의\n확률변수 X와 Y가 동일한 분포(identically distributed) 를 갖는다고 하려면 임의의 집합 \\(A \\in \\mathcal{B}^{1}\\)에 대해 다음이 성립해야 한다. \\(P(X \\in A) = P(Y \\in A)\\). 즉, 모든 사건 A에 대해 확률변수 X와 Y가 동일한 확률을 가질 때, 확률변수 X와 Y는 동일한 분포를 따른다고 정의한다.\n정리\n다음 두 명제는 동치(equivalent) 이다.\n\n확률변수 X와 Y는 동일한 분포 를 갖는다.\n모든 \\(x\\)에 대해 \\(F_{X}(x) = F_{Y}(x),\\text{for every x}\\) 성립한다.\n\n즉, 확률변수 X 와 Y 가 동일한 분포를 갖는다는 것은, 모든 x 에 대해 누적분포함수가 동일하다는 것과 동치이다.\n\n\n(2) CDF 성질\n\n\\(F(x)\\)는 비감소 non-deceasing 함수이다. \\(x_{1} &lt; x_{2} \\leftrightarrow F\\left( x_{1} \\right) \\leq F(x_{2})\\)\n\\(F( - \\infty) = 0,F(\\infty) = 1\\)\n이산형 확률변수의 누적확률분포는 계단 함수이다. \\(P(X = x) = F(x) - F(x - )\\)\n연속형 확률변수에서는 누적확률밀도함수의 미분함수가 확률밀도함수이다. \\(F'(x) = f(x)\\)\n\n\n\n\n3. 확률밀도함수\n확률변수 \\(X\\)와 그 누적분포함수 \\(F_{X}\\)에 연관된 또 다른 함수가 존재하는데, 이는 확률밀도함수(probability density function, pdf) 또는 확률질량함수(probability mass function, pmf) 라고 불린다. 여기서 pdf 와 pmf 라는 용어는 각각 연속형 및 이산형 확률변수에 적용된다.\n\n(1) 확률질량함수\n【PMF 정의】 이산형 확률변수 \\(X\\)의 확률질량함수는 다음과 같이 정의된다. \\(p_{X}(x) = P(X = x),\\text{for all}x.\\). 즉, \\(p_{X}(x)\\)는 특정 값 \\(x\\)에서 확률변수 \\(X\\)가 그 값을 가질 확률이다.\n【예제】 이산형 확률밀도함수: 4면(1, 2, 3, 4) 주사위 2개를 동시에 던져 큰 수를 확률변수 \\(X\\)라 하자. 확률밀도함수는 다음과 같다.\n\\(p(x) = P(X = x) = \\frac{2x - 1}{16},x = 1,2,3,4\\)\n\n\n\n\n\n\n\n\n\n\n\\[x\\]\n1\n2\n3\n4\n\n\n\n\n\\[p(x)\\]\n1/16\n3/16\n5/16\n7/16\n\n\n\\[F(x)\\]\n1/16\n4/16\n9/16\n16/16\n\n\n\n【예제】 이산형 확률밀도함수: 앞면이 나올 확률이 \\(p\\)인 동전을 던지는 실험을 생각해 보자. ”앞면이 처음 나올 때까지 동전을 몇 번 던졌는지” 를 확률변수 \\(X\\)로 정의하면 확률질량함수는 \\(P(X = x) = (1 - p)^{x - 1}p,x = 1,2,...\\)\n누적확률밀도함수: \\(F_{X}(x) = P(X \\leq x) = \\overset{x}{\\sum_{i = 1}}(1 - p)^{x - 1}p\\)이므로 \\(F_{X}(x) = 1 - (1 - p)^{x},x = 1,2,3,...\\)\n\n\n(2) 확률밀도함수\n【PDF 정의】 연속형 확률변수 \\(X\\)의 확률밀도함수 \\(f_{X}(x)\\)는 다음 조건을 만족하는 함수이다. \\(F_{X}(x) = \\int_{- \\infty}^{x}f_{X}(t)dt\\text{for all}x\\). 즉, 누적분포함수 \\(F_{X}(x)\\)는 확률밀도함수 \\(f_{X}(x)\\)의 적분으로 정의된다.. \\(\\frac{d}{dx}F_{X}(x) = f_{X}(x)\\)이다.\n\n\n(3) 확률계산\n\n이산형: \\(P(X = x) = f_{X}(x) = F_{X}(x) - F_{X}(x^{-})\\), 여기서 \\(F_{X}(x^{-})\\)는 \\(x\\)직전의 누적확률 값이다.\n\n\n\n\n\n\n\n연속형: \\(P(a &lt; X &lt; b) = F_{X}(b) - F_{X}(a),\\text{a&lt;x&lt;b}\\)..\n\n\n\n\n\n\n\n\n(4) 확률밀도함수 관련 정리\n【정리】 함수 \\(f_{X}(x),p_{X}(x)\\)가 확률변수 \\(X\\)의 확률밀도함수 또는 확률질량함수가 되기 위한 필요충분조건은 다음과 같다.\n\n\\(f_{X}(x) \\geq 0\\text{for all x}\\) / \\(p_{X}(x) \\geq 0\\text{for all x}\\)\n\\(\\int_{- \\infty}^{\\infty}f_{X}(x)dx = 1\\) / \\(\\sum_{x}f_{X}(x) = 1\\)\n\n\n\n\n\nchapter 2. 데이터 확률밀도함수\n데이터로부터 확률밀도함수를 추정하는 데 사용되는 대표적인 방법으로는 히스토그램(histogram)과 커널 밀도 추정(kernel density estimator, KDE)이 있다.\n\n히스토그램은 데이터를 구간으로 나누고 각 구간에 속한 데이터의 개수를 세어 상대도수 형태로 표현하는 방법으로, 확률밀도의 초기적 형태를 시각화하는 데 유용하다.\n커널 밀도 추정은 각 데이터 지점에 부드러운 커널 함수를 적용하여 전체 분포를 매끄럽게 추정하는 방법으로, 연속적인 확률밀도함수의 형태를 보다 정교하게 표현할 수 있다.\n\n이 두 방법은 실제 관측 데이터를 바탕으로 모수적 가정을 하지 않고(비모수적 방법), 데이터의 분포 형태를 직관적으로 파악하고 분석하는 데 널리 활용된다.\n\n1. 확률모형\n확률모형이란 확률변수의 확률밀도함수 또는 확률질량함수를 의미한다. 이 모형은 확률변수가 어떻게 분포하는지를 수리적으로 표현해주며, 이산형과 연속형의 경우 해석 방식에 차이가 있다.\n\n이산형 확률모형에서는 막대그래프 형태로 표현되며, 각 막대의 높이가 해당 값이 나올 확률을 나타낸다. 히스토그램과 유사하게 보일 수 있지만, 확률질량함수는 각 이산적인 값에 대해 정확한 확률을 정의한다.\n연속형 확률모형에서는 확률밀도함수의 곡선 아래 면적이 확률을 나타낸다. 이때, 특정한 값 하나에서의 확률은 면적이 0이므로 \\(P(X = x) = 0\\) 이다. 대신, 확률은 구간에 대해 정의되며, 예를 들어 P(a X b)는 곡선 아래 a부터 b까지의 면적으로 해석된다.\n\n따라서 연속형 확률분포에서는 개별 값보다는 구간 단위의 확률 해석이 핵심이 되며, 이는 이산형과의 가장 큰 차이점 중 하나이다.\n\n\n\n\n\n모집단 확률분포함수 \\(f(x)\\) 가정\n모집단 전체를 전수 조사한 경우, 해당 자료로부터 그린 히스토그램은 모집단의 실제 확률분포함수를 시각적으로 표현한 것이라고 볼 수 있다. 그러나 현실에서 다루는 대부분의 연속형 데이터는 모집단이 따르는 정확한 함수 형태를 알 수 없기 때문에, 통계 분석에서는 모집단의 확률분포를 이론적으로 가정한 뒤 이를 바탕으로 통계량의 분포, 즉 샘플링 분포를 구하여 분석을 수행한다.\n이러한 이론적 분포는 단지 분석 도구로서만 사용되는 것이 아니라, 모집단과 유사한 특성을 지닌 데이터를 시뮬레이션을 통해 생성할 때도 활용된다. 즉, 모집단의 확률 구조를 기반으로 가상의 데이터를 생성하여 반복 실험이나 검증에 이용한다.\n회귀분석이나 분산분석과 같은 모형 기반 분석에서는 오차항이 어떤 확률분포를 따른다고 가정하는 것이 필수적이다. 일반적으로 오차항이 정규분포를 따른다고 가정하면, 회귀계수에 대한 검정통계량은 t-분포를 따르게 되며, 모형 전체의 유의성을 검정할 때는 F-분포가 사용된다. 이러한 분포 가정은 추론의 타당성을 확보하고, 모형의 설명력을 평가하는 데 핵심적인 역할을 한다.\n또한 소표본 상황에서는 확률분포에 대한 가정이 더욱 중요하다. 표본의 크기가 20에서 30 미만일 경우, 모집단이 정규분포를 따른다고 가정하면 표본평균은 t-분포를 따르게 되며, 이를 이용하여 모평균에 대한 가설 검정을 수행할 수 있다. 이와 같이 확률분포에 대한 적절한 가정은 불확실한 모집단 구조를 이론적으로 보완하고, 통계적 추론을 가능하게 해주는 중요한 기반이 된다.\n모집단 확률분포함수 가정이 불가능한 경우\n모집단의 분포를 사전에 가정하지 않는 경우, 모집단의 확률분포를 직접 알 수는 없다. 그러나 통계적 접근을 통해 표본으로부터 도출한 확률분포를 바탕으로 모집단의 분포를 추정할 수 있다. 이때 사용되는 핵심 개념은, 확률표본의 표본 확률분포가 모집단의 분포를 반영한다는 점이다. 즉, 무작위로 추출된 대표성 있는 표본이 주어진다면, 이 표본에서 도출된 확률분포를 통해 모집단의 분포를 추정할 수 있다.\n이러한 추정이 타당한지를 검토하는 방법으로는 다음과 같은 절차가 활용된다.\n\n적합성 검정(Goodness-of-fit test)\n\n표본이 특정 분포(예: 정규분포, 이항분포 등)를 따른다고 가정할 수 있는지를 검정하는 방법이다. 대표적으로 카이제곱(χ²) 검정이 사용되며, 교차표 형태로 관측빈도와 기대빈도를 비교하여 유의미한 차이가 있는지를 판단한다.\n\n그래프 기반 방법\n\n\nP-P plot (Probability-Probability plot): 표본의 누적상대도수와 이론적 누적분포를 비교\nQ-Q plot (Quantile-Quantile plot): 분위수를 비교하여 직선에 가까우면 분포가 유사함을 시사\n그 외에도 히스토그램이나 커널 밀도 추정 그래프를 통해 대략적인 분포 형태를 확인할 수 있다.\n\n\n경험적 판단 기준 (rule of thumb) 완전한 검정 대신, 자료의 분포 특성을 직관적으로 파악하기 위한 경험적 기준이 사용되기도 한다. 예를 들어, 자료가 좌우 대칭을 띠고 중심이 뚜렷하면 정규분포로 가정할 수 있다는 등의 판단이다.\n\n요약하면, 모집단의 분포를 직접 알 수 없을 때는 표본으로부터 확률분포를 도출하고, 그 분포가 특정 이론적 분포와 얼마나 잘 부합하는지를 검정과 시각화 도구를 통해 판단하게 된다. 이를 통해 통계 분석의 전제가 되는 분포 가정을 정당화하거나 조정할 수 있다.\n\n\n2. 히스토그램 histogram\n\n(1) 이산형 데이터\n이산형 데이터의 히스토그램은 확률변수의 분포를 시각적으로 표현하는 그래프이다. 이때 가로축(x축)은 확률변수가 가질 수 있는 값이며, 세로축(y축)은 각 값에 대응하는 확률을 막대의 높이로 나타낸다. 이러한 그래프를 확률분포함수라고 한다.\n예를 들어, 어느 도시의 가구를 대상으로 가구원 수를 조사한 결과를 빈도표로 정리하고, 각 가구원 수에 대한 상대빈도를 계산하면, 이를 이용해 히스토그램을 그릴 수 있다. 이때 상대빈도는 각 값이 나타날 확률을 의미하며, 히스토그램은 확률변수의 분포를 직관적으로 보여주는 도구가 된다.\n이산형 데이터의 히스토그램은 막대그래프와 유사한 형태를 가지며, 일반적으로 바차트(bar chart)라고도 불린다. 다만, 확률적 의미를 강조할 경우 이를 확률분포함수의 시각적 표현으로 이해한다.\n\n\n\n\n\n\n\n\n\n\n\n\n(2) 연속형 데이터\n연속형 데이터의 경우, 데이터를 그대로 나열하거나 개별 값을 기준으로 분석하는 것은 어렵기 때문에, 데이터의 전체 범위를 일정한 폭의 구간으로 나누어 (상대)빈도표를 작성한 후, 이를 바탕으로 히스토그램을 그린다. 각 구간의 상대빈도는 해당 구간에 속한 데이터의 비율을 나타내며, 이를 시각화하면 전체 분포의 형태를 파악할 수 있다.\n히스토그램을 구성한 뒤, 각 구간의 중앙값을 기준으로 막대의 꼭짓점을 연결하면 분포의 윤곽을 따라가는 곡선, 즉 폴리곤 형태의 그림이 생성된다. 이 곡선을 통해 전체 데이터의 확률분포 형태를 추정해볼 수 있지만, 이 곡선은 데이터에 따라 달라지며 불연속적이고 매끄럽지 않기 때문에, 이를 바탕으로 정확한 확률분포함수 식을 도출하는 것은 불가능하다.\n따라서 실제 분석에서는 연속형 데이터를 이론적으로 잘 설명할 수 있는 정규분포, 지수분포, 감마분포 등의 분포를 가정하고, 관측된 데이터가 이러한 이론적 분포에 얼마나 잘 부합하는지를 검토한다. 이 과정을 적합성 검정이라고 하며, 대표적으로 카이제곱 검정, Q-Q plot 등의 방법이 활용된다. 이를 통해 분석자는 모집단이 특정 분포를 따른다는 가정의 타당성을 평가하고, 필요한 경우 대안을 모색할 수 있다.\n\n\n\n\n\n연속형 데이터에 대한 히스토그램을 작성하는 과정은 다음과 같다.\n\n데이터를 크기 순으로 정렬한 후, 최소값과 최대값을 구하고 이를 바탕으로 전체 범위(range)를 계산한다. \\(\\text{범위} = \\text{최대값} - \\text{최소값}\\)\n빈(bin) 또는 계급(class)의 개수를 결정한다. 일반적으로 빈의 개수는 8개에서 12개 사이가 적절하며, 데이터 크기에 따라 달라질 수 있다. 빈의 개수를 결정하기 위한 경험적인 기준 중 하나는 Sturges의 규칙으로, 데이터 개수를 N이라 할 때 \\(K = 1 + 3.322 \\log_{10}(N)\\) 와 같이 계산하여 적정한 빈 개수 K를 구할 수 있다.\n전체 범위를 빈 개수로 나누어 각 계급 구간의 폭(interval width)을 구한다. 이때 구간 폭은 해석의 편의를 위해 가급적 정수 단위로 조정하는 것이 좋다.\n정해진 구간에 따라 데이터를 분할하고, 각 구간에 속하는 도수(빈도)를 계산한다. 이 도수를 전체 관측값의 수로 나누어 상대빈도를 구할 수 있다.\n각 구간의 상대빈도 값을 세로축에 대응하여 막대그래프 형태로 표현하면 히스토그램이 완성된다. 이때 각 막대의 중앙값(계급의 중심값)을 점으로 연결하면, 연속형 확률변수의 분포를 나타내는 확률분포함수의 형태를 추정할 수 있다.\n\n이와 같은 과정은 실제 확률분포함수를 알 수 없는 상황에서, 데이터를 바탕으로 분포의 형태를 시각적으로 파악하고 분석하는 데 매우 유용하다.\n\n\n\n3. Kernel 추정\n유한개의 표본 데이터를 이용하여 모집단의 확률분포함수를 추정할 때, 데이터의 불연속성을 부드럽게 처리하는 평활화 기법(smoothing method)이 사용된다. 이러한 방법은 실제 모집단 분포가 무엇인지 알 수 없을 때, 관측된 데이터를 바탕으로 그 분포를 근사하는 데 활용된다.\n신호처리 분야에서는 이러한 방법을 Parzen–Rosenblatt window 방법이라 부르며, 통계학에서는 커널 밀도 추정(Kernel Density Estimator, KDE)이라는 이름으로 널리 알려져 있다.\nKDE는 각 관측값에 대해 커널 함수(예: 정규 커널)를 중심으로 확률밀도를 부여하고, 이를 전체적으로 합산하여 스무드한 곡선 형태의 확률밀도함수를 생성한다. 이는 히스토그램이나 히스토그램의 폴리곤처럼 계단 모양이나 뾰족한 꺾임이 있는 형태보다 훨씬 부드럽고 연속적인 분포 형태를 제공한다.\n결과적으로 KDE는 히스토그램보다 해상도와 유연성이 높으며, 주어진 데이터의 분포 구조를 더 자연스럽게 반영할 수 있는 장점이 있다. 커널 함수의 선택과 함께 대역폭(bandwidth) 설정이 밀도 추정의 형태와 정밀도에 큰 영향을 미친다.\n【커널함수 정의】 커널함수 \\(\\int_{- \\infty}^{\\infty}K(x)dx = 1\\)는 양의실수 적분가능한 좌우 대칭인 함수이며 Gaussian, Epanechnikov 등이 유명한 커널 함수이다.\n\n가우스 커널 = ’gaussian' : \\(K(x;h) \\propto e^{\\frac{x^{2}}{2h^{2}}}\\)\nEpanechnikov 커널 = ’epanechnikov' : \\(K(x;h) \\propto 1 - \\frac{x^{2}}{h^{2}}\\)\n선형 커널 = ’linear' : \\(K(x;h) \\propto 1 - \\frac{x}{h},x &lt; h\\)\n\n확률분포함수 \\(f(x)\\)로 부터 확률표본 데이터 \\(x_{1},x_{2},\\cdots,x_{n}\\)가 주어진 경우 커널추정량은 \\({\\widehat{f}}_{h}(x) = \\frac{1}{n}\\overset{n}{\\sum_{i = 1}}K_{h}(x - x_{i}) = \\frac{1}{nh}\\sum K(\\frac{x - x_{i}}{h})\\)이다. \\(K()\\)는 커널함수이고 \\(h\\)는 bandwith 모수이다. \\(h\\)가 크면 완만한 형태가 되고 작으면 뾰족한 형태이다.\n최적 \\(h = (\\frac{4{\\widehat{\\sigma}}^{5}}{3n})^{\\frac{1}{5}} \\approx 1.06\\widehat{\\sigma}n^{- 0.2}\\) 이다.\n\n\n\n\n\n\n\n4. 봉우리\n확률분포함수의 경우 발생 확률이 높은 곳을 봉우리라 하고 가장 높은 봉우리가 발생하는 확률변수 값을 최빈값 mode이라 한다. 일반적으로 확률분포함수는 봉우리를 중심으로 좌우로 확률이 작아지는 형태를 갖는다.\n\n\n\n\n\n확률분포함수에서 발생 확률이 높은 구간은 곡선 위에서 봉우리로 나타나며, 그 중 가장 높은 봉우리에 해당하는 확률변수의 값을 최빈값(mode)이라고 한다. 일반적으로 확률분포함수는 봉우리를 중심으로 양쪽으로 갈수록 확률이 점차 작아지는 형태를 보인다.\n봉우리의 개수에 따라 확률분포함수의 형태를 다음과 같이 분류할 수 있다.\n\n단봉(unimodal): 봉우리가 하나인 분포. 대부분의 정규분포는 이 유형에 속한다.\n쌍봉(bimodal): 봉우리가 두 개인 분포. 서로 다른 두 집단이 섞여 있을 때 자주 발생한다.\n다봉(multimodal): 세 개 이상의 봉우리를 가지는 분포. 여러 이질적인 집단이 혼합된 경우 발생할 수 있다.\n\n봉우리가 2개 이상인 분포는 일반적으로 서로 다른 성격을 가진 하위 집단이 한 데이터셋에 혼재되어 있을 때 나타난다. 예를 들어, 중간고사 점수를 분석할 때, 공부한 학생들과 그렇지 않은 학생들을 함께 조사하면 성적 분포가 두 개의 뚜렷한 봉우리를 가진 쌍봉 형태로 나타날 수 있다. 이와 유사하게 남녀의 몸무게를 함께 측정한 경우에도 평균 체격 차이로 인해 두 개의 봉우리가 나타나는 현상이 관찰된다.\n이러한 분포 형태는 단순한 평균이나 분산으로는 설명하기 어려운 구조적 특성을 보여주기 때문에, 데이터를 분해하거나 군집별 분석을 고려해야 할 필요가 있음을 시사한다.\n쌍봉 분포의 한계\n봉우리가 두 개인 이봉(bimodal) 분포에서는 단순히 평균을 추론하는 방식이 다음과 같은 여러 문제점을 야기할 수 있다.\n첫째, 대표성 부족 문제가 발생할 수 있다. 평균은 보통 데이터의 중심 경향을 요약하는 지표로 사용되지만, 이봉 분포에서는 두 개의 봉우리 사이에 위치하게 되며, 실제로는 데이터가 거의 존재하지 않는 구간의 값을 대표값으로 제시할 위험이 있다. 이 경우 평균은 중심 경향을 적절히 반영하지 못한다.\n둘째, 왜곡된 해석의 가능성이 크다. 이봉 분포는 일반적으로 서로 다른 두 모집단이 섞여 있을 때 나타나는데, 이 경우 전체 데이터를 대상으로 계산한 단일 평균은 각 모집단의 특성을 반영하지 못하고, 두 집단을 모두 왜곡한 해석을 초래할 수 있다.\n셋째, 분산이 증가하는 문제가 있다. 두 개의 중심에서 데이터가 분포되기 때문에 전체 분산이 커지며, 이로 인해 평균에 대한 신뢰구간이 넓어지고 추론의 정확성이 낮아진다.\n넷째, 단일 정규분포 가정의 위반이 발생할 수 있다. 많은 평균 비교를 위한 통계적 방법들(예: t-검정, 분산분석)은 정규성을 전제로 하지만, 이봉 분포는 이러한 정규분포 가정을 충족하지 않을 가능성이 높아, 해당 검정의 사용이 부적절해질 수 있다.\n다섯째, 혼합 모집단의 가능성이 존재한다. 두 개 이상의 서로 다른 모집단이 하나의 데이터셋으로 결합된 경우라면, 전체 평균을 하나로 제시하기보다는 각 하위 집단의 평균을 개별적으로 추정하고 해석하는 것이 더 적절하다. 단일 평균을 제시하면 오히려 중요한 정보가 손실될 수 있다.\n따라서 이봉 분포를 갖는 데이터를 분석할 때는 단순한 평균 추론보다는, 군집 분석, 혼합분포 모형(Gaussian Mixture Model), 또는 중앙값(median)과 같은 대체적인 대표값을 사용하는 것이 보다 타당하며, 데이터의 구조를 반영한 해석이 가능해진다.\n\n\n5. 치우침\n\n(1) 정의\n확률분포함수는 일반적으로 중심에 가까운 값에서 높은 확률을 가지며, 중심에서 멀어질수록 확률이 점차 작아지는 형태를 띤다. 이러한 특성은 자연 현상이나 사회적 현상에서 관찰되는 많은 변수들이 극단적인 값보다는 평균이나 중심값 부근에서 자주 발생하는 경향을 잘 반영한다.\n특히 종 모양(bell-shaped)이며 좌우 대칭인 분포는 봉우리를 중심으로 양쪽이 완전히 대칭을 이루는 형태로, 평균, 중앙값, 최빈값이 모두 같은 위치에 존재한다. 이와 같은 분포는 데이터가 한쪽으로 치우치지 않고 균형 있게 분포되어 있음을 의미하며, 대표적으로 정규분포가 이에 해당한다.\n반면, 분포가 우측으로 치우친 경우(positive skewed, 또는 right-skewed)는 오른쪽 꼬리가 길게 늘어지는 모양을 보인다. 이는 일부 큰 값들이 전체 분포의 끝부분에 위치함을 나타내며, 이 값들이 평균을 끌어올려 평균이 중앙값보다 크게 된다. 예를 들어 소득, 부동산 가격과 같은 변수는 이런 형태의 분포를 보이는 경우가 많다.\n반대로 좌측으로 치우친 분포(negative skewed, 또는 left-skewed)는 왼쪽 꼬리가 길며, 극단적으로 작은 값들이 존재하는 경우이다. 이로 인해 평균이 중앙값보다 작아지는 경향이 나타난다. 예를 들어 시험 점수가 대부분 높은 수준이지만 일부 낮은 점수가 포함된 경우 이런 형태가 관찰될 수 있다.\n이처럼 확률분포함수의 형태와 치우침(skewness)은 데이터가 어떻게 퍼져 있는지를 보여주는 중요한 특징으로, 중심 경향을 해석하고 적절한 분석 방법을 선택하는 데 유용한 정보를 제공한다. 분포의 대칭성, 비대칭성 여부는 평균, 중앙값, 최빈값 간의 관계를 통해 파악할 수 있으며, 데이터의 구조와 특성을 이해하는 데 핵심적인 역할을 한다.\n\n\n(2) 치우침과 중앙 위치 통계량 관계\n\n\n\n\n\n평균(mean)은 데이터의 크기를 반영하는 중심 척도로, 모든 관측값을 더한 뒤 데이터의 개수로 나눈 값으로 정의된다. 이는 데이터 전체의 절대적인 크기 수준을 대표하며, 분포의 중심이 어디에 위치하는지를 수치적으로 표현해준다. 다만, 극단적으로 크거나 작은 값이 포함될 경우 평균은 그 영향을 크게 받게 된다.\n이에 비해 중앙값(median)은 관측값을 크기 순으로 정렬했을 때 정중앙에 위치하는 값이다. 이는 순서에 기반한 중심 척도로서, 이상치나 극단값의 영향을 거의 받지 않는다. 특히 데이터 분포가 비대칭이거나 한쪽으로 치우쳐 있을 때도 안정적으로 중심 경향을 나타낼 수 있어 평균보다 더 적절한 대표값이 될 수 있다.\n최빈값(mode)은 전체 데이터 중 가장 자주 나타나는 값으로, 확률분포의 봉우리에 해당한다. 최빈값은 특정 값이 빈번하게 반복되는 패턴을 보여주며, 특히 이산형 데이터나 분포가 다봉(multi-modal)인 경우에 유용한 정보를 제공한다.\n이 세 가지 중심 척도는 모두 데이터의 중심 경향을 요약하는 데 사용되며, 분포의 형태에 따라 서로 다른 위치에 나타날 수 있다. 정규분포처럼 좌우 대칭인 경우에는 평균, 중앙값, 최빈값이 모두 같은 값을 가지지만, 우측 또는 좌측으로 치우친 분포에서는 평균이 극단값에 의해 끌려가 중앙값 및 최빈값과 차이를 보이게 된다.\n따라서 데이터의 분포 특성과 목적에 따라 적절한 중심 척도를 선택하는 것이 중요하다. 평균은 수치의 크기를 중심으로 한 대표값, 중앙값은 순서의 중심을 나타내는 값, 최빈값은 가장 빈번하게 나타나는 데이터의 특성을 반영하는 값으로 각각의 특성을 고려해 활용해야 한다.\n\n\n(3) 치우침 척도\n평균 기반\n\n\\(\\mu_{3} = E\\left\\lbrack \\left( \\frac{X - \\mu}{\\sigma} \\right)^{3} \\right\\rbrack = \\frac{\\mu_{3}}{\\sigma^{3}}\\)\n\n좌우 대칭인 정규분포의 왜도는 0이고 지수분포의 왜도 2로, 이는 강한 우측 치우침을 나타냄을 의미한다.\n중앙값 기반 척도\n\nPearson’s First Skewness : \\(skew = \\frac{\\text{mean} - \\text{mode}}{\\text{std}}\\)\nPearson’s Second Skewness : \\(skew = \\frac{3(\\text{mean} - \\text{median})}{\\text{std}}\\)\n\n사분위 기반 Bowley’s Skewness\n\nskew = \\(\\frac{(Q_{3} + Q_{1} - 2Q_{2})}{\\text{IQR}}\\)\n\n중앙값을 중심으로 사분위수 간의 차이를 활용한 왜도 측정 방식. 이상치의 영향을 덜 받는다.\nGroeneveld & Meeden’s Coefficient\n\n\\(skew = \\frac{\\text{mean} - \\text{median}}{E(|X - \\text{median}|)}\\)\n\n평균과 중앙값의 차이를 절대 편차로 정규화한 값으로, 비대칭성을 측정하는 지표이다.\n\n\n(4) 치우침 판단 기준\n\n\n\n\n\n\n\n\n\nPearson 2차\n분포 형태\nBowley’s\nGroeneveld & Meeden’s\n\n\nskew=0\n대칭 분포\nskew=0\nskew=0\n\n\n0 &lt; skew &lt; 0.5\n거의 대칭\n0 &lt; skew &lt; 0.25\n0 &lt; skew &lt; 0.1\n\n\n0.5 ≤ skew &lt; 1.0\n약한 우측 치우침\n0.25 ≤ skew &lt; 0.5\n0.1 ≤ skew &lt; 0.3\n\n\nskew ≥ 1.0\n강한 우측 치우침\nskew ≥ 0.5\nskew ≥ 0.3\n\n\n-0.5 &lt; skew &lt; 0\n약한 좌측 치우침\n-0.25 &lt; skew &lt; 0\n-0.1 &lt; skew &lt; 0\n\n\n-1.0 &lt; skew ≤ -0.5\n중간 정도의 좌측 치우침\n-0.5 &lt; skew ≤ -0.25\n-0.3 &lt; skew ≤ -0.1\n\n\nskew ≤ -1.0\n강한 좌측 치우침\nskew ≤ -0.5\nskew ≤ -0.3\n\n\n\n\n\n\n6. 확률밀도함수 관련 법칙\n\n(1) 실증적 법칙 empirical rule\n확률변수 \\(X\\)의 확률밀도함수 형태가 벨모양의 좌우 대칭이고 평균 \\(\\mu\\), 표준편차 \\(\\sigma\\)를 갖는다고 하면 다음이 성립한다.\n\\(P(|X - \\mu| &lt; k\\sigma) \\geq \\alpha\\)\n\n\\(k = 1\\) : 데이터는 평균을 중심으로 \\(\\pm \\sigma\\) 구간, \\((\\mu - \\sigma,\\mu + \\sigma)\\)에 적어도 68%(\\(\\alpha\\))가 포함된다.\n\\(k = 2\\) : 데이터는 평균을 중심으로 \\(\\pm 2\\sigma\\) 구간, \\((\\mu - 2\\sigma,\\mu + 2\\sigma)\\)에 적어도 95%(\\(\\alpha\\))가 포함된다.\n\\(k = 3\\) : 데이터는 평균을 중심으로 \\(\\pm 3\\sigma\\) 구간, \\((\\mu - 3\\sigma,\\mu + 3\\sigma)\\)에 적어도 99.8%(\\(\\alpha\\))가 포함된다.\n\n만약 \\(k = 6\\) : 100만개 데이터 중 2개만 이 구간을 벗어남. \\(6\\sigma\\) 품질 운동이다.\n\n\n\n\n\n\n\n(2) chebyshev 부등식\n평균 \\(\\mu\\), 표준편차 \\(\\sigma\\)을 갖는 확률변수 \\(X\\)의 확률밀도함수 형태를 알 수 없는 경우 다음이 성립한다.\n\\[P(|X - \\mu| &lt; k\\sigma) \\geq 1 - \\frac{1}{k^{2}}\\]\n\n\\(k = 2\\) : 데이터는 평균을 중심으로 \\(\\pm 2\\sigma\\) 구간, \\((\\mu - 2\\sigma,\\mu + 2\\sigma)\\)에 적어도 75%가 포함된다.\n\\(k = 3\\) : 데이터는 평균을 중심으로 \\(\\pm 3\\sigma\\) 구간, \\((\\mu - 3\\sigma,\\mu + 3\\sigma)\\)에 적어도 89%가 포함된다.\n\n#데이터 읽기 (연속형 데이터)\nimport pandas as pd\ndata=pd.read_csv('https://vincentarelbundock.github.io/Rdatasets/csv/fpp2/ausair.csv')\ndata.info()\n(10, 20] 0.361702 (30, 40] 0.148936 (60, 70] 0.127660 (20, 30] 0.106383 (40, 50] 0.106383 (0, 10] 0.085106 (50, 60] 0.042553\n#상대빈도표\nimport numpy as np\npd.cut(data['value'],np.arange(0,80,10)).value_counts()/len(data)\n#확률밀도함수, KDE\nimport matplotlib.pyplot as plt\ndf = pd.DataFrame(data,columns=['value'])\nax = df.plot.hist(bins=10)\ndf.plot.kde(ax=ax,secondary_y=True,bw_method=0.3)\nplt.show()\n\n\n\n\n\n#CDF, PDF 그리기\nimport matplotlib.pyplot as plt\nfig, ax = plt.subplots()\nax2 = ax.twinx()\nn, bins, patches = ax.hist(data['value'], bins=10)\nn, bins, patches = ax2.hist(data['value'], cumulative=1, histtype='step', bins=10, color='tab:orange')\nplt.show()\n\n\n\n\n\n\n\n\n\nchapter 3. 변수변환\n모집단의 관심 현상을 누적분포함수를 가진 확률변수 X의 형태로 모델링할 수 있다면, 자연스럽게 X 자체뿐만 아니라 X의 함숫값, 즉 g(X)의 특성에도 관심을 갖게 된다. 예를 들어, 이자율 변화에 따른 수익률, 측정값의 제곱, 또는 로그 변환과 같이 실제 응용에서는 원래의 확률변수 X가 아닌, 그것의 함수 형태인 g(X)가 분석의 대상이 되는 경우가 많다.\n따라서 확률변수 X가 어떤 분포를 따를 때, 그 함수 g(X)가 가지는 분포나 평균, 분산, 모멘트 등과 같은 정보를 어떻게 도출할 수 있을지를 체계적으로 다루는 기법들이 필요하다. 여기서는 이러한 목적을 위해 사용되는 확률변수의 함수에 대한 변환법, 기댓값 계산법, 모멘트 생성 함수, 변수 변환을 통한 분포 도출 등의 통계적 기법들을 다루게 된다.\n\n1. 개념\n만약 확률변수 \\(X\\) 의 임의의 함수 \\(g(X)\\) 또한 확률변수가 된다. 종종 \\(g(X)\\) 자체도 관심의 대상이 되며 이를 새로운 확률변수 \\(Y = g(X)\\)로 표기하며, 임의의 집합 \\(A\\)에 대하여 \\(P(Y \\in A) = P(g(X) \\in A)\\)로 나타낼 수 있으며, 이는 \\(Y\\)의 분포가 함수 \\(F_{X}\\)와 \\(g\\)에 의해 결정됨을 의미한다.\n\\(y = g(x)\\)라고 쓸 때 함수 \\(g(x)\\)는 원래 확률변수 \\(X\\)의 표본공간 \\(\\mathcal{X}\\) 에서 새로운 표본공간 \\(\\mathcal{Y}\\), 즉 확률변수 \\(Y\\)의 표본공간으로 가는 매핑을 정의한다. 이를 수식으로 표현하면, \\(g(x):\\mathcal{X} \\rightarrow \\mathcal{Y}\\). \\(g\\)에 대해 역함수 매핑 \\(g^{- 1}\\)을 정의하는데, 이는 \\(\\mathcal{Y}\\)의 부분집합을 \\(\\mathcal{X}\\)의 부분집합으로 변환하는 함수이다. \\(g^{- 1}(A) = \\{ x \\in \\mathcal{X}:g(x) \\in A\\}\\).\n즉, 사상 \\(g^{- 1}\\)은 집합을 집합으로 변환하는데, \\(g^{- 1}(A)\\)는 \\(g(x)\\)가 집합 \\(A\\)로 보내는 \\(\\mathcal{X}\\)의 점들의 집합을 의미한다. 집합 \\(A\\)가 단일 원소 집합, 즉 \\(A = \\{ y\\}\\)인 경우도 가능하다. 이때는, \\(g^{- 1}(\\{ y\\}) = \\{ x \\in \\mathcal{X}:g(x) = y\\}\\) 로 표현된다. 이 경우, 우리는 종종 \\(g^{- 1}(y)\\)를 \\(g^{- 1}(\\{ y\\})\\) 대신 사용하여 표기한다.\n\\(g^{- 1}(y)\\)는 여전히 집합이 될 수 있는데, 이는 \\(g(x) = y\\)를 만족하는 \\(x\\)가 하나 이상 존재할 수 있기 때문이다. 만약 \\(g(x) = y\\)를 만족하는 \\(x\\)가 오직 하나뿐이라면, \\(g^{- 1}(y)\\)는 단일 원소 집합 \\(\\{ x\\}\\)가 되고, 이때는 단순히 \\(g^{- 1}(y) = x\\)라고 표기할 수도 있다.\n확률변수 \\(Y = g(X)\\), 임의의 집합 \\(A \\subset \\mathcal{Y}\\)에 대해 다음과 같이 표현할 수 있다.\n\\(\\begin{matrix}\nP(Y \\in A) & = P(g(X) \\in A) \\\\\n& = P(\\{ x \\in \\mathcal{X}:g(x) \\in A\\}) \\\\\n& = P(X \\in g^{- 1}(A))\n\\end{matrix}\\).\n\n\n2. 이산형 확률변수 변수변환\n확률변수 \\(X\\)가 이산 확률변수라면 표본공간 \\(\\mathcal{X}\\)는 가산 집합이다. 그러므로 확률변수 \\(Y = g(X)\\)의 표본공간은 \\(\\mathcal{Y} = \\{ y:y = g(x),x \\in \\mathcal{X}\\}\\)이며, 이 또한 가산 집합이므로 \\(Y = g(X)\\) 역시 이산 확률변수가 된다. \\(Y = g(X)\\)의 확률질량함수는 다음과 같이 주어진다.\n\\(f_{Y}(y) = P(Y = y) = \\sum_{x \\in g^{- 1}(y)}P(X = x) = \\sum_{x \\in g^{- 1}(y)}f_{X}(x),\\text{for}y \\in \\mathcal{Y}\\)그리고, \\(y \\notin \\mathcal{Y}\\)인 경우 \\(f_{Y}(y) = 0\\)이다. 즉, \\(Y = g(X)\\)의 pmf를 구하는 과정은 각 \\(y \\in \\mathcal{Y}\\)에 대해 \\(g^{- 1}(y)\\)를 찾아서 해당 확률들을 모두 합산하는 것으로 간단하게 해결된다.\n【예제】 (동전 던지기 실험회수)\n확률변수 \\(X\\)를 첫 번째 앞면이 나온 동전 던지기의 횟수라 정의하면 \\(P(X = x) = (1/2)^{x},x = 1,2,...\\)이다. 첫 번째 앞면이 나오기 전까지의 동전 던진 횟수를 나타내는 확률변수 \\(Y\\)를 정의하자. 즉, \\(Y = g(x) = X - 1\\)이다.\n\n역함수 구하기: \\(g^{- 1}(y) = y + 1\\)\n확률변수 \\(Y\\)의 표본공간: \\(\\mathcal{Y} = \\{ y:y = g(x),x \\in \\mathcal{X}\\} = \\{ 0,1,2,\\ldots\\}\\)\n확률변수 \\(Y = X - 1\\)의 확률질량함수:\n\n\\[p_{Y}(y) = p_{X}(y + 1) = \\left( \\frac{1}{2} \\right)^{y + 1},\\text{for}y = 0,1,2,\\ldots\\]\n【예제】(이항분포)\n이산 확률변수 \\(X\\)(성공확률이 \\(p\\)인 \\(n\\)번의 베르누이 시행 결과 성공회수)가 이항 분포를 가지면, 그 확률질량함수는 다음과 같은 형태를 가진다(\\(n\\)은 양의 정수, \\(0 &lt; p &lt; 1\\)).\n\\[f_{X}(x) = P(X = x) = \\binom{n}{x}p^{x}(1 - p)^{n - x},x = 0,1,\\ldots,n\\]\n새로운 확률변수 \\(g(x) = n - x\\)(실패회수)의 확률질량함수를 구하자.\n\n역함수 구하기: \\(g^{- 1}(y) = n - y\\)\n확률변수 \\(Y\\)의 표본공간: \\(\\mathcal{Y} = \\{ y:y = g(x),x \\in \\mathcal{X}\\} = \\{ 0,1,\\ldots,n\\}\\)\n\n\\[\\begin{matrix}\np_{Y}(y) & = \\sum_{x \\in g^{- 1}(y)}p_{X}(x) = p_{X}(n - y) \\\\\n& = \\binom{n}{n - y}p^{n - y}(1 - p)^{n - (n - y)} \\\\\n& = \\binom{n}{y}(1 - p)^{y}p^{n - y}\n\\end{matrix}\\]\n\n\n3. 연속형 확률변수 변수변환\n만약 \\(X\\)가 연속 확률변수라면, 일부 경우에는 \\(Y = g(X)\\)의 누적분포함수와 확률밀도함수를 \\(X\\)의 cdf와 pdf 및 함수 \\(g(X)\\)를 이용하여 간단한 형태로 표현하는 것이 가능하다.\n\n(1) 분포함수 이용하기\n확률변수 \\(Y = g(X)\\)의 누적분포함수는 다음과 같이 주어진다.\n\\[\\begin{matrix}\nF_{Y}(y) & = P(Y \\leq y) = P(g(X) \\leq y) \\\\\n& = P(\\{ x \\in \\mathcal{X}:g(x) \\leq y\\}) \\\\\n& = \\int_{\\{ x \\in \\mathcal{X}:g(x) \\leq y\\}}f_{X}(x)dx\n\\end{matrix}\\]\n【예제】 \\(g(X) = X^{2}\\) 변환\n확률변수 \\(X\\)의 확률밀도함수 \\(f_{X}(x) = \\frac{1}{2}, - 1 &lt; x &lt; 1\\)라 하자. 새로운 확률변수 \\(Y = X^{2}\\)의 확률밀도함수를 구하고자 한다.\n\n확률변수 \\(Y\\)의 표본공간: \\(\\mathcal{Y} = \\{ y:y = g(x),x \\in \\mathcal{X}\\} = \\{ y:0 \\leq y &lt; 1\\}\\)\n확률변수 \\(Y = X^{2}\\)의 확률분포함수:\n\n\\(P(X^{2} \\leq y) = P( - \\sqrt{y} \\leq X \\leq \\sqrt{y})\\)이므로 \\(F_{Y}(y)\\)다음과 같이 주어진다.\n\\[F_{Y}(y) = \\{\\begin{matrix}\n0, & y &lt; 0 \\\\\n\\int_{- \\sqrt{y}}^{\\sqrt{y}}\\frac{1}{2}dx = \\sqrt{y}, & 0 \\leq y &lt; 1 \\\\\n1, & 1 \\leq y.\n\\end{matrix}\\]\n따라서, Y 의 확률밀도함수(pdf)는 다음과 같다.\n\\[f_{Y}(y) = \\{\\begin{matrix}\n\\frac{1}{2\\sqrt{y}}, & 0 &lt; y &lt; 1 \\\\\n0, & \\text{그 외의 경우}.\n\\end{matrix}\\]\n\n\n(2) Jacobian 이용하기\n확률변수 \\(X\\)가 확률밀도함수 \\(f_{X}(x)\\)와 영역 \\(S_{X}\\)를 가지는 연속 확률변수라고 하자. 그리고 새로운 확률변수 \\(Y = g(X)\\) 로 정의하자. 여기서, 함수 \\(g(x)\\)는 일대일 미분 가능 함수이며, \\(X\\)의 영역 \\(S_{X}\\)에서 정의된다.\n【정리】 \\(g(X)\\)의 역함수를 \\(x = g^{- 1}(y)\\)라고 나타내고, \\(\\frac{dx}{dy} = \\frac{d\\lbrack g^{- 1}(y)\\rbrack}{dy}\\)로 정의하자. 그러면, \\(Y = g(X)\\)의 확률밀도함수는 다음과 같이 주어진다.\n\\(f_{Y}(y) = f_{X}(g^{- 1}(y))\\left| \\frac{dx}{dy} \\right|,\\text{for}y \\in S_{Y}\\).\n여기서, \\(Y\\)의 영역은 \\(S_{Y} = \\{ y = g(x):x \\in S_{X}\\}\\)이다.\n\n\\(f_{Y}(y) = f_{X}(x)|\\frac{dx}{dy}|\\) (비감소 함수\n\\(f_{Y}(y) = {- f}_{X}(x)|\\frac{dx}{dy}|\\) (비증가 함수)\n\n【정리】 확률변수 \\(X\\)가 누적분포함수 \\(F_{X}(x)\\)를 가진다고 하자. 또한, \\(Y = g(X)\\) 로 정의하고, 표본공간 \\(\\mathcal{X}\\)와 \\(\\mathcal{Y}\\)는 \\(\\mathcal{Y} = \\{ y:y = g(x),x \\in \\mathcal{X}\\}\\) 정의된다고 하자.\n\n만약 \\(g(X)\\)가 증가 함수이면, \\(F_{Y}(y) = F_{X}(g^{- 1}(y)),\\text{for}y \\in \\mathcal{Y}\\)\n만약 \\(g(X)\\)가 감소 함수이면, \\(F_{Y}(y) = 1 - F_{X}(g^{- 1}(y)),\\text{for}y \\in \\mathcal{Y}\\).\n\n【예제】 균일분포 지수분포 변환\n만약 \\(X \\sim f(x) = 1,0 &lt; x &lt; 1\\)이면 확률변수 \\(Y = - ln(1 - X)\\) 확률밀도함수를 구하라.\n\n확률변수 \\(Y = - ln(1 - X)\\) 은 \\(X\\)의 증가 함수이고 \\(X = 1 - e^{- y}\\)이다. 그러므로 \\(\\frac{dx}{dy} = e^{- y}\\)이다.\n\\(Y\\)의 영역 : \\(0 \\leq x \\leq 1\\) ⇨ \\(0 &lt; y &lt; \\infty\\).\n\\(f_{Y}(y) = f_{X}(x)|\\frac{dx}{dy}| = e^{- y},0 &lt; y\\).\n\n【예제】 정규분포와 카이제곱분포\n확률변수 \\(X\\)가 표준정규분포를 따른다고 가정하자.\n\\(f_{X}(x) = \\frac{1}{\\sqrt{2\\pi}}e^{- x^{2}/2}, - \\infty &lt; x &lt; \\infty\\).\n함수 \\(y = g(x) = X^{2}\\)는 구간 \\(( - \\infty,0),(0,\\infty)\\)에서 단조함수이고, \\(Y\\)의 표본공간은 \\(\\mathcal{Y} = (0,\\infty)\\)이다.\n\\(A_{1} = ( - \\infty,0),g_{1}(x) = x^{2},g_{1}^{- 1}(y) = - \\sqrt{y}\\);\n\\(A_{2} = (0,\\infty),g_{2}(x) = x^{2},g_{2}^{- 1}(y) = \\sqrt{y}\\).\n\\(Y\\)의 확률밀도함수는 다음과 같이 구해지며, 자유도 1의 카이제곱 분포와 동일하다\n\\[f_{Y}(y) = \\frac{1}{\\sqrt{2\\pi}}e^{- ( - \\sqrt{y})^{2}/2}\\left| \\frac{1}{2\\sqrt{y}} \\right| + \\frac{1}{\\sqrt{2\\pi}}e^{- (\\sqrt{y})^{2}/2}\\left| \\frac{1}{2\\sqrt{y}} \\right|\\]\n\\(= \\frac{1}{\\sqrt{2\\pi}}\\frac{1}{\\sqrt{y}}e^{- y/2},0 &lt; y &lt; \\infty\\).\n\n\n\n\nchapter 4. 기대값\n확률밀도함수는 확률변수의 분포 형태와 흩어진 정도, 즉 데이터가 어떻게 퍼져 있는지를 시각적으로나 구조적으로 보여주는 데 유용하다. 그러나 이러한 함수 자체는 중심 경향이나 대표값과 같은 요약 정보를 직접적으로 제공하지는 않는다. 이러한 요약 정보를 수치로 표현한 값을 기술 요약값(descriptive summary statistics)이라 하며, 그 대표적인 예가 기대값(expected value)이다.\n기대값은 확률변수의 평균값을 의미한다. 이때의 평균이란 단순한 산술 평균이 아니라, 확률분포에 따라 각 값에 가중치를 적용한 평균값이다. 다시 말해, 각 값이 발생할 가능성(확률)을 고려하여 계산된 가중 평균이라 할 수 있다.\n확률분포의 기대값은 분포의 중심 위치를 나타내는 지표로서, 통계학에서 중심 척도(measure of central tendency)로 해석된다. 이는 우리가 일반적으로 평균을 중심 경향의 대표값으로 사용하는 것과 같은 개념이다. 기대값은 단지 수학적 정의를 넘어서, 확률적 사건의 장기적 평균 결과를 설명하는 데 핵심적인 역할을 한다.\n\n1. 기대값 정의\n【기대값 정의】 확률변수 \\(g(X)\\)의 기대값은 \\(E\\lbrack g(X)\\rbrack\\)로 표기되며, 다음과 같이 정의된다.\n\\[E\\lbrack g(X)\\rbrack = \\{\\begin{matrix}\n\\int_{- \\infty}^{\\infty}g(x)f_{X}(x)dx, & \\text{if}X\\text{is continuous} \\\\\n\\sum_{x \\in \\mathcal{X}}g(x)f_{X}(x) = \\sum_{x \\in \\mathcal{X}}g(x)P(X = x), & \\text{if}X\\text{is discrete}.\n\\end{matrix}\\]\n단, 위의 적분 또는 합이 존재해야 한다. 만약 \\(E|g(X)| = \\infty\\)이면, 기대값이 존재하지 않는다고 말한다.\n【예제】 지수분포 평균\n확률변수 \\(X\\)가 모수가 \\(\\lambda &gt; 0\\)인 지수분포를 따른다고 가정하자.\n\\(f_{X}(x) = \\frac{1}{\\lambda}e^{- x/\\lambda},0 \\leq x &lt; \\infty\\).\n\\[E\\lbrack X\\rbrack = \\int_{0}^{\\infty}\\frac{1}{\\lambda}xe^{- x/\\lambda}dx = - xe^{- x/\\lambda}|_{0}^{\\infty} + \\int_{0}^{\\infty}e^{- x/\\lambda}dx\\]\n\\(= \\int_{0}^{\\infty}e^{- x/\\lambda}dx = \\lambda\\).\n【예제】 이항분포 평균\n확률변수 \\(X\\)가 모수가 \\((n,p)\\)인 이항분포를 따른다고 가정하자(단, \\(n\\)은 양의 정수이며, \\(0 \\leq p \\leq 1\\)이다).\n\\(P_{X}(x) = \\binom{n}{x}p^{x}(1 - p)^{n - x},dx = 0,1,\\ldots,n\\).\n\\[E\\lbrack X\\rbrack = \\overset{n}{\\sum_{x = 0}}x\\binom{n}{x}p^{x}(1 - p)^{n - x} = \\overset{n}{\\sum_{x = 1}}x\\binom{n}{x}p^{x}(1 - p)^{n - x}\\]\n(\\(x = 0\\)항은 0이므로 제외하였고) \\(x\\binom{n}{x} = n\\binom{n - 1}{x - 1}\\)을 이용하여 \\(E\\lbrack X\\rbrack = \\overset{n}{\\sum_{x = 1}}n\\binom{n - 1}{x - 1}p^{x}(1 - p)^{n - x}\\).\n변수 치환 \\(y = x - 1\\)을 적용하면 \\(E\\lbrack X\\rbrack = \\overset{n - 1}{\\sum_{y = 0}}n\\binom{n - 1}{y}p^{y + 1}(1 - p)^{n - (y + 1)}\\)이므로\n\\(E\\lbrack X\\rbrack = np\\overset{n - 1}{\\sum_{y = 0}}\\binom{n - 1}{y}p^{y}(1 - p)^{n - 1 - y}\\).\n위의 식에서 마지막 합은 이항 분포 \\(\\text{Binomial}(n - 1,p)\\)의 확률 질량 함수의 전체 합이므로 1이 되어 \\(E(X) = np\\)가 된다.\n\n\n2. 주요 기대값\n【평균 정의】 \\(X\\)를 기대값이 존재하는 확률 변수라고 하자. \\(X\\)의 평균 값을 \\(\\mu\\)는 \\(\\mu = E(X)\\)로 정의된다.\n평균(mean)은 자료 전체의 중심 위치를 나타내는 대표값으로, 관측된 값들의 크기를 모두 고려하여 계산되는 중심 경향의 척도이다. 데이터의 전체적인 수준이나 규모를 요약하는 데 사용되며, 가장 일반적이고 직관적인 요약 지표 중 하나이다.\n평균은 모든 값을 동일하게 반영하므로, 자료에 극단적인 값(이상치)이 있을 경우 그 영향을 크게 받을 수 있다. 따라서 분포가 대칭이고 이상치가 없는 경우에는 평균이 중심을 잘 대표하지만, 비대칭 분포나 이상치가 있는 경우에는 중앙값이나 최빈값 등 다른 중심 척도와 함께 해석하는 것이 바람직하다.\n【분산 정의】 \\(X\\)를 유한한 평균 \\(\\mu\\)를 가지며, \\(E\\lbrack(X - \\mu)^{2}\\rbrack\\)가 유한한 확률 변수라고 하자. 그러면 \\(X\\)의 분산은 \\(E\\lbrack(X - \\mu)^{2}\\rbrack\\)로 정의된다. 보통 \\(\\sigma^{2}\\) 또는 \\(\\text{Var}(X)\\)로 표기된다.\n분산은 확률변수가 가질 수 있는 값들이 평균으로부터 얼마나 퍼져 있는지를 나타내는 지표로, 데이터의 변동성 또는 흩어진 정도를 수치적으로 측정한다. 이는 확률변수의 값이 평균을 중심으로 얼마나 멀리 떨어져 있는지를 나타내며, 분포의 폭이나 안정성을 이해하는 데 중요한 역할을 한다.\n분산이 클수록 확률변수의 값들이 평균을 기준으로 넓게 퍼져 있다는 것을 의미하며, 이는 불확실성이나 데이터의 변동성이 크다는 신호로 해석된다. 반면, 분산이 작을수록 값들이 평균 부근에 가깝게 밀집되어 있음을 나타내며, 비교적 안정적인 분포 구조를 가진다.\n분산 간편식: \\(\\sigma^{2} = E\\lbrack(X - \\mu)^{2}\\rbrack = E(X^{2}) - \\mu^{2}\\)\n표준편차: 분산의 양의 제곱근 \\(\\sigma = \\sqrt{E\\lbrack(X - \\mu)^{2}\\rbrack}\\)\n【정리】 만약 확률변수 \\(X\\)가 유한한 분산을 가지면, 임의의 상수 \\(a\\)와 \\(b\\)에 대해 다음이 성립한다. \\(\\text{Var}(aX + b) = a^{2}\\text{Var}(X)\\).\n【왜도 정의】 확률변수 \\(X\\)의 왜도(Skewness)는 확률분포의 비대칭성을 측정하는 지표로, 평균 \\(\\mu\\)를 기준으로 데이터가 어느 방향으로 치우쳐 있는지를 나타내며 다음과 같이 정의된다.\n\\[\\text{Skewness} = E\\left\\lbrack \\left( \\frac{X - \\mu}{\\sigma} \\right)^{3} \\right\\rbrack\\]\n왜도(skewness)는 확률분포가 좌우 대칭인지, 혹은 한쪽으로 치우쳐 있는지를 나타내는 척도이다. 이는 분포의 비대칭성을 수치적으로 표현한 것으로, 분포의 꼬리가 어느 방향으로 더 길게 늘어져 있는지를 판단하는 데 사용된다.\n왜도의 값이 0이면 분포는 완전히 대칭이며, 평균과 중앙값이 일치하는 경향을 보인다. 왜도가 양수일 경우, 분포는 오른쪽 꼬리가 길어지는 형태로, 이를 우측으로 치우친 분포(positive skewed)라고 한다. 이 경우 상대적으로 작은 값들이 많이 분포하고, 드물게 큰 값들이 존재한다.\n반대로 왜도가 음수이면 분포는 왼쪽 꼬리가 길어지는 형태로, 이는 좌측으로 치우친 분포(negative skewed)를 의미한다. 이 경우 큰 값들이 상대적으로 많고, 작은 값들이 드물게 존재한다.\n왜도는 단순한 중심 경향이나 분산만으로는 파악할 수 없는 분포의 형태적 특성을 파악하게 해주며, 데이터의 구조를 이해하고 적절한 분석 방법을 선택하는 데 중요한 역할을 한다. 특히, 정규분포를 가정하는 통계 분석에서는 왜도 값을 통해 정규성 가정이 타당한지 여부를 사전에 점검할 수 있다.\n【첨도 정의】 확률변수 \\(X\\)의 첨도(Kurtosis)는 분포의 꼬리의 두터움 정도를 측정하는 지표로, 일반적으로 정규분포의 첨도 값인 3을 기준으로 비교되며, 보정된 첨도는 다음과 같이 정의된다.\n\\[\\text{Kurtosis} = E\\left\\lbrack \\left( \\frac{X - \\mu}{\\sigma} \\right)^{4} \\right\\rbrack - 3\\]\n첨도(kurtosis)는 확률분포의 꼬리가 두껍거나 얇은 정도, 즉 극단적인 값의 발생 빈도를 나타내는 척도이다. 이는 분포의 중심부와 꼬리 부분이 얼마나 뾰족하거나 평평한지를 측정함으로써, 이상치(outliers)의 존재 가능성을 파악하는 데 활용된다.\n첨도의 값이 0이면 해당 분포는 정규분포와 동일한 첨도를 가지며, 꼬리의 두께나 중심의 뾰족함이 정규분포와 유사하다는 것을 의미한다.\n첨도가 양수인 경우(leptokurtic)는 꼬리가 두껍고 중심이 뾰족한 분포를 나타낸다. 이는 평균 근처에 값들이 밀집되어 있는 반면, 극단적인 값도 상대적으로 더 자주 나타나는 분포 형태로 해석할 수 있다.\n반대로 첨도가 음수인 경우(platykurtic)는 꼬리가 얇고 중심이 평평한 분포를 의미한다. 이 경우 극단값의 발생 빈도는 적고, 데이터가 평균 주변에 상대적으로 덜 집중되어 넓게 퍼져 있는 형태를 보인다.\n첨도는 단순한 분산이나 왜도만으로는 파악하기 어려운 분포의 꼬리 특성과 극단값의 민감도를 파악하는 데 중요한 지표로, 분포의 형태를 정밀하게 분석하고 이상치의 영향을 고려한 해석을 도와준다.\n【적률 정의】 각 정수 \\(n\\)에 대해, 확률변수 \\(X\\)의 \\(n\\)차 적률(moment)은 다음과 같이 정의된다. \\(\\mu_{n}' = E\\lbrack X^{n}\\rbrack\\)\n\\(n\\)차 중심적률: \\(\\mu_{n} = E\\lbrack(X - \\mu)^{n}\\rbrack\\)\n【적률생성함수 정의】 확률변수 \\(X\\)의 누적분포함수가 주어졌을 때, \\(X\\)의 적률생성함수(moment generating function)는 다음과 같이 정의된다. \\(M_{X}(t) = E\\lbrack e^{tX}\\rbrack\\), 적률생성함수는 \\(t\\)의 함수이다.\n이는 \\(t\\)가 0의 근방에서 정의될 때 유효하다. 즉, 어떤 \\(h &gt; 0\\)가 존재하여 \\(- h &lt; t &lt; h\\)인 모든 \\(t\\)에 대해 \\(E\\lbrack e^{tX}\\rbrack\\)가 존재하면 적률생성함수가 존재한다고 한다.\n【정리】 확률변수 \\(X\\)가 적률생성함수를 가진다면, 다음이 성립한다. \\(E\\lbrack X^{n}\\rbrack = M_{X}^{(n)}(0)\\), 여기서 \\(M_{X}^{(n)}(0) = {\\frac{d^{n}}{dt^{n}}M_{X}(t)|}_{t = 0}\\)이다. 즉, n차 적률은 적률생성함수 \\(M_{X}(t)\\)의 n차 도함수를 t=0 에서 계산한 값과 같다.\n【정리】 확률변수 \\(X\\)와 \\(Y\\)가 각각 적률생성함수 \\(M_{X}\\)와 \\(M_{Y}\\)를 가지며, 이 함수들이 0을 포함하는 개구간에서 존재한다고 하자.\n\\(F_{X}(z) = F_{Y}(z)\\text{for all}z \\in \\mathbb{R}\\) 필요 충분조건은\n\\(M_{X}(t) = M_{Y}(t)\\text{for all}t \\in ( - h,h)\\text{for some}h &gt; 0\\)이다.\n이 정리는 적률생성함수(moment generating function, MGF)를 이용하여 확률분포의 동일성을 판별할 수 있는 중요한 결과를 말한다.\n즉, 두 확률변수의 적률생성함수가 어떤 열린 구간 내에서 서로 일치한다면, 이 두 확률변수는 동일한 확률분포를 따른다. 이는 적률생성함수가 존재하고, 일정한 조건을 만족할 경우, 확률분포를 완전히 특징짓는 함수라는 사실을 의미한다.\n따라서 적률생성함수는 단순히 기대값이나 분산 등 몇몇 요약 값을 계산하는 도구를 넘어, 확률분포의 고유한 형태를 정의하고 구별하는 수단으로 사용될 수 있다. 이러한 성질은 이론적 추론뿐 아니라 분포의 일치 여부를 검정하는 데에도 유용하게 활용된다.\n【예제】 감마분포 적률생성함수\n모수 \\((\\alpha,\\beta)\\)인 감마분포의 확률밀도함수는 다음과 같다.\n\\(f(x) = \\frac{1}{\\Gamma(\\alpha)\\beta^{\\alpha}}x^{\\alpha - 1}e^{- x/\\beta}, &lt; x &lt; \\infty,\\alpha &gt; 0,\\beta &gt; 0\\).\n적률생성함수: \\(M_{X}(t) = \\frac{1}{\\Gamma(\\alpha)\\beta^{\\alpha}}\\int_{0}^{\\infty}e^{tx}x^{\\alpha - 1}e^{- x/\\beta}dx\\)\n\\[= \\frac{1}{\\Gamma(\\alpha)\\beta^{\\alpha}}\\int_{0}^{\\infty}x^{\\alpha - 1}e^{- (\\frac{1}{\\beta} - t)x}dx = \\frac{1}{\\Gamma(\\alpha)\\beta^{\\alpha}}\\int_{0}^{\\infty}x^{\\alpha - 1}e^{- x/(\\frac{\\beta}{1 - \\beta t})}dx\\]\n확률밀도함수의 적분값 \\(\\int_{0}^{\\infty}\\frac{1}{\\Gamma(a)b^{a}}x^{a - 1}e^{- x/b}dx = 1\\)이므로\n\\(\\int_{0}^{\\infty}x^{a - 1}e^{- x/b}dx = \\Gamma(a)b^{a}\\)이다. 이를 이용하면\n\\[M_{X}(t) = \\frac{1}{\\Gamma(\\alpha)\\beta^{\\alpha}}\\Gamma(\\alpha)\\left( \\frac{\\beta}{1 - \\beta t} \\right)^{\\alpha} = \\left( \\frac{1}{1 - \\beta t} \\right)^{\\alpha},\\text{단,}t &lt; \\frac{1}{\\beta}\\]\n평균: \\(E\\lbrack X\\rbrack = {\\frac{d}{dt}M_{X}(t)|}_{t = 0} = {\\frac{\\alpha\\beta}{(1 - \\beta t)^{\\alpha + 1}}|}_{t = 0} = \\alpha\\beta\\)\n【예제】 이항분포 적률생성함수\n모수 \\((n,p)\\)인 이항분포의 적률생성함수는 다음과 같다.\n\\[M_{X}(t) = \\overset{n}{\\sum_{x = 0}}e^{tx}\\binom{n}{x}p^{x}(1 - p)^{n - x} = \\overset{n}{\\sum_{x = 0}}\\binom{n}{x}(pe^{t})^{x}(1 - p)^{n - x}\\]\n이항 정리에 의해 \\(\\overset{n}{\\sum_{x = 0}}\\binom{n}{x}u^{x}v^{n - x} = (u + v)^{n}\\) 성립하므로 \\(u = pe^{t},v = 1 - p\\)로 설정하면 \\(M_{X}(t) = \\lbrack pe^{t} + (1 - p)\\rbrack^{n}\\).\n\\[E\\lbrack X\\rbrack = {\\frac{d}{dt}M_{X}(t)|}_{t = 0} = {npe^{t}\\lbrack pe^{t} + (1 - p)\\rbrack^{n - 1}|}_{t = 0} = np\\]\n\n\n3. 기대값 관련 정리 및 부등식\n【정리】 \\(X\\)를 확률변수, \\(a,b,c\\)를 상수라고 하자. 또한, 기댓값이 존재하는 임의의 함수 \\(g_{1}(x)\\)와 \\(g_{2}(x)\\)에 대해 다음이 성립한다.\n\n\\(E(ag_{1}(X) + bg_{2}(X) + c) = aE\\lbrack g_{1}(X)\\rbrack + bE\\lbrack g_{2}(X)\\rbrack + c\\)\\(2)\\) 만약 \\(g_{1}(x) \\geq 0\\)이 모든 \\(x\\)에 대해 성립하면, \\(E\\lbrack g_{1}(X)\\rbrack \\geq 0\\).\n$ 만약 \\(g_{1}(x) \\geq g_{2}(x)\\) 가 성립하면, \\(E\\lbrack g_{1}(X)\\rbrack \\geq E\\lbrack g_{2}(X)\\rbrack\\).\n만약 \\(a \\leq g_{1}(x) \\leq b\\) 가 성립하면, \\(a \\leq E\\lbrack g_{1}(X)\\rbrack \\leq b\\).\n\n【정리】 \\(X\\)를 확률변수라고 하고, \\(m\\)을 양의 정수라고 하자. 만약 \\(E\\lbrack X^{m}\\rbrack\\)이 존재한다고 가정하면, \\(k\\)가 양의 정수이고 \\(k \\leq m\\)일 때, \\(E\\lbrack X^{k}\\rbrack\\) 도 존재한다.\n(Markov’s Inequality) \\(u(X)\\)를 확률변수 \\(X\\)의 비음수 함수라고 하자. 만약 \\(E\\lbrack u(X)\\rbrack\\)가 존재하면, 모든 양의 상수 \\(c\\)에 대해 다음이 성립한다. \\(P\\lbrack u(X) \\geq c\\rbrack \\leq \\frac{E\\lbrack u(X)\\rbrack}{c}\\).\n(Chebyshev’s Inequality) 확률변수 \\(X\\)가 유한한 분산 \\(\\sigma^{2}\\)를 가지는 확률분포를 따른다고 가정하자. 그러면, 모든 \\(k &gt; 0\\)에 대해 다음이 성립한다. \\(P(|X - \\mu| &lt; k\\sigma) \\geq 1 - \\frac{1}{k^{2}}\\)."
  },
  {
    "objectID": "notes/mldl_method/mldlmethod_introduction.html",
    "href": "notes/mldl_method/mldlmethod_introduction.html",
    "title": "MLDL 방법론 1. 서론",
    "section": "",
    "text": "Chapter 1. 표본, 모집단 그리고 일반화\n\n1. 모집단\n통계적 학습은 모집단에 대한 가정을 출발점으로 한다. 모집단은 분석의 대상이 되는 전체를 의미하지만, 통계적 학습에서는 이를 단순한 개체들의 집합이 아니라 확률적 생성 구조로 이해한다. 즉, 설명변수와 반응변수는 일정한 확률 규칙에 따라 생성되며, 이 확률 규칙이 모집단을 규정한다.\n통계적 학습에서는 설명변수 X와 반응변수 Y가 어떤 결합확률분포를 따른다고 가정한다.\n\\[(X,Y) \\sim P_{XY}\\]\n이 분포는 직접 관측할 수 없으며, 오직 표본을 통해 간접적으로 추론할 수 있다. 회귀, 분류, 머신러닝, 딥러닝을 포함한 모든 학습 방법은 이 보이지 않는 확률 구조를 요약하거나 근사하는 것을 목표로 한다.\n모집단을 확률분포로 이해한다는 관점은 이후에 등장하는 모든 학습 문제를 하나의 공통된 틀 안에서 다루게 해준다.\n\n\n2. 표본\n현실에서는 모집단 전체를 관측할 수 없기 때문에, 우리는 모집단으로부터 추출된 유한한 개수의 관측값을 사용한다. 이 관측값들의 집합을 표본이라고 한다. 표본은 모집단 분포로부터 우연히 생성된 결과로 간주되며, 다음과 같은 가정을 바탕으로 한다.\n\\[(x_{1},y_{1}),(x_{2},y_{2}),\\ldots,(x_{n},y_{n})\\overset{iid}{\\sim}P_{XY}\\]\n이 가정은 통계적 학습 이론의 기본 전제이며, 이후에 등장하는 추론, 예측, 검증 절차의 이론적 기반이 된다. 표본의 크기와 구성은 학습 결과의 안정성과 신뢰성에 직접적인 영향을 미친다. 표본이 작을수록 우연 변동의 영향은 커지며, 동일한 방법을 적용하더라도 결과가 크게 달라질 수 있다.\n중요한 점은, 표본이 아무리 많아지더라도 모집단 그 자체가 되지는 않는다는 사실이다. 데이터 분석에서 발생하는 불확실성은 대부분 이 표본과 모집단 사이의 간극에서 비롯된다.\n\n\n3. 일반화\n통계적 학습의 목적은 표본을 가능한 한 잘 설명하는 데 있지 않다. 학습의 궁극적인 목표는 표본을 통해 학습한 규칙이나 모형이 아직 관측하지 않은 새로운 데이터에 대해서도 잘 작동하도록 만드는 데 있다. 이를 일반화라고 한다.\n학습 알고리즘은 입력 변수 X로부터 출력 변수 Y를 예측하는 함수 f(X)를 학습한다. 훈련 데이터에서 계산되는 평균 손실은 다음과 같다.\n\\[\\frac{1}{n}\\overset{n}{\\sum_{i = 1}}L(y_{i},f(x_{i}))\\]\n이 값은 모델이 이미 관측한 데이터에 얼마나 잘 맞는지를 보여주는 지표이다. 그러나 우리가 실제로 관심을 가지는 것은 모집단 전체에서의 평균적인 성능이다. 이는 다음과 같은 기댓값으로 표현된다.\n\\[\\mathbb{E}_{(X,Y) \\sim P_{XY}}\\lbrack L(Y,f(X))\\rbrack\\]\n훈련 데이터에서의 성능과 모집단에서의 성능은 일반적으로 일치하지 않는다. 표본에 지나치게 맞춰진 모델은 새로운 데이터에 대해 성능이 급격히 저하될 수 있으며, 이를 과적합이라고 한다. 일반화는 이러한 위험을 인식하고 통제하는 개념이다.\n\n\n4. 모집단에 대한 관심의 차이: 통계, 머신러닝, 딥러닝\n모집단을 확률적 생성 구조로 이해한다는 점에서는 통계학, 머신러닝, 딥러닝이 동일한 출발점을 공유한다. 그러나 이들 방법론은 모집단의 무엇에 관심을 두는지에 따라 서로 다른 방향으로 발전해 왔다.\n전통적인 통계학은 모집단 분포의 특정한 요약량에 주된 관심을 둔다. 평균, 분산, 회귀계수와 같은 모수는 모집단 구조를 간결하게 설명하는 핵심 요소로 간주된다. 통계적 추론의 목적은 이러한 모수를 추정하고, 그 불확실성을 정량화하는 데 있다. 이 관점에서 모델은 모집단을 설명하기 위한 수단이며, 해석 가능성이 중요한 위치를 차지한다.\n머신러닝은 모집단 분포 자체의 해석보다는, 모집단으로부터 생성될 새로운 관측값에 대해 얼마나 정확한 예측을 할 수 있는지에 더 큰 관심을 둔다. 즉, 조건부 분포의 구조를 직접 해석하기보다는 예측 오차를 최소화하는 함수 f(X)를 찾는 데 초점을 맞춘다. 이 과정에서 일반화 성능은 핵심 평가 기준이 된다.\n딥러닝은 머신러닝의 이러한 관점을 더욱 확장한 접근법이다. 딥러닝 모델은 모집단 분포에 대한 명시적 가정보다는, 고차원 입력 공간에서의 복잡한 함수 관계를 직접 근사하는 데 집중한다. 이때 주요 관심사는 해석 가능한 모수보다는 표현 학습과 대규모 데이터 환경에서의 예측 성능이다.\n요약하면, 통계학은 모집단의 구조와 모수에, 머신러닝은 모집단에서의 예측 성능에, 딥러닝은 고차원 데이터에서의 표현과 함수 근사 능력에 더 큰 비중을 둔다. 이러한 관점의 차이는 이후 장들에서 각 방법론의 장점과 한계를 이해하는 데 중요한 기준이 된다.\n\n\n\nChapter 2. 손실함수와 위험\n통계적 학습에서 ”모형이 얼마나 잘 작동하는가”라는 질문은 반드시 수식으로 정의되어야 한다. 그렇지 않으면 서로 다른 방법을 비교하거나, 학습의 목표를 명확히 설정할 수 없다. 손실함수와 위험은 이러한 질문에 체계적으로 답하기 위해 도입되는 개념이다. 이 절에서는 모든 학습 문제를 하나의 공통된 최소화 문제로 이해하는 관점을 소개한다.\n\n1. 손실함수\n손실함수는 예측값과 실제값 사이의 차이를 수치로 표현하는 함수이다. 학습 알고리즘은 이 손실을 기준으로 자신의 성능을 판단하며, 손실이 작을수록 더 좋은 예측을 수행한다고 간주한다.\n입력 변수 X에 대해 예측 함수 f(X)를 사용한다고 하자. 관측된 실제 값이 Y일 때, 손실함수는 다음과 같은 형태를 가진다.\n\\[L(Y,f(X))\\]\n손실함수의 선택은 학습 문제의 성격에 따라 달라진다. 회귀 문제에서는 제곱오차 손실이나 절대오차 손실이 자주 사용되며, 분류 문제에서는 0–1 손실이나 로그 손실이 사용된다. 중요한 점은 손실함수가 단순한 계산 도구가 아니라, 무엇을 ”잘 맞춘다”고 볼 것인가에 대한 기준을 명확히 규정한다는 것이다.\n예를 들어 제곱오차 손실은 큰 오차에 더 큰 벌점을 부여하며, 이는 평균적인 성능을 중시하는 관점을 반영한다. 반면 절대오차 손실은 이상치의 영향을 상대적으로 덜 받는다. 이처럼 손실함수는 학습의 철학을 수식으로 표현한 것이라고 볼 수 있다.\n\n\n2. 경험적 위험\n모집단 전체에 대해 손실을 계산하는 것은 현실적으로 불가능하다. 우리가 실제로 사용할 수 있는 정보는 유한한 표본 뿐이다. 따라서 통계적 학습에서는 표본을 이용해 손실의 평균을 계산하고, 이를 최소화하는 방향으로 학습을 수행한다.\n표본 \\((x_{1},y_{1}),\\ldots,(x_{n},y_{n})\\)이 주어졌을 때, 평균 손실은 다음과 같이 정의된다.\n\\[\\widehat{R}(f) = \\frac{1}{n}\\overset{n}{\\sum_{i = 1}}L(y_{i},f(x_{i}))\\]\n이 값을 경험적 위험이라고 한다. 경험적 위험은 관측된 데이터에 기반해 계산된 손실의 평균이며, 학습 알고리즘이 직접적으로 최소화하는 대상이다. 선형회귀의 최소제곱법, 로지스틱 회귀의 최대우도추정, 신경망 학습에서의 경사하강법은 모두 경험적 위험 최소화의 구체적인 구현이다.\n그러나 경험적 위험이 작다고 해서 반드시 좋은 모델이라고 말할 수는 없다. 경험적 위험은 표본에 대한 성능만을 반영하며, 표본에 지나치게 맞춰진 모델은 새로운 데이터에 대해 나쁜 성능을 보일 수 있다. 이 지점에서 일반화 문제가 다시 등장한다.\n\n\n3. 모집단 위험\n우리가 궁극적으로 관심을 가지는 것은 표본이 아니라 모집단 전체에서의 평균적인 성능이다. 이를 모집단 위험이라고 하며, 다음과 같이 정의된다.\n\\[R(f) = \\mathbb{E}_{(X,Y) \\sim P_{XY}}\\lbrack L(Y,f(X))\\rbrack\\]\n모집단 위험은 예측 함수 f가 모집단 분포로부터 생성될 임의의 관측값에 대해 얼마나 잘 작동하는지를 나타내는 척도이다. 이상적으로는 이 값을 직접 최소화하는 것이 목표이지만, 모집단 분포는 알 수 없기 때문에 모집단 위험을 직접 계산할 수는 없다.\n통계적 학습의 핵심 문제는 경험적 위험과 모집단 위험 사이의 관계에 있다. 표본의 크기가 충분히 크고, 모델의 복잡도가 적절히 통제된다면 경험적 위험은 모집단 위험의 좋은 근사값이 될 수 있다. 반대로 표본이 작거나 모델이 지나치게 복잡하면 두 값 사이의 차이는 커질 수 있다.\n이러한 관점에서 통계적 학습은 다음과 같은 문제로 요약된다. 경험적 위험을 최소화하면서도, 모집단 위험이 함께 작아지도록 학습 과정을 설계하는 것이다. 이후 절에서 다룰 규제, 재표본 방법, 검증 기법들은 모두 이 목표를 달성하기 위한 장치들이다.\n\n\n\nChapter 3. Resampling과 일반화 성능 추정\n앞 절에서 살펴본 바와 같이, 통계적 학습에서 우리가 직접 계산할 수 있는 것은 경험적 위험이며, 실제로 관심을 가지는 대상은 모집단 위험이다. 문제는 모집단 분포를 알 수 없기 때문에 모집단 위험을 직접 계산할 수 없다는 점이다. 여기서는 이러한 한계를 극복하기 위해 사용되는 방법인 재표본과 일반화 성능 추정의 개념을 다룬다.\n\n1. 재표본 개념\n재표본 방법(resampling methods)은 현대 통계학과 통계적 학습에서 핵심적인 역할을 하는 도구이다. 재표본 방법이란, 하나의 학습 데이터로부터 반복적으로 표본을 추출하고, 각 표본마다 동일한 통계적 모형을 다시 적합함으로써 이미 적합된 모형에 대해 추가적인 정보를 얻는 방법을 의미한다. 이러한 절차를 통해 단일 표본에 기반한 분석만으로는 파악하기 어려운 모형의 성질을 보다 체계적으로 이해할 수 있다.\n예를 들어, 선형회귀 모형의 적합 결과가 얼마나 안정적인지를 평가하고자 할 때, 학습 데이터로부터 서로 다른 표본을 반복적으로 추출한 뒤 각 표본에 대해 선형회귀 모형을 적합할 수 있다. 이후 각 표본에서 얻어진 회귀 계수나 예측 성능이 서로 어느 정도 차이를 보이는지를 분석함으로써, 추정치의 변동성과 불확실성을 평가할 수 있다. 이러한 접근은 원래의 학습 표본을 한 번만 사용하여 모형을 적합하는 경우에는 얻기 어려운 정보를 제공한다.\n재표본 방법은 동일한 통계적 방법을 서로 다른 데이터 부분집합에 대해 여러 번 반복 적용해야 하므로 계산 비용이 큰 방법으로 인식되기도 한다. 그러나 최근 계산 자원의 발전으로 인해, 재표본 방법에 필요한 계산량은 대부분의 응용 문제에서 실질적인 제약이 되지 않는다. 이에 따라 재표본 방법은 이론적 논의에 머무르지 않고, 실제 데이터 분석과 통계적 학습 문제에서 널리 활용되고 있다.\n가장 단순한 재표본 방법은 주어진 표본을 두 부분으로 나누는 방식이다. 일반적으로 하나는 학습용 데이터로, 다른 하나는 평가용 데이터로 사용되며, 이를 hold-out 방법이라고 한다. 이 방법은 구현이 간단하고 계산 비용이 적다는 장점이 있지만, 데이터 분할 방식에 따라 평가 결과가 크게 달라질 수 있다는 한계를 가진다. 특히 표본의 크기가 작은 경우에는 우연한 분할로 인해 일반화 성능이 과대 또는 과소 평가될 위험이 있다.\n본 장에서는 재표본 방법 가운데 가장 널리 사용되는 두 가지 방법인 교차검증과 부트스트랩을 다룬다. 이 두 방법은 다양한 통계적 학습 절차를 실제로 적용하는 과정에서 중요한 도구로 활용된다. 교차검증은 특정 통계적 학습 방법의 일반화 성능, 즉 테스트 오차를 추정하는 데 사용되며, 동시에 모형의 적절한 복잡도나 유연성 수준을 선택하는 데에도 활용된다. 모형의 예측 성능을 평가하는 과정을 모형 평가라고 하며, 여러 후보 모형 가운데 적절한 모형을 선택하는 과정을 모형 선택이라고 한다.\n한편, 부트스트랩은 여러 상황에서 활용되는 재표본 방법으로, 특히 모수 추정치의 정확도와 변동성을 평가하는 데 자주 사용된다. 또한 특정 통계적 학습 방법이 표본의 변동에 얼마나 민감한지를 파악하는 데에도 유용하다. 이처럼 재표본 방법은 일반화 성능의 평가와 추정의 불확실성 분석을 연결하는 중요한 역할을 수행한다.\n\n\n2. 일반화 성능 추정의 필요성\n학습된 모형의 성능을 평가하는 가장 단순한 방법은 훈련 데이터에서의 평균 손실을 계산하는 것이다. 이 값은 모형이 주어진 학습 데이터에 얼마나 잘 적합되었는지를 보여주는 지표로서 의미를 가진다. 그러나 훈련 데이터에서의 성능이 우수하다는 사실만으로, 해당 모형이 새로운 데이터에 대해서도 동일한 수준의 성능을 보일 것이라고 기대할 수는 없다.\n특히 모형의 복잡도가 증가할수록 이러한 문제는 더욱 두드러진다. 복잡한 모형은 훈련 데이터의 패턴을 정교하게 따라갈 수 있기 때문에 훈련 오차를 매우 작게 만들 수 있다. 그러나 이 과정에서 표본에 포함된 우연한 변동까지 함께 학습하게 되면, 새로운 데이터에 대해서는 오히려 예측 성능이 저하될 수 있다. 이와 같은 현상은 통계적 학습에서 과적합으로 알려져 있으며, 일반화 성능을 평가할 필요성을 분명하게 보여준다.\n통계적 학습의 관점에서 중요한 것은 훈련 데이터에 대한 적합도가 아니라, 아직 관측되지 않은 데이터에 대한 평균적인 성능이다. 즉, 학습된 모형이 모집단으로부터 새롭게 생성될 관측값에 대해 얼마나 안정적으로 작동하는지를 평가해야 한다. 이를 일반화 성능 추정이라고 한다.\n이상적으로는 훈련 데이터와는 독립적인 새로운 데이터를 이용하여 모형의 성능을 평가하는 것이 바람직하다. 그러나 현실적인 분석 상황에서는 이러한 별도의 데이터셋을 확보하기가 쉽지 않다. 따라서 통계적 학습에서는 하나의 표본을 여러 방식으로 나누거나 반복적으로 재구성하여, 새로운 데이터가 주어졌을 때의 상황을 간접적으로 모의하는 방법을 사용한다.\n이러한 접근법이 바로 재표본 방법이며, 재표본은 일반화 성능을 추정하기 위한 실질적인 도구로 사용된다. 즉, 재표본 방법은 단순한 계산 기법이 아니라, 훈련 오차와 모집단 성능 사이의 간극을 줄이기 위한 핵심적인 방법론이다.\n\n\n3. 재표본의 기본 아이디어\n재표본 방법의 핵심 아이디어는 하나의 표본을 고정된 데이터셋으로 취급하지 않고, 모집단으로부터 추출될 수 있는 여러 가능한 표본 가운데 하나로 간주하는 데 있다. 즉, 현재 관측된 데이터는 모집단으로부터 우연히 생성된 결과이며, 동일한 모집단에서라면 다른 표본이 관측될 수도 있었음을 전제로 한다.\n이러한 관점에서 재표본 방법은 주어진 데이터셋을 여러 방식으로 나누거나, 혹은 중복을 허용하여 다시 추출함으로써, 새로운 데이터가 관측되는 상황을 간접적으로 모의한다. 이를 통해 하나의 표본만을 사용했을 때는 관찰하기 어려운 학습 결과의 변동성과 불확실성을 평가할 수 있다.\n재표본 과정에서는 일반적으로 데이터의 일부를 모형 학습에 사용하고, 나머지를 모형 평가에 사용한다. 학습에 사용되지 않은 데이터는 훈련 과정에 관여하지 않기 때문에, 새로운 데이터에 대한 관측값과 유사한 역할을 한다. 이 평가 데이터에서 계산된 손실은, 모형이 실제로 새로운 데이터에 대해 보일 성능을 간접적으로 반영한다.\n중요한 점은, 재표본 방법이 새로운 정보를 인위적으로 만들어내는 것이 아니라, 동일한 표본을 서로 다른 역할로 반복적으로 활용함으로써 일반화 성능을 추정한다는 점이다. 이러한 절차를 통해 통계적 학습은 훈련 오차와 모집단 성능 사이의 간극을 보다 현실적으로 이해할 수 있게 된다.\n이 기본 아이디어를 구체화한 것이 이후에 다룰 hold-out 방법, 교차검증, 그리고 부트스트랩이다. 각 방법은 재표본을 구성하는 방식과 평가 전략에서 차이를 보이지만, 모두 일반화 성능을 추정하기 위한 공통된 목적을 공유한다.\n\n\n\nChapter 4. 재표본 방법\n\n1. 훈련 오차, 테스트 오차 그리고 재표본\n회귀 문제를 중심으로 다루어진 편향–분산 트레이드오프와 같은 핵심 개념은 분류 문제에도 거의 그대로 적용된다. 분류 문제에서 차이가 있다면 반응변수가 연속형이 아니라 범주형이라는 점이며, 이에 따라 성능을 측정하는 방식이 오차율(error rate)의 형태로 정의된다는 점이다.\n훈련 관측값 \\(\\{(x_{1},y_{1}),\\ldots,(x_{n},y_{n})\\}\\)을 이용하여 분류기 \\(\\widehat{f}\\)를 학습했다고 하자. 이때 분류기의 성능을 가장 간단하게 평가하는 방법은 훈련 데이터에 대해 발생한 오분류의 비율을 계산하는 것이다. 이를 훈련 오차율(training error rate)이라 하며, 다음과 같이 정의된다.\n\\[\\frac{1}{n}\\overset{n}{\\sum_{i = 1}}I(y_{i} \\neq {\\widehat{y}}_{i})\\]\n여기서 \\({\\widehat{y}}_{i}\\)는 i번째 관측값에 대해 분류기가 예측한 범주 레이블이고,\n\\(I(y_{i} \\neq {\\widehat{y}}_{i})\\)는 실제 값과 예측 값이 다를 경우 1, 같을 경우 0을 취하는 지시변수이다. 따라서 훈련 오차율은 학습에 사용된 데이터 중 잘못 분류된 관측값의 비율을 의미한다.\n훈련 오차율은 계산이 간단하고 항상 구할 수 있다는 장점이 있다. 그러나 통계적 학습의 관점에서 훈련 오차는 모형의 실제 성능을 평가하는 데 적합한 지표라고 보기는 어렵다. 이는 훈련 오차가 이미 관측한 데이터에 대해 계산된 값이기 때문이다. 특히 모형의 복잡도가 높아질수록, 모형은 데이터의 일반적인 구조뿐 아니라 표본에 포함된 우연한 변동까지 학습할 수 있으며, 그 결과 훈련 오차는 지나치게 작아질 수 있다.\n우리가 실제로 관심을 가지는 것은 학습에 사용되지 않은 새로운 관측값에 대해 분류기가 얼마나 잘 작동하는가이다. 이러한 관점에서 정의되는 성능 지표가 테스트 오차율(test error rate)이다. 테스트 관측값 \\((x_{0},y_{0})\\)에 대해 테스트 오차율은 다음과 같이 정의된다.\n\\[\\text{Ave}(I(y_{0} \\neq {\\widehat{y}}_{0}))\\]\n여기서 \\({\\widehat{y}}_{0}\\)는 새로운 입력 \\(x_{0}\\)에 대해 분류기가 예측한 범주 레이블이다. 좋은 분류기란 훈련 오차가 작은 분류기가 아니라, 테스트 오차가 작은 분류기이다. 즉, 통계적 학습의 궁극적인 목표는 훈련 데이터에 대한 적합도를 높이는 것이 아니라, 새로운 데이터에 대한 오차를 최소화하는 데 있다.\n문제는 테스트 오차율이 정의상 학습에 사용되지 않은 데이터에 대해 계산되는 값이라는 점이다. 모집단 분포를 알 수 없는 현실적인 상황에서는 충분히 큰 테스트 데이터셋을 확보하기가 어렵고, 따라서 테스트 오차를 직접 계산하는 것은 대부분의 경우 불가능하다. 이로 인해 훈련 오차와 테스트 오차 사이에는 본질적인 간극이 존재한다.\n이 간극을 메우기 위해 등장한 방법이 재표본(resampling) 방법이다. 재표본 방법은 훈련 데이터의 일부를 의도적으로 학습 과정에서 제외한 뒤, 이를 새로운 데이터인 것처럼 사용하여 오차를 계산한다. 이렇게 계산된 오차는 훈련 오차보다 덜 낙관적이며, 테스트 오차에 대한 추정치로 해석될 수 있다.\n요약하면, 훈련 오차는 항상 계산 가능하지만 일반화 성능을 과대평가하는 경향이 있고, 테스트 오차는 우리가 진정으로 최소화하고자 하는 대상이지만 직접 계산할 수 없다. 재표본 방법은 이 둘 사이의 간극을 체계적으로 연결함으로써, 유한한 표본만을 이용해 일반화 성능을 평가할 수 있게 해주는 통계적 학습의 핵심 도구이다.\n\n\n2. 교차검증\n\n(1) 개념\n테스트 오차는 통계적 학습 방법을 사용하여 새로운 관측값, 즉 모형 학습에 사용되지 않은 데이터에 대한 반응변수를 예측할 때 발생하는 평균적인 오차를 의미한다. 주어진 데이터셋에 대해 특정 통계적 학습 방법을 사용하는 것이 타당하다고 판단되기 위해서는 테스트 오차가 작아야 한다.\n만약 별도의 테스트 데이터셋이 존재한다면, 테스트 오차는 비교적 쉽게 계산할 수 있다. 그러나 현실적인 분석 상황에서는 이러한 테스트 데이터가 충분히 확보되는 경우가 드물다. 반면, 훈련 오차는 학습에 사용된 관측값에 동일한 통계적 학습 방법을 적용함으로써 쉽게 계산할 수 있다. 하지만 훈련 오차는 테스트 오차와 상당히 다를 수 있으며, 특히 테스트 오차를 크게 과소추정하는 경향이 있다.\n충분히 큰 테스트 데이터셋이 없어 테스트 오차를 직접 추정할 수 없는 경우, 이용 가능한 훈련 데이터를 활용하여 테스트 오차를 추정하는 다양한 방법들이 사용된다. 본 절에서는 학습 과정에서 일부 관측값을 의도적으로 제외한 뒤, 해당 관측값에 대해 모형을 적용함으로써 테스트 오차를 추정하는 방법들을 살펴본다.\n\n\n(2) 검증 세트 방법 (The Validation Set Approach)\n특정 통계적 학습 방법을 하나의 데이터셋에 적용할 때 발생하는 테스트 오차를 추정하고자 한다고 가정하자.\n검증 세트 방법은 사용 가능한 관측값들을 무작위로 두 부분으로 나누는 절차를 포함한다. 하나는 학습용 데이터셋(training set)이고, 다른 하나는 검증 데이터셋(validation set), 또는 홀드아웃 세트(hold-out set)이다. 모형은 학습 데이터셋을 이용해 적합되며, 적합된 모형은 검증 데이터셋에 포함된 관측값들의 반응변수를 예측하는 데 사용된다. 이때 계산된 검증 세트 오차율은, 연속형 반응변수의 경우 일반적으로 평균제곱오차(MSE)를 사용하여 측정되며, 테스트 오차에 대한 추정치를 제공한다.\n\n\n\n\n\n예제 자료 Auto 데이터셋에 적용하면, mpg와 horsepower 사이의 비선형적 관계를 고려할 때 단순 선형 회귀보다 다항 회귀 모형이 더 적절한지 평가할 수 있다. 이를 위해 전체 392개의 관측값을 무작위로 학습 세트와 검증 세트로 나눈 뒤, 다양한 차수의 다항 회귀 모형을 학습 데이터에 적합하고 검증 데이터에서 평균제곱오차로 성능을 비교한다. 그 결과, 이차 다항 모형은 선형 모형에 비해 검증 세트 MSE가 크게 감소하였으나, 삼차 이상의 다항항을 추가해도 예측 성능은 더 이상 개선되지 않는 것으로 나타났다.\n한편, 검증 세트 방법은 데이터 분할 방식에 따라 성능 추정 결과가 달라질 수 있다는 한계를 가진다. 서로 다른 무작위 분할을 반복하면 각 경우마다 검증 세트 MSE가 달라지며, 어떤 모형이 최적이라고 단정하기는 어렵다. 그럼에도 불구하고 반복된 결과를 종합하면, 선형 모형이 이 데이터에 적합하지 않다는 점만큼은 일관되게 확인된다.\n검증 세트 방법은 개념적으로 단순하고 구현이 쉽다는 장점을 지닌다. 그러나 두 가지 잠재적인 단점이 존재한다.\n첫째, 검증 세트 오차 추정치는 어떤 관측값이 학습 데이터에 포함되고 어떤 관측값이 검증 데이터에 포함되는지에 따라 크게 달라질 수 있다.\n둘째, 검증 세트 방법에서는 전체 데이터 중 일부만을 사용하여 모형을 학습한다. 일반적으로 통계적 방법은 더 많은 관측값을 사용하여 학습할수록 성능이 향상되므로, 검증 세트 오차는 전체 데이터를 사용하여 학습한 모형의 테스트 오차를 과대추정하는 경향이 있다.\n\n\n(3) 하나를 제외한 교차검증 (Leave-One-Out Cross-Validation)\n\n\n\n\n\n검증 세트 방법과 마찬가지로, LOOCV 역시 전체 관측값을 두 부분으로 나누어 사용한다. 그러나 검증 세트 방법에서 두 부분이 비교적 비슷한 크기를 갖는 것과 달리, LOOCV에서는 단 하나의 관측값 \\((x_{1},y_{1})\\)만을 검증 데이터로 사용하고, 나머지 n-1개의 관측값 \\(\\{(x_{2},y_{2}),\\ldots,(x_{n},y_{n})\\}\\)을 학습 데이터로 사용한다. 통계적 학습 방법은 이 n-1개의 관측값에 대해 적합되며, 제외된 관측값에 대해 예측값 \\({\\widehat{y}}_{1}\\)이 계산된다.\n이때 \\((x_{1},y_{1})\\)은 학습 과정에 사용되지 않았으므로, \\(\\text{MSE}_{1} = (y_{1} - {\\widehat{y}}_{1})^{2}\\)은 테스트 오차에 대한 거의 비편향(unbiased) 추정값으로 해석할 수 있다. 그러나 \\(\\text{MSE}_{1}\\)은 단 하나의 관측값에 기반한 값이므로 변동성이 매우 크며, 그 자체로는 신뢰할 만한 추정치라고 보기 어렵다.\n이 과정을 두 번째 관측값 \\((x_{2},y_{2})\\)에 대해서도 반복할 수 있다. 즉, \\((x_{2},y_{2})\\)를 검증 데이터로 두고, 나머지 n-1개의 관측값 \\(\\{(x_{1},y_{1}),(x_{3},y_{3}),\\ldots,(x_{n},y_{n})\\}\\)을 학습 데이터로 사용하여 모형을 적합한 뒤, \\(\\text{MSE}_{2} = (y_{2} - {\\widehat{y}}_{2})^{2}\\)를 계산한다. 이와 같은 절차를 모든 관측값에 대해 반복하면, 총 n개의 제곱오차 \\(\\text{MSE}_{1},\\ldots,\\text{MSE}_{n}\\)를 얻게 된다.\nLOOCV에서의 테스트 평균제곱오차 추정치는 이 값들의 평균으로 정의된다.\n\\[\\text{CV}_{(n)} = \\frac{1}{n}\\overset{n}{\\sum_{i = 1}}\\text{MSE}_{i}\\]\nLOOCV는 검증 세트 방법에 비해 몇 가지 중요한 장점을 가진다. 첫째, 편향이 훨씬 작다. LOOCV에서는 매번 n-1개의 관측값을 사용하여 모형을 학습하므로, 이는 전체 데이터셋을 사용한 경우와 거의 동일한 규모의 학습에 해당한다. 반면 검증 세트 방법에서는 학습 데이터가 전체 데이터의 절반 정도로 줄어드는 경우가 많아, 테스트 오차를 과대추정하는 경향이 있다. 이러한 이유로 LOOCV는 검증 세트 방법보다 테스트 오차에 대한 편향이 훨씬 작다.\n둘째, LOOCV는 반복 수행 시 항상 동일한 결과를 제공한다. 검증 세트 방법은 학습/검증 데이터 분할이 무작위로 이루어지기 때문에, 분할 방식에 따라 결과가 달라질 수 있다. 반면 LOOCV에서는 각 관측값이 정확히 한 번씩 검증 데이터로 사용되므로, 분할에 따른 임의성이 존재하지 않는다.\nLOOCV의 단점은 계산 비용이 크다는 점이다. LOOCV에서는 모형을 총 n번 적합해야 하므로, 관측값의 수가 크거나 각 모형의 적합에 많은 시간이 소요되는 경우 계산 부담이 매우 커질 수 있다.\n그러나 최소제곱 선형 회귀나 다항 회귀의 경우에는 놀라운 계산적 단축 방법이 존재하여, LOOCV의 계산 비용을 단일 모형 적합과 거의 동일한 수준으로 줄일 수 있다. 이 경우 다음 식이 성립한다.\n\\(\\text{CV}_{(n)} = \\frac{1}{n}\\overset{n}{\\sum_{i = 1}}\\left( \\frac{y_{i} - {\\widehat{y}}_{i}}{1 - h_{i}} \\right)^{2}\\), 여기서 \\({\\widehat{y}}_{i}\\)는 전체 데이터에 대해 적합한 최소제곱 회귀모형에서의 i번째 적합값이며, \\(h_{i}\\)는 레버리지(leverage) 값이다. 레버리지는 1/n과 1 사이의 값을 가지며, 특정 관측값이 자기 자신의 적합값에 얼마나 큰 영향을 미치는지를 나타낸다. 이 식에서는 레버리지가 큰 관측값에 대한 잔차가 적절히 보정되어 반영된다.\nLOOCV는 매우 일반적인 방법으로, 로지스틱 회귀나 선형 판별분석 등 다양한 예측 모형에 적용할 수 있다.\n\n\n(4) k-겹 교차검증 (k-Fold Cross-Validation)\n\n\n\n\n\nLOOCV의 대안으로 k-겹 교차검증(k-fold CV)이 있다. 이 방법은 전체 관측값 집합을 크기가 거의 같은 k개의 그룹, 즉 폴드(fold)로 무작위 분할하는 절차를 포함한다. 첫 번째 폴드를 검증 세트로 사용하고, 나머지 k-1개의 폴드를 학습 세트로 사용하여 통계적 학습 방법을 적합한다. 이후 제외된 폴드에 포함된 관측값들에 대해 평균제곱오차 \\(\\text{MSE}_{1}\\)를 계산한다.\n이 절차를 총 k번 반복하며, 매번 서로 다른 폴드가 검증 세트로 사용된다. 그 결과 테스트 오차에 대한 k개의 추정값 \\(\\text{MSE}_{1},\\text{MSE}_{2},\\ldots,\\text{MSE}_{k}\\)를 얻게 된다. k-겹 교차검증에 의한 테스트 오차 추정치는 이 값들의 평균으로 정의된다.\n\\[\\text{CV}_{(k)} = \\frac{1}{k}\\overset{k}{\\sum_{i = 1}}\\text{MSE}_{i}\\]\nLOOCV는 k=n인 k-겹 교차검증의 특수한 경우로 볼 수 있다. 실제 응용에서는 보통 k=5 또는 k=10을 사용한다. 그렇다면 k=n 대신 k=5나 k=10을 사용하는 장점은 무엇일까? 가장 분명한 장점은 계산 효율성이다. LOOCV는 학습 방법을 총 n번 적합해야 하므로, 계산 비용이 매우 클 수 있다. 반면 10-겹 교차검증은 학습을 10번만 수행하면 되므로, 훨씬 현실적인 계산 비용으로 적용할 수 있다.\n교차검증을 수행하는 목적은 상황에 따라 다를 수 있다. 어떤 경우에는 독립적인 데이터에 대해 해당 학습 방법이 얼마나 잘 작동할지를 평가하기 위해 테스트 MSE의 정확한 추정치에 관심을 가질 수 있다. 그러나 다른 경우에는 여러 학습 방법이나 동일한 방법의 서로 다른 유연성 수준을 비교하여, 테스트 오차를 최소화하는 설정을 선택하는 것이 목적일 수 있다. 이 경우에는 추정된 테스트 MSE의 정확한 값보다는, 테스트 MSE 곡선에서 최소점이 위치하는 지점이 더 중요하다.\n교차검증 곡선이 때로는 참된 테스트 MSE를 과소 또는 과대추정하더라도, 대부분의 경우 테스트 MSE를 최소화하는 유연성 수준을 비교적 정확하게 식별해 준다. 이는 교차검증이 모형 선택을 위한 도구로서 매우 효과적임을 의미한다.\nk-겹 교차검증에서의 편향–분산 절충\nk&lt;n인 k-겹 교차검증이 LOOCV에 비해 계산상 유리하다는 점을 언급하였다. 그러나 계산 비용을 제외하더라도, k-겹 교차검증이 LOOCV보다 테스트 오차율을 더 정확하게 추정하는 경우가 많다는 점은 덜 직관적이지만 잠재적으로 더 중요한 장점이다. 이는 k-겹 교차검증에서 나타나는 편향–분산 절충과 관련이 있다.\nLOOCV에서는 각 학습 세트가 n-1개의 관측값을 포함하므로, 전체 데이터셋의 크기와 거의 동일한 규모로 학습이 이루어진다. 따라서 LOOCV는 테스트 오차에 대해 거의 비편향에 가까운 추정치를 제공한다. 반면, k=5 또는 k=10과 같은 k-겹 교차검증에서는 각 학습 세트가 대략 (k-1)n/k개의 관측값을 포함하므로, LOOCV보다는 적지만 검증 세트 방법보다는 훨씬 많은 데이터를 사용하게 된다. 이로 인해 k-겹 교차검증은 편향의 관점에서 중간 정도의 특성을 갖는다. 따라서 편향을 최소화하는 측면에서는 LOOCV가 k-겹 교차검증보다 유리하다고 할 수 있다.\n그러나 오차 추정에서 편향만이 유일한 고려 요소는 아니다. 분산 또한 중요한 역할을 한다. 실제로 LOOCV는 k&lt;n인 k-겹 교차검증보다 더 큰 분산을 가지는 것으로 알려져 있다. 그 이유는 LOOCV에서 평균을 내는 대상이 되는 n개의 모형들이 거의 동일한 학습 데이터를 사용하여 적합되기 때문이다. 이 경우 각 모형의 결과는 서로 강한 양의 상관관계를 가지게 되며, 이러한 상관된 값들의 평균은 분산이 크게 나타난다.\n반면, k&lt;n인 k-겹 교차검증에서는 각 모형의 학습 세트 간 중복이 상대적으로 적어, 서로 다른 모형들의 결과가 LOOCV에 비해 덜 상관되어 있다. 일반적으로 서로 높은 상관을 가지는 값들의 평균은, 상관이 낮은 값들의 평균보다 분산이 더 크다. 이로 인해 LOOCV에서 얻은 테스트 오차 추정치는 k-겹 교차검증에서 얻은 추정치보다 분산이 더 크게 나타나는 경향이 있다.\n요약하면, k-겹 교차검증에서 k의 선택에는 명확한 편향–분산 절충이 존재한다. 일반적으로 이러한 점들을 종합적으로 고려할 때, k=5 또는 k=10을 사용하는 것이 경험적으로 바람직한 것으로 알려져 있다. 이러한 값들은 과도하게 큰 편향이나 매우 큰 분산을 동시에 피하면서, 안정적인 테스트 오차 추정치를 제공하는 것으로 확인되어 왔다.\n\n\n(5) 분류 문제에서의 교차검증\n지금까지 이 장에서는 반응변수 Y가 연속형인 회귀 문제를 중심으로 교차검증의 사용법을 설명하였으며, 테스트 오차를 정량화하기 위해 평균제곱오차(MSE)를 사용하였다. 그러나 반응변수가 범주형인 분류 문제에서도 교차검증은 매우 유용한 방법이 될 수 있다. 이 경우에도 교차검증의 절차 자체는 앞서 설명한 내용과 동일하며, 다만 테스트 오차를 측정하는 지표만 달라진다. 즉, 분류 문제에서는 MSE 대신 잘못 분류된 관측값의 개수를 사용하여 오차를 정의한다.\n예를 들어 분류 문제에서 LOOCV에 의한 오차율은 다음과 같은 형태를 갖는다.\n\\(\\text{CV}_{(n)} = \\frac{1}{n}\\overset{n}{\\sum_{i = 1}}\\text{Err}_{i}\\), 여기서 \\(\\text{Err}_{i} = I(y_{i} \\neq {\\widehat{y}}_{i})\\)이며, 이는 i번째 관측값이 잘못 분류되었을 경우 1, 올바르게 분류되었을 경우 0의 값을 갖는 지시변수이다. k-겹 교차검증과 검증 세트 방법에서의 분류 오차율 역시 이와 동일한 방식으로 정의된다.\n로지스틱 회귀는 회귀 문제에서와 마찬가지로 설명변수의 다항 함수를 포함시켜 비선형 결정경계를 갖도록 확장할 수 있다. 예를 들어, 다음과 같은 이차 로지스틱 회귀 모형을 고려할 수 있다.\n\\[\\log\\left( \\frac{p}{1 - p} \\right) = \\beta_{0} + \\beta_{1}X_{1} + \\beta_{2}X_{1}^{2} + \\beta_{3}X_{2} + \\beta_{4}X_{2}^{2}\\]\n실제 데이터 분석 상황에서는 베이즈 결정경계나 참된 테스트 오차율을 알 수 없다. 아래 제시된 네 가지 로지스틱 회귀 모형 중 어떤 모형을 선택해야 할지를 판단하는 문제는 교차검증을 통해 해결할 수 있다.\n\n\n\n\n\n아래 제시된 이차원 분류 데이터에 대해 계산된 테스트 오차(갈색), 훈련 오차(파란색), 그리고 10-겹 교차검증 오차(검은색)를 나타낸 것이다. 왼쪽 패널은 설명변수들의 다항함수를 사용한 로지스틱 회귀 모형의 결과를 보여주며, 가로축에는 사용된 다항식의 차수가 표시되어 있다. 오른쪽 패널은 서로 다른 K 값에 대한 KNN 분류기의 결과를 나타내며, 여기서 K는 KNN 분류기에 사용된 이웃의 수를 의미한다.\n\n\n\n\n\n\n\n(6) 분류 문제에서의 교차검증 오차 지표\n분류 문제에서 교차검증의 목적은 학습에 사용되지 않은 새로운 관측값에 대해 분류기가 얼마나 잘 작동하는지를 평가하는 데 있다. 이를 위해 회귀 문제에서 평균제곱오차(MSE)를 사용하듯, 분류 문제에서는 반응변수의 특성에 맞는 오차 지표를 사용한다. 어떤 지표를 선택하느냐에 따라 교차검증의 해석과 모형 선택 결과가 달라질 수 있다.\n1. 오차율 (Misclassification Error Rate)\n가장 직관적이고 널리 사용되는 분류 오차 지표는 오차율이다. 오차율은 전체 관측값 중에서 잘못 분류된 비율로 정의된다.\n\\[\\text{Error Rate} = \\frac{1}{n}\\overset{n}{\\sum_{i = 1}}I(y_{i} \\neq {\\widehat{y}}_{i})\\]\n여기서 \\(I( \\cdot )\\)는 지시변수이며, 예측값과 실제 값이 다를 경우 1, 같을 경우 0을 취한다. 교차검증에서는 각 검증 세트에서 계산된 오차율을 평균하여 테스트 오차율의 추정치로 사용한다.\n오차율은 해석이 매우 간단하다는 장점이 있으나, 예측 확률의 크기나 불확실성은 반영하지 못한다. 예를 들어, 매우 확신을 가지고 틀린 예측과 거의 우연에 가까운 예측 실패를 동일하게 취급한다는 한계를 가진다.\n2. 로그손실 (Log Loss, Cross-Entropy Loss)\n로그손실은 분류기가 출력한 확률 예측의 품질을 평가하는 지표로, 특히 확률 기반 분류기에서 중요하다. 이항 분류의 경우 로그손실은 다음과 같이 정의된다.\n\\[\\text{Log Loss} = - \\frac{1}{n}\\overset{n}{\\sum_{i = 1}}\\left\\lbrack y_{i}\\log({\\widehat{p}}_{i}) + (1 - y_{i})\\log(1 - {\\widehat{p}}_{i}) \\right\\rbrack\\]\n여기서 \\({\\widehat{p}}_{i} = P(Y = 1 \\mid X = x_{i})\\)는 분류기가 예측한 확률이다. 다중 분류의 경우에는 softmax 출력과 함께 일반화된 형태의 교차엔트로피 손실을 사용한다.\n로그손실은 잘못된 예측뿐만 아니라, 얼마나 확신을 가지고 예측했는지까지 함께 반영한다. 따라서 확률 추정이 중요한 문제, 예를 들어 위험 예측이나 의사결정 문제에서 특히 유용하다. 반면 값의 크기가 직관적으로 해석되기 어렵다는 단점이 있다.\n3. 이진 교차엔트로피와 다중 클래스 교차엔트로피\n로그손실은 신경망과 같은 딥러닝 분류 모형에서 손실함수로 직접 사용되며, 교차검증에서도 동일한 기준으로 활용된다.\n\n이진 분류: Binary Cross-Entropy\n다중 분류: Categorical Cross-Entropy\n\n교차검증에서는 각 폴드에서 계산된 교차엔트로피 손실을 평균하여 테스트 손실의 추정치로 사용한다. 이 지표는 오차율보다 미세한 모형 차이를 감지하는 데 유리하다.\n4. ROC-AUC 기반 지표\nROC 곡선 아래 면적(AUC)은 분류 임계값에 의존하지 않는 성능 지표로, 확률 예측의 순위 품질을 평가한다. 교차검증에서는 각 폴드에서 AUC를 계산한 뒤 평균 AUC를 사용한다.\nAUC는 클래스 불균형 문제가 있는 경우에도 비교적 안정적인 지표라는 장점이 있다. 다만 이는 엄밀한 의미의 손실 함수는 아니며, 직접적인 확률 예측 오차를 측정하지는 않는다.\n5. Precision, Recall, F1-score 기반 지표\n불균형 데이터에서 단순 오차율은 오해를 불러일으킬 수 있으므로, 정밀도(precision), 재현율(recall), 그리고 이들의 조화평균인 F1-score가 사용된다. 교차검증에서는 각 폴드에서 해당 지표를 계산하고 평균값을 사용한다.\n이들 지표는 특정 클래스에 대한 성능을 강조하고자 할 때 유용하지만, 하나의 단일 손실 함수로 해석하기는 다소 어렵다.\n\n\n\n3. 부트스트랩\n부트스트랩은 주어진 추정량이나 통계적 학습 방법에 수반되는 불확실성을 정량화하는 데 사용할 수 있는, 매우 강력하고 폭넓게 적용 가능한 통계적 도구이다. 간단한 예로, 부트스트랩은 선형회귀 모형에서 회귀계수의 표준오차를 추정하는 데 사용될 수 있다. 그러나 선형회귀의 경우에는 \\통계 소프트웨어가 이러한 표준오차를 자동으로 제공하므로, 부트스트랩의 활용 가치가 크지 않을 수 있다. 그럼에도 불구하고 부트스트랩의 진정한 장점은, 통계 소프트웨어가 자동으로 변동성 정보를 제공하지 않거나 이론적 계산이 어려운 다양한 통계적 학습 방법에도 손쉽게 적용할 수 있다는 점에 있다.\n이제 두 개의 금융 자산 X와 Y에 고정된 금액을 투자한다고 가정하자. 이때 X와 Y는 각각 확률변수로서 수익률을 나타낸다. 우리는 전체 투자금의 \\(\\alpha\\) 비율을 자산 X에 투자하고, 나머지 \\(1 - \\alpha\\)를 자산 Y에 투자한다. 두 자산의 수익률에는 변동성이 존재하므로, 우리는 투자 전체의 위험, 즉 분산을 최소화하는 \\(\\alpha\\)를 선택하고자 한다. 이는 다음 분산을 최소화하는 문제로 표현된다.\n\\(\\text{Var}(\\alpha X + (1 - \\alpha)Y)\\). 이 분산을 최소화하는 \\(\\alpha\\)는 다음과 같이 주어진다. \\(\\alpha = \\frac{\\sigma_{Y}^{2} - \\sigma_{XY}}{\\sigma_{X}^{2} + \\sigma_{Y}^{2} - 2\\sigma_{XY}}\\), 여기서 \\(\\sigma_{X}^{2} = \\text{Var}(X),\\sigma_{Y}^{2} = \\text{Var}(Y),\\sigma_{XY} = \\text{Cov}(X,Y)\\)이다.\n현실적으로 \\(\\sigma_{X}^{2},\\sigma_{Y}^{2},\\sigma_{XY}\\)는 알려져 있지 않다. 대신 우리는 과거의 X와 Y 수익률을 포함한 데이터셋을 이용해 이들에 대한 추정값 \\({\\widehat{\\sigma}}_{X}^{2},{\\widehat{\\sigma}}_{Y}^{2},{\\widehat{\\sigma}}_{XY}\\)를 계산할 수 있다. 이를 대입하면, 투자 분산을 최소화하는 \\(\\alpha\\)의 추정값은 다음과 같이 얻어진다.\n\\[\\widehat{\\alpha} = \\frac{{\\widehat{\\sigma}}_{Y}^{2} - {\\widehat{\\sigma}}_{XY}}{{\\widehat{\\sigma}}_{X}^{2} + {\\widehat{\\sigma}}_{Y}^{2} - 2{\\widehat{\\sigma}}_{XY}}\\]\n이제 우리는 \\(\\widehat{\\alpha}\\)의 정확도를 정량화하고자 한다. 이를 위해, X와 Y의 관측값 100쌍을 생성하고 \\(\\widehat{\\alpha}\\)를 계산하는 과정을 1,000번 반복하였다. 그 결과 1,000개의 \\(\\widehat{\\alpha}\\) 추정값을 얻을 수 있다. 이들 추정값의 평균은\n\\(\\overline{\\alpha} = \\frac{1}{1000}\\overset{1000}{\\sum_{r = 1}}{\\widehat{\\alpha}}_{r}\\), 추정값들의 표준편차는 \\(\\sqrt{\\frac{1}{1000 - 1}\\overset{1000}{\\sum_{r = 1}}({\\widehat{\\alpha}}_{r} - \\overline{\\alpha})^{2}}\\)으로 계산된다.\n그러나 실제 데이터 분석에서는 이러한 모의실험 절차를 수행할 수 없다. 현실에서는 모집단으로부터 새로운 표본을 반복적으로 생성할 수 없기 때문이다. 이때 부트스트랩 접근법은 컴퓨터를 이용해 새로운 표본을 생성하는 과정을 모의함으로써, 추가적인 데이터 없이도 \\(\\widehat{\\alpha}\\)의 변동성을 추정할 수 있게 해준다.\n\n부트스트랩 알고리즘 요약\n부트스트랩은 하나의 관측 데이터셋을 이용해 표본 추출 과정을 반복적으로 모의함으로써, 추정량의 변동성과 불확실성을 평가하는 방법이다. 절차는 다음과 같이 정리할 수 있다.\n1단계: 원자료 준비\n크기 n인 원래 데이터셋 \\(Z = \\{(x_{1},y_{1}),\\ldots,(x_{n},y_{n})\\}\\)을 준비한다. 이 데이터는 모집단으로부터 추출된 하나의 표본으로 간주한다.\n2단계: 부트스트랩 표본 생성\n원자료 Z로부터 크기 n의 표본을 복원추출하여 하나의 부트스트랩 데이터셋 \\(Z^{*1}\\)을 생성한다. 이때 동일한 관측값이 여러 번 선택될 수도 있고, 어떤 관측값은 전혀 선택되지 않을 수도 있다.\n3단계: 관심 통계량 계산\n부트스트랩 표본 \\(Z^{*1}\\)을 이용해 관심 있는 통계량 또는 추정량 \\({\\widehat{\\theta}}^{*1}\\)을 계산한다. 예를 들어 회귀계수, 예측값, 최적 파라미터 등이 이에 해당한다.\n4단계: 반복 수행\n2단계와 3단계를 총 B번 반복하여, \\({\\widehat{\\theta}}^{*1},{\\widehat{\\theta}}^{*2},\\ldots,{\\widehat{\\theta}}^{*B}\\)와 같은 B개의 부트스트랩 추정값을 얻는다.\n5단계: 변동성 추정\n얻어진 부트스트랩 추정값들의 분포를 이용해 표준오차, 분산, 신뢰구간 등을 계산한다. 예를 들어 표준오차는 다음과 같이 추정된다.\n\\[\\text{SE}_{B}(\\widehat{\\theta}) = \\sqrt{\\frac{1}{B - 1}\\overset{B}{\\sum_{r = 1}}\\left( {\\widehat{\\theta}}^{*r} - \\frac{1}{B}\\overset{B}{\\sum_{r' = 1}}{\\widehat{\\theta}}^{*r'} \\right)^{2}}\\]\n6단계: 해석\n계산된 표준오차나 분포를 바탕으로, 원래 데이터에서 계산된 추정량 \\(\\widehat{\\theta}\\)의 불확실성과 안정성을 해석한다.\n\n\n\n\n\n\n\n부트스트랩과 교차검증의 개념적 차이\n부트스트랩과 교차검증은 모두 재표본(resampling)에 기반한 방법이지만, 두 방법이 해결하고자 하는 문제와 그 철학은 본질적으로 다르다. 교차검증은 주어진 모형의 일반화 성능을 추정하는 데 초점을 맞추는 반면, 부트스트랩은 추정량이나 학습 결과에 수반되는 불확실성을 정량화하는 데 목적이 있다.\n\n\n교차검증: ”이 모형은 새로운 데이터에서 얼마나 잘\n작동하는가”\n교차검증은 학습 데이터의 일부를 의도적으로 제외한 뒤, 해당 데이터를 새로운 관측값처럼 사용하여 테스트 오차를 추정하는 방법이다. 핵심 관심사는 예측 성능이며, 특히 훈련 데이터에 포함되지 않은 관측값에 대해 모형이 얼마나 잘 일반화되는지를 평가하는 데 있다.\n이 과정에서 각 관측값은 학습과 평가의 역할을 번갈아 수행하며, 이를 통해 훈련 오차와 구분되는 테스트 오차의 추정치를 얻는다. 따라서 교차검증은 주로 모형 선택이나 하이퍼파라미터 튜닝과 같이, 여러 후보 모형 중에서 어떤 모형이 더 나은지를 판단하는 데 사용된다.\n요약하면, 교차검증은 ”어떤 모형이 더 잘 맞는가”라는 질문에 답하기 위한 도구이다.\n\n\n부트스트랩: ”이 추정값은 얼마나 불확실한가”\n부트스트랩은 하나의 데이터셋으로부터 복원추출을 반복함으로써, 동일한 모집단에서 여러 표본이 관측되는 상황을 모의하는 방법이다. 여기서 관심의 대상은 예측 성능이 아니라, 추정량 자체의 변동성이다.\n부트스트랩은 특정 모형이나 통계량이 표본에 따라 얼마나 달라질 수 있는지를 평가하는 데 사용된다. 예를 들어, 회귀계수의 표준오차, 예측값의 변동성, 혹은 복잡한 학습 알고리즘의 출력이 얼마나 안정적인지를 파악하는 데 유용하다. 이는 추가적인 표본을 실제로 수집할 수 없는 상황에서도 불확실성을 정량화할 수 있게 해준다.\n요약하면, 부트스트랩은 ”이 추정값을 얼마나 믿을 수 있는가”라는 질문에 답하기 위한 도구이다.\n\n\n관점의 차이\n교차검증과 부트스트랩의 가장 큰 차이는, 재표본을 사용하는 이유에 있다. 교차검증은 학습 과정에서 일부 데이터를 제거함으로써 미래의 데이터 상황을 모의한다. 반면, 부트스트랩은 하나의 데이터셋을 반복적으로 재구성함으로써 동일한 표본 추출 과정을 모의한다.\n이로 인해 교차검증은 모형의 예측 성능에 대한 평가와 선택에 적합하며, 부트스트랩은 추정량의 분산이나 표준오차를 추정하는 데 적합하다.\n\n\n\n\nChapter 5. 재표본 사례\n\n1. 부스트랩\n\n사례1\n관측값 \\(X_{1},\\ldots,X_{n}\\)의 표본평균 \\(\\overline{X}\\)에 대해 표본 하나만 주어졌을 \\(\\overline{X}\\)의 불확실성(표준오차)을 어떻게 추정할 수 있는가?\nimport numpy as np\n\n# -----------------------------\n# 1. 원자료 생성 (예제용)\n# -----------------------------\nnp.random.seed(42)\nX = np.random.normal(loc=0, scale=1, size=50)  # n=50 표본\nn = len(X)\n\n# 원래 표본 평균\ntheta_hat = np.mean(X)\n\n# -----------------------------\n# 2. 부트스트랩 수행\n# -----------------------------\nB = 1000  # 부트스트랩 반복 횟수\ntheta_star = np.empty(B)\n\nfor b in range(B):\n    X_star = np.random.choice(X, size=n, replace=True)  # 복원추출\n    theta_star[b] = np.mean(X_star)\n\n# -----------------------------\n# 3. 부트스트랩 표준오차\n# -----------------------------\nbootstrap_se = np.std(theta_star, ddof=1)\n\nprint(f\"원래 표본 평균 = {theta_hat:.4f}\")\nprint(f\"부트스트랩 표준오차 = {bootstrap_se:.4f}\")\n원래 표본 평균 = -0.2255  부트스트랩 표준오차 = 0.1334\n\n\n사례2\n선형회귀 \\(Y = \\beta_{0} + \\beta_{1}X + \\varepsilon\\) 에서 \\({\\widehat{\\beta}}_{1}\\)의 불확실성을 부트스트랩으로 평가한다.\nimport numpy as np\nimport statsmodels.api as sm\n\n# -----------------------------\n# 1. 데이터 생성\n# -----------------------------\nnp.random.seed(0)\nn = 100\nX = np.random.normal(size=n)\nY = 2 + 3*X + np.random.normal(scale=1, size=n)\n\nX_mat = sm.add_constant(X)\n\n# 원래 회귀계수\nmodel = sm.OLS(Y, X_mat).fit()\nbeta_hat = model.params[1]\n\n# -----------------------------\n# 2. 부트스트랩\n# -----------------------------\nB = 1000\nbeta_star = np.empty(B)\n\nfor b in range(B):\n    idx = np.random.choice(np.arange(n), size=n, replace=True)\n    X_star = X_mat[idx]\n    Y_star = Y[idx]\n    \n    model_star = sm.OLS(Y_star, X_star).fit()\n    beta_star[b] = model_star.params[1]\n\n# -----------------------------\n# 3. 부트스트랩 표준오차\n# -----------------------------\nbootstrap_se = np.std(beta_star, ddof=1)\n\nprint(f\"원래 회귀계수 추정값 = {beta_hat:.4f}\")\nprint(f\"부트스트랩 표준오차 = {bootstrap_se:.4f}\")\n원래 회귀계수 추정값 = 3.1147  부트스트랩 표준오차 = 0.0947\n\n\n\n2. 검증세트 방법, LOOCV, k-겹 교차검증\nseaborn auto 데이터의 mpg(Y)와 horsepower(X)의 함수 관계가 1차, 2차, 3차 관계 중 최소 MSE를 갖는 함수 관계는? 3 방법 모두 2차 함수관계가 최적 모형이다.\nimport numpy as np\nimport pandas as pd\n\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split, LeaveOneOut, KFold, cross_val_score\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\n\n# 1) 데이터 로드: seaborn mpg (Auto와 유사)\ndf = sns.load_dataset(\"mpg\")\n\n# horsepower 결측 제거 후 mpg, horsepower만 사용\ndf = df[[\"mpg\", \"horsepower\"]].dropna()\n\nX = df[[\"horsepower\"]].values\ny = df[\"mpg\"].values\n\ndegrees = [1, 2, 3]\n\ndef make_model(deg: int):\n    return Pipeline([\n        (\"poly\", PolynomialFeatures(degree=deg, include_bias=False)),\n        (\"lr\", LinearRegression())\n    ])\n\nresults = []\n\n# 2) 검증 세트 방법(Validation Set Approach): 한 번만 랜덤 분할\nX_tr, X_val, y_tr, y_val = train_test_split(\n    X, y, test_size=0.5, random_state=42, shuffle=True\n)\n\nfor d in degrees:\n    model = make_model(d)\n    model.fit(X_tr, y_tr)\n    pred_val = model.predict(X_val)\n    mse_val = mean_squared_error(y_val, pred_val)\n    results.append({\"method\": \"Validation set (50/50 split)\", \"degree\": d, \"MSE\": mse_val})\n\n# 3) LOOCV\nloo = LeaveOneOut()\nfor d in degrees:\n    model = make_model(d)\n    scores = cross_val_score(model, X, y, cv=loo, scoring=\"neg_mean_squared_error\")\n    mse_loo = -scores.mean()\n    results.append({\"method\": \"LOOCV\", \"degree\": d, \"MSE\": mse_loo})\n\n# 4) k-겹 교차검증 (예: 10-fold)\nk = 10\nkf = KFold(n_splits=k, shuffle=True, random_state=42)\nfor d in degrees:\n    model = make_model(d)\n    scores = cross_val_score(model, X, y, cv=kf, scoring=\"neg_mean_squared_error\")\n    mse_kfold = -scores.mean()\n    results.append({\"method\": f\"{k}-fold CV\", \"degree\": d, \"MSE\": mse_kfold})\n\n# 5) 결과 정리\nres = pd.DataFrame(results).sort_values([\"method\", \"degree\"]).reset_index(drop=True)\nprint(res)\n\nprint(\"\\n방법별 최소 MSE(가장 좋은 차수):\")\nbest = res.loc[res.groupby(\"method\")[\"MSE\"].idxmin(), [\"method\", \"degree\", \"MSE\"]]\nprint(best.sort_values(\"method\").reset_index(drop=True))\nmethod degree MSE  0 10-fold CV 1 24.199808  1 10-fold CV 2 19.228637  2 10-fold CV 3 19.266265\n3 LOOCV 1 24.231514  4 LOOCV 2 19.248213  5 LOOCV 3 19.334984\n6 Validation set (50/50 split) 1 25.573878  7 Validation set (50/50 split) 2 22.218020  8 Validation set (50/50 split) 3 22.667675 \n방법별 최소 MSE(가장 좋은 차수):  method degree MSE  0 10-fold CV 2 19.228637  1 LOOCV 2 19.248213  2 Validation set (50/50 split) 2 22.218020\n\np-value와 resampling 비교\nstatsmodels로 1차/2차/3차 다항회귀를 적합하고 p-value(특히 최고차 항의 p-value)를 확인한 뒤 동일한 세 모형에 대해 10-fold CV MSE를 계산하여 ”p-value 기반 선택”과 ”CV 기반 선택”을 한눈에 비교한다.\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\n\nimport statsmodels.api as sm\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\n\n# -----------------------------\n# 1) 데이터 준비\n# -----------------------------\ndf = sns.load_dataset(\"mpg\")[[\"mpg\", \"horsepower\"]].dropna()\n\ny = df[\"mpg\"].to_numpy()\nx = df[\"horsepower\"].to_numpy()\n\ndegrees = [1, 2, 3]\n\n# -----------------------------\n# 2) statsmodels: p-value 기반(중첩모형) 비교\n#    - 1차: mpg ~ hp\n#    - 2차: mpg ~ hp + hp^2\n#    - 3차: mpg ~ hp + hp^2 + hp^3\n# -----------------------------\nsm_rows = []\n\nfor d in degrees:\n    X = np.column_stack([x**k for k in range(1, d+1)])\n    X = sm.add_constant(X)  # 절편 포함\n    model = sm.OLS(y, X).fit()\n    \n    # 최고차항 p-value(예: 2차면 hp^2의 p-value)\n    top_term_p = model.pvalues[-1]\n    \n    sm_rows.append({\n        \"degree\": d,\n        \"AIC\": model.aic,\n        \"BIC\": model.bic,\n        \"R2\": model.rsquared,\n        \"Adj_R2\": model.rsquared_adj,\n        \"Top-term p-value\": float(top_term_p)\n    })\n\nsm_table = pd.DataFrame(sm_rows).sort_values(\"degree\")\n\nprint(\"=== statsmodels (p-value / information criteria) ===\")\nprint(sm_table.to_string(index=False))\n\n# -----------------------------\n# 3) sklearn: 10-fold CV MSE 비교\n# -----------------------------\nX_skl = df[[\"horsepower\"]].to_numpy()\n\ndef poly_model(deg: int):\n    return Pipeline([\n        (\"poly\", PolynomialFeatures(degree=deg, include_bias=False)),\n        (\"lr\", LinearRegression())\n    ])\n\nkf = KFold(n_splits=10, shuffle=True, random_state=42)\n\ncv_rows = []\nfor d in degrees:\n    model = poly_model(d)\n    scores = cross_val_score(model, X_skl, y, cv=kf, scoring=\"neg_mean_squared_error\")\n    cv_rows.append({\n        \"degree\": d,\n        \"10-fold CV MSE (mean)\": float(-scores.mean()),\n        \"10-fold CV MSE (std)\": float(scores.std())  # neg MSE의 std이므로 부호 유의: 아래에서 절대값 처리해도 됨\n    })\n\ncv_table = pd.DataFrame(cv_rows).sort_values(\"degree\")\n# std는 부호가 섞일 수 있어 MSE 기준으로 보기 쉽게 변환\ncv_table[\"10-fold CV MSE (std)\"] = np.abs(cv_table[\"10-fold CV MSE (std)\"])\n\nprint(\"\\n=== 10-fold CV (MSE) ===\")\nprint(cv_table.to_string(index=False))\n\n# -----------------------------\n# 4) 한 표로 합치기 (p-value vs CV)\n# -----------------------------\nmerged = sm_table.merge(cv_table, on=\"degree\")\nprint(\"\\n=== Combined: p-value vs CV ===\")\nprint(merged.to_string(index=False))\n\n# -----------------------------\n# 5) 선택 기준 예시 출력\n# -----------------------------\n# p-value 기준: 최고차항 p-value가 유의하지 않으면 더 높은 차수는 불필요하다고 보는 단순 규칙\nalpha = 0.05\npv_choice = merged.loc[merged[\"Top-term p-value\"] &lt; alpha, \"degree\"].max()\npv_choice = int(pv_choice) if pd.notna(pv_choice) else 1\n\n# CV 기준: 평균 CV MSE 최소\ncv_choice = int(merged.loc[merged[\"10-fold CV MSE (mean)\"].idxmin(), \"degree\"])\n\nprint(\"\\n=== Simple choices ===\")\nprint(f\"p-value rule (alpha={alpha}): choose degree = {pv_choice}\")\nprint(f\"CV rule (min mean CV MSE): choose degree = {cv_choice}\")\n=== statsmodels (p-value / information criteria) ===  degree AIC BIC R2 Adj_R2 Top-term p-value  1 2361.323658 2369.266182 0.605948 0.604938 7.031989e-81  2 2272.353522 2284.267308 0.687559 0.685953 2.196340e-21  3 2273.531297 2289.416344 0.688214 0.685803 3.672973e-01\n=== 10-fold CV (MSE) ===  degree 10-fold CV MSE (mean) 10-fold CV MSE (std)  1 24.199808 4.902966  2 19.228637 4.783560  3 19.266265 4.818808\n=== Combined: p-value vs CV ===\ndegree AIC BIC R2 Adj_R2 Top-term p-value 10-fold CV MSE (mean) 10-fold CV MSE (std)  1 2361.323658 2369.266182 0.605948 0.604938 7.031989e-81 24.199808 4.902966  2 2272.353522 2284.267308 0.687559 0.685953 2.196340e-21 19.228637 4.783560  3 2273.531297 2289.416344 0.688214 0.685803 3.672973e-01 19.266265 4.818808\n=== Simple choices ===  p-value rule (alpha=0.05): choose degree = 2  CV rule (min mean CV MSE): choose degree = 2",
    "crumbs": [
      "기초수학",
      "📄 방법론 소개"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "세상 모든 통계 이야기",
    "section": "",
    "text": "🎓 Welcome to Prof. Kwon’s 통계이야기\n\n\n\n\n\n1995년부터 한남대학교에서 제공해온 통계학 강의노트를 데이터 사이언스 중심으로 새롭게 구성했습니다. since 1999.03(첫 웹이지 구축) / 2023.01 (1차 수정) / 2026.01 (최종본)\n\n| put a ding in the universe | 세상 모든 사람이 통계를 사랑하는 그 날까지\n\n\n\n\n👤 Who am I?\n\n\n1979 - 1983 성균관대학교 통계학 학사  1983 - 1985 성균관대학교 통계학 석사  1986 - 1992 미국 North Carolina State University 통계학 박사  1993 - 1995. 전자통신연구원 선임연구원  1995 - 2026. 한남대학교 통계학과 교수"
  },
  {
    "objectID": "notes/mldl_method/mldlmethod_prediction_intro.html",
    "href": "notes/mldl_method/mldlmethod_prediction_intro.html",
    "title": "MLDL 예측방법. 1.서론",
    "section": "",
    "text": "Chapter 1. 예측모형 통계적 정의\n\n1. 예측 문제의 통계적 정의\n예측이란 설명변수 X가 주어졌을 때 반응변수 Y의 값을 가능한 한 정확하게 추정하는 문제이다. 여기서는 반응변수가 연속형인 경우, 즉 \\(Y \\in \\mathbb{R}\\)인 상황을 다룬다.\n통계적 관점에서 예측 문제는 함수 추정 문제로 정의된다. 확률변수 (X, Y)에 대해, 다음의 기대제곱오차를 최소화하는 함수 f를 찾는 것이 목표이다.\n\\(f^{*} = \\arg\\min_{f}\\mathbb{E}\\lbrack(Y - f(X))^{2}\\rbrack\\), 이때 제곱오차 손실 하에서의 최적 해는 잘 알려진 바와 같이 \\(f^{*}(x) = \\mathbb{E}\\lbrack Y \\mid X = x\\rbrack\\)이다. 그러나 실제 데이터 분석에서는 \\(\\mathbb{E}\\lbrack Y \\mid X = x\\rbrack\\)를 정확히 알 수 없으므로, 제한된 표본 데이터를 이용해 이를 근사(approximation)한다.\n즉, 예측 문제는 본질적으로 ”미지의 함수 \\(f^{*}\\)를 데이터로부터 얼마나 잘 근사할 수 있는가”라는 질문으로 귀결된다.\n\n\n2. 불가약오차(Irreducible error)와 잡음 모델\n현실의 데이터에서 Y는 X만으로 완전히 결정되지 않는다. 동일한 X가 주어져도 관측되는 Y는 여러 요인의 영향으로 변동할 수 있으며, 이 변동은 예측모형이 아무리 정교해도 완전히 제거할 수 없다. 이러한 관점은 예측 문제를 다음과 같은 잡음(noise) 포함 모델로 표현하게 한다.\n1. 잡음 모델 \\(Y = f^{*}(X) + \\varepsilon\\)\n예측의 이론적 최적함수 \\(f^{*}(x) = \\mathbb{E}\\lbrack Y \\mid X = x\\rbrack\\)를 기준으로, \\(Y = f^{*}(X) + \\varepsilon\\)라고 쓸 수 있다. 여기서 \\(\\varepsilon\\)는 X로 설명되지 않는 변동(잡음)을 나타내며, 정의상 \\(\\mathbb{E}\\lbrack\\varepsilon \\mid X\\rbrack = 0\\)을 만족한다. 즉, \\(\\varepsilon\\)는 ”평균적으로는 0이지만, 개별 관측치에서는 예측을 흔드는 요인”이다. 이 표현은 예측문제가 결국 신호(signal) \\(f^{*}(X)\\)와 잡음(noise) \\(\\varepsilon\\)를 구분하는 작업임을 보여준다.\n2. 불가약오차의 의미\n어떤 예측함수 f를 사용하더라도 기대제곱오차는 0이 될 수 없다. 그 이유는 X가 동일하더라도 Y가 확률적으로 변동하기 때문이다. 특히 제곱오차 기준에서 최적함수 \\(f^{*}\\)를 사용하더라도 남는 오차는 \\(\\mathbb{E}\\lbrack(Y - f^{*}(X))^{2}\\rbrack\\)이며, 이것이 바로 불가약오차이다.\n조건부 분산을 이용하면 이 값은 더 명확히 해석된다:\n\\[\\mathbb{E}\\lbrack(Y - f^{*}(X))^{2}\\rbrack = \\mathbb{E}\\lbrack Var(Y \\mid X)\\rbrack\\]\n즉 불가약오차는 ”X가 주어졌을 때 Y가 본질적으로 가지는 변동성(조건부 분산)“의 평균이며, 데이터 생성과정 자체가 갖는 한계이다. 따라서 예측모형의 목표는 불가약오차를 없애는 것이 아니라, 그 위에 추가로 발생하는 오차를 최소화하는 것이다.\n3. (중요) 오차 분해: 줄일 수 있는 것 vs 줄일 수 없는 것\n임의의 예측함수 f에 대해 제곱오차는 다음과 같이 분해된다:\n\\[\\mathbb{E}\\lbrack(Y - f(X))^{2}\\rbrack = \\underset{\\text{불가약오차}}{\\underbrace{\\mathbb{E}\\lbrack(Y - f^{*}(X))^{2}\\rbrack}} + \\underset{\\text{근사/추정 오차}}{\\underbrace{\\mathbb{E}\\lbrack(f^{*}(X) - f(X))^{2}\\rbrack}}\\]\n두 번째 항은 f가 \\(f^{*}\\)를 얼마나 잘 근사하는지에 따라 달라지는 부분으로, 모형 선택·규제·데이터 크기 등에 의해 줄일 수 있다. 반면 첫 번째 항은 데이터 생성 과정의 잡음 크기에 의해 결정되며, 모형을 바꿔도 사라지지 않는다.\n이 관점은 ”왜 아무리 복잡한 모델도 테스트오차가 0이 되지 않는가”, ”왜 과적합을 피해야 하는가”를 설명하는 이론적 근거가 된다.\n4. 실무적 해석과 함의\n불가약오차가 크다는 것은, X만으로는 Y를 정밀하게 예측하기 어렵다는 뜻이다. 이때의 대응은 대체로 두 가지 방향이다.\n정보를 추가한다(특징 확장): 더 유의미한 설명변수(센서, 설문, 로그 등)를 확보하면 \\(Var(Y \\mid X)\\)를 줄일 수 있다.\n목표를 조정한다: 점예측의 한계를 인정하고, 예측구간/확률예측처럼 불확실성을 함께 보고한다.\n요약하면, 예측모형의 성능에는 근본적 하한이 존재하며(불가약오차), 우리가 할 수 있는 일은 그 하한 위에서 f^*에 최대한 가까운 \\hat f를 학습하도록 복잡도 조절과 검증 기반 선택을 수행하는 것이다.\n\n\n3. Y가 이진형 혹은 범주형인 경우 예측은 분류이다.\n반응변수가 이진형인 경우, 예측 문제는 분류(classification) 문제로 구분된다. 즉, \\(Y \\in \\{ 0,1\\}\\) 또는 보다 일반적으로 \\(Y \\in \\{ 1,2,\\ldots,K\\}\\)와 같이 유한한 범주 값을 갖는 경우, 목표는 연속값을 추정하는 것이 아니라 어느 범주에 속하는지를 결정하는 것이다.\n다만 여기서 중요한 점은, 분류 문제 역시 본질적으로는 함수 추정 문제라는 사실이다. 분류에서는 일반적으로 다음과 같은 조건부 확률 함수를 추정한다. \\(\\eta(x) = P(Y = 1 \\mid X = x)\\)\n그리고 실제 분류 규칙은 이 확률 함수에 대한 임계값(threshold)을 통해 정의된다.\n\\(\\widehat{Y} = \\{\\begin{matrix}\n1, & \\eta(x) \\geq c \\\\\n0, & \\eta(x) &lt; c\n\\end{matrix}\\). 이때 임계값 c는 흔히 0.5로 설정되지만, 비용 구조나 문제 맥락에 따라 달라질 수 있다.\n이 관점에서 보면, 회귀=예측은 \\(f(x) \\approx \\mathbb{E}\\lbrack Y \\mid X = x\\rbrack\\)를 직접 근사하는 문제이고, 분류는 \\(f(x) \\approx P(Y = 1 \\mid X = x)\\)와 같은 확률 함수를 근사한 뒤, 이를 의사결정 규칙으로 변환하는 문제이다.\n따라서 회귀와 분류의 차이는 출력 공간과 손실함수의 차이이지, ”예측 문제라는 본질”이 다른 것은 아니다. 연속형 반응변수에서는 제곱오차(MSE)가 자연스럽고, 이진형 반응변수에서는 로그손실(log-loss)이나 크로스엔트로피가 자연스럽게 등장한다.\n\n\n4. 예측문제는 본질적으로 함수 근사 문제이다.\n통계적 예측 문제는 겉으로 보면 ”주어진 설명변수로 반응변수를 맞히는 문제”처럼 보이지만, 그 본질은 훨씬 일반적인 함수 근사 문제로 이해할 수 있다.\n예측문제는 반응변수가 연속형일 경우, \\(Y \\in \\mathbb{R}\\), 위험 함수, \\(\\min_{f}\\mathbb{E}\\lbrack(Y - f(X))^{2}\\rbrack\\)를 최소화하는 함수 \\(f\\)를 찾는 문제로 정식화된다. 이때 제곱오차 손실 하에서 이론적으로 최적인 함수는 조건부 기댓값 \\(f^{*}(X) = \\mathbb{E}\\lbrack Y \\mid X\\rbrack\\)임이 알려져 있다. 즉, 모든 예측 문제의 이상적인 해는 조건부 평균 함수이다. 그러나 실제 데이터 분석에서는 모집단의 분포를 알 수 없고, 오직 유한한 표본 \\(\\{(x_{1},y_{1}),\\ldots,(x_{n},y_{n})\\}\\)만이 주어진다. 따라서 실제 목표는 \\(f^{*}\\) 자체를 구하는 것이 아니라, 데이터로부터 이를 근사하는 함수 \\(\\widehat{f}\\)를 구성하는 데 있다.\n이 관점에서 보면, 예측 문제는 다음과 같이 재해석된다.\n미지의 함수 \\(f^{*}:\\mathcal{X} \\rightarrow \\mathbb{R}\\)를 제한된 데이터와 제한된 함수 공간 내에서 얼마나 잘 근사할 수 있는가? 이때 핵심은 어떤 함수 공간을 선택하느냐이다. 함수 공간의 선택은 곧 예측모형의 성격을 결정한다.\n\n선형회귀는 \\(\\mathcal{F} = \\{ f(x) = \\beta_{0} + \\beta^{\\top}x\\}\\)와 같은 유한차원 선형 함수 공간을 가정한다.\n다항회귀는 입력변수를 비선형 변환하여 보다 큰 함수 공간을 구성한다.\n트리 기반 모형은 입력 공간을 분할한 뒤 각 영역에서 상수 또는 단순 함수를 사용함으로써 비연속적 함수 근사를 수행한다.\n신경망은 다층 비선형 변환을 통해 매우 큰 함수 공간을 형성하며, 이론적으로 임의의 연속함수를 근사할 수 있다.\n\n함수 공간이 커질수록 근사 오차)는 감소하지만, 동시에 추정 오차는 증가한다. 이는 예측 문제의 핵심 딜레마인 편향–분산 트레이드오프(Bias–Variance Trade-off) 로 이어진다. 이를 분해하여 표현하면, 임의의 점 x에서의 평균 제곱오차는 다음과 같이 나눌 수 있다.\n\\[\\mathbb{E}\\lbrack(Y - \\widehat{f}(x))^{2}\\rbrack = \\underset{\\text{모형 한계}}{\\underbrace{\\text{Bias}^{2}}} + \\underset{\\text{추정 불안정성}}{\\underbrace{\\text{Variance}}} + \\underset{\\text{불가피한 잡음}}{\\underbrace{\\sigma^{2}}}\\]\n따라서 예측모형의 설계란 단순히 ”복잡한 모형을 쓰는 것”이 아니라, 함수 공간의 크기, 데이터 양, 노이즈 수준, 그리고 일반화 성능을 함께 고려하여 적절한 함수 근사 수준을 선택하는 과정이다.\n\n\n5. \\(Y \\in \\mathbb{R}\\)에 대한 규제는 함수 공간을 줄이는 장치이다.\n왜 규제가 필요한가: 고차원과 불안정성\n예측변수의 개수 \\(p\\)가 크거나 \\(X^{\\top}X\\)가 거의 특이(singular)하면(이를 다중공선성(multicollinearity) 문제) \\(\\widehat{\\beta}\\) 분산이 커지고 예측이 흔들린다.\n\n훈련오차는 줄어도 테스트오차가 증가하는 과적합이 발생한다.\n규제는 ”계수의 크기를 제한”하여 함수 공간을 사실상 축소한다.\n\n제한형 문제와 패널티형 문제의 동치\n규제는 다음 두 형태로 동치가 성립한다(라그랑주 관점).\n\n제한형(함수 공간 축소의 직접 표현) \\(\\min_{\\beta} \\parallel y - X\\beta \\parallel^{2}\\text{s.t.} \\parallel \\beta \\parallel_{q} \\leq t\\)\n패널티형(목적함수에 벌점 추가) \\(\\min_{\\beta} \\parallel y - X\\beta \\parallel^{2} + \\lambda \\parallel \\beta \\parallel_{q}^{q}\\)\n\n즉, 규제는 ”좋은 함수”의 정의에 복잡도 비용을 포함시키는 방식이다.\nRidge: L_2 규제와 수축(shrinkage)\n\n목적함수: \\(\\min_{\\beta} \\parallel y - X\\beta \\parallel^{2} + \\lambda \\parallel \\beta \\parallel_{2}^{2}\\)\n해: \\({\\widehat{\\beta}}_{\\text{ridge}} = (X^{\\top}X + \\lambda I)^{- 1}X^{\\top}y\\)\n해석: \\(\\lambda I\\)를 더해 역행렬 안정화(수치적·통계적 안정성) 하고 계수는 0에 가깝게 연속적으로 수축한다(변수 선택은 직접적이지 않음).\n기하학: 제곱오차 등고선(타원)과 L_2 제약(원/구)의 접점.\n\nLasso: L_1 규제와 희소성(sparsity)\n\n목적함수: \\(\\min_{\\beta} \\parallel y - X\\beta \\parallel^{2} + \\lambda \\parallel \\beta \\parallel_{1}\\)\n특징: 일부 계수가 정확히 0이 되며 변수 선택 효과가 나타난다.\n기하학: L_1 제약은 마름모(고차원에서는 뾰족한 다면체)라 접점이 축에 걸리기 쉬워 0이 자주 발생.\n\nElastic Net: 혼합 규제의 실용적 타협\n\\[\\min_{\\beta} \\parallel y - X\\beta \\parallel^{2} + \\lambda\\left( \\alpha \\parallel \\beta \\parallel_{1} + (1 - \\alpha) \\parallel \\beta \\parallel_{2}^{2} \\right)\\]\n\nLasso의 변수 선택 + Ridge의 안정성(상관 높은 변수군에서 그룹화 경향).\n\n규제의 핵심 메시지\n규제는 단순히 ”벌점 추가”가 아니라, 허용하는 함수 공간을 ’계수의 크기 제약’으로 축소하여 일반화 성능을 높이는 장치 로 이해해야 한다. 그러나 규제 강도 \\(\\lambda\\)와 같은 하이퍼파라미터는 데이터가 알려주지 않으면 정할 수 없다. 이 선택의 기준이 바로 검증오차이며, 결국 함수 선택은 검증오차 최소화로 구현된다.\n\n\n6. 검증오차 관점의 함수 선택\n훈련오차 vs 일반화오차\n예측모형의 성능을 논할 때 가장 먼저 구분해야 할 개념은 훈련오차(training error) 와 일반화오차(generalization error) 이다.\n훈련오차란, 학습에 사용된 데이터에 대해 모형이 얼마나 잘 맞는지를 측정한 값으로, 주어진 함수 f에 대해 다음과 같이 정의된다.\n\\(\\text{TrainErr}(f) = \\frac{1}{n}\\overset{n}{\\sum_{i = 1}}(y_{i} - f(x_{i}))^{2}\\). 이는 관측된 표본에서 계산되는 경험적 오차로서, 데이터가 주어지면 언제든지 계산할 수 있다.\n그러나 예측의 궁극적인 목적은 훈련 데이터에 잘 맞추는 것이 아니라, 아직 관측되지 않은 새로운 데이터에서도 잘 작동하는 것이다. 이를 수학적으로 표현한 것이 일반화오차 또는 테스트오차이다. 일반화오차는 테스트 분포, 즉 모집단 분포에서의 평균 제곱오차로 정의된다.\n\\(\\text{TestErr}(f) = \\mathbb{E}\\lbrack(Y - f(X))^{2}\\rbrack\\) 이 값은 확률변수 X, Y의 분포에 대한 기댓값이므로, 실제로는 직접 계산할 수 없고 추정의 대상이 된다.\n중요한 점은 훈련오차가 작다고 해서 일반화오차도 반드시 작은 것은 아니라는 사실이다. 함수 공간 \\(\\mathcal{F}\\)가 커질수록, 즉 모형이 복잡해질수록 훈련오차는 단조롭게 감소한다. 충분히 유연한 함수라면 훈련 데이터의 모든 점을 거의 완벽하게 통과하도록 만들 수 있기 때문이다. 그러나 이러한 복잡한 모형은 데이터에 포함된 우연한 잡음까지 함께 학습하게 되어, 새로운 데이터에 대해서는 오히려 예측 성능이 나빠질 수 있다.\n이 현상을 시각적으로 표현하면, 함수 공간의 복잡도에 따라 훈련오차는 지속적으로 감소하는 반면, 테스트오차는 U자 형태를 띠는 경우가 많다. 복잡도가 너무 작은 경우에는 모형이 데이터의 구조를 충분히 표현하지 못해 편향이 크고, 반대로 복잡도가 지나치게 큰 경우에는 분산이 커져 과적합이 발생한다. 이 두 효과의 균형점에서 테스트오차가 최소가 된다.\n따라서 예측모형 학습의 핵심은 훈련오차를 최소화하는 것이 아니라, 일반화오차를 최소화하는 함수의 복잡도를 선택하는 것이다. 이 관점에서 검증셋과 교차검증은 보이지 않는 테스트오차를 데이터로부터 간접적으로 추정하기 위한 핵심 도구가 된다.\n왜 훈련오차 최소화만으로는 충분하지 않은가\n예측모형의 학습 과정은 흔히 훈련오차를 최소화하는 문제로 표현된다. 실제로 많은 모형은 다음과 같은 경험적 위험을 최소화하도록 설계되어 있다.\n\\(\\text{TrainErr}(f) = \\frac{1}{n}\\overset{n}{\\sum_{i = 1}}(y_{i} - f(x_{i}))^{2}\\). 이 식은 주어진 데이터에 대해 모형이 얼마나 잘 맞는지를 정량적으로 측정해 주며, 계산도 간단하다. 그러나 훈련오차를 최소화하는 것만으로는 좋은 예측모형을 보장할 수 없다.\n그 이유는 훈련오차가 본질적으로 과거 데이터에 대한 적합도만을 반영하기 때문이다. 훈련 데이터는 모집단 분포에서 우연히 추출된 하나의 표본에 불과하며, 그 안에는 구조적인 신호뿐 아니라 우연적 잡음도 함께 포함되어 있다. 훈련오차를 지나치게 줄이려는 과정은 이 잡음까지 함께 학습하도록 모형을 유도할 수 있다.\n이를 극단적으로 생각해 보면, 충분히 유연한 함수 공간 \\(\\mathcal{F}\\)를 허용할 경우, 훈련 데이터의 모든 관측값을 정확히 통과하는 함수 \\(f\\)를 항상 구성할 수 있다. 이 경우 훈련오차는 거의 0이 된다. 그러나 이러한 함수는 데이터의 일반적인 구조를 학습했다기보다, 특정 표본의 우연한 패턴을 외운 것에 가깝다. 이처럼 훈련오차는 작지만 새로운 데이터에 대한 예측 성능이 나쁜 현상을 과적합이라 한다.\n이 문제를 수학적으로 이해하기 위해, 테스트오차를 다음과 같이 분해할 수 있다.\n\\[\\mathbb{E}\\lbrack(Y - \\widehat{f}(X))^{2}\\rbrack = \\underset{\\text{모형의 구조적 한계}}{\\underbrace{\\text{Bias}^{2}}} + \\underset{\\text{표본에 대한 민감도}}{\\underbrace{\\text{Variance}}} + \\underset{\\text{불가피한 오차}}{\\underbrace{\\sigma^{2}}}\\]\n훈련오차 최소화는 주로 편향을 줄이는 방향으로 작동한다. 모형을 복잡하게 만들수록 데이터에 더 잘 맞게 되어 편향은 감소한다. 그러나 동시에 추정된 함수 \\hat f는 표본의 작은 변화에도 크게 달라지게 되어 분산이 급격히 증가할 수 있다. 결국 테스트오차는 감소하지 않고 오히려 증가할 수 있다.\n또한 훈련오차는 함수 공간의 크기에 대해 단조 감소하는 성질을 가진다. 즉, 허용되는 함수의 수가 많아질수록 훈련오차는 항상 같거나 더 작아진다. 반면 테스트오차는 이러한 단조성을 가지지 않으며, 적절한 복잡도 수준에서 최소값을 가진다. 따라서 훈련오차만을 기준으로 모형을 선택하면, 필연적으로 지나치게 복잡한 모형을 선택하게 된다.\n이러한 이유로 예측 문제에서의 학습 목표는 훈련오차 최소화가 아니라, \\(\\text{TestErr}(f) = \\mathbb{E}\\lbrack(Y - f(X))^{2}\\rbrack\\)를 최소화하는 것이다. 그러나 이 값은 관측할 수 없으므로, 실제로는 검증오차나 교차검증 오차를 통해 간접적으로 추정한다.\n결론적으로, 훈련오차 최소화는 예측모형 학습의 필요조건일 수는 있으나 충분조건은 아니다. 좋은 예측모형이란, 훈련 데이터에 잘 맞는 모형이 아니라, 보이지 않는 데이터에서도 안정적으로 작동하는 모형이다. 이 차이를 인식하는 것이 통계적 예측과 머신러닝 방법론을 이해하는 출발점이다.\n검증오차는 왜 테스트오차의 대리 변수인가\n예측모형의 궁극적인 목표는 훈련 데이터에 대한 적합도가 아니라, 모집단 분포에서의 예측 성능, 즉 일반화오차 또는 테스트오차를 최소화하는 것이다. \\(\\text{TestErr}(f) = \\mathbb{E}\\lbrack(Y - f(X))^{2}\\rbrack\\) 값은 확률변수 (X, Y)의 분포에 대한 기댓값이므로, 모집단 분포를 알지 못하는 현실에서는 직접 계산할 수 없다. 예측 문제에서 발생하는 근본적인 어려움은 바로 이 지점에 있다. 우리가 최소화하고 싶은 대상은 관측 불가능하다.\n이러한 상황에서 등장하는 개념이 검증오차이다. 검증오차란, 학습에 사용되지 않은 데이터에 대해 계산한 예측오차로, 일반적으로 다음과 같이 정의된다.\n\\(\\text{ValErr}(f) = \\frac{1}{m}\\overset{m}{\\sum_{j = 1}}(y_{j}^{(v)} - f(x_{j}^{(v)}))^{2}\\), 여기서 \\(\\{(x_{j}^{(v)},y_{j}^{(v)})\\}\\)는 훈련 과정에서 사용하지 않은 검증 데이터이다. 검증오차가 테스트오차의 대리 변수로 사용될 수 있는 이유는, 검증 데이터가 훈련 데이터와 동일한 모집단 분포로부터 독립적으로 추출되었다는 가정에 있다. 이 가정이 성립한다면, 검증오차는 테스트오차에 대한 하나의 표본 평균으로 해석할 수 있다.\n즉, \\(\\mathbb{E}\\lbrack\\text{ValErr}(f)\\rbrack = \\text{TestErr}(f)\\)가 성립한다. 이 의미는 매우 중요하다. 검증오차는 불편추정량으로서 테스트오차를 추정하고 있는 것이다. 물론 검증오차는 표본 평균이므로 변동성을 가진다. 검증 데이터의 크기 m이 작을수록 검증오차의 분산은 커지며, 이로 인해 우연한 표본 변동에 의해 잘못된 모형을 선택할 위험이 존재한다. 이러한 이유로 단순한 단일 분할은 데이터가 충분히 클 때에만 안정적으로 작동한다.\n그럼에도 불구하고 검증오차는 훈련오차와 본질적으로 다른 정보를 제공한다. 훈련오차는 항상 훈련 데이터에 대해 낙관적으로 편향되어 있으며, 함수 공간이 커질수록 단조 감소한다. 반면 검증오차는 새로운 데이터에 대한 성능을 반영하므로, 함수의 복잡도 증가에 따라 증가하거나 감소할 수 있다. 이로 인해 검증오차는 과적합이 시작되는 지점을 감지할 수 있는 지표가 된다.\n요약하면, 검증오차는 다음 두 조건이 만족될 때 테스트오차의 합리적인 대리 변수로 작동한다.\n첫째, 검증 데이터가 훈련 데이터와 독립적이며 동일한 분포에서 추출되어야 한다.\n둘째, 검증 데이터가 학습 과정에 사용되지 않아야 한다.\n이 두 조건이 깨질 경우, 검증오차는 더 이상 테스트오차를 정확히 반영하지 못하며, 이는 데이터 누수로 이어진다.\n결국 검증오차를 사용하는 이유는 단순히 ”계산 가능하기 때문”이 아니다. 검증오차는 관측 불가능한 일반화오차를 데이터로부터 추정할 수 있게 해주는 유일한 통로이기 때문이다. 예측모형 선택이란, 검증오차를 통해 테스트오차를 간접적으로 최소화하는 과정이라고 이해할 수 있다.\n교차검증은 왜 검증오차의 분산을 줄이는가?\n검증오차는 테스트오차를 추정하기 위한 중요한 도구이지만, 단일한 검증셋을 사용하는 경우 그 값은 우연한 표본 분할에 크게 의존할 수 있다. 특히 데이터의 크기가 크지 않은 상황에서는, 어떤 관측치가 검증셋에 포함되었는지에 따라 검증오차가 상당히 달라질 수 있다. 이는 검증오차가 표본 평균으로서 상당한 분산을 가질 수 있음을 의미한다.\n이를 보다 명확히 이해하기 위해, 단일 검증셋을 사용하는 경우의 검증오차를 다시 적어보자.\n\\(\\text{ValErr}(f) = \\frac{1}{m}\\overset{m}{\\sum_{j = 1}}(y_{j}^{(v)} - f(x_{j}^{(v)}))^{2}\\). 이 값은 모집단 오차 \\(\\text{TestErr}(f)\\)의 불편추정량이지만, 표본 크기 m이 작을수록 분산이 커진다. 즉, 기대값은 맞지만 추정의 안정성은 떨어질 수 있다. 이로 인해, 우연히 낮은 검증오차를 보인 모형이 실제로는 더 나쁜 일반화 성능을 가질 가능성도 존재한다.\n교차검증(cross-validation)은 이러한 문제를 해결하기 위한 방법으로, 검증오차를 여러 번 계산하여 평균을 내는 절차로 이해할 수 있다. 가장 널리 사용되는 K-겹 교차검증에서는 데이터를 K개의 서로 겹치지 않는 부분집합으로 나눈 뒤, 각 부분집합을 한 번씩 검증셋으로 사용한다. 이때의 교차검증 오차는 다음과 같이 정의된다.\n\\(\\text{CV}_{K}(f) = \\frac{1}{K}\\overset{K}{\\sum_{k = 1}}\\text{Err}^{(k)}(f)\\), 여기서 \\(\\text{Err}^{(k)}(f)\\)는 k번째 분할에서 계산된 검증오차이다.\n교차검증이 검증오차의 분산을 줄이는 이유는 크게 두 가지로 설명할 수 있다.\n첫째, 여러 검증셋을 평균내는 효과이다. 각 \\(\\text{Err}^{(k)}(f)\\)는 동일한 테스트오차를 추정하지만, 서로 다른 표본 변동을 가진다. 이들을 평균내면, 분산은 대략 \\(\\text{Var}(\\text{CV}_{K}(f)) \\approx \\frac{1}{K^{2}}\\overset{K}{\\sum_{k = 1}}\\text{Var}(\\text{Err}^{(k)}(f))\\)의 형태로 감소한다. 완전히 독립은 아니지만, 평균화 효과 자체만으로도 단일 검증오차보다 훨씬 안정적인 추정이 가능해진다.\n둘째, 데이터 활용의 균형이다. 단일 검증셋 방식에서는 학습에 사용되지 않는 데이터가 고정되어 있지만, 교차검증에서는 모든 관측치가 학습과 검증에 번갈아 사용된다. 이로 인해 특정 관측치의 영향력이 과도하게 커지는 현상이 완화되며, 특정 분할에 대한 의존성이 줄어든다.\n이러한 이유로 교차검증 오차는 단일 검증오차보다 표본 분할에 덜 민감하며, 테스트오차의 형태를 보다 안정적으로 반영한다. 다만 교차검증 역시 완전한 해법은 아니다. K가 너무 작으면 분산 감소 효과가 제한적이고, K가 너무 크면 계산 비용이 증가하며 분할 간 상관이 커질 수 있다. LOOCV(K=n)의 경우, 편향은 작지만 분산이 오히려 커질 수 있다는 점도 함께 주의해야 한다.\n결론적으로 교차검증은 테스트오차를 직접 관측할 수 없는 상황에서, 검증오차의 변동성을 줄여 보다 신뢰할 수 있는 모형 선택 기준을 제공하는 방법이다. 예측모형 선택에서 교차검증이 표준 도구로 사용되는 이유는, 그것이 가장 이론적으로 타당하고 실용적인 일반화오차 추정 방식이기 때문이다.\n검증오차 최소화가 곧 최적 예측을 보장하지 않는 이유\n검증오차는 관측할 수 없는 테스트오차를 추정하기 위한 핵심 도구이며, 실제 예측모형 선택 과정에서는 검증오차를 최소화하는 모형이 선택된다. 그러나 검증오차를 최소화했다고 해서, 그 모형이 반드시 최적의 예측 성능을 가진다고 보장할 수는 없다. 이는 검증오차가 테스트오차의 추정값이지, 그 자체가 참값은 아니기 때문이다.\n이를 수식으로 표현하면, 우리가 실제로 최소화하고자 하는 대상은 \\(\\text{TestErr}(f) = \\mathbb{E}\\lbrack(Y - f(X))^{2}\\rbrack\\)이지만, 현실에서는 이를 직접 계산할 수 없으므로 다음과 같은 검증오차를 사용한다.\n\\(\\text{ValErr}(f) = \\frac{1}{m}\\overset{m}{\\sum_{j = 1}}(y_{j}^{(v)} - f(x_{j}^{(v)}))^{2}\\) 문제는 \\(\\text{ValErr}(f)\\)가 확률변수라는 점이다. 즉, 동일한 함수 f에 대해서도 어떤 검증 데이터가 선택되었는지에 따라 값이 달라질 수 있다.\n특히 여러 후보 모형 \\(\\mathcal{F} = \\{ f_{1},\\ldots,f_{M}\\}\\) 중에서 검증오차가 가장 작은 모형 \\(\\widehat{f} = \\arg\\min_{f \\in \\mathcal{F}}\\text{ValErr}(f)\\)을 선택하는 과정 자체가 확률적 선택 과정이라는 점이 중요하다. 이때 선택된 모형은 실제 테스트오차가 가장 작은 모형이 아니라, 우연히 검증오차가 작게 관측된 모형일 수 있다.\n이 현상은 다중 비교의 관점에서 이해할 수 있다. 후보 모형의 수 M이 많아질수록, 일부 모형은 단지 표본 변동에 의해 검증오차가 과도하게 작게 나타날 가능성이 커진다. 즉, \\(\\min_{f \\in \\mathcal{F}}\\text{ValErr}(f)\\)은 평균적으로 \\(\\min_{f \\in \\mathcal{F}}\\text{TestErr}(f)\\)보다 낙관적으로 편향된다. 이로 인해 검증오차를 기준으로 선택한 모형의 실제 일반화 성능은 기대보다 낮아질 수 있다.\n또한 검증오차 최소화는 본질적으로 모형 선택 편향을 수반한다. 검증 데이터를 사용해 모형을 선택한 후, 동일한 검증 데이터로 해당 모형의 성능을 평가하면, 그 평가는 선택 과정의 영향을 받은 값이 된다. 이는 검증오차가 ”평가 지표”가 아니라 ”선택 기준”으로 사용되었기 때문이다.\n이러한 이유로, 검증오차 최소화는 최적 예측을 위한 합리적인 전략이기는 하지만, 확률적 불확실성을 내포한 전략이기도 하다. 검증오차는 테스트오차의 기대값을 맞추지만, 최소값을 선택하는 순간 그 분포는 왜곡될 수 있다.\n결론적으로, 검증오차 최소화가 보장하는 것은 ”주어진 후보들 중에서 평균적으로 성능이 좋을 가능성이 높은 모형”이지, 항상 최적의 예측모형은 아니다. 이 한계를 인식하는 것이 중요하며, 이를 보완하기 위해 테스트셋 분리, 중첩 교차검증(nested cross-validation), 또는 독립적인 외부 검증이 사용된다.\n모형 선택 편향과 중첩 교차검증\n검증오차를 이용한 모형 선택은 예측 문제에서 필수적인 절차이지만, 이 과정에는 구조적인 편향이 내재되어 있다. 이를 모형 선택 편향이라 한다. 모형 선택 편향이란, 검증 데이터를 사용해 모형을 선택한 뒤, 동일한 데이터를 이용해 그 모형의 예측 성능을 평가할 때 발생하는 낙관적 편향을 의미한다.\n이 현상을 이해하기 위해, 후보 모형의 집합을 \\(\\mathcal{F} = \\{ f_{1},f_{2},\\ldots,f_{M}\\}\\)이라 하자. 각 모형에 대해 계산된 검증오차 \\(\\text{ValErr}(f_{m})\\)는 테스트오차 \\(\\text{TestErr}(f_{m})\\)의 불편추정량이지만, 이는 각 모형을 고정했을 때의 이야기이다. 실제로는 다음과 같은 선택 연산이 추가된다.\n\\(\\widehat{f} = \\arg\\min_{f \\in \\mathcal{F}}\\text{ValErr}(f)\\). 이때 문제가 되는 점은, 최소 연산이 확률변수의 분포를 왜곡한다는 사실이다. 여러 개의 확률변수 중 최소값을 취하면, 그 기대값은 각 확률변수의 기대값보다 체계적으로 작아진다. 즉, \\(\\mathbb{E}\\lbrack\\min_{f \\in \\mathcal{F}}\\text{ValErr}(f)\\rbrack &lt; \\min_{f \\in \\mathcal{F}}\\text{TestErr}(f)\\)가 일반적으로 성립한다. 이로 인해, 검증오차를 기준으로 선택된 모형의 성능은 실제보다 과도하게 좋아 보이게 된다.\n이 편향은 후보 모형의 수가 많을수록, 하이퍼파라미터 탐색 범위가 넓을수록, 그리고 교차검증을 반복적으로 사용할수록 더 심각해진다. 특히 머신러닝과 딥러닝에서는 수많은 하이퍼파라미터 조합을 탐색하므로, 모형 선택 편향의 위험이 더욱 커진다.\n이 문제를 해결하기 위한 표준적인 방법이 중첩 교차검증(nested cross-validation) 이다. 중첩 교차검증의 핵심 아이디어는 모형 선택과 성능 평가를 서로 다른 데이터 분할에서 수행하는 것이다.\n중첩 교차검증은 두 개의 반복 구조를 가진다.\n먼저 바깥쪽 교차검증(outer loop)에서는 데이터를 K개로 나누고, 각 분할을 한 번씩 테스트셋으로 사용한다. 이 바깥쪽 테스트셋은 오직 최종 성능 평가에만 사용되며, 모형 선택에는 전혀 관여하지 않는다.\n그 다음 안쪽 교차검증(inner loop)에서는, 바깥쪽에서 남겨진 학습 데이터를 다시 여러 분할로 나누어 교차검증을 수행한다. 이 안쪽 교차검증을 통해 하이퍼파라미터를 선택하고, 최적의 모형을 결정한다.\n절차를 요약하면 다음과 같다.\n1. 바깥쪽 분할에서 하나의 폴드를 테스트셋으로 분리한다.\n2. 나머지 데이터에 대해 안쪽 교차검증을 수행하여 최적의 모형을 선택한다.\n3. 선택된 모형을 바깥쪽 테스트셋에 적용하여 예측오차를 계산한다.\n4. 이 과정을 모든 바깥쪽 폴드에 대해 반복하고, 테스트오차를 평균낸다.\n이때 바깥쪽에서 계산된 오차는 모형 선택 과정을 포함한 전체 학습 절차의 일반화 성능을 추정하게 된다. 즉, 중첩 교차검증은 \\(\\mathbb{E}\\lbrack\\text{TestErr}(\\widehat{f})\\rbrack\\)를 직접적으로 추정하는 방법이라고 볼 수 있다.\n중첩 교차검증의 중요한 장점은, 모형 선택 편향을 구조적으로 차단한다는 점이다. 테스트셋이 모형 선택에 전혀 사용되지 않기 때문에, 평가 결과가 선택 과정의 영향을 받지 않는다. 이는 예측 성능을 보고하거나, 서로 다른 알고리즘을 공정하게 비교해야 하는 상황에서 특히 중요하다.\n다만 중첩 교차검증은 계산 비용이 크다는 단점이 있다. 데이터 크기가 크거나 모형 학습 비용이 높은 경우에는 실무적으로 부담이 될 수 있다. 이러한 경우에는 충분히 큰 독립 테스트셋을 별도로 확보하는 방식이 대안이 될 수 있다.\n결론적으로, 모형 선택 편향은 검증오차 기반 학습에서 피할 수 없는 구조적 문제이며, 중첩 교차검증은 이를 이론적으로 가장 깔끔하게 해결하는 방법이다. 예측 성능을 엄밀하게 평가하고자 할 때, ”교차검증을 했다”는 사실보다 교차검증이 어떤 역할로 사용되었는지가 훨씬 더 중요하다.\n\n\n7. 예측모형: 통계적 사고와 ML 사고\n예측과 추론의 목적 차이: 왜 p-value는 여기서 중요하지 않은가\n통계 분석에서 예측과 추론은 종종 같은 방법을 사용함에도 불구하고, 그 목적과 평가 기준은 근본적으로 다르다. 이 차이를 명확히 구분하지 않으면, 예측 문제에서 부적절한 기준을 적용하거나, 추론 문제에서 잘못된 결론에 도달할 위험이 있다.\n추론의 목적은 설명변수와 반응변수 사이의 관계에 대한 불확실성을 정량화하는 데 있다. 예를 들어 선형회귀 모형 \\(Y = \\beta_{0} + \\beta_{1}X_{1} + \\cdots + \\beta_{p}X_{p} + \\varepsilon\\)에서 추론의 핵심 질문은 다음과 같다. ”계수 \\(\\beta_{j}\\)는 0과 유의미하게 다른가?“, ”이 변수의 효과는 통계적으로 존재한다고 말할 수 있는가?”\n이러한 질문에 답하기 위해 추론에서는 표본분포, 표준오차, 신뢰구간, 그리고 p-value가 사용된다. p-value는 특정 귀무가설 하에서 관측된 통계량이 얼마나 극단적인지를 나타내는 지표로, 모수에 대한 가설 검정을 위해 설계된 개념이다.\n반면 예측의 목적은 전혀 다르다. 예측 문제에서의 핵심 질문은 ”이 모형이 새로운 데이터에 대해 얼마나 정확한 값을 산출하는가?“이다. 즉 관심의 대상은 개별 계수의 해석이나 통계적 유의성이 아니라, \\(\\widehat{f}(X) \\approx \\mathbb{E}\\lbrack Y \\mid X\\rbrack\\)라는 함수 근사가 얼마나 잘 이루어졌는가에 있다.\n이 관점에서 보면, p-value는 예측 성능을 직접적으로 평가하는 정보가 아니다. 어떤 변수의 회귀계수가 통계적으로 유의하지 않더라도, 그 변수가 포함된 모형이 전체적으로 더 나은 예측 성능을 보일 수 있다. 반대로 매우 작은 p-value를 가진 변수만으로 구성된 모형이 반드시 좋은 예측모형이 되는 것도 아니다.\n이 차이는 손실함수의 관점에서 더욱 명확해진다. 예측 문제에서의 학습과 평가는 다음과 같은 손실함수를 중심으로 이루어진다.\n\\(\\mathbb{E}\\lbrack(Y - f(X))^{2}\\rbrack,\\mathbb{E}\\lbrack|Y - f(X)|\\rbrack\\). 즉, 예측 오차의 크기가 유일한 평가 기준이다. 이때 모형 내부의 계수가 어떤 분포를 가지는지, 또는 특정 계수가 0인지 아닌지는 부차적인 문제이다.\n또한 p-value는 모형이 올바르게 지정되었다는 가정 하에서만 해석 가능하다. 선형성, 등분산성, 독립성, 정규성 등의 가정이 위배될 경우, p-value의 의미는 급격히 약화된다. 그러나 예측 문제에서는 이러한 가정이 반드시 성립할 필요가 없다. 비선형 모형, 트리, 신경망과 같이 p-value 자체가 정의되지 않는 모형들도 뛰어난 예측 성능을 가질 수 있다.\n더 나아가, 예측 문제에서 변수 선택을 p-value에 의존할 경우 심각한 문제가 발생할 수 있다. p-value 기반 변수 선택은 훈련 데이터에 대한 통계적 유의성을 기준으로 하기 때문에, 일반화 성능과는 직접적인 연관이 없다. 이는 앞서 논의한 모형 선택 편향을 더욱 악화시킬 수 있다.\n이러한 이유로 예측 문제에서의 모형 선택 기준은 p-value가 아니라 검증오차, 교차검증 오차, 그리고 테스트오차와 같은 일반화 성능 지표가 되어야 한다.\n결론적으로, p-value는 ”이 변수가 통계적으로 의미 있는가?“라는 질문에는 적절한 도구이지만, ”이 모형이 미래를 잘 예측하는가?”라는 질문에는 답을 주지 않는다. 예측과 추론은 서로 경쟁하는 개념이 아니라, 서로 다른 질문에 답하는 서로 다른 목적의 분석이다. 예측 문제에서 중요한 것은 설명의 그럴듯함이 아니라, 보이지 않는 데이터에 대한 성능이다.\n통계적 사고에서 ML 사고로의 전환\n전통적인 통계 분석과 머신러닝은 동일한 데이터와 유사한 모형을 사용하는 경우가 많지만, 문제를 바라보는 사고방식에는 본질적인 차이가 있다. 이 차이를 이해하는 것은 예측모형을 올바르게 설계하고 해석하는 데 있어 매우 중요하다.\n통계적 사고의 출발점은 모형 가정이다. 선형회귀를 예로 들면, 분석자는 먼저 반응변수와 설명변수 사이의 관계가 선형이라는 가정을 세우고, 오차항에 대해 독립성, 등분산성, 정규성 등의 조건을 부여한다. 이러한 가정 하에서 관심의 대상은 모수 \\(\\beta\\)이며, 데이터는 이 모수를 추론하기 위한 수단으로 사용된다. 즉, 통계적 사고에서의 핵심 질문은 ”이 모수는 무엇이며, 얼마나 정확하게 추론할 수 있는가?“이다.\n이와 달리 머신러닝적 사고의 출발점은 예측 성능이다. ML에서는 모형이 현실을 얼마나 정확히 설명하는가보다, 새로운 데이터에 대해 얼마나 잘 작동하는가가 우선적인 관심사이다. 따라서 특정한 함수 형태나 분포 가정을 강하게 두기보다는, 다양한 함수 공간을 열어두고 데이터가 그중에서 적절한 함수를 선택하도록 한다. 이때 관심의 대상은 개별 모수가 아니라, 전체 함수 \\(\\widehat{f}(X)\\)의 일반화 성능이다.\n이 차이는 손실함수의 역할에서도 분명히 드러난다. 통계적 추론에서는 우도 함수와 그로부터 유도되는 검정통계량이 중심이 되는 반면, 머신러닝에서는 명시적인 손실함수 \\(L(Y,f(X))\\)를 정의하고, 그 기댓값을 최소화하는 방향으로 학습이 이루어진다. 이 관점에서 보면, 예측 문제는 본질적으로 최적화 문제이며, 확률모형은 그중 하나의 선택지일 뿐이다.\n또 하나의 중요한 전환점은 모형 복잡도에 대한 태도이다. 통계적 사고에서는 과도한 복잡성이 해석을 어렵게 하고 가정 위반의 위험을 높이기 때문에, 가능한 한 단순한 모형이 선호된다. 반면 머신러닝에서는 복잡한 모형 자체를 문제로 보지 않는다. 대신, 복잡도가 일반화 성능을 해치지 않도록 규제와 검증을 통해 통제한다. 즉, ML 사고에서는 ”모형은 복잡해도 좋지만, 검증오차로 검증되어야 한다”는 원칙이 작동한다.\n데이터의 역할 역시 다르게 인식된다. 통계적 추론에서 데이터는 불확실성을 줄이기 위한 표본이며, 표본의 크기가 작더라도 이론적 분포에 근거한 추론이 가능하다. 반면 머신러닝에서는 데이터의 양이 모형 성능을 결정하는 핵심 자원이며, 데이터가 많아질수록 더 유연한 함수 공간을 사용할 수 있다. 이 때문에 ML에서는 ”좋은 가정”보다 ”충분한 데이터와 검증 절차”가 더 중요해진다.\n이러한 차이를 종합하면, 통계적 사고에서 머신러닝 사고로의 전환은 다음과 같이 요약할 수 있다.\n통계적 사고는 모형 가정 → 모수 추론 → 해석과 검정의 흐름을 따른다면, 머신러닝 사고는 손실함수 정의 → 함수 공간 선택 → 검증 기반 성능 평가의 흐름을 따른다.\n이 전환은 통계를 부정하는 것이 아니라, 통계적 사고를 더 넓은 예측 프레임 안으로 확장하는 것이다. 실제로 많은 머신러닝 방법은 통계적 원리에서 출발하며, 규제, 교차검증, 편향–분산 트레이드오프와 같은 핵심 개념은 양쪽을 연결하는 공통 언어이다.\n결론적으로, 예측 문제에서 요구되는 사고방식은 ”이 모형이 의미 있는가?“가 아니라 ”이 모형이 미래에도 잘 작동하는가?”이다. 이 질문에 답하기 위해서는 통계적 추론의 도구를 넘어, 머신러닝적 검증과 선택의 논리를 함께 받아들여야 한다.",
    "crumbs": [
      "기초수학",
      "📄 예측방법: 서론"
    ]
  },
  {
    "objectID": "notes/mldl_method/index.html",
    "href": "notes/mldl_method/index.html",
    "title": "AI · ML 방법론",
    "section": "",
    "text": "이 섹션에서는 머신러닝·딥러닝을 통계학적 방법론의 확장으로 이해하는 관점을 다룬다.\n예측·분류 모형의 설계, 학습과 검증, 불확실성 평가, 설명가능성 등\n현대 AI 방법론의 핵심 논의를 체계적으로 정리한다.\n☚ 왼쪽 메뉴에서 주제를 선택하시오.",
    "crumbs": [
      "기초수학",
      "【HOME】"
    ]
  },
  {
    "objectID": "notes/math_stat/estimation.html",
    "href": "notes/math_stat/estimation.html",
    "title": "수리 통계 6. 추정",
    "section": "",
    "text": "chapter 1. 데이터 축소 원칙\n확률표본 \\(X_{1},\\ldots,X_{n}\\)으로부터 미지의 모수 \\(\\theta\\)에 대해 추론을 시도한다. 표본크기 \\(n\\)이 크면 관찰된 표본 데이터 \\(x_{1},\\ldots,x_{n}\\)은 해석하기 어려운 긴 수열이 될 수 있으므로, 데이터 내 정보를 요약하기 위해 확률표본의 함수인 통계량을 계산한다. 데이터 축소 또는 요약의 한 형태인 \\(T(\\mathbf{X})\\)는 동일한 값을 갖더라도 상이한 표본일 수 있다.\n여기서는 데이터 축소의 세 가지 원리에 대해 살펴본다. 미지의 모수 \\(\\theta\\)에 대한 중요한 정보를 버리지 않고 데이터 요약을 수행하는 방법과, 반대로 \\(\\theta\\)에 대한 지식 획득에 무관한 정보를 성공적으로 제거하는 방법이다.\n\n충분성 원리 sufficiency principle: 데이터를 요약하는 과정에서도 \\(\\theta\\)에 관한 정보를 버리지 않는 방법을 제시\n우도 원리 likelihood principle: 관찰된 확률표본을 통해 얻어진 \\(\\theta\\)에 대한 모든 정보를 담고 있는 파라미터의 함수를 기술\n등분산성 원리 equivariance principle: 모형의 중요한 특성들을 유지하면서 또 다른 형태의 데이터 축소를 가능하게 하는 방법을 제시\n\n\n1. 충분성 원리\n충분통계량은 어떤 모수 \\(\\theta\\)에 대해, 확률표본에 포함된 \\(\\theta\\)에 관한 모든 정보를 포착하는 통계량을 의미한다. 확률표본의 충분통계량 값 이외에 추가로 얻을 수 있는 표본의 다른 정보는 \\(\\theta\\)에 대해 더 이상 아무런 정보를 제공하지 않는다.\n【정의】 \\(T(\\mathbf{X})\\)가 모수 \\(\\theta\\)에 대한 충분통계량이라면 \\(\\theta\\)에 대한 모든 추론은 확률표본 \\(\\mathbf{X}\\)의 전체 값이 아니라 \\(T(\\mathbf{X})\\)의 값만을 통해 이루어져야 한다. 즉, 두 표본 점 \\(\\mathbf{x}\\)와 \\(\\mathbf{y}\\)가 \\(T(\\mathbf{x}) = T(\\mathbf{y})\\)를 만족하면, \\(\\theta\\)에 대한 추론은 \\(\\mathbf{X} = \\mathbf{x}\\)가 관측되었을 때와 \\(\\mathbf{X} = \\mathbf{y}\\)가 관측되었을 때 동일해야 한다.\n\n(1) 충분통계량\n【정의】 통계량 \\(T(\\mathbf{X})\\)가 모수 \\(\\theta\\)에 대한 충분통계량이 되기 위한 조건은 다음과 같다: 조건부 분포 \\(f_{\\mathbf{X}|T(\\mathbf{X})}(\\mathbf{x}|T(\\mathbf{X}) = t)\\)가 모수 \\(\\theta\\)에 의존하지 않을 때, \\(T(\\mathbf{X})\\)는 \\(\\theta\\)에 대한 충분통계량이다.\n즉, 확률표본 \\(\\mathbf{X}\\)에 대한 조건부 분포가 통계량 \\(T(\\mathbf{X})\\)의 값만 주어진 상태에서 \\(\\theta\\)와 무관하다면, \\(T(\\mathbf{X})\\)만 가지고도 \\(\\theta\\)에 대해 모든 정보를 담고 있다고 본다.\n【정리】 \\(p(\\mathbf{x}|\\theta)\\)를 확률표본 \\(\\mathbf{X}\\)의 결합확률밀도함수라 하고, 통계량 \\(T(\\mathbf{X})\\)의 확률밀도함수를 \\(q(t|\\theta)\\)라 하자. 통계량 \\(T(\\mathbf{X})\\)가 모수 \\(\\theta\\)에 대한 충분통계량이 되기 위한 필요충분조건은, 모든 표본 \\(\\mathbf{x}\\)에 대하여 비율 \\(\\frac{p(\\mathbf{x}|\\theta)}{q(T(\\mathbf{X})|\\theta)} = H(x_1,x_2,...,x_n)\\) 이 모수 \\(\\theta\\)에 의존하지 않는 상수일 때이다.\n【예제 ①】 \\(X_{1},\\ldots,X_{n}\\)을 모수 \\(\\theta \\in (0,1)\\)인 베르누이 분포를 따르는 확률표본이라 하자. \\(T(\\mathbf{X}) = \\sum X_{i}\\)는 \\(\\theta\\)에 대한 충분통계량이다.\n\\(T(\\mathbf{X}) = \\sum X_{i} \\sim B(n,\\theta)\\)이므로 \\(q(T(\\mathbf{x})|\\theta) \\sim \\binom{n}{t}\\theta^{t}(1 - \\theta)^{n - t}\\)\n결합확률밀도함수는 \\(p(\\mathbf{x}|\\theta) = \\prod\\theta^{x_{i}}(1 - \\theta)^{1 - x_{i}}\\)이다.\n\\[\\frac{p(\\mathbf{x}|\\theta)}{q(T(\\mathbf{x})|\\theta)} = = \\frac{\\theta^{t}(1 - \\theta)^{n - t}}{\\binom{n}{t}\\theta^{t}(1 - \\theta)^{n - t}} = \\frac{1}{\\binom{n}{t}} = \\frac{1}{\\binom{n}{\\sum x_{i}}}\\]\n【예제 ②】 \\(X_{1},\\ldots,X_{n}\\)을 \\(\\sigma^{2}\\)가 알려진 \\(N(\\mu,\\sigma^{2})\\) 정규분포를 따르는 확률표본이라 하자. \\(T(\\mathbf{X}) = \\sum X_{i}/n\\)는 \\(\\mu\\)에 대한 충분통계량이다.\n\\[f(\\mathbf{x}|\\mu) = \\overset{n}{\\prod_{i = 1}}(2\\pi\\sigma^{2})^{- 1/2}\\exp\\left( - \\frac{(x_{i} - \\mu)^{2}}{2\\sigma^{2}} \\right)\\]\n\\[\\overline{X} \\sim N(\\mu,\\sigma^{2}/n)\\]\n\\[\\frac{f(\\mathbf{x}|\\theta)}{q(T(\\mathbf{x})|\\theta)} = (2\\pi\\sigma^{2})^{- n/2}\\exp\\left( - \\frac{\\sum_{i = 1}^{n}(x_{i} - \\overline{x})^{2} + n(\\overline{x} - \\mu)^{2}}{2\\sigma^{2}} \\right)/(2\\pi\\sigma^{2}/n)^{- 1/2}\\exp\\left( - \\frac{n(\\overline{x} - \\mu)^{2}}{2\\sigma^{2}} \\right)\\]\n\\[= n^{- 1/2}(2\\pi\\sigma^{2})^{- (n - 1)/2}\\exp\\left( - \\frac{\\sum_{i = 1}^{n}(x_{i} - \\overline{x})^{2}}{2\\sigma^{2}} \\right)\\]\n【예제 ③】 지수족 분포를 벗어나면 순서통계량보다 작은 차원의 충분통계량을 찾는 것은 불가능 하다. 비모수 검정이 필요하다.\n\\(X_{1},\\ldots,X_{n}\\)이 임의의 확률밀도함수 \\(f(x)\\)로부터 확률분포로 추출되었다고 하자. 이때 \\(f(x)\\)에 대한 추가적인 정보가 없는 경우(비모수 추정 상황)에는 확률표본의 순서통계량만이 정보를 담는다. 즉,\n\\(f(\\mathbf{x}) = \\overset{n}{\\prod_{i = 1}}f(x_{i}) = \\overset{n}{\\prod_{i = 1}}f(x_{(i)})\\)가 성립하므로 위의 정리에 따라 순서통계량이 충분통계량이 된다.\n(분해 정리, Factorization Theorem) \\(f(\\mathbf{x}|\\theta)\\)를 표본 \\(\\mathbf{X}\\)의 결합확률밀도함수라 하자. 통계량 \\(T(\\mathbf{X})\\)가 \\(\\theta\\)에 대한 충분통계량이 되기 위한 필요충분조건은 다음과 같다. 모든 \\(\\mathbf{x}\\)와 \\(\\theta\\)에 대해, 함수 \\(g(t|\\theta)\\)와 \\(h(\\mathbf{x})\\)가 존재하여 \\(f(\\mathbf{x}|\\theta) = g(T(\\mathbf{x})|\\theta)h(\\mathbf{x})\\)를 만족할 때, \\(T(\\mathbf{X})\\)는 \\(\\theta\\)에 대한 충분통계량이다.\n표본의 확률함수를 충분통계량만을 통한 함수 \\(g\\)와 \\(\\mathbf{x}\\)에만 의존하는 함수 \\(h\\)로 분해할 수 있으면, 그 통계량은 충분하다. 분해정리를 이용하여 충분통계량을 찾기 위해서는 확률표본의 결합확률밀도함수를 두 부분으로 분해한다. 한 부분은 모수 \\(\\theta\\)에 의존하지 않는 부분이고 다른 부분은 \\(\\theta\\)에 의존하는 부분이다.\n【예제 ② 계속】 \\(X_{1},\\ldots,X_{n}\\)을 \\(\\sigma^{2}\\)가 알려진 \\(N(\\mu,\\sigma^{2})\\) 정규분포를 따르는 확률표본이라 하자. \\(T(\\mathbf{X}) = \\sum X_{i}/n\\)는 \\(\\mu\\)에 대한 충분통계량이다.\n\\[f(\\mathbf{x}|\\mu) = (2\\pi\\sigma^{2})^{- n/2}\\exp\\left( - \\overset{n}{\\sum_{i = 1}}(x_{i} - \\overline{x})^{2}/(2\\sigma^{2}) \\right)\\exp\\left( - n(\\overline{x} - \\mu)^{2}/(2\\sigma^{2}) \\right)\\]\n\\[h(\\mathbf{x}) = (2\\pi\\sigma^{2})^{- n/2}\\exp\\left( - \\frac{1}{2\\sigma^{2}}\\overset{n}{\\sum_{i = 1}}(x_{i} - \\overline{x})^{2} \\right)\\]\n\\(g(t|\\mu) = \\exp\\left( - \\frac{n(t - \\mu)^{2}}{2\\sigma^{2}} \\right)\\)이므로 \\(f(\\mathbf{x}|\\mu) = g(t|\\mu)h(\\mathbf{x})\\)\n【예제 ④】 \\(X_{1},\\ldots,X_{n}\\)을 \\(f(x|\\theta) = \\frac{1}{\\theta},x = 1,2,...,\\theta\\) 이산형 균일분포를 따르는 확률표본이라 하자. \\(max(x_{i})\\)는 \\(\\theta\\)에 대한 충분통계량이다.\n\\[f(\\mathbf{x}|\\theta) = \\{\\begin{matrix}\n\\theta^{- n}, & \\text{if}x_{i} \\in \\{ 1,2,\\ldots,\\theta\\}\\text{for all}i = 1,\\ldots,n \\\\\n0, & \\text{otherwise}\n\\end{matrix}\\]\n\\[h(\\mathbf{x}) = \\{\\begin{matrix}\n1, & \\text{if}x_{i} \\in \\{ 1,2,\\ldots\\}\\text{for all}i = 1,\\ldots,n \\\\\n0, & \\text{otherwise}\n\\end{matrix}\\]\n\\(T(\\mathbf{x}) = \\max_{i}x_{i}\\)이면, \\(g(t|\\theta) = \\{\\begin{matrix}\n\\theta^{- n}, & \\text{if}t \\leq \\theta \\\\\n0, & \\text{otherwise}\n\\end{matrix}\\)\n【예제 ⑤】 \\(X_{1},\\ldots,X_{n}\\)을 \\(N(\\mu,\\sigma^{2})\\) 정규분포를 따르는 확률표본이라 하자. \\(T_{1}(\\mathbf{x}) = \\overline{X}\\), \\(T_{2}(\\mathbf{x}) = S^{2} = \\frac{1}{n - 1}\\overset{n}{\\sum_{i = 1}}(X_{i} - \\overline{X})^{2}\\)은 모수 \\((\\mu,\\sigma^{2})\\)에 대한 충분통계량이다.\n\\(T_{1}(\\mathbf{x}) = \\overline{X}\\), \\(T_{2}(\\mathbf{x}) = S^{2}\\)에 대하여 \\(h(x) = 1\\)이고\n\\[g(t_{1},t_{2} \\mid \\mu,\\sigma^{2}) = (2\\pi\\sigma^{2})^{- n/2}\\exp\\left( - \\frac{n(t_{1} - \\mu)^{2} + (n - 1)t_{2}}{2\\sigma^{2}} \\right)\\]\n\\[f(\\mathbf{x} \\mid \\mu,\\sigma^{2}) = g\\left( T_{1}(\\mathbf{x}),T_{2}(\\mathbf{x}) \\mid \\mu,\\sigma^{2} \\right)h(\\mathbf{x})\\]\n【정리】 \\(f(x \\mid \\theta) = h(x)c(\\mathbf{\\theta})\\exp\\left( \\overset{k}{\\sum_{i = 1}}w_{i}(\\mathbf{\\theta})t_{i}(x) \\right)\\) 지수족 분포로부터의 충분통계량은 \\(T(\\mathbf{X}) = \\left( \\overset{n}{\\sum_{j = 1}}t_{1}(X_{j}),\\overset{n}{\\sum_{j = 1}}t_{2}(X_{j}),\\ldots,\\overset{n}{\\sum_{j = 1}}t_{k}(X_{j}) \\right)\\)이다.\n\n\n(2) 최소 충분통계량\n충분통계량은 모수 \\(\\theta\\)에 대한 정보를 표본에서 손실 없이 요약할 수 있는 통계량이다. 그런데 모든 충분통계량이 ”작거나 간단한” 것은 아닙니다. 어떤 충분통계량은 더 많은 정보를 담고 있을 수도 있다. 최소 minimal 충분통계량은 다음을 만족한다.\n정보를 모두 보존하면서 가장 작고 요약된 형태로 되어 있는 충분통계량이다.\n즉, 중복 없이 핵심 정보만 유지하는 가장 효율적인 통계량이다.\n【정의】 어떤 충분통계량 \\(T(\\mathbf{X})\\)이 모든 다른 충분통계량 \\(T'(\\mathbf{X})\\)에 대해, \\(T(\\mathbf{X})\\)이 \\(T'(\\mathbf{X})\\)의 함수로 표현될 수 있으면 \\(T(\\mathbf{X})\\)을 최소 충분통계량이라고 한다.\n【정리】 확률표본 \\(\\mathbf{X}\\)의 확률밀도함수가 \\(f(\\mathbf{x}|\\theta)\\)로 주어졌다고 하자.\n어떤 함수 \\(T(\\mathbf{x})\\)가 존재하여, 모든 표본점 \\(\\mathbf{x},\\mathbf{y}\\)에 대해 \\(\\frac{f(\\mathbf{x}|\\theta)}{f(\\mathbf{y}|\\theta)}\\)가 \\(\\theta\\)에 대해 상수가 되는 경우가 \\(T(\\mathbf{x}) = T(\\mathbf{y})\\)일 때와 정확히 일치한다면, \\(T(\\mathbf{X})\\)는 \\(\\theta\\)에 대한 최소 충분통계량이다.\n【예제 ⑥】 확률표본 \\(X_{1},\\ldots,X_{n}\\)이 \\(\\text{N}(\\mu,\\sigma^{2})\\)에서 추출되었고 모수 \\((\\mu,\\sigma^{2})\\) 둘 다 모를 경우 \\((\\overline{x},s^{2})\\)는 최소 충분통계량이다.\n\\[\\frac{f(\\mathbf{x}|\\mu,\\sigma^{2})}{f(\\mathbf{y}|\\mu,\\sigma^{2})} = \\exp\\left( \\left\\lbrack - n({\\overline{x}}^{2} - {\\overline{y}}^{2}) + 2n\\mu(\\overline{x} - \\overline{y}) - (n - 1)(s_{x}^{2} - s_{y}^{2}) \\right\\rbrack/(2\\sigma^{2}) \\right)\\]\n이 비가 \\((\\mu,\\sigma^{2})\\)에 대해 상수가 되려면 \\(\\overline{x} = \\overline{y},s_{x}^{2} = s_{y}^{2}\\)\n이어야 한다. 따라서, 표본평균 \\(\\overline{X}\\)와 표본분산 \\(S^{2}\\)는 \\((\\mu,\\sigma^{2})\\)\n에 대한 최소 충분통계량이다.\n【예제 ⑦】 확률표본 \\(X_{1},\\ldots,X_{n}\\)이 \\(U(\\theta,\\theta + 1)\\)에서 추출되었다고 하자. \\(T(\\mathbf{X}) = (X_{(1)},X_{(n)})\\)은 모수 \\(\\theta\\)에 대한 최소 충분통계량이다.\n\\[f(\\mathbf{x}|\\theta) = \\{\\begin{matrix}\n1 & \\text{if}\\theta &lt; x_{i} &lt; \\theta + 1,\\text{for all}i = 1,\\ldots,n \\\\\n0 & \\text{otherwise}\n\\end{matrix}\\]\n\\[f(\\mathbf{x}|\\theta) = \\{\\begin{matrix}\n1 & \\text{if}\\max_{i}x_{i} - 1 &lt; \\theta &lt; \\min_{i}x_{i} \\\\\n0 & \\text{otherwise}\n\\end{matrix}\\]\n두 표본 \\(\\mathbf{x},\\mathbf{y}\\)에 대하여, 비율 \\(\\frac{f(\\mathbf{x}|\\theta)}{f(\\mathbf{y}|\\theta)}\\)이 \\(\\theta\\)에 대해 항상 일정하려면 \\(\\min_{i}x_{i} = \\min_{i}y_{i},\\max_{i}x_{i} = \\max_{i}y_{i}\\)이어야 한다.\n최소 충분통계량은 유일하지 않다.\n\\(T'(\\mathbf{X}) = (X_{(n)} - X_{(1)},(X_{(n)} + X_{(1)})/2)\\),\n\\(T'(\\mathbf{X}) = \\left( \\overset{n}{\\sum_{i = 1}}X_{i},\\overset{n}{\\sum_{i = 1}}X_{i}^{2} \\right)\\) 또한 최소 충분통계량이다.\n\n\n(3) 보조 통계량\n【정의】 통계량 \\(S(\\mathbf{X})\\)의 분포가 모수 \\(\\theta\\)와 무관할 때, 이를 보조 ancillary 통계량이라고 한다.\n【예제 ⑦ 계속】 확률표본 \\(X_{1},\\ldots,X_{n}\\)이 \\(U(\\theta,\\theta + 1)\\)에서 추출되었다고 하자. \\(T(\\mathbf{X}) = (X_{(1)},X_{(n)})\\)은 모수 \\(\\theta\\)에 대한 최소 충분통계량이므로 \\(R = X_{(n)} - X_{(1)}\\)은 보조 통계량이다.\n\\[g(x_{(1)},x_{(n)} \\mid \\theta) = \\{\\begin{matrix}\nn(n - 1)(x_{(n)} - x_{(1)})^{n - 2} & \\text{if}\\theta &lt; x_{(1)} &lt; x_{(n)} &lt; \\theta + 1 \\\\\n0 & \\text{otherwise}.\n\\end{matrix}\\]\n\\(R = X_{(n)} - X_{(1)},M = \\frac{X_{(1)} + X_{(n)}}{2}\\) 변수변환 하면,\n\\[h(r,m \\mid \\theta) = \\{\\begin{matrix}\nn(n - 1)r^{n - 2}, & \\text{if}0 &lt; r &lt; 1,\\theta + \\frac{r}{2} &lt; m &lt; \\theta + 1 - \\frac{r}{2} \\\\\n0, & \\text{otherwise}.\n\\end{matrix}\\]\n\\[h(r \\mid \\theta) = n(n - 1)r^{n - 2}(1 - r),0 &lt; r &lt; 1 \\sim Beta(n - 1,2)\\]\n\\(R = X_{(n)} - X_{(1)}\\)의 확률밀도함수는 모수 \\(\\theta\\)에 의존하지 않는다.\n\n\n(4) 완비 통계량\n최소 충분통계량은 표본으로부터 모수 \\(\\theta\\)에 대한 모든 정보를 유지하면서, 그 외의 불필요한 정보를 최대한 제거한 통계량이다. 즉, 표본에서 모수와 관련된 핵심 정보만을 남기는 데이터 축약 방법이다. 반면, 보조 통계량은 그 분포가 모수 \\(\\theta\\)에 의존하지 않는 통계량으로 모수에 대한 정보를 전혀 담고 있지 않다.\n이 둘은 개념적으로 구별되지만, 반드시 독립적이지는 않다. 예를 들어, \\(uniform(\\theta,\\theta + 1)\\)에서, 최소값과 최대값의 조합인 \\((X_{(n)} - X_{(1)},(X_{(n)} + X_{(1)})/2)\\)은 최소 충분통계량이 되고, 그 중 \\(X_{(n)} - X_{(1)}\\)은 보조 통계량이 된다. 이 경우, 최소 충분통계량과 보조 통계량은 서로 독립하지 않으며, 오히려 하나의 구성요소가 된다.\n보조 통계량은 모수 추정 정밀도에 기여\n\\(X_{1},X_{2}\\)가 다음 이산분포 에서 독립적으로 관측되었다.\\(P_{\\theta}(X = \\theta) = P_{\\theta}(X = \\theta + 1) = P_{\\theta}(X = \\theta + 2) = \\frac{1}{3}\\). 순서 통계량 \\(X_{(1)},X_{(2)}\\)으로 \\(R = X_{(2)} - X_{(1)},M = (X_{(1)} + X_{(2)})/2\\)를 정의하면, \\((R,M)\\)은 최소 충분통계량이고, \\(R\\)은 보조 통계량이다.\n그러나 \\(R\\)이 보조 통계량임에도 불구하고 모수 \\(\\theta\\)에 대해 간접적으로 중요한 정보를 제공할 수 있다. 예를 들어, 단순히 \\(M = m\\)이라는 정보만 알고 있을 때, 가능한 \\(\\theta\\) 값은 \\(m,m - 1,m - 2\\) 세 가지가 된다. 하지만 추가로 \\(R = 2\\)라는 정보를 알게 되면, \\(X_{(1)} = m - 1,X_{(2)} = m + 1\\)이 되어, 가능한 \\(\\theta\\) 값이 유일하게 \\(m - 1\\)로 결정된다. 즉, 보조 통계량 \\(R\\)이 모수 추정의 정밀도를 높이는 데 기여한 것이다.\n【정의】 어떤 통계량 \\(T(\\mathbf{X})\\)에 대해 확률분포족 \\(f(t|\\theta)\\)가 있을 때, 이 분포족을 완비라고 부른다. 완비란, 모든 \\(\\theta\\)에 대해 \\(\\mathbb{E}\\theta\\lbrack g(T)\\rbrack = 0\\)이면서도, \\(P_{\\theta}(g(T) = 0) = 1\\)이 되는 경우를 말한다. 즉, 기대값이 0인 함수 \\(g(T)\\)는 거의 확률 1로 항상 0이어야 한다는 뜻이다. 이 경우, \\(T(\\mathbf{X})\\)를 완비 통계량 complete 이라고 한다.\n【예제 ①】 \\(X_{1},\\ldots,X_{n}\\)을 모수 \\(\\theta \\in (0,1)\\)인 베르누이 분포를 따르는 확률표본이라 하자. \\(T(\\mathbf{X}) = \\sum X_{i} \\sim B(n,\\theta)\\)는 \\(\\theta\\)에 대한 완비 충분 통계량이다.\n【예제 ②】 확률표본 \\(X_{1},\\ldots,X_{n}\\)이 \\(U(0,\\theta)\\)에서 추출되었다고 하자. \\(T(\\mathbf{X}) = max(x_{i}) = x_{(n)}\\)은 모수 \\(\\theta\\)에 대한 완비 충분 통계량이다.\n(Basu’s Theorem) 만약 \\(T(\\mathbf{X})\\)가 완비하고 최소 충분 통계량이라면, \\(T(\\mathbf{X})\\)는 모든 보조 통계량과 서로 독립이다.\n완비성과 최소충분성이라는 강력한 조건을 만족할 경우, 매개변수 \\(\\theta\\)와 무관하게 분포하는 보조통계량들과 \\(T(\\mathbf{X})\\)사이에는 어떠한 의존성도 존재하지 않음을 의미한다. 최소충분 통계량과 모수와 무관한 정보보조통계량를 분리할 수 있게 해주기 때문에, 통계 추론이나 신뢰구간 설정에 매우 유용하게 사용된다.\n【정리】 \\(X_{1},\\ldots,X_{n}\\)이 지수족 분포를 따르는 확률표본이라고 하자.\n\\(f(x|\\theta) = h(x)c(\\theta)\\exp\\left( \\overset{k}{\\sum_{j = 1}}w(\\theta_{j})t_{j}(x) \\right)\\). 다음 \\(T(\\mathbf{X})\\)는 완비통계량이다. \\(T(\\mathbf{X}) = \\left( \\overset{n}{\\sum_{i = 1}}t_{1}(X_{i}),\\overset{n}{\\sum_{i = 1}}t_{2}(X_{i}),\\ldots,\\overset{n}{\\sum_{i = 1}}t_{k}(X_{i}) \\right)\\)\n(Minimal Complete Statistic) 만약 최소 충분통계량이 존재한다면, 모든 완비통계량도 최소 충분통계량이다.\n【예제 ③】 \\(X_{1},\\ldots,X_{n}\\)을 \\(exp(\\theta)\\), 지수분포(지수족)를 따르는 확률표본이라 하자. \\(T(\\mathbf{X}) = \\sum X_{i} \\sim Gamma(n,\\theta)\\)는 \\(\\theta\\)에 대한 완비 최소 충분 통계량이다.\n【예제 ④】 \\(X_{1},\\ldots,X_{n}\\)을 \\(N(\\mu,\\sigma^{2})\\), 정규분포(지수족)를 따르는 확률표본이라 하자. \\(T(\\mathbf{X}) = (\\overline{X},S^{2})\\)는 \\(\\theta\\)에 대한 완비 최소 충분 통계량이다.\n\n\n\n2. 우도함수\n통계적 추론에서는 데이터로부터 정보를 요약하는 방법이 중요하다. 우도함수는 단순히 하나의 요약 방법이 아니라, 특정 원칙을 수용할 경우 필수적인 데이터 축약 장치로 간주된다.\n\n충분성 원칙: 관찰된 데이터가 어떤 충분한 통계량에 의해서만 정보를 제공한다면, 모든 추론은 이 충분한 통계량에만 의존해야 한다.\n조건화 원칙: 실험 설계상 복수의 실험이 가능한 경우, 실제로 수행된 실험의 결과만을 기반으로 추론해야 한다.\n우도 원칙: 주어진 데이터에 대한 우도함수의 형태만이 추론에 중요하며, 데이터가 관찰된 경로는 중요하지 않다.\n\n위의 원칙들을 받아들인다면, 우도함수는 주어진 데이터로부터 정보를 요약하는 유일하고 필수적인 수단이 된다.\n\n(1) 우도함수\n【정의】 확률표본 \\(\\mathbf{X} = (X_{1},\\ldots,X_{n})\\)의 결합 확률밀도함수를 \\(f(\\mathbf{x}|\\theta)\\)라고 하자. 이때 표본 데이터 \\(\\mathbf{X} = \\mathbf{x}\\)가 관측되었을 때, 모수 \\(\\theta\\)의 함수로 정의되는 \\(L(\\theta|\\mathbf{x}) = f(\\mathbf{x}|\\theta)\\)를 우도함수 likelihood function 라고 한다.\n우도함수는 관측된 데이터 \\(\\mathbf{x}\\)를 기준으로 다양한 \\(\\theta\\) 값들에 대해 상대적 타당성을 비교하는 도구이다. 이산형, 연속형 모두 우도비를 통해 두 모수에 대한 비교가 가능하다. 즉, 실제 데이터가 관측되어 우도 값이 계산된다면 우도 값이 큰 모수가 진짜 모수일 가능성이 높다.\n【예제 ①】 \\(NB(r = 3,p)\\), 음이항분포로부터 \\(X_{1} = 2\\) 관측되었다면 우도함수는 \\(P_{p}(X = 2) = \\binom{4}{2}p^{3}(1 - p)^{2}\\)이다.\n【우도 원리】 표본점 \\(\\mathbf{x},\\mathbf{y}\\)가 다음 조건을 만족한다고 하자.\n두 표본에 대해 우도함수 \\(L(\\theta|\\mathbf{x})\\), \\(L(\\theta|\\mathbf{y})\\)가 서로 비례한다.\n즉, 모든 \\(\\theta\\)에 대해 다음을 만족하는 상수 \\(C(\\mathbf{x},\\mathbf{y})\\)가 존재한다.\n\\[L(\\theta|\\mathbf{x}) = C(\\mathbf{x},\\mathbf{y})L(\\theta|\\mathbf{y}),\\text{for all}\\theta\\]\n두 표본 \\(\\mathbf{x},\\mathbf{y}\\)가 관찰되었을 때, 만약 이들의 우도함수가 비례한다면, 이 두 표본은 동일한 정보를 제공한다. 통계적 결론은 오직 우도함수 에만 의존해야 하며, 표본의 다른 세부사항에는 의존하지 않는다.\n\n\n(2) 공식 formal 충분 통계량 원칙\n어떤 실험 \\(E = (X,\\theta,\\{ f(x|\\theta)\\})\\)이 수행되었고, \\(T(X)\\)이 \\(\\theta\\)에 대한 충분통계량이라 할 때, 만약 두 관측값 \\(x\\)와 \\(y\\)가 \\(T(x) = T(y)\\)를 만족한다면, 이 두 관측값이 제공하는 증거는 동일해야 한다는 것이다. 즉, 관측 데이터 전체 \\(x\\) 자체가 아니라, 그로부터 계산된 충분통계량 \\(T(x)\\)만이 \\(\\theta\\)에 관한 모든 정보를 요약하므로, 두 데이터가 동일한 충분통계량 값을 가질 때는, 둘 모두 \\(\\theta\\)에 대해 동일한 결론을 가져야 한다.\n\n\n(3) 조건화 conditionality 원칙\n여러 개의 가능한 실험이 있을 때, 어떤 실험이 실제로 수행되었는지가 매우 중요하다는 사실을 강조한다. 예를 들어, 두 개의 실험 \\(E_{1},E_{2}\\) 중 무작위로 하나를 선택하여 시행한다고 가정하자. 이때, 어느 실험이 선택되었는지는 관측값과 함께 반드시 고려되어야 하며, 실제로 수행된 실험에 기반하여 추론이 이루어져야 한다.\n보다 공식적으로, 혼합 실험 \\(E\\)이 정의될 때, 실험 \\(E_{j}(j = 1,2)\\)가 수행되고 관측값 \\(x_{j}\\)가 주어진 경우, \\(\\text{Ev}(E^{,}(j,x_{j})) = \\text{Ev}(E_{j},x_{j})\\)이어야 한다. 즉, 실험 \\(E\\)로부터 얻어진 데이터라도 실제로 수행된 \\(E_{j}\\)에 기반하여 해석되어야 한다. 조건화 원칙은 ”오직 수행된 실험만이 중요하며, 선택되지 않은 실험들은 전혀 고려되어서는 안 된다”는 점을 명확히 한다. 이는 실험 설계 단계에서 무작위성이 개입되더라도, 실제로 수행된 실험만이 추론의 근거가 되어야 한다는 점에서 자연스럽고 설득력 있는 원칙이다.\n\n\n(4) 우도 원칙\n공식 충분성 원칙과 조건화 원칙을 함께 받아들이면, 우도 원칙이 도출된다. 즉, 두 실험에서 수집된 두 데이터 \\(x_{1}^{*},x_{2}^{*}\\)가 생성하는 우도함수가 다음과 같은 비례 관계를 만족할 때, \\(L(\\theta|x_{2}^{*}) = CL(\\theta|x_{1}^{*})\\)이 두 데이터는 \\(\\theta\\)에 대해 동일한 증거를 제공해야 한다.\n따라서 관측 데이터가 생성하는 우도함수만이 파라미터에 관한 모든 정보를 담고 있으며, 우도함수가 같으면 추론 결과도 같아야 한다는 결론에 도달한다. 이는 바로 우도 원칙의 본질이다.\n\n\n(5) 동등성 Equivariance 원칙\n동등성 원칙에서는 함수 \\(T(x)\\)가 지정되지만, \\(T(x) = T(y)\\)일 때 \\(x\\)를 관찰했을 경우와 \\(y\\)를 관찰했을 경우 추론 결과가 ”일정한 관계”를 가져야 한다고 요구한다. 반드시 동일할 필요는 없지만, 정해진 관계를 따라야 한다는 점이 특징이다. 또한 동등성 원칙은 실제로 두 가지 다른 고려사항을 결합한 것으로 이해할 수 있다:\nMeasurement Equivariance\n측정 단위에 의존하지 않는 추론을 요구한다. 예를 들어, 두 산림 조사원이 각각 나무의 평균 직경을 측정한다고 하자. 한 명은 인치 단위로, 다른 한 명은 미터 단위로 데이터를 수집하였다. 비록 단위가 다르더라도, 최종적으로 동일한 추정값을 제시해야 한다. 즉, 단위 변환(예: 인치를 미터로 변환) 이후 결과가 일치해야 한다.\nFormal Invariance\n수학적 모델의 구조가 동일하다면 추론 절차 역시 동일해야 한다고 요구한다. 이는 물리적 의미(예: 단위 등)와는 무관하게, 다음 세 가지가 같다면, 동일한 추론 방법을 사용해야 한다는 것이다.\n\n모수 공간 \\(\\Theta\\)\n확률밀도함수 \\(f(x|\\theta)\\)\n허용 가능한 추론 및 오차\n\n만약 \\(Y = g(X)\\)가 \\(X\\)의 측정 단위 변환이고, \\(Y\\)의 모델이 \\(X\\)의 모델과 동일한 수학적 구조를 갖는다면, 추론 절차는 측정 단위 변화에 대해 불변하며 동시에 수학적 구조에 대해 불변해야 한다.\n【예제 ①】 \\(X \\sim \\text{Binomial}(n,p)\\)일 때, 성공 횟수 \\(x\\)를 관찰한 경우를 생각한다. 실패 횟수는 \\(Y = n - X\\)로 표현할 수 있으며, 역시 \\(\\text{Binomial}(n,q = 1 - p)\\)분포를 따른다.\nMeasurement Equivariance 요구\n성공 수 \\(x\\)를 기반으로 한 추정값 \\(T(x)\\)와 실패 수 \\(y = n - x\\)를 기반으로 한 추정값 \\(T(y)\\)는 다음을 만족해야 한다.\n\\[T(x) = 1 - T(n - x)\\text{or}T(x) = 1 - T(n - x)\\]\n\n\n\n\nchapter 2. 점 추정\n\n1. 개념\n첫 번째 부분은 추정량을 찾는 방법을, 두 번째 부분은 추정량(및 기타 다른 추정량)을 평가하는 방법을 다룬다. 점추정의 논리는 매우 단순하다. 모집단이 확률밀도함수 \\(f(x|\\theta)\\)로 기술될 때 \\(\\theta\\)에 대한 지식은 모집단 전체에 대한 정보를 제공한다. 따라서, \\(\\theta\\)의 좋은 추정량을 찾는 방법을 모색하는 것은 자연스러운 일이다. 또한, 경우에 따라서는 \\(\\theta\\)의 함수, 즉 \\(\\tau(\\theta)\\)가 관심 대상이 될 수도 있다.\n【정의】 점추정량은 확률표본의 함수 \\(W(X_{1},\\ldots,X_{n})\\)이다. 즉, 모든 통계량은 점추정량이다.\n모집단 확률분포함수 \\(f(x;\\theta),\\theta \\in \\Omega\\)의 확률표본에서 얻은 통계량이 추정에 사용된다면 이를 추정량 estimator 이라 한다. 근사할 것이라고 생각하는 하나의 값으로 제시한다면 이를 점추정 point estimate, \\(\\theta\\)을 포함하고 있을 가능성이 높은 구간을 제시하는 것은 구간추정 interval estimate이라 한다. 계산되는 공식을 추정량, 실제 데이터를 이용하여 계산된 값을 추정치 estimates 이라 한다.\n【정의】 모집단 확률분포함수 \\(f(x;\\theta),\\theta \\in \\Omega\\)의 확률표본에서 얻은 통계량 \\(T = T\\left( X_{1},X_{2},\\ldots,X_{n} \\right)\\)이 모수 추정에 사용되면 이를 추정량 이라 하고 \\(\\overset{\\hat{}}{\\theta}\\)이라 표현한다.\n\n추정량 : \\(\\overset{\\hat{}}{\\theta} = T(X_{1},X_{2},\\ldots,X_{n})\\) 대문자로 표현\n추정치 : : \\(t(x_{1},x,\\ldots,x_{n})\\) 관측된 값으로 소문자로 표현\n\n\n\n2. 추정량 구하는 방법\n\n(1) 적률법\n가장 오랜 방법으로 적률을 이용하여 모수 추정하는 방법으로 매우 간단하나 좋은 추정량의 조건을 갖추지 않을 수 있다.\n알려지지 않은 모집단 확률분포함수 \\(f(x;\\theta),\\theta \\in \\Omega\\), 확률표본 \\(\\left( X_{1},X_{2},\\ldots,X_{n} \\right)\\)에서 모집단의 \\(k\\)차 적률 \\({\\mu'}_{k} = E(X^{k})\\)과 표본의 \\(k\\)차 적률 \\(m_{k}' = E(X_{i}^{k})\\)이라 하자.\n\\(\\mu_{k}' = m_{k}\\)이라 놓고 풀면 모수 추정량 얻게 된다. 만약 모수 한 개 이상이면 적률에 의한 방정식을 모수 수만큼 얻어 사용하면 된다.\n【예제 ①】 모집단 \\(B(n,p)\\)으로부터 확률표본 \\(\\left( X_{1},X_{2},\\ldots,X_{n} \\right)\\)이다. 적률방법으로 추정량 \\(\\overset{\\hat{}}{n},\\overset{\\hat{}}{p}\\) 구하라.\n\n모집단 적률: \\(\\mu_{1}' = E(X) = np,\\mu_{2}' = E\\left( X^{2} \\right) = np(1 - p) + {(np)}^{2}\\)\n표본적률: \\(m_{1}' = E(X) = \\overset{¯}{X}\\), \\(m_{2}' = E\\left( X^{2} \\right) = \\frac{1}{n}\\sum X_{i}^{2}\\)\n방정식: \\(np = \\overset{¯}{X}\\), \\(\\frac{1}{n}\\sum X_{i}^{2}\\)=\\(= np(1 - p) + {(np)}^{2}\\)\n모비율 추정량 : \\(\\widehat{p} = \\overline{x}\\)\n\n【예제 ②】 모집단 \\(N(\\mu,\\sigma^{2})\\)으로부터 확률표본 \\(\\left( X_{1},X_{2},\\ldots,X_{n} \\right)\\)이다. 적률방법으로  \\(추정량\\overset{\\hat{}}{\\mu},\\overset{\\hat{}}{\\sigma^{2}}\\) 구하라.\n\n모집단 적률: \\(\\mu_{1}' = E(X) = \\mu,\\mu_{2}' = E\\left( X^{2} \\right) = \\sigma^{2} + \\mu^{2}\\)\n표본 적률: \\(m_{1}' = E(X) = \\overset{¯}{X}\\), \\(m_{2}' = E\\left( X^{2} \\right) = \\frac{1}{n}\\sum X_{i}^{2}\\)\n방정식: \\(\\mu = \\overset{¯}{X}\\), \\(\\sigma^{2} + \\mu^{2} = \\frac{\\sum X_{i}^{2}}{n}\\)\n평균 추정량: \\(\\overset{\\hat{}}{\\mu} = \\overset{¯}{x}\\)\n분산 추정량: \\(\\overset{\\hat{}}{\\sigma^{2}} = \\frac{1}{n}\\sum X_{i}^{2} - {\\overset{¯}{X}}^{2} = \\frac{1}{n}\\sum\\left( X_{i} - \\overset{¯}{X} \\right)^{2}\\)\n\n【예제 ③】 모집단 \\(U(0,\\theta)\\)으로부터 확률표본 \\(\\left( X_{1},X_{2},\\ldots,X_{n} \\right)\\)이다. 적률방법으로  추정량 \\(\\widehat{\\theta}\\) 구하라.\n\n모집단 적률: \\(\\mu_{1}' = E(X) = \\frac{\\theta}{2}\\)\n표본 적률: \\(m_{1}' = \\overset{¯}{X}\\)\n\\(\\overset{¯}{X} = \\frac{\\theta}{2}\\) 이므로 적률에 의한 추정량은 \\(\\overset{\\hat{}}{\\theta} = 2\\overset{¯}{X}\\)이다.\n\n\n\n\n\n\n추정량 \\(\\overset{\\hat{}}{\\theta} = 2\\overset{¯}{X}\\)은 불편 추정량은 (\\(E\\left( 2\\overset{¯}{X} \\right) = 2\\theta \\neq \\theta\\))아니지만 일치 추정량이다. 【정리】 만약 \\(\\lim_{n \\rightarrow \\infty}{V\\left( \\overset{\\hat{}}{\\theta} \\right)( = 4\\frac{\\theta^{2}}{12n}) = 0}\\)이면  \\(\\overset{\\hat{}}{\\theta}\\)는 일치 추정량이다.\n\n\n\n\n【예제 ④】 모집단 \\(Gamma(\\alpha,\\beta)\\)으로부터 확률표본 \\(\\left( X_{1},X_{2},\\ldots,X_{n} \\right)\\)이다. 적률방법으로 모수 \\(\\alpha,\\beta\\) 추정량을 구하라.\n\n모집단 적률: \\(\\mu_{1}' = E(X) = \\alpha\\beta,\\mu_{2}' = E\\left( X^{2} \\right) = \\alpha\\beta^{2} + (\\alpha{\\beta)}^{2}\\)\n표본 적률: \\(m_{1}' = E(X) = \\overset{¯}{X}\\), \\(m_{2}' = E\\left( X^{2} \\right) = \\frac{1}{n}\\sum X_{i}^{2}\\)\n방정식: \\(\\alpha\\beta = \\overset{¯}{X}\\), \\(\\alpha\\beta^{2} + \\alpha\\beta^{2} = \\frac{\\sum X_{i}^{2}}{n}\\)\n추정량: \\(\\overset{\\hat{}}{\\alpha} = \\frac{n\\overset{¯}{X}}{n\\sum\\left( X_{i} - \\overset{¯}{X} \\right)^{2}}\\), \\(\\overset{\\hat{}}{\\beta} = \\frac{\\sum\\left( X_{i} - \\overset{¯}{X} \\right)^{2}}{n\\overset{¯}{X}}\\)\n\n불편 추정량은 아니지만 \\(\\overset{¯}{X}\\)는 \\(\\alpha\\beta\\)의 일치 추정량이고 \\(\\frac{1}{n}\\sum X_{i}^{2}\\)은 \\(\\alpha\\beta^{2} + (\\alpha{\\beta)}^{2}\\)의 일치 추정량이다. 적률에 의해 구한 추정량은 일치 추정량이기는 하지만 불편성 보장은 물론 MVUE라는 보장이 없다. 쉽게 얻을 수 있다는 장점으로 인하여 추정량을 이해하기 위하여 시작점이 된다.\n\n\n(2) 최대우도 추정량 MLE\n개념\n최종적으로 최량 추정량, MVUE(minimum variance unbiased estimator 최소분산 불편 추정량)를 구하기 위하여 ⑴Factorial criterion에 의해 충분 통계량을 구하고 ⑵충분 통계량의 함수이면서 불편성을 갖는 추정량을 구하면 Rao-Blackwell 정리에 의해 이것이 MVUE이다. 그러나 불편 추정량을 구하는 것이 그렇게 쉽지만은 않다.\n한편, 적률에 의한 추정량은 일치성은 보장하지만 불편성, MVUE는 아닐 가능성이 높다. 이제 MVUE일 가능성이 높은 추정 방법을 소개하고자 한다. 추출된(수집된) 확률표본(데이터)이 어떤 모수 값일 경우 그 값들이 추정될 가능성이 가장 높은가? 이를 최대 우도 추정량이라 한다. 통계추론에서 사용되는 추정량은 대부분 MLE이다.\n주머니 속에 공이 3개 들어 있다. 공의 색깔은 하양, 파랑일 수 있다. 그러나 각 몇 개씩 들어 있는지는 모른다고 가정하자. 2개의 공을 뽑아 색을 보고 주머니에 있는 공의 색을 맞춘다고 하자. 공 2개를 뽑았더니 파랑이었다. 그럼? 주머니의 공은?\n하얀 공일 확률: 1/3(\\(= \\binom{2}{2}\\binom{1}{0}/\\binom{3}{2}\\)), 파란 공일 확률: 1(\\(= \\binom{3}{2}/\\binom{3}{2}\\)) 파랑 공이 가능성이 높다. 이렇게 모수에 대한 추정량을 구하는 방법이 최대우도 추정법이다.\nMLE 구하기\n【우도함수】 알려지지 않은 모집단 확률분포함수 \\(f(x;\\theta),\\theta \\in \\Omega\\)에 대한 정보를 얻기 위하여 추출한 크기 \\(n\\)의 (확률)표본 \\(\\overline{X} = \\left( X_{1},X_{2},\\ldots,X_{n} \\right)\\)의 결합 확률밀도함수를 모수 포함한 함수로 표현한 것을 우도함수 likelihood function 이라 하며 모수의 함수이다.\n\\(L\\left( \\theta;x_{1},x_{2},\\ldots,x_{n} \\right) = L(\\theta;\\overline{x}) = \\prod_{i}^{n}{f(x_{i};}\\theta)\\)\n확률표본(데이터) 결합 확률(표본 데이터가 수집되었다면 어떤 모수 값일 가능성이 가장 높은가)을 최대화 하는 모수 값을 MLE라 한다.\n【MLE】 우도함수 최대화 하는 \\(\\theta\\)를 최대우도 maximum likelihood 추정량 이라 한다.\n\\(\\frac{\\partial L(\\theta)}{\\partial\\theta} = 0\\)을 만족하는 추정량 \\(\\overset{\\hat{}}{\\theta}(\\overline{x})\\)을 \\(MLE\\) 이라 한다.\n【로그 우도함수】 우도 함수는 항상 0보다 크므로 우도함수 최대화 하는 \\(\\theta\\) 계산 ⬄ 로그 우도함수 최대화 하는 \\(\\theta\\) 계산\n【예제 ①】 어느 지역의 암 환자 비율 \\(p\\)을 추정하려고 한다. 모수 \\(p\\)인 베르누이 확률밀도함수로부터 확률표본을 추출하였다고 하자.\n\n모집단 확률밀도함수 : \\(f(x;\\theta = p) = p^{x}(1 - p)^{1 - x},x = 0,1\\)\n우도 함수: \\(L(\\theta;\\overline{x}) = \\prod_{i}^{n}{f(x_{i};p)} = \\sum_{i}^{n}{p^{x_{i}}(1 - p)^{1 - x_{i}}} = p^{\\sum x_{i}}(1 - p)^{n - \\sum x_{i}}\\)\n로그 우도함수: \\(l(\\theta) = \\ln\\left( L(\\theta) \\right) = \\sum x_{i}\\ln(p) + (n - \\sum x_{i})ln(1 - p)\\)\nMLE: \\(\\frac{\\partial l(\\theta)}{\\partial\\theta} = \\sum x_{i}\\left( \\frac{1}{p} \\right) + (n - \\sum x_{i})\\frac{1}{1 - p}( - 1) = 0\\),\n\n그러므로 \\(\\overset{\\hat{}}{p} = \\frac{\\sum x_{i}}{n}\\)이다.\n【예제 ②】 빼빼로 중량이 \\(N(\\mu,\\sigma^{2})\\)을 따른다고 하자. 중량 평균을 추정하기 위하여 확률표본을 추출하였다고 하였다. 모집단 모수 \\(\\overline{\\theta} = (\\mu,\\sigma)\\)는 2개이나 평균에 관심이 있으므로 \\(\\mu\\)는 목표 모수, 분산 \\(\\sigma^{2}\\)은 불필요 nuisance 모수 이다.\n\n모집단 확률밀도함수: \\(f(x;\\mu,\\sigma) = \\frac{1}{2\\sqrt{}\\pi\\sigma}\\exp\\left( - \\frac{(x - \\mu)^{2}}{2\\sigma^{2}} \\right), - \\infty &lt; x &lt; \\infty\\)\n로그 우도함수: \\(l\\left( \\mu,\\sigma^{2} \\right) = \\frac{n}{2}\\ln(2\\pi) - \\frac{1}{2}\\sum\\left( \\frac{x_{i} - \\mu}{\\sigma} \\right)^{2}\\)\nMLE: \\(\\frac{\\partial l(\\mu,\\sigma)}{\\partial\\mu} = 0,\\frac{\\partial l\\left( \\mu,\\sigma^{2} \\right)}{\\partial\\sigma^{2}} = 0\\) 그러므로 \\(\\overset{\\hat{}}{\\mu} = \\overset{¯}{X},{\\overset{\\hat{}}{\\sigma}}^{2} = \\frac{\\sum\\left( x_{i} - \\overset{¯}{x} \\right)^{2}}{n}\\)이다.\n\n【예제 ③】 모집단 확률분포 \\(U(0,\\theta)\\), 확률표본에서 MLE을 구하시오.\n\n모집단 확률밀도함수 : \\(f(x;\\theta) = \\frac{1}{\\theta},0 &lt; x &lt; \\theta\\)\n우도함수 : \\(L(\\theta) = \\Pi\\left( \\frac{1}{\\theta} \\right)I_{(x_{i},\\theta)} = \\Pi\\left( \\frac{1}{\\theta} \\right)I_{(\\max\\left\\{ x_{i} \\right\\},\\theta)}\\)\n\n\\(I_{\\lbrack a,b\\rbrack}\\)은 지시 indicator 함수로 \\((a &lt; b)\\)이면 1, 그렇지 않으면 0이다. 우도함수 최대화 되려면 \\(\\overset{\\hat{}}{\\theta} = \\max\\left\\{ x_{i} \\right\\} = x_{(n)}\\)\n【예제 ④】 라플라스분포 \\(f(x;\\theta) = \\frac{1}{2}e^{- |x - \\theta|}, - \\infty &lt; x &lt; \\infty, - \\infty &lt; \\theta &lt; \\infty\\)을 따르는 확률표본을 이용하여 \\(\\theta\\)에 대한 MLE 구하라.\n\n로그 우도함수: \\(l(\\theta) = - nln(2) - \\sum_{i}^{n}{|x_{i} - \\theta|}\\)\n미분: $ = {i}^{n}{sgn(x{i} - )} = 0,wheresgn(t) = { \\begin{array}{r}\n1,t &lt; 0 \\ 0,t = 0 \\ 1,t &gt; 0 \\end{array} . $\n\n그러므로 \\(\\overset{\\hat{}}{\\theta} = Median\\), MLE이다.\n【예제 ⑤】 \\(N(\\mu,1),where\\mu &gt; 0\\)을 따르는 확률표본을 이용하여 \\(\\mu\\)에 대한 MLE 구하라.\n\n로그 우도함수 최대화 하는 MLE \\(\\overset{\\hat{}}{\\mu} = \\overline{X}\\) 이므로 \\(\\overset{\\hat{}}{\\mu} = \\left\\{ \\begin{array}{r}\n\\overline{X}if\\overline{X} \\geq 0 \\\\\n0if\\overline{X} &lt; 0\n\\end{array} \\right.\\ \\)\n\n【예제 ⑥】 \\(B(n,p)\\)을 따르는 확률표본을 이용하여 \\(n\\)에 대한 MLE 구하라(단, \\(p\\)는 알려져 있음). (적용) 동전의 공정성을 평가하기 위하여 몇 번을 던져야 하나?\n우도함수 : \\(L\\left( k;\\overline{x},p \\right) = \\prod_{i}^{n}{\\binom{k}{x_{i}}p^{x_{i}}(1 - p)^{k - x_{i}}}\\)\n\\(k\\)에 대한 우도함수 미분은 쉽지 않다. 만약 \\(k &lt; x_{(n)}\\)이면 \\(L\\left( k;\\overline{x},p \\right) = 0\\) 이므로 다음 조건을 만족하는 \\(k \\geq x_{(n)}\\)이 MLE이다.\n\\(\\frac{L\\left( k;\\overline{x},p \\right)}{L\\left( k - 1;\\overline{x},p \\right)} \\geq 1,\\frac{L\\left( k + 1;\\overline{x},p \\right)}{L\\left( k;\\overline{x},p \\right)} &lt; 1\\).\n그러므로 최대화 조건은 다음과 같다.\\(\\left( k(1 - p) \\right)^{n} \\geq \\prod_{1}^{n}\\left( k - x_{i} \\right)and\\left( (k + 1)(1 - p) \\right)^{n} \\geq \\prod_{1}^{n}{(k + 1 - x_{i})}\\)이다. 결론적으로 \\((1 - p)^{n} = \\overset{n}{\\prod_{i = 1}}(1 - x_{i}z)\\). 구간 \\(0 \\leq z \\leq 1/\\max_{i}x_{i}\\) 범위 내에서 MLE 구하면 \\(\\widehat{k} = \\lfloor 1/\\widehat{z}\\rfloor\\) 소숫점 버리고 내림한 값이다.\nMLE 성질\n【invariance property】 \\(f(x;\\theta),\\theta \\in \\Omega\\)을 따르는 확률표본으로부터 \\(\\overset{\\hat{}}{\\theta}\\)은 MLE, \\(\\theta \\rightarrow \\tau(\\theta)\\) 일대일 맵핑이라면 \\(\\tau(\\theta)\\) MLE은 \\(\\tau\\left( \\overset{\\hat{}}{\\theta} \\right)\\)이다.\n【예제 ①】 \\(N\\left( \\mu,\\sigma^{2} \\right)\\)에서 \\(\\overline{\\theta} = (\\mu,\\sigma^{2})\\) MLE는 \\(\\overset{\\hat{}}{\\mu} = \\overset{¯}{X},{\\overset{\\hat{}}{\\sigma}}^{2} = \\frac{\\sum\\left( x_{i} - \\overset{¯}{x} \\right)^{2}}{n}\\).\ninvariance property에 의해 표준편차 \\(\\sigma = \\sqrt{\\sigma^{2}}(\\tau(\\theta))\\) MLE는 \\(\\overset{\\hat{}}{\\sigma} = \\sqrt{\\frac{\\sum\\left( x_{i} - \\overset{¯}{x} \\right)^{2}}{n}}\\)이다.\n【예제 ②】 \\(B(p)\\)에서 확률표본 \\((X_{1},X_{2},\\ldots,X_{n})\\)이다. \\(V(X)\\)의 MLE 구하라.\n\\(V(X) = p(1 - p)\\)이고 모수 \\(p\\)에 대한 MLE은 \\(\\overset{\\hat{}}{p} = \\frac{\\sum X_{i}}{n}\\) 이므로 분산의 MLE는 \\(\\overset{\\hat{}}{V(X)} = \\frac{\\sum X_{i}}{n}(1 - \\frac{\\sum X_{i}}{n})\\)이다.\n【정의】 모수 \\(\\theta\\)에 대한 MLE \\({\\overset{\\hat{}}{\\theta}}_{mle}\\)은 일치 추정량이다.\n\n\n(3) 베이즈 추정량\n베이지안 접근법은 통계학에 대한 고전적인 접근법과 근본적으로 다른데 고전적인 접근법에서는 모수 \\(\\theta\\)가 알려지지 않지만 고정된 값으로 간주된다. 모수 \\(\\theta\\)에 대한 정보는 확률표본 \\(\\overline{X} = \\left( X_{1},X_{2},\\ldots,X_{n} \\right)\\)을 추출한 후 계산된 통계량을 기초하여 얻어진다.\n베이지안 접근법에서는 모수 \\(\\theta\\)는 확률변수로서의 변동성을 갖는 양으로 간주되며 이를 사전 확률밀도함수라 한다. 이는 분석자의 믿음에 기반한 주관적인 분포로서 데이터가 관찰되기 전에 정의하고 그런 다음 모수 \\(\\theta\\)인 모집단에서 표본을 추출하고 이 표본 정보를 사용하여 사전 분포를 업데이트한다. 이 업데이트된 사전 분포를 사후 분포라고 하고 이러한 업데이트는 베이즈 정리를 사용하여 수행한다.\n【사후확률】 \\(\\pi(\\theta)\\) 모수 사전 prior 확률밀도함수, \\(L(\\theta;\\overline{x})\\)을 우도 함수이면 확률표본 \\(\\overline{x} = \\left( x_{1},x_{2},\\ldots,x_{n} \\right)\\)이 주어진 경우 모수에 대한 조건부 확률밀도함수를 사후 posterior 확률밀도함수라 한다.\n\\(\\pi\\left( \\theta \\middle| \\overline{x} \\right) = \\frac{\\pi(\\theta)L(\\theta;\\overline{x})}{m(\\overline{x})} \\propto \\pi(\\theta)L(\\theta;\\overline{x}),wherem\\left( \\overline{x} \\right) = \\int\\pi(\\theta)L\\left( \\theta;\\overline{x} \\right)d\\theta\\)\n【bayes estimator 베이지안 추정량 정의】\n\n최소 squared error loss function(제곱 오차 손실함수) \\(Loss\\left( \\theta,\\overset{\\hat{}}{\\theta} \\right) = \\left( \\theta - \\overset{\\hat{}}{\\theta} \\right)^{2}\\): 사후 확률함수 평균\n최소 absolute error loss function(절대 오차 손실함수) \\(Loss\\left( \\theta,\\overset{\\hat{}}{\\theta} \\right) = |\\theta - \\overset{\\hat{}}{\\theta}|\\): 사후 확률함수 중앙값\n\n【예제 ①】 \\(B(p)\\)에서 추출한 확률표본 \\(\\overline{X} = \\left( X_{1},X_{2},\\ldots,X_{n} \\right)\\)을 이용하여 베이즈 추정량 구하라.\n\n우도함수 : \\(L\\left( p;\\overline{x} \\right) = \\binom{n}{y}p^{y}(1 - p)^{n - y}wherey = \\sum_{i}^{n}x_{i}\\)\n사전확률 : (1) uniform prior \\(\\pi(p) \\sim U(0,1)\\), (2) conjugate prior \\(\\pi(p) \\sim Beta(\\alpha,\\beta)\\)\n\nconjugate prior: 사후 확률밀도함수와 동일한 분포를 갖는 사전 확률밀도함수를 conjugate prior라 한다. 비율의 사후 확률밀도함수가 베타분포 이므로 사전 확률밀도함수를 베타분포이면 이를 conjugate prior라 한다.\n\nuniform prior 사후확률: \\(\\pi\\left( p \\middle| \\overline{x} \\right) \\propto 1_{(0,1)}^{p}\\binom{n}{y}p^{y}(1 - p)^{n - y} \\sim Beta(y + 1,n - y + 1)\\)\nconjugate prior 사후확률: \\(\\pi\\left( p \\middle| \\overline{x} \\right) \\propto Beta(\\alpha,\\beta)\\binom{n}{y}p^{y}(1 - p)^{n - y} \\sim Beta(\\alpha + y,\\beta + n - y)\\)\n베이즈 추정량 (제곱 오차 손실 함수 적용): \\(\\overset{\\hat{}}{p} = \\frac{y + 1}{(n + 2)}\\)(uniform prior), \\(\\overset{\\hat{}}{p} = \\frac{\\alpha + y}{(n + \\alpha + \\beta)}\\)(conjugate prior)\n\n【예제 ②】 \\(N\\left( \\theta,\\sigma^{2} \\right)\\)에서 추출한 확률표본 \\(\\overline{X} = \\left( X_{1},X_{2},\\ldots,X_{n} \\right)\\)을 이용하여 \\(\\left( \\theta,\\sigma^{2} \\right)\\)베이즈 추정량 구하라.\n\nConjugate 사전 확률밀도 함수: \\(\\pi(\\theta) \\sim N(\\mu,\\tau^{2})\\)\n사후 확률밀도함수: \\(\\pi\\left( \\theta \\middle| \\overline{x} \\right) \\sim N(\\frac{\\tau^{2}}{\\tau^{2} + \\sigma^{2}}\\overline{x} + \\frac{\\sigma^{2}}{\\tau^{2} + \\sigma^{2}}\\mu,\\frac{\\sigma^{2}\\tau^{2}}{\\tau^{2} + \\sigma^{2}})\\)\n제곱 오차 손실함수 최소화 베이즈 추정량: \\(\\overset{\\hat{}}{\\theta} = \\frac{\\tau^{2}}{\\tau^{2} + \\sigma^{2}}\\overline{x} + \\frac{\\sigma^{2}}{\\tau^{2} + \\sigma^{2}}\\mu\\).\n\n\n\n\n3. 추정량 평가\n앞 절에서는 모수의 점추정량을 구하는 여러 가지 합리적인 방법들을 소개하였다. 그러나 실제 통계 분석에서는 동일한 상황에 대해 서로 다른 추정 방법을 적용할 수 있는 경우가 많다. 따라서 여러 추정량 후보 중에서 어떤 추정량을 선택할 것인지 결정해야 하는 중요한 과제가 남는다.\n점추정은 흔히 과녁에 화살을 쏘는 것에 비유된다. 모집단으로부터 확률표본을 얻고, 이를 바탕으로 모수를 추정하는 것은 마치 과녁을 향해 한 발의 화살을 쏘는 것과 같다. 과연 이 화살은 과녁 한가운데, 즉 bull-eye에 명중했을까? 만약 단 한 번의 시도에서 bull-eye에 명중했다고 해도, 그 사람을 진정한 명궁이라 부를 수 있을까? 아마도 아닐 것이다. 동일한 조건에서 여러 차례 화살을 쏘아, 일관되게 bull-eye에 가까운 결과를 보여줘야 비로소 실력을 인정받을 수 있을 것이다.\n\n\n\n\n\n이와 마찬가지로, 한 번의 점추정으로는 그 추정치가 얼마나 좋은지를 판단할 수 없다. 좋은 추정량인지 판단하려면, 동일한 절차를 여러 번 반복하여 얻은 추정치들의 분포를 살펴보아야 한다. 즉, 추정치들의 평균과 분산, 그리고 그 특성을 분석함으로써 해당 추정량의 성능을 평가할 수 있다.\n목표 모수 \\(\\theta\\)에 대한 추정량 \\(\\overset{\\hat{}}{\\theta}\\)을 여러 번 얻는다면(\\(\\overset{\\hat{}}{\\theta}\\) 확률밀도함수, \\(f(\\overset{\\hat{}}{\\theta})\\),샘플링 분포도 얻을 수 있음) 그 추정량은 모수 \\(\\theta\\)을 중심으로 흩어져 있을 것이다. 모수 부근에 있을 가능성은 높고 멀어질수록 가능성은 떨어질 것이다.\n\n(1) 평균제곱오차\n【MSE】 추정량 \\(W\\)와 모수 \\(\\theta\\)에 대해, 평균제곱오차(Mean Squared Error, MSE)는 \\(MSE(W) = E_{\\theta}(W - \\theta)^{2}\\)로 정의된다.\n평균 절대오차 \\(E_{\\theta}(|W - \\theta|)\\))도 점추정량 성능 척도의 대안이 될 수 있으나, MSE는 다음과 같은 두 가지 강점을 가진다.\n\n수학적으로 다루기 쉬움\n분산과 편향이라는 명확한 해석 가능\n\n\\[MSE = E_{\\theta}(W - \\theta)^{2} = {Var}_{\\theta}(W) + (E_{\\theta}W - \\theta)^{2}\\]\n\\({Var}_{\\theta}(W)\\): 추정량 \\(W\\)의 분산(추정분산) - 추정량의 변동성\n\\((E_{\\theta}W - \\theta)^{2}\\): 추정량 \\(W\\)의 편향 bias의 제곱 - 추정량이 모수에 얼마나 가까운지\n【편향】 추정량 \\(W\\) 의 편향은 \\({Bias}_{\\theta}(W) = E_{\\theta}W - \\theta\\) 으로 정의된다.\n만약 \\({Bias}_{\\theta}(W) = 0\\)이면, 추정량 W는 불편 unbiased 추정량이라 한다. 이는 모든 \\(\\theta\\)에 대해 \\(E_{\\theta}W = \\theta\\)를 만족한다는 뜻이다. 불편 추정량인 경우 \\(MSE_{\\theta}(W) = V_{\\theta}(W)\\)이다.\n【예제 ①】 \\(f(x;\\theta) \\sim N(\\mu,\\sigma^{2})\\)에서 표본크기 \\(n\\)인 확률표본을 추출하였다. MLE 추정량 \\(\\overline{X},S^{2}\\)이 불편 추정량 인지 보이고 MSE 구하라.\n\n\\(E(\\overline{X}) = \\mu,E(S^{2}) = \\sigma^{2}\\)\n\\(MSE(\\overline{X}) = E(\\overline{X} - \\mu)^{2} = Var(\\overline{X}) = \\frac{\\sigma^{2}}{n}\\)\n\\(MSE(S^{2}) = E(S^{2} - \\sigma^{2})^{2} = Var(S^{2}) = \\frac{2\\sigma^{4}}{n - 1}\\)\n\n【예제 ②】 \\(f(x;\\theta) \\sim U(\\theta,\\theta + 1)\\)에서 표본크기 \\(n\\)인 확률표본을 추출하였다. 추정량 \\(\\overset{¯}{X}\\)가 편의 추정량 임을 보이고 MSE 구하라.\n\n\\(E(X) = \\frac{2\\theta + 1}{2},V(X) = \\frac{1}{12}\\).\n편의: \\(B\\left( \\overline{x} \\right) = \\frac{2\\theta + 1}{2} - \\theta = \\frac{1}{2}\\).\n추정분산 \\(V\\left( \\overline{x} \\right) = \\frac{1}{12n}\\) 이므로 \\(MSE\\left( \\overline{x} \\right) = \\frac{1}{12n} + \\frac{1}{4}\\)이다.\n\n【예제 ③】 \\(f(x;\\theta) = \\frac{1}{\\theta}e^{- x/\\theta},0 &lt; x\\) 에서 표본크기 3인 확률표본 \\((X_{1},X_{2},X_{3})\\) 추출하였다. 4개 추정량 중 MSE가 가장 작은 것은?\n\\((1){\\overset{\\hat{}}{\\theta}}_{1} = X_{1}(2){\\overset{\\hat{}}{\\theta}}_{2} = \\frac{X_{1} + X_{2}}{2}(3){\\overset{\\hat{}}{\\theta}}_{3} = \\frac{X_{1} + 2X_{2}}{3}(4){\\overset{\\hat{}}{\\theta}}_{4} = \\frac{X_{1} + X_{2} + X_{3}}{3}\\)\n\\(E(X) = \\theta,V(X) = \\theta^{2}\\)이므로, \\(E\\left( {\\overset{\\hat{}}{\\theta}}_{1} \\right) = \\theta,E\\left( {\\overset{\\hat{}}{\\theta}}_{2} \\right) = \\theta,E\\left( {\\overset{\\hat{}}{\\theta}}_{3} \\right) = \\theta,E\\left( {\\overset{\\hat{}}{\\theta}}_{4} \\right) = \\theta\\) 이므로 모두 불편 추정량 이다.\n\\(MSE\\left( {\\overset{\\hat{}}{\\theta}}_{1} \\right) = V\\left( {\\overset{\\hat{}}{\\theta}}_{1} \\right) = \\theta^{2}\\), \\(MSE\\left( {\\overset{\\hat{}}{\\theta}}_{2} \\right) = \\frac{\\theta^{2}}{2}\\), \\(MSE\\left( {\\overset{\\hat{}}{\\theta}}_{3} \\right) = \\frac{5\\theta^{2}}{9}\\), \\(MSE\\left( {\\overset{\\hat{}}{\\theta}}_{4} \\right) = \\frac{\\theta^{2}}{3}\\), 4번째 추정량 MSE가 최소\n【예제 ④】 \\(f(x;p) = p^{x}(1 - p)^{1 - x},x = 0,1\\) 베르누이 분포에서 표본크기 n인 확률표본을 추출하였다. MLE 추정량과 베이지 추정량의 MLE을 구하시오.\n\n모집단 평균 및 분산: \\(E(X) = p,V(X) = p(1 - p)\\)\nMLE: \\(\\widehat{p} = \\frac{\\sum X_{i}}{n}\\), \\(MSE = E_{p}(\\widehat{p} - p)^{2} = {Var}_{p}(\\overline{X}) = \\frac{p(1 - p)}{n}\\)\n베이지 추정량: \\({\\widehat{p}}_{B} = \\frac{Y + \\alpha}{\\alpha + \\beta + n}\\)\n\n\\[E_{p}({\\widehat{p}}_{B} - p)^{2} = {Var}_{p}({\\widehat{p}}_{B}) + ({Bias}_{p}({\\widehat{p}}_{B}))^{2}\\]\n\\[= {Var}_{p}\\left( \\frac{Y + \\alpha}{\\alpha + \\beta + n} \\right) + \\left( E_{p}\\left( \\frac{Y + \\alpha}{\\alpha + \\beta + n} \\right) - p \\right)^{2} = \\frac{np(1 - p)}{(\\alpha + \\beta + n)^{2}} + \\left( \\frac{np + \\alpha}{\\alpha + \\beta + n} - p \\right)^{2}\\]\n\n\n(2) 상대효율\n앞에서 살펴보았듯이 모수 \\(\\theta\\)에 대한 불편 추정량은 무수히 많이 존재한다. 두 불편 추정량 \\({\\overset{\\hat{}}{\\theta}}_{1},{\\overset{\\hat{}}{\\theta}}_{2}\\)을 생각해 보자. 만약 \\({V(\\overset{\\hat{}}{\\theta}}_{1}) \\leq V({\\overset{\\hat{}}{\\theta}}_{2})\\)라면 \\({\\overset{\\hat{}}{\\theta}}_{1}\\)은 \\({\\overset{\\hat{}}{\\theta}}_{2}\\)에 비해 상대적으로 효율적 efficient 이라고 정의한다.\n【상대효율】 \\({eff(\\overset{\\hat{}}{\\theta}}_{1},{\\overset{\\hat{}}{\\theta}}_{2}) = \\frac{{V(\\overset{\\hat{}}{\\theta}}_{2})}{V({\\overset{\\hat{}}{\\theta}}_{1})}\\), 추정량 \\({\\overset{\\hat{}}{\\theta}}_{2}\\)에 대한 \\({\\overset{\\hat{}}{\\theta}}_{1}\\)의 상대효율이라 한다.\n\\({\\overset{\\hat{}}{\\theta}}_{1},{\\overset{\\hat{}}{\\theta}}_{2}\\)가 불편 추정량이면 추정 분산과 추정 평균제곱오차은 동일하므로 추정 분산이 적은 추정량이 (즉 효율적인 추정량) 좋은 추정량이다.\n【예제 ①】 \\(f(x;\\theta) \\sim U(0,\\theta)\\)에서 표본크기 \\(n\\)인 확률표본을 추출하였다. \\(({\\overset{\\hat{}}{\\theta}}_{1} = 2\\overline{X},{\\overset{\\hat{}}{\\theta}}_{2} = \\frac{n + 1}{n}X_{(n)})\\) 추정량이 불편 추정량임을 보이고 상대효율을 구하라.\n\n\\(E(X) = \\frac{\\theta}{2},V(\\theta) = \\frac{\\theta^{2}}{12}\\). \\(E\\left( {\\overset{\\hat{}}{\\theta}}_{1} \\right) = E\\left( 2\\overline{X} \\right) = 2\\frac{\\theta}{2} = \\theta\\) 불편 추정량이다.\n\\(V\\left( {\\overset{\\hat{}}{\\theta}}_{1} \\right) = V\\left( 2\\overline{X} \\right) = 4\\frac{\\theta^{2}}{12n} = \\frac{\\theta^{2}}{3n}\\).\n순서 통계량 \\(Y = X_{(n)}\\) 확률밀도함수: \\(f(y) = n\\left( \\frac{y}{\\theta} \\right)^{n - 1}\\left( \\frac{1}{\\theta} \\right),0 &lt; y &lt; \\theta\\).\n\\(E(Y) = \\frac{n}{n + 1}\\theta,V(Y) = \\left( \\frac{n}{n + 2} - \\left( \\frac{n}{n + 2} \\right)^{2} \\right)\\theta^{2}\\).\n\\(E\\left( {\\overset{\\hat{}}{\\theta}}_{2} \\right) = E\\left( \\frac{n + 1}{n}X_{(n)} \\right) = \\frac{n + 1}{n}\\frac{n}{n + 1}\\theta = \\theta\\) 불편 추정량이다. .\n\\(V\\left( {\\overset{\\hat{}}{\\theta}}_{2} \\right) = \\frac{1}{n(n + 2)}\\theta^{2}\\).\n\\({eff(\\overset{\\hat{}}{\\theta}}_{1},{\\overset{\\hat{}}{\\theta}}_{2}) = \\frac{{V(\\overset{\\hat{}}{\\theta}}_{2})}{V({\\overset{\\hat{}}{\\theta}}_{1})} = \\frac{3}{n + 2}\\) 이므로 \\(n \\geq 1\\) 이면 \\({\\overset{\\hat{}}{\\theta}}_{1}\\)이 \\({\\overset{\\hat{}}{\\theta}}_{2}\\)에 비해 상대적으로 효율적이다.\n\n【예제 ②】 \\(f(x;\\theta) = \\frac{1}{\\theta}e^{- x/\\theta},0 &lt; x\\)에서 표본크기 \\(n\\)인 확률표본 \\((X_{1},X_{2},\\ldots,X_{n})\\) 추출하였다. \\(({\\overset{\\hat{}}{\\theta}}_{1} = \\frac{X_{1} + X_{2}}{2},{\\overset{\\hat{}}{\\theta}}_{2} = {\\overline{X}}_{n})\\) 불편 추정량임을 보이고 상대효율을 구하라.\n\n\\(E(X) = \\theta,V(\\theta) = \\theta^{2}\\)\n\\(E\\left( {\\overset{\\hat{}}{\\theta}}_{1} \\right) = E\\left( \\frac{X_{1} + X_{2}}{2} \\right) = \\theta\\) 불편 추정량이다. \\(V\\left( {\\overset{\\hat{}}{\\theta}}_{1} \\right) = V\\left( \\frac{X_{1} + X_{2}}{2} \\right) = \\frac{\\theta^{2}}{2}\\)\n\\(E\\left( {\\overset{\\hat{}}{\\theta}}_{2} \\right) = E\\left( \\frac{X_{1} + X_{2} + \\ldots + X_{n}}{n} \\right) = \\theta\\) 불편 추정량이다 \\(V\\left( {\\overset{\\hat{}}{\\theta}}_{1} \\right) = V\\left( {\\overline{X}}_{n} \\right) = \\frac{\\theta^{2}}{n}\\)\n\\({eff(\\overset{\\hat{}}{\\theta}}_{1},{\\overset{\\hat{}}{\\theta}}_{2}) = \\frac{{V(\\overset{\\hat{}}{\\theta}}_{2})}{V({\\overset{\\hat{}}{\\theta}}_{1})} = \\frac{n}{2}\\) 이므로 \\(n \\geq 2\\) 이면 \\({\\overset{\\hat{}}{\\theta}}_{2}\\)이 \\({\\overset{\\hat{}}{\\theta}}_{1}\\)에 비해 상대적으로 효율적이다.\n\n\n\n(3) 최량 불편 추정량\n최소 MSE을 갖는 추정량을 최량 추정량으로 정의하였는데 실제 MSE을 최소화 하는 추정량을 구하는 것은 쉽지 않거나(수학적 접근 매우 어려움) 실제 ”최고의 MSE 추정량”은 존재하지 않는다. 이는 후보 추정량의 범위가 너무 넓기 때문인데, \\(\\widehat{\\theta} = 17\\)은 \\(\\theta = 17\\)일 때 MSE가 최솟값이지만 다른 값에서는 매우 나쁜 추정량이다.\n불편추정량으로 범위를 제한하면 후보 추정량의 범위를 불편추정량으로 제한한다. MSE 비교는 단순히 분산 비교로 귀결되므로 추정분산이 더 작은 불편추정량을 선택하면 된다.\n【정의】 추정량 \\(W^{*}\\)가 다음 조건을 만족하면 \\(\\tau(\\theta)\\)에 대한 최량 불편추정량 best unbiased estimator 이라 한다.\n모든 \\(\\theta\\)에 대해 \\(E_{\\theta}W^{*} = \\tau(\\theta)\\)\n임의의 다른 추정량 \\(W\\)에 대해 \\(Var\\theta(W^{*}) \\leq Var\\theta(W)\\text{for all}\\theta\\)\n【예제 ①】 \\(f(x;\\theta) \\sim B(n,p)\\)에서 표본크기 \\(n\\)인 확률표본을 추출하였다. 다음 2개 추정량에 대하여 (1) 불편 추정량인지 보이고 (2) MSE을 비교하라. \\((1){\\overset{\\hat{}}{p}}_{1} = \\frac{\\sum X_{i}}{n}(2){\\overset{\\hat{}}{p}}_{2} = \\frac{\\sum X_{i} + 1}{n + 2}\\).\n\n\\(E(X) = p,V(X) = p(1 - p)\\).\n\\(E\\left( \\frac{\\sum X_{i}}{n} \\right) = p\\) 이므로 \\({\\overset{\\hat{}}{p}}_{1}\\)는 불편 추정량 이다.\n\\(E\\left( \\frac{\\sum X_{i} + 1}{n + 2} \\right) = \\frac{p + 1}{n + 2}\\) 이므로 \\({\\overset{\\hat{}}{p}}_{2}\\)는 불편 추정량 아니다.\n\\(B\\left( {\\overset{\\hat{}}{p}}_{2} \\right) = \\frac{1 - np - p}{n + 2}\\).\n\\(MSE\\left( {\\overset{\\hat{}}{p}}_{1} \\right) = MSE\\left( \\frac{\\sum X_{i}}{n} \\right) = V\\left( \\frac{\\sum X_{i}}{n} \\right) = \\frac{p(1 - p)}{n}\\)\n\\(MSE\\left( {\\overset{\\hat{}}{p}}_{2} \\right) = MSE\\left( \\frac{\\sum X_{i} + 1}{n + 2} \\right) = V\\left( \\frac{\\sum X_{i} + 1}{n + 2} \\right) + B^{2}\\left( \\frac{\\sum X_{i} + 1}{n + 2} \\right) = \\frac{np(1 - p)}{(n + 2)^{2}} + \\frac{(1 - np - p)^{2}}{(n + 2)^{2}}\\)\n\n그러므로 \\(MSE\\left( {\\overset{\\hat{}}{p}}_{1} \\right) &gt; MSE\\left( {\\overset{\\hat{}}{p}}_{2} \\right)for0 &lt; p &lt; 1\\).\n【예제 ②】 \\(f(x;\\theta) \\sim Poisson(\\lambda)\\)에서 표본크기 \\(n\\)인 확률표본을 추출하였다. 포아송 분포는 평균, 분산이 동일하므로 표본평균(\\(\\overline{x}\\)), 표본분산(\\(S^{2}\\)) 모두 불편 추정량이다. 어느 추정량을 사용할 것인가? 추정 분산이 적은 통계량을 사용해야 한다.\n\n표본평균 추정분산: \\(V\\left( \\overset{¯}{x} \\right) = \\frac{\\lambda}{n}\\).\n표본분산 추정분산: \\(\\frac{(n - 1)S^{2}}{\\sigma^{2}( = \\lambda)} \\sim \\chi^{2}(n - 1)\\) 이므로 \\(V(S^{2}) = \\frac{2\\lambda^{2}}{n - 1}\\)이다.\n\\(V(\\overline{X}) \\leq V(S^{2})\\)\n\n최량 불편추정량을 찾는 과정은 매우 복잡하다. 만약 어떤 분포 \\(f(x|\\theta)\\)에 대해 모수 \\(\\tau(\\theta)\\)의 불편추정량의 분산에 대한 하한 \\(B(\\theta)\\)를 설정할 수 있다면,\\({Var}_{\\theta}(W) = B(\\theta)\\)를 만족하는 추정량을 찾으면 최량 불편추정량을 찾은 것이 된다.\n【Cramér–Rao Lower Bound, CRLB】 확률밀도함수 \\(f(x|\\theta)\\)의 확률표본으로부터의 추정량 \\(W(\\mathbf{X}) = W(X_{1},\\ldots,X_{n})\\)는 다음을 만족한다면, \\({Var}_{\\theta}(W(\\mathbf{X})) \\geq \\frac{\\left( \\frac{d}{d\\theta}E_{\\theta}W(\\mathbf{X}) \\right)^{2}}{nE_{\\theta}\\left( \\left( \\frac{\\partial}{\\partial\\theta}\\log f(\\mathbf{X}|\\theta) \\right)^{2} \\right)}\\)\n\n\\(\\frac{d}{d\\theta}E_{\\theta}W(\\mathbf{X}) = \\int_{x}\\frac{\\partial}{\\partial\\theta}\\lbrack W(x)f(x|\\theta)\\rbrack dx\\)\n\\({Var}_{\\theta}(W(\\mathbf{X})) &lt; \\infty\\)\n\n【Fisher Information】 확률밀도함수 \\(f(x|\\theta)\\)가 지수족을 따르다면 \\(E_{\\theta}\\left( \\left( \\frac{\\partial}{\\partial\\theta}\\log f(X|\\theta) \\right)^{2} \\right) = - E_{\\theta}\\left( \\frac{\\partial^{2}}{\\partial\\theta^{2}}\\log f(X|\\theta) \\right)\\)\n【예제 ② 계속】 \\(f(x;\\theta) \\sim Poisson(\\lambda)\\)에서 표본크기 \\(n\\)인 확률표본을 추출하였다. 모수 \\(\\lambda\\)에 대한 추정량의 분산 그레머 라오 하한을 구하시오.\n포아송분포는 지수족이므로 Fisher Information은 다음과 같다.\n\\(E_{\\lambda}\\left( \\left( \\frac{\\partial}{\\partial\\lambda}\\log\\overset{n}{\\prod_{i = 1}}f(X_{i}|\\lambda) \\right)^{2} \\right) = - nE_{\\lambda}\\left( \\frac{\\partial^{2}}{\\partial\\lambda^{2}}\\log f(X|\\lambda) \\right)\\)\n\\(= - nE_{\\lambda}\\left( \\frac{\\partial^{2}}{\\partial\\lambda^{2}}\\log\\left( \\frac{e^{- \\lambda}\\lambda^{X}}{X!} \\right) \\right) = - nE_{\\lambda}\\left( \\frac{\\partial^{2}}{\\partial\\lambda^{2}}\\left( - \\lambda + X\\log\\lambda - \\log X! \\right) \\right) = \\frac{n}{\\lambda}\\)\n\\(V_{\\lambda}(\\overline{X}) = \\frac{\\lambda}{n}\\)이므로 표본평균이 크레머 라오 하한을 보장한다.\n【예제 ③】 \\(f(x;\\theta) \\sim N(\\mu,\\sigma^{2})\\)에서 표본크기 \\(n\\)인 확률표본을 추출하였다. 모수 \\(\\sigma^{2}\\)에 대한 추정량의 분산 그레머 라오 하한을 구하시오.\n정규분포는 지수족이므로 \\[\\frac{\\partial^{2}}{\\partial(\\sigma^{2})^{2}}\\log\\left( \\frac{1}{(2\\pi\\sigma^{2})^{\\frac{1}{2}}}e^{- (1/2)(x - \\mu)^{2}/\\sigma^{2}} \\right) = \\frac{1}{2\\sigma^{4}} - \\frac{(x - \\mu)^{2}}{\\sigma^{6}}\\] \\[- E\\left( \\frac{\\partial^{2}}{\\partial(\\sigma^{2})^{2}}\\log f(X|\\mu,\\sigma^{2}) \\right) = - E\\left( \\frac{1}{2\\sigma^{4}} - \\frac{(X - \\mu)^{2}}{\\sigma^{6}} \\right) = \\frac{1}{2\\sigma^{4}}\\]\n표본분산의 추정분산은 \\(Var(S^{2} \\mid \\mu,\\sigma^{2}) = \\frac{2\\sigma^{4}}{n - 1}\\)이므로 표본분산은 그레머 라오 하한을 만족하지 못한다.\n\n\n\n4. Rao balckwell 정리 & MVUE\n【MVUE 정의】 \\(f(x;\\theta)\\) 에서 표본크기 \\(n\\)인 확률표본 \\((X_{1},X_{2},\\ldots,X_{n})\\) 추출하였고 \\(T(\\overline{x})\\)은 모수 \\(\\theta\\)의 충분 통계량이다. 만약 \\(T(\\overline{x})\\) 불편 추정량이고 다른 불편 추정량의 추정 분산보다 적은 추정 분산을 가진다면 \\(T(\\overline{x})\\)를 최소분산 불편 추정량 minimum variance unbiased estimator 이라 한다.\n충분 통계량은 모수에 대한 좋은 추정량을 발견하는데 주요 역할을 한다. 추정량 \\(\\widehat{\\theta}\\)을 모수 \\(\\theta\\)의 불편 추정량, 통계량 \\(U\\)을 모수 \\(\\theta\\)에 대한 충분 통계량이라 하자. 불편 추정량인 충분 통계량 함수는 불편 추정량 중 최소 분산을 갖는다. 만약 최소 분산을 갖는 불편 추정량을 찾는 것은 충분 통계량의 함수인 추정량에 한정하며 된다. 이에 관련된 이론이 Rao-Blackwell 정리라 한다.\n【rao-blackwell theorem】 추정량  \\(\\overset{\\hat{}}{\\theta}\\)는 모수 \\(\\theta\\)의 불편 추정량이고 추정 분산을 \\(V(\\overset{\\hat{}}{\\theta})\\)이라 하자. 만약 통계량 \\(U\\)을 모수 \\(\\theta\\)에 대한 충분 통계량이라 하면 \\(E(\\overset{\\hat{}}{\\theta}|U)\\)은 불편 추정량이고 불편 추정량 중 최소 분산을 갖는다.\n\\(X_{1},X_{2}\\) 확률변수에 대하여 (1) \\(E\\left( X_{2} \\right) = E(E\\left( X_{2}|X_{1} \\right))\\) (2) \\(V\\left( X_{2} \\right) \\geq V(E\\left( X_{2}|X_{1} \\right))\\)\n\\(X_{1}\\)=모수 \\(\\theta\\) 충분 통계량 \\(U\\), \\(X_{2}\\)=모수 \\(\\theta\\) 불편 통계량 \\(\\overset{\\hat{}}{\\theta}\\)이라 하자.\n= \\(E\\left( \\overset{\\hat{}}{\\theta} \\right) = E(E\\left( \\overset{\\hat{}}{\\theta}|U \\right))\\) 이므로 \\(E(\\overset{\\hat{}}{\\theta}|U)\\) 불편 추정량이다.\n\n\\(V\\left( \\overset{\\hat{}}{\\theta} \\right) \\geq V(E(\\overset{\\hat{}}{\\theta}|U))\\) 이므로 불편 추정량이면서 이전보다 추정분산이 적은 추정량을 얻는다.\n\nR-B 정리는 최소분산을 갖는 불편 추정량은 충분 통계량으로 만들어질 수 있다. 만약 우리가 불편 추정량을 갖고 있다면 R-B 정리를 이용하여 이 불편 추정량을 향상 시킬 수 있다. 이렇게 얻는 추정량에 R-B 정리를 반복 적용하면 된다. 그러나 만약 동일한 충분 통계량을 사용한다면 더 이상 나아지는 것도 없다.\n\\({\\overset{\\hat{}}{\\theta}}^{*} = E(\\overset{\\hat{}}{\\theta}|U)\\)을 새로 얻은 불편 추정량이라 하자. \\(E\\left( {\\overset{\\hat{}}{\\theta}}^{*} \\middle| U \\right) = {\\overset{\\hat{}}{\\theta}}^{*}\\) 이므로 충분 통계량은 수없이 많다. 그럼 어떤 충분 통계량을 시작점으로 하여 R-B 정리에 사용될까? Factorization criterion이 가장 좋은 충분 통계량을 얻게 한다. 가장 좋은 통계량이란 데이터(확률표본)에 있는 모수에 대한 정보를 가장 잘(best) 요약한 것을 의미하며 이를 Minimal 충분 통계량이라 한다.\n【정리】 \\(f(x;\\theta)\\) 에서 표본크기 \\(n\\)인 확률표본 \\((X_{1},X_{2},\\ldots,X_{n})\\) 추출하였고 \\(T(\\overline{x})\\)을 모수 \\(\\theta\\)의 충분 통계량이라 하자. 또 다른 확률표본 \\((Y_{1},Y_{2},\\ldots,Y_{n})\\)에 대하여 \\(\\frac{L(x_{1},x_{2},\\ldots,x_{n};\\theta)}{L(y_{1},y_{2},\\ldots,y_{n};\\theta)}\\)가 모수 \\(\\theta\\)의 함수가 성립한다. ⬄ (필요 충분 조건) \\(T\\left( \\overline{x} \\right) = T(\\overline{y})\\). 그리고 \\(T(\\overline{x})\\)을 최소 minimal 충분 통계량이라 한다.\n일반적으로 Factorization criterion에서 얻은 충분 통계량과 Minimal 충분 통계량은 같다. 이런 통계량이 갖는 성질을 Completeness(완비성)라 한다.\n【ancillary statistics】 모수 \\(\\theta\\)에 의존하지 않는 통계량 \\(S(\\overline{x})\\)을 보조 ancillary 통계량이라 한다.확률 분포의 모수에 관련된 정보가 아닌 추가적인 정보를 제공하는 통계량을 나타내고 모수 추정이나 가설 검정과 같은 통계적 추론에서 사용되는데, 주로 추정된 모수들의 분포나 특성을 이해하고 분석하는 데 활용된다.\n【예제】 \\(f(x;\\theta) \\sim U(\\theta,\\theta + 1)\\)에서 표본크기 \\(n\\)인 확률표본 \\((X_{1},X_{2},\\ldots,X_{n})\\) 추출하였다. 통계량 \\(R = x_{(n)} - x_{(1)}\\)의 확률밀도함수가 모수 \\(\\theta\\)에 의존하지 않으므로 보조 통계량이다.\n【예제】 \\(f(x;\\theta) \\sim B(\\theta = p)\\)에서 표본크기 \\(n\\)인 확률표본 \\((X_{1},X_{2},\\ldots,X_{n})\\) 추출하였다. 충분 통계량 \\(\\sum X_{i}\\)의 확률밀도함수는 \\(B(np,np(1 - p))\\)로 모수 \\(\\theta = p\\)에 의존하므로 보조 통계량은 아니다. 그러나 \\(\\frac{\\sum X_{i} - np}{\\sqrt{np(1 - p)}}\\) 확률밀도함수는 표준정규분포(\\(N(0,1)\\))에 근사하므로 모수에 의존하지 않아 보조 통계량이다. 다음 장에서 이를 검정 통계량이라 한다.\n【완비성 completeness】 충분 통계량 \\(T\\left( \\overline{x} \\right) \\sim f(t;\\theta)\\)을 갖는다고 하자. 만약 \\(E_{\\theta}\\left( g(T) \\right) = 0forall\\theta\\)가 \\(P_{\\theta}\\left( g(T) = 0 \\right) = 1forall\\theta\\)을 포함하면 \\(T\\left( \\overline{x} \\right)\\)는 완비 통계량이다.\n【예제】 \\(f(x;\\theta) \\sim B(\\theta = p)\\)에서 표본크기 \\(n\\)인 확률표본을 추출하였다. 충분 통계량 \\(\\sum X_{i}\\)은 완비 통계량임을 보이시오.\n\n\\(T = \\sum X_{i} \\sim B(n,p)\\) 이다.\n\\(0 = E_{p}\\left( g(T) \\right) = \\sum_{t}^{n}{g(t)\\binom{n}{t}p^{t}(1 - p)^{n - t} = (1 - p)^{n}}\\sum_{t}^{n}{g(t)\\binom{n}{t}{(\\frac{p}{1 - p})}^{t}forall0 &lt; p &lt; 1}\\)\n\\(0 = \\sum_{t}^{n}{g(t)\\binom{n}{t}{(\\frac{p}{1 - p})}^{t}}\\) 이 조건이 만족하기 위해서는 \\(P_{\\theta}\\left( g(T) = 0 \\right) = 1\\) 이므로 완비 통계량이다.\n\n【예제】 \\(f(x;\\theta) \\sim U(0,\\theta)\\)에서 표본크기 \\(n\\)인 확률표본 \\((X_{1},X_{2},\\ldots,X_{n})\\) 추출하였다. 충분 통계량 \\({T = x}_{(n)}\\)은 완비 통계량이다.\n\\(T \\sim f(t;\\theta) = nt^{n - 1}\\theta^{- n},0 &lt; t &lt; \\theta\\) 이다. \\(E_{\\theta}\\left( g(T) \\right)\\)은 모수 \\(\\theta\\)의 함수이고 상수이므로 \\(E_{p}\\left( g(T) \\right) = 0\\)을 보이는 것은 \\({\\frac{\\partial}{\\partial\\theta}E}_{\\theta}\\left( g(T) \\right) = 0\\)을 보이는 것은 동일하다.\n\\({0 = \\frac{\\partial}{\\partial\\theta}E}_{\\theta}\\left( g(T) \\right) = \\frac{\\partial}{\\partial\\theta}\\int_{0}^{\\theta}{g(t)}nt^{n - 1}\\theta^{- n}dt = \\theta^{- 1}ng(\\theta)\\)\n\\(\\theta^{- 1}n \\neq 0\\) 이므로 \\(g(\\theta) = 0\\)이어야 한다. 그러므로 \\({T = x}_{(n)}\\) 완비 통계량이다.\n【basu theorem】 최소 충분 통계량이고 완비 통계량 \\(T\\left( \\overline{x} \\right)\\)는 다른 모든 보조 통계량과 독립이다.\n【정리】 완비 통계량 \\(T\\left( \\overline{x} \\right)\\)는 최소 충분 통계량이다.\n⑴ Factorization에 의해 충분 통계량 \\(U\\)을 구하고 (2) \\(U\\) 확률밀도함수의 완비성을 보이고 (3) 완비 충분 통계량 \\(U\\)의 함수로 된 불편 추정량을 얻으면 이것이 Rao-Blackwell 정리에 의하여 MVUE가 된다.\n【lehmann and scheffe theorem】 \\(f(x;\\theta),\\theta \\in \\Omega\\) 에서 표본크기 \\(n\\)인 확률표본을 추출하였고 \\(T(x_{1},x_{2},\\ldots,x_{n})\\)은 모수 \\(\\theta\\)의 충분 통계량이라 하자. \\(T(\\overline{x})\\)의 확률밀도함수 \\(f(t;\\theta)\\)가 완비성을 갖는다면 불편성을 갖는 \\(T(\\overline{x})\\) 함수, \\(g(T\\left( \\overline{x} \\right),E(g\\left( T\\left( \\overline{x} \\right) \\right) = \\theta)\\)는 유일 최소분산불편 추정량(MVUE)이다.\n【정리】 \\(f(x;\\theta)\\) 에서 표본크기 \\(n\\)인 확률표본을 추출하였고 \\(T(\\overline{x})\\)을 모수 \\(\\theta\\)의 충분 통계량, 그리고 \\({\\overset{\\hat{}}{\\theta}}_{mle}\\)는 MLE 추정량이라 하자. \\({\\overset{\\hat{}}{\\theta}}_{mle}\\)는 충분 통계량, \\(T(\\overline{x})\\)의 함수이다.\n충분 통계량의 완비성을 증명하는 것은 쉽지 않다. 단 지수족 모집단으로부터 확률표본의 통계량 \\(T = \\sum K(X_{i})\\)는 모수 \\(\\theta\\)의 완비 충분 통계량이다.\n【정리】 지수족 확률밀도함수를 갖는 경우 \\(c(\\theta)\\)의 최소 충분 통계량은 \\(\\sum K(x)\\)이다.\n지수족 확률밀도함수를 다음과 같이 쓸 수 있다.\n\\(f(x;\\theta) = h(x)g(\\theta)\\exp{\\left( c(\\theta)K(x) \\right) \\Longleftrightarrow}exp(c(\\theta)K(x) + h(x) + g(\\theta))\\)\n\\(h(x),K(x)\\) : 확률변수 \\(x\\)의 함수, \\(g(\\theta),c(\\theta)\\) : 모수 \\(\\theta\\)의 함수\n【예제】 \\(f(x;\\theta) = \\frac{1}{\\theta},0 &lt; x &lt; \\theta\\)에서 표본크기 \\(n\\)인 확률표본을 추출하였다. MVUE 구하라.\n\n\\({T = X}_{(n)} \\sim f(t;\\theta) = \\frac{nt^{n - 1}}{\\theta^{n}},0 &lt; t &lt; \\theta\\)는 충분 통계량이다.\n\\[E_{\\theta}\\left( g(T) \\right) = \\int_{0}^{\\theta}{g(t)\\frac{nt^{n - 1}}{\\theta^{n}}dt} = (\\theta &gt; 0,n \\geq 1)\\int_{0}^{\\theta}{g(t)t^{n - 1}dt} = 0\\] \\(0 = g(\\theta)\\theta^{n - 1}\\)을 만족하려면 \\(g(\\theta) = 0\\)이어야 하므로 \\({T = X}_{(n)}\\) 완비 통계량이다.\n\\(E(T) = \\int_{0}^{\\theta}{t\\frac{nt^{n - 1}}{\\theta^{n}}dt = \\frac{n}{n + 1}\\theta}\\) 이므로 \\(\\frac{n + 1}{n}X_{(n)}\\)은 MVUE\n\n【예제】 \\(f(x;\\theta) = B(\\theta = p)\\)에서 표본크기 \\(n\\)인 확률표본을 추출하였다. MVUE 구하라.\n\\(f(x;\\theta = p) = p^{x}(1 - p)^{1 - x} = 1(1 - p)exp(ln(\\frac{p}{1 - p})x)\\) 이므로 지수족이고 \\(K\\left( x_{i} \\right) = x_{i}\\). 그러므로 \\(\\sum x_{i}\\) 완비 충분 통계량이고 \\(\\sum x_{i} \\sim B(n,p)\\) 이다. \\(E\\left( \\sum x_{i} \\right) = np\\) 이므로 \\(\\overline{X} = \\frac{\\sum x_{i}}{n}\\)는 MVUE이다.\n【예제】 \\(f(x;\\theta = \\lambda) = e^{- \\lambda}\\frac{\\lambda^{x}}{x!}\\)에서 표본크기 \\(n\\)인 확률표본을 추출하였다. MVUE 구하라.\n\\(f(x;\\lambda) = {\\frac{1}{x!}e}^{- \\lambda}(ln(\\lambda)x)\\) 이므로 지수족이고 \\(K\\left( x_{i} \\right) = x_{i}\\). 그러므로 \\(\\sum x_{i}\\) 완비 충분 통계량이고 \\(\\sum x_{i} \\sim P(n\\lambda)\\) 이다.\n\\(E\\left( \\sum x_{i} \\right) = n\\lambda\\) 이므로 \\(\\overline{X} = \\frac{\\sum x_{i}}{n}\\)는 MVUE이다.\n【예제】 \\(f(x;\\theta) = \\left( \\frac{2x}{\\theta} \\right)e^{- x^{2}/\\theta} \\sim Weibull(\\gamma = 2,\\theta)\\)에서 표본크기 \\(n\\)인 확률표본을 추출하였다. MVUE 구하라.\n\\(f(x;\\theta) = 2x\\left( \\frac{1}{\\theta} \\right)exp( - x^{2}/\\theta)\\) 이므로 지수족이고 \\(K\\left( x_{i} \\right) = x_{i}^{2}\\). 그러므로 \\(\\sum x_{i}^{2}\\) 완비 충분 통계량이다. 변수 변환 방법 \\(W = X^{2},X = \\sqrt{W},J = \\frac{1}{2\\sqrt{w}}\\) 이므로 \\(W \\sim exponential(\\theta)\\)이다. \\(E\\left( \\sum x_{i}^{2} \\right) = n\\theta\\) 이므로 \\(\\frac{\\sum x_{i}^{2}}{n}\\) 은 MVUE이다.\n【예제】 \\(f(x;\\theta = \\mu) = N\\left( \\mu,\\sigma^{2} \\right),where\\sigma^{2}isknown\\)에서 표본크기 \\(n\\)인 확률표본을 추출하였다. 모수 \\(\\theta = \\mu\\)MVUE 구하라.\n\\(f(x;\\theta) = \\frac{1}{\\sqrt{2\\pi}\\sigma}e^{( - \\mu/2\\sigma^{2})}exp(\\frac{\\mu}{\\sigma^{2}}x - \\frac{x^{2}}{2\\sigma^{2}})\\) 이므로 지수족이고 \\(K\\left( x_{i} \\right) = x_{i}\\). \\(E\\left( \\sum x_{i} \\right) = n\\mu\\) 이므로 \\(\\overline{X} = \\frac{\\sum x_{i}}{n}\\)는 MVUE이다.\n【정리】 추정량 \\(\\overset{\\hat{}}{\\theta}\\)은 모수 \\(\\theta\\)에 대한 MVUE이고 \\(g(.)\\)은 일대일 함수이면 \\(g(\\theta)\\)의 MVUE는 \\(g(\\overset{\\hat{}}{\\theta})\\) 중 불편성을 갖는 추정량이다.\n【예제】 \\(f(x;\\theta) = B(\\theta = p)\\)에서 표본크기 \\(n\\)인 확률표본을 추출하였다. \\(\\frac{p(1 - p)}{n}\\)에 대한 MVUE 구하라.\n모수 \\(\\theta = p\\)에 대한 MVUE는 \\(\\overline{X} = \\frac{\\sum x_{i}}{n}\\) 임을 보였다. 그러므로 \\(\\frac{p(1 - \\theta p)}{n}\\)에 대한 MVUE을 구하기 위하여 분포를 알고 있는 \\(Y = \\sum x_{i} \\sim B(n,p)\\) 함수를 이용하는 것이 적절하다. \\(Y\\)는 완비 충분 통계량이므로 \\(Y(1 - Y)\\)도 완비 충분 통계량이다.\n\\(E\\left( Y(1 - Y) \\right) = E(Y) - E\\left( Y^{2} \\right) = E(Y) - \\left( V(X) + E(Y)^{2} \\right) = np - np(1 - p) - n^{2}p^{2}\\)\n\\(= (n - 1)p(1 - p)\\) 이므로 \\(\\frac{\\theta(1 - \\theta)}{n}\\) 의 MVUE는 \\(\\frac{Y(1 - Y)}{n(n - 1)}\\)\n【예제】 \\(f(x;\\theta) = \\left( \\frac{1}{\\theta} \\right)e^{- x/\\theta} \\sim exponential(\\theta)\\)에서 표본크기 \\(n\\)인 확률표본을 추출하였다. \\(V(X) = \\theta^{2}\\) MVUE 구하라.\n지수분포는 지수족이고 \\(K\\left( x_{i} \\right) = x_{i}\\)이므로\\(\\sum x_{i}\\) 완비 충분 통계량이므로 \\(\\overline{X}\\)는 모수 \\(\\theta\\)의 MVUE이다. 그러므로 \\(V(X) = \\theta^{2}\\)의 MVUE을 \\({\\overline{X}}^{2}\\)의 함수 중 불편 추정량을 찾으면 된다.\\(E\\left( {\\overline{X}}^{2} \\right) = V\\left( \\overline{X} \\right) + E\\left( \\overline{X} \\right)^{2} = \\frac{\\theta^{2}}{n} + \\theta^{2} = \\frac{n + 1}{n}\\theta^{2}\\) 이므로 \\(\\frac{n\\overline{X}}{n + 1}\\) 은 MVUE이다."
  },
  {
    "objectID": "notes/math_stat/random_sample.html",
    "href": "notes/math_stat/random_sample.html",
    "title": "수리 통계 5. 확률 표본",
    "section": "",
    "text": "chapter 1. 데이터와 확률표본\n\n1. 데이터\n실험을 통해 수집된 자료는 일반적으로 어떤 하나의 변수에 대한 여러 관측값으로 구성되며, 이러한 자료를 우리는 데이터(data)라고 한다. 데이터는 자연현상이나 사회현상에 대한 체계적 이해를 가능하게 해주는 가장 기초적인 재료이며, 과학적 탐구의 출발점이라 할 수 있다.\n과학 이론은 때때로 아인슈타인의 상대성 이론처럼 탁월한 이론적 통찰이나, 케플러의 행성 궤도 법칙처럼 새로운 자연현상의 관찰, 또는 Student의 t-분포처럼 실험과 경험에서 비롯된 혁신적인 아이디어를 통해 탄생하기도 한다. 그러나 대부분의 경우 과학 이론은 관찰, 실험, 그리고 분석의 반복을 통해 점진적으로 정립된다. 이러한 과정에서 수집되고 해석되는 데이터는 과학적 주장과 이론의 정당성을 뒷받침하는 핵심적인 증거가 된다.\n예를 들어, 벼 품종의 수량성을 높이기 위한 품종 개량 연구, 신약의 효과와 부작용을 평가하기 위한 임상시험, 혹은 산업 현장에서의 공정 개선 실험 등은 모두 체계적인 실험 설계에 따라 데이터를 수집하고 분석함으로써 유의미한 결론에 도달하게 된다. 이러한 연구들은 데이터의 수집과 활용이 단순한 관찰을 넘어서, 실질적인 과학적·기술적 진보를 이끌어낸다는 점을 잘 보여준다.\n요약하면, 데이터란 실험이나 관찰을 통해 얻은 정량적 또는 정성적 측정값의 집합이며, 이는 과학적 이론을 정립하고 검증하는 데 필수적인 도구이다. 따라서 데이터에 대한 명확한 이해와 올바른 수집·분석 방법의 습득은 과학 연구 및 통계학 학습의 핵심이라 할 수 있다.\n데이터(data)란 추론, 토론, 계산 등의 목적을 위해 활용되는 실제 정보의 집합으로, 일반적으로 측정값 또는 통계적 수치의 형태를 갖는 숫자들의 모임을 의미한다. 이러한 데이터는 관찰이나 실험을 통해 수집되며, 연구 가설의 검정이나 현상의 이해를 위한 객관적 근거로 기능한다. 즉, 데이터란 변수의 값을 수치 또는 범주 형태로 표현한 관측값들의 집합으로, 이는 모집단에 대한 추론이나 가설 검정 등의 통계적 분석의 기초 자료가 된다.\n【정의】 통계학에서 분석의 대상이 되는 데이터는 일반적으로 행과 열로 구성된 숫자 행렬의 형태를 가진다. 이때 각 행은 하나의 개체 또는 관측 단위를 나타내며, 각 열은 해당 개체에 대해 측정된 변수(확률변수) 또는 특성을 의미한다. 이러한 데이터 행렬에서 행의 첨자는 서로 다른 개체를 구분하는 데 사용되며, 열의 첨자는 어떤 변수를 나타내는지를 구별하는 역할을 한다. 따라서 행렬의 각 원소는 특정 개체에 대해 특정 변수의 값을 의미하며, 분석은 이 구조를 기반으로 이루어진다.\n\\[\\mathbf{X}n \\times p = \\begin{bmatrix}\nx11 & x_{12} & \\cdots & x_{1p} \\\\\nx_{21} & x_{22} & \\cdots & x_{2p} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nx_{n1} & x_{n2} & \\cdots & x_{np}\n\\end{bmatrix}\\]\n\n\\(n\\): 관측값의 수 (또는 개체 수)\n\\(p\\): 변수의 수 (측정된 특성의 수)\n\\(x_{ij}\\): i번째 개체의 j번째 변수에 대한 관측값\n\n\n\n2. 데이터 수집 및 모형\n\n\n\n\n\nVeracity(데이터 정확성) + Value(데이터 가치) = 5V of Big Data\n데이터는 객관적인가? NO &lt;- 수집되는 데이터는 목적이 있다. even 빅데이터 - 분석자의 의도없이 매초 단위로 엄청난 자동 저장되는 데이터도 일단 분석 대상이 되는 순간 그 데이터는 목적을 가지게 되므로 객관성을 상실한다.\n데이터는 관심을 갖는 모집단 개체로부터 분석 대상 특성을 관측, 측정 등을 통하여 얻어지는 숫자(고전적 데이터), 문자(텍스트 마이닝), 음성, 이미지(빅데이터) 형식이다.\n\n\n\n\n\n(전통적인 통계방법) 과학에서 이론이 제안되고 데이터 분석이 이루어지는 경우보다는 (빅데이터 통계방법)데이터로부터 새로운 이론이나 모형을 도출하는 경우가 많고 탐색적 자료 분석에 의해 제안된 이론이나 모형은 다시 확증적 방법에 의해 유의성이 (연구가설이 적합하다) 검증되므로 모형과 데이터는 순환 사이클을 갖는다.\n통계적 모형은 과학적 진실이기 보다는 분석 대상이 되는 사실(현황)의 대표적 모형이다. 예를 들어, 회귀모형에서는 선형함수(모형)이 설명하지 못하는 오차항(e)이존재하고 이 오차항은 평균 0, 분산 \\(\\sigma^{2}\\)인 정규분포를 따른다고 가정한다.\n\n\n3. 데이터와 모집단\n\n(1) 모집단과 확률변수\n관심의 대상이 되는 개체 전체를 모집단이라 한다. 예를 들어 코스피 지수와 등록된 기업의 주가에 관심이 있다면 코스피 등록된 기업들이 모집단이 된다. 전국 대학생들의 흡연율, 일주일 공부시간, 폰에 저장된 친구 전화번호 개수에 관심이 있다면 조사시점 기준 대학에 등록한 대학생이 모집단이 된다.\n모집단을 구성하는 개체의 관심 특성을 (확률)변수라 한다. 코스피 예제에서 확률변수는 주가(시작가, 최고가, 최저가, 종가)이고 관측은 일별(주 중 5일)로 관측된다. 주가는 연속형 확률변수이고 측정형이다. 시간적 측면에서는 시계열 데이터이다. 대학생 예제의 확률변수는 흡연여부, 일주일 공부시간이다. 흡연여부와 친구번호 개수는 이산형 확률변수, 공부시간은 연속형 확률변수이다.\n\n\n(2) 모집단 관심 정보\n\n\n\n\n\n모집단 관심 특성 확률변수의 모든 정보는 \\(f(x;\\theta)\\), 확률분포함수와 모수로 요약된다. \\(f(x)\\)에 대하여 모르는 상황은 다음 2가지 중 하나이다.\n\n모집단 확률분포함수 \\(f(x)\\)는 알려져 있지 않다.\n\\(f(x)\\)의 형태는 모수 \\(\\theta\\)까지는 알려져 있다.\n\n일반적으로 \\(f(x)\\)에 대한 관심이보다는 요약값인 \\(\\theta\\)에 관심을 가지므로 위의 (2) 상황에서 추론을 한다. (예) 가구소득은 로그정규분포를 따른다고 가정하고 평균과 표준편차에 대한 추론을 하게 된다.\n모집단 확률분포함수 \\(f(x)\\)\n모집단 확률변수에 대한 모든 정보는 확률분포함수를 얻으면 얻을 수 있다. 이산형 확률변수의 모집단 확률밀도함수는 베르누이 시행 가정 하에 구할 수 있지만 연속형 모집단 확률분포함수는 이론적으로 가정하거나 수집된 데이터(확률표본)로부터 얻을 수 있는데 이를 실증적 확률분포함수라 한다.\n모수 \\(f(x;\\theta)\\)\n확률분포함수의 모든 개체의 관측값의 정보를 가지고 있으므로 모집단의 확률분포함수를 구할 수 있다면 모집단에 대한 원하는 정보를 얻을수 있다. 일반적으로 이산형 데이터 모집단에 대한 확률분포함수를 얻는 것은 가능하나 연속형인 경우에는 불가능하다. 그러므로 일반적으로 모집단 확률분포함수에 대하여는 가정하게 된다.\n통계학은 모집단 개체 하나 하나의 정보에 관심을 갖기보다는 확률변수의 요약 특성(예를 들면 중앙위치, 흩어진 정도 등) 값에 관심을 갖는다. 이를 모수라 하고 \\(\\theta\\)라 표현한다.\n\n\n\n\nchapter 2. 확률표본 개념\n\n1. 확률표본 정의 및 활용\n\n(1) 확률표본 정의\n확률변수 \\(X_{1},\\ldots,X_{n}\\)이 다음 조건을 만족할 때, 이들을 모집단 \\(f(x)\\)로부터의 크기 n인 확률표본 random sample 이라 한다.\n\n\\(X_{1},\\ldots,X_{n}\\)은 서로 독립이며\n각각의 \\(X_{i}\\)는 동일한 확률밀도함수 또는 확률질량함수를 갖는다.\n\n이러한 확률변수들을 우리는 독립이고 동일한 분포(iid: independent and identically distributed)를 따르는 변수들이라고 하고 다음과 같이 표기한다. \\(X_{1},\\ldots,X_{n} \\sim \\text{iid}f(x)\\)\n모집단의 확률함수 \\(f(x)\\)가 어떤 모수 모형에 속한다고 가정할 수 있다면, 확률밀도함수는 \\(f(x \\mid \\theta)\\)의 형태로 표현될 수 있다. 이 경우 결합 확률함수는 다음과 같다. 여기서 동일한 모수 \\(\\theta\\)가 모든 항에서 동일하게 사용된다.\n\\[f(x_{1},\\ldots,x_{n} \\mid \\theta) = \\overset{n}{\\prod_{i = 1}}f(x_{i} \\mid \\theta)\\]\n통계적 분석에서는 모집단이 어떤 특정한 모수 모형에 속한다고 가정하되, 실제 모수 값 \\(\\theta\\)는 미지수로 취급한다. 이러한 상황에서 임의표본은 위와 같은 형태의 결합 확률함수를 가지며, \\(\\theta\\)는 모르는 상태로 남는다. 따라서 다양한 가능한 \\(\\theta\\) 값을 고려함으로써, 서로 다른 모집단들로부터 추출된 임의표본이 어떻게 행동할지를 연구할 수 있다.\n\n\n(2) 확률표본 활용\n통계학에서는 확률변수, 확률변수의 관측치를 데이터(열, 변수x행, 관측치)라 하고 데이터가 가진 모든 정보는 확률밀도함수 \\(f(x;\\theta)\\)에 의해 요약된다. 통계추론에서는 확률밀도함수를 요약하는 값을 모수(\\(\\theta\\))라 하며 추론 관심 대상이 되는 값으로 모집단 평균, 분산, 비율 등이 대표적인 예이다. 확률밀도함수의 모수는 확률밀도함수의 형태를 결정한다.\n예들 들어 이항분포의 모수는 실험 회수(\\(n\\))와 성공확률((\\(p\\))이고, 감마분포의 모수는 형태모수, \\(\\alpha\\)와 비율모수, \\(\\beta\\)이다. 모집단 모수는 \\(\\theta\\)로 표현되고 \\(\\theta\\) 값을 추정 estimation하거나 가설검정 hypothesis testing을 통계적 추론이라 하고 이를 위하여 모집단으로부터 확률표본(이를 데이터라 함)을 추출하게 된다.\n확률표본으로부터 모수에 ”가장” 적절한 값, 통계량(확률표본의 함수, \\(T = T(X_{1},X_{2},\\ldots,X_{n})\\))을 계산하고 이를 이용하여 추론 을 하게 된다. 통계적 추론에서 모집단의 확률밀도함수를 \\(f(x;\\theta),p(x;\\theta)\\)로 하여 모수 포함하여 표현한다.\n\\(f(x),p(x)\\)의 형태는 알려져 있지 않다.\n모수 \\(\\theta\\)는 고정된 값이나 모른다. 베이지안 추론에서는 모수 \\(\\theta\\)을 확률변수 하여 추론한다.\n데이터_확률표본\n\\[(x_{1},x_{2},...,x_{n})\\]\n모집단\n\\[f(x;\\theta)\\]\n통계량\n\\[\\widehat{\\theta} = h(x_{1},x_{2},...,x_{n})\\]\n\n\n\n\n\n\n\n(3) 확률표본 성질\n확률표본의 확률분포함수는 모집단 확률분포함수와 동일하다.\ni-번째 관측치 \\(x_{i}\\) 의 확률분포함수는 모집단의 확률분포 \\(f(x;\\theta)\\) 와 동일하고 다른 관측치와 서로 독립이다.\n확률표본 결합확률밀도함수 \\(f(x_{1},x_{2},...,x_{n})\\)\n확률표본이 갖는 정보인 확률분포함수는 다음과 같이 구해지며 이를 확률표본 결합밀도함수라 한다.\n\\[f(x_{1},x_{2},...,x_{n}) = (independent)f(x_{1})f(x_{2})...f(x_{n})\\]\n\\[= (identical)f(x;\\theta)f(x;\\theta)...f(x;\\theta) = \\lbrack f(x;\\theta)\\rbrack^{n}\\]\n\n모집단 확률분포함수 \\(f(x)\\)를 알지 못하므로 결합확률밀도함수도 알수 없다.\n확률표본 결합확률밀도함수가 추정해야 하는 모수를 포함하고 있는 경우 이를 우도함수 likelihood function 라 한다.\n\n【예제 ①】 확률변수 \\(X_{1},\\ldots,X_{n}\\)이 평균이 \\(\\beta\\)인 지수분포로부터 추출된 크기 n의 확률표본이라고 하자. 결합확률밀도함수는 다음과 같다.\n\\[f(x_{1},\\ldots,x_{n} \\mid \\beta) = \\overset{n}{\\prod_{i = 1}}f(x_{i} \\mid \\beta) = \\overset{n}{\\prod_{i = 1}}\\frac{1}{\\beta}e^{- x_{i}/\\beta} = \\frac{1}{\\beta^{n}}e^{- (x_{1} + \\cdots + x_{n})/\\beta}\\]\n만약 모든 \\(X_{i}\\)가 1 이상 값을 가질 확률은\n\\[P(X_{1} &gt; 1,\\ldots,X_{n} &gt; 1) = \\int_{1}^{\\infty}\\cdots\\int_{1}^{\\infty}\\overset{n}{\\prod_{i = 1}}\\frac{1}{\\beta}e^{- x_{i}/\\beta}dx_{1}\\cdots dx_{n}\\]\n\\[= \\left( \\int_{1}^{\\infty}\\frac{1}{\\beta}e^{- x/\\beta}dx \\right)^{n} = \\left( e^{- 1/\\beta} \\right)^{n} = e^{- n/\\beta}\\]\n\n\n\n2. 복원추출과 비복원추출\n무한 모집단\n앞에서 정의된 확률표본 모형은 때때로 무한 모집단으로부터의 표본추출로 간주된다. 예를 들어, \\(X_{1},\\ldots,X_{n}\\)의 값을 순차적으로 얻는 과정을 생각해 보자. 먼저 실험을 수행하여 \\(X_{1} = x_{1}\\)이 관측되고, 다음으로 실험을 반복하여 \\(X_{2} = x_{2}\\)가 관측된다.\n이때 무작위 표본추출에서의 독립성 가정은 \\(X_{1} = x_{1}\\)이 먼저 관측되었더라도 \\(X_{2}\\)의 확률분포에는 아무런 영향을 주지 않는다는 것을 의미한다. 다시 말해, \\(x_{1}\\)을 무한 모집단에서 ”제거”하더라도 모집단의 성질은 바뀌지 않기 때문에, \\(X_{2} = x_{2}\\) 역시 여전히 동일한 모집단에서의 무작위 추출로 간주된다.\n유한모집단 복원추출\n반면, 유한 모집단으로부터 표본을 추출하는 경우에는 실제 자료가 어떻게 수집되었는지에 따라 달라진다. 모집단의 크기가 유한한 \\(N\\)개일 때, 각 값이 동일한 확률 \\(\\frac{1}{N}\\)로 선택되는 방식으로 표본을 추출한다고 하자. 선택된 첫 번째 값은 \\(X_{1} = x_{1}\\)로 기록된다. 그 다음 동일한 과정을 반복하여 \\(X_{2} = x_{2}\\)를 선택한다. 만약 동일한 값이 다시 선택된다면 \\(x_{1} = x_{2}\\)가 될 수도 있다. 이러한 표본추출 방식은 각 단계에서 선택된 값을 다시 모집단에 되돌려 놓는다는 의미에서 복원추출이라 부른다. 추출 선택 과정이 서로 영향을 미치지 않기 때문에\\(X_{1},\\ldots,X_{n}\\)은 상호 독립이다.\n유한모집단 비복원추출\n비복원추출은 한 번 선택된 값은 이후의 선택에서 제외되는 방식이다. 처음에는 모집단 \\(\\{ x_{1},\\ldots,x_{N}\\}\\)중 하나를 확률 \\(\\frac{1}{N}\\)로 선택하여 \\(X_{1} = x_{1}\\) 로 기록한다. 그 다음 두 번째 값은 나머지 \\(N - 1\\)개의 값 중에서 선택되며, 각 값은 \\(1/(N - 1)\\)의 확률로 선택된다. 이런 방식으로 \\(X_{1},\\ldots,X_{n}\\)까지 계속 선택해나간다. 하지만 한 번 선택된 값은 다시 선택되지 않으므로 중복은 허용되지 않으므로 \\(P(X_{2} = y \\mid X_{1} = y) = 0\\) 상호 독립이 아니다.\n하지만 흥미로운 점은, 이러한 경우에도 \\(X_{1},\\ldots,X_{n}\\)은 여전히 동일한 분포를 따른다. 즉, 각각의 주변분포는 같다.\n\\(X_{1}\\)의 경우, \\(P(X_{1} = x) = \\frac{1}{N}\\)이다.\n\\(X_{2}\\)의 주변확률분포: \\(P(X_{2} = x) = \\overset{N}{\\sum_{i = 1}}P(X_{2} = x \\mid X_{1} = x_{i}) \\cdot P(X_{1} = x_{i})\\)\n이때 \\(x_{i} = x\\)인 경우, \\(P(X_{2} = x \\mid X_{1} = x) = 0\\)\n나머지 \\(j \\neq k\\)일 때는 \\(P(X_{2} = x \\mid X_{1} = x_{j}) = \\frac{1}{N - 1}\\).\n\\(P(X_{2} = x) = (N - 1)\\left( \\frac{1}{N - 1} \\cdot \\frac{1}{N} \\right) = \\frac{1}{N}\\) (동일분포)\n다음 예제는 유한 모집단에서의 복잡한 정확 확률 계산을, 독립성이라는 가정 하에 단순한 곱셈식으로 근사할 수 있음을 보여주는 유용한 사례이다.\n【예제 ②】 유한 모집단 \\(\\{ 1,2,\\ldots,1000\\}\\)에서 표본을 추출한다고 하자. 이 모집단에서 크기 n = 10인 표본을 비복원 추출로 선택한다고 가정하고 \\(P(X_{1} &gt; 200,\\ldots,X_{10} &gt; 200)\\)을 구해보자.\n(독립성 가정) \\(\\left( P(X &gt; 200) \\right)^{10} = \\left( \\frac{800}{1000} \\right)^{10} = 0.107374\\)\n(초기하분포) \\(P(Y = 10) = \\frac{\\binom{800}{10} \\cdot \\binom{200}{0}}{\\binom{1000}{10}} = 0.106164\\)\n\n\n\nchapter 3. 확률표본의 함수\n확률표본 \\(X_{1},\\ldots,X_{n}\\)이 주어졌을 때, 일반적으로 이 값들에 대한 어떤 요약값을 계산하게 된다. \\(T(x_{1},\\ldots,x_{n})\\)\n여기서 함수 \\(T\\)의 정의역은 확률벡터(\\(X_{1},\\ldots,X_{n}\\))의 표본공간 전체를 포함한다. 함수 \\(T\\)는 확률변수의 함수이므로 확률변수가 된다.\n표본 \\(X_{1},\\ldots,X_{n}\\)이 독립이고 동일한 분포를 따른다는 구조적 단순성 덕분에, 이로부터 유도되는 확률변수 \\(Y = T(X_{1},\\ldots,X_{n})\\)의 분포는 비교적 다루기 쉬운 편이다. 이러한 분포는 일반적으로 표본 안의 변수들의 분포로부터 유도되므로, 이를 샘플링 sampling 분포라 부른다. 여기서는 확률변수의 합(\\(Y = \\sum X_{i}\\)) 형태로 정의되는 함수를 중심으로, 샘플링분포의 성질에 대해 논의할 것이다.\n\n1. 통계량\n【정의】 \\(X_{1},\\ldots,X_{n}\\)이 어떤 모집단으로부터 크기 n인 임의표본이라고 하자. 또한 \\(T(x_{1},\\ldots,x_{n})\\)이 표본공간 \\(X_{1},\\ldots,X_{n}\\)을 정의역으로 갖는 실수값 또는 벡터값 함수라고 하자. \\(Y = T(X_{1},\\ldots,X_{n})\\)으로 정의되는 확률변수 또는 확률벡터 \\(Y\\)를 통계량 statistic 이라 한다. 이 통계량 \\(Y\\)의 확률분포를 샘플링 분포라 한다.\n통계량의 정의는 매우 폭넓지만, 단 하나의 제한 조건은 통계량은 모수의 함수가 될 수 없다는 점이다. 즉, 통계량은 오직 표본 데이터만을 기반으로 정의되어야 한다.\n확률표본인 데이터로부터 실제 값이 계산되면 소문자 \\(t\\)로 표시하며 이를 모수 \\(\\theta \\subseteq \\Omega\\)에 대한 점 추정량 point estimator 으로 사용한다.\n통계량을 추정에 사용하면 추정량 , 가설 검정에 사용하면 검정통계량 이라 한다.\n확률표본의 함수인 통계량도 확률변수의 함수이므로 확률변수이다. 그러므로 확률분포함수를 갖게 되므로 이를 샘플링분포라 한다.\n확률변수의 선형함수의 평균과 분산\n\\((X_1, \\ldots, X_n)\\) 이 어떤 확률실험에서 얻어진 확률벡터(random vector)라고 하자. 우리는 종종 \\(T = T(X_1, \\ldots, X_n)=\\sum_{i=1}^{n} a_i X_i,\\) 과 같은 선형결합(linear combinations)에 관심을 갖는다.\n【정리】 \\(T = \\sum_{i=1}^{n} a_i X_i\\) 라고 하자. 또한 모든 \\(i = 1, \\ldots, n에 대해 E[|X_i|] &lt; \\infty\\) 라고 하자. 그러면, \\(E(T) = \\sum_{i=1}^{n} a_i E(X_i)\\).\n【정리】 \\(T = \\sum_{i=1}^{n} a_i X_i\\) 이고, \\(W = \\sum_{j=1}^{m} b_j Y_j\\) 라고 하자. 만약 모든 \\(i = 1, \\ldots, n\\) 에 대해 \\(E[X_i^2] &lt; \\infty\\), 그리고 모든 \\(j = 1, \\ldots, m\\) 에 대해 \\(E[Y_j^2] &lt; \\infty\\) 라면,\n\\(\\operatorname{Cov}(T, W)\n= \\sum_{i=1}^{n} \\sum_{j=1}^{m} a_i b_j \\operatorname{Cov}(X_i, Y_j)\\).\n【따름정리】 \\(T = \\sum_{i=1}^{n} a_i X_i\\) 라고 하자. 또한 모든 \\(i = 1, \\ldots, n에 대해 E[X_i^2] &lt; \\infty\\) 라고 하자. 그러면, \\(\\operatorname{Var}(T)\n= \\operatorname{Cov}(T, T)\n= \\sum_{i=1}^{n} a_i^{2}\\,\\operatorname{Var}(X_i)\n\\;+\\; 2 \\sum_{i&lt;j} a_i a_j\\, \\operatorname{Cov}(X_i, X_j)\\).\n【따름정리】 만약 \\(X_1, \\ldots, X_n\\) 이 서로 독립이며 유한한 분산을 가진 확률변수들이라면, \\(\\operatorname{Var}(T)\n= \\sum_{i=1}^{n} a_i^{2}\\, \\operatorname{Var}(X_i)\\).\n\n\n2. 확률표본의 평균과 분산\n【정의】 확률표본 \\(\\left( X_{1},X_{2},\\ldots,X_{n} \\right)\\)의 함수 \\(\\overset{¯}{X} = \\frac{1}{n}\\sum_{i}^{n}X_{i}\\)통계량을 표본평균이라 한다.\n【정의】 확률표본 \\(\\left( X_{1},X_{2},\\ldots,X_{n} \\right)\\)의 함수 \\(S^{2} = \\frac{1}{n - 1}\\sum_{i}^{n}\\left( X_{i} - \\overset{¯}{X} \\right)^{2}\\)통계량을 표본분산이라 한다. \\(\\sum_{i}^{n}\\left( x_{i} - \\overset{¯}{x} \\right)^{2} = \\sum_{1}^{n}{x_{i}^{2} - n\\overset{¯}{x}}\\)\n【정리】표본분산을 최소화 하는 \\(a = \\overset{¯}{x}\\) 이다.\n【정리】 평균 \\(\\mu\\), 분산 \\(\\sigma^{2}\\)인 모집단으로부터 확률표본 \\(\\left( X_{1},X_{2},\\ldots,X_{n} \\right)\\)의 표본평균, 표본분산에 대하여 다음이 성립한다.\n\\[E\\left( \\overline{X} \\right) = E\\left( \\frac{1}{n}\\sum_{1}^{n}X_{i} \\right) = \\frac{1}{n}\\sum_{i}^{n}{E\\left( X_{i} \\right) =}\\frac{1}{n}\\sum_{i}^{n}{\\mu =}\\mu\\]\n\\[V\\left( \\overline{X} \\right) = V\\left( \\frac{1}{n}\\sum x_{i} \\right) = (ind)\\frac{1}{n^{2}}\\sum V\\left( X_{i} \\right) = \\frac{1}{n^{2}}\\sum\\sigma^{2} = \\frac{\\sigma^{2}}{n}\\]\n\\[ES^{2} = \\mathbb{E}\\left( \\frac{1}{n - 1}\\left\\lbrack \\overset{n}{\\sum_{i = 1}}X_{i}^{2} - n{\\overline{X}}^{2} \\right\\rbrack \\right) = \\frac{1}{n - 1}\\left( n\\mathbb{E}\\lbrack X_{1}^{2}\\rbrack - n\\mathbb{E}\\lbrack{\\overline{X}}^{2}\\rbrack \\right)\\]\n\\[ES^{2} = \\frac{1}{n - 1}\\left( n(\\sigma^{2} + \\mu^{2}) - n\\left( \\frac{\\sigma^{2}}{n} + \\mu^{2} \\right) \\right) = \\sigma^{2}\\]\n표본분산은 모집단 분산의 불편 추정량이다.\n\n\n3. 샘플링 분포\n확률표본 함수인 통계량 \\(T = T\\left( X_{1},X_{2},\\ldots,X_{n} \\right)\\)확률밀도함수를 샘플링 분포라 한다.\n통계량을 활용하여 가설검정하거나 신뢰구간 추정을 하려면 통계량의 확률분포함수를 알아야 한다. 통계량의 확률분포함수 \\(f(x)\\)의 샘플링 확률분포함수라 한다.\n모집단의 확률분포=표본확률분포이나 샘플링 확률분포함수는 상이하다. 모집단의 확률분포함수는 몰라도 샘플링 확률분포함수는 알 수 있어, 이를 이용하여 추정과 검정을 한다.\n(중심극한정리) 모집단의 확률분포함수와 무관하게 표본평균, 표본합의 확률분포함수는 정규분포에 근사한다.\n【정리】 확률표본 \\(X_{1},\\ldots,X_{n}\\)이 모멘트생성함수 \\(M_{X}(t)\\)를 갖는 모집단으로부터 추출되었다고 하자. 그렇다면, 표본평균 \\(\\overline{X} = \\frac{1}{n}\\overset{n}{\\sum_{i = 1}}X_{i}\\)의 모멘트생성함수는 \\(M_{\\overline{X}}(t) = \\left\\lbrack M_{X}\\left( \\frac{t}{n} \\right) \\right\\rbrack^{n}\\)이다.\n【예제 ①】 확률표본 \\(X_{1},\\ldots,X_{n}\\)이 \\(\\mathcal{N}(\\mu,\\sigma^{2})\\) 분포를 따르는 모집단으로부터 추출되었다고 하자. 이때, 표본평균 \\(\\overline{X}\\)의 모멘트생성함수는 다음과 같다.\n\\[M_{\\overline{X}}(t) = \\left\\lbrack \\exp\\left( \\mu \\cdot \\frac{t}{n} + \\frac{\\sigma^{2}(t/n)^{2}}{2} \\right) \\right\\rbrack^{n} = \\exp\\left( n\\left( \\mu \\cdot \\frac{t}{n} + \\frac{\\sigma^{2}(t/n)^{2}}{2} \\right) \\right) = \\exp\\left( \\mu t + \\frac{\\sigma^{2}}{2n}t^{2} \\right)\\]\n이로부터, \\(\\overline{X} \\sim \\mathcal{N}\\left( \\mu,\\frac{\\sigma^{2}}{n} \\right)\\).\n【예제 ②】 확률표본 \\(X_{1},\\ldots,X_{n}\\)이 감마분포 \\(\\text{Gamma}(\\alpha,\\beta)\\)를 따른다고 하자. 표본평균 \\(\\overline{X}\\)의 모멘트생성함수는 다음과 같으므로 표본평균의 샘플링분포는 \\(\\overline{X} \\sim \\text{Gamma}(n\\alpha,\\beta/n)\\) 또한 감마분포이다.\n\\[M_{\\overline{X}}(t) = \\left\\lbrack \\left( \\frac{1}{1 - \\beta(t/n)} \\right)^{\\alpha} \\right\\rbrack^{n} = \\left( \\frac{1}{1 - (\\beta/n)t} \\right)^{n\\alpha}\\]\n【정리】 만약 \\((X,Y)\\)가 서로 독립이며 연속형 확률변수이고, 각각의 확률밀도함수가 \\(f_{X}(x),f_{Y}(y)\\)라고 하자. 그러면 이들의 합 \\(Z = X + Y\\)의 확률밀도함수는 다음과 같은 컨볼루션 convolution으로 주어진다. \\(f_{Z}(z) = \\int_{- \\infty}^{\\infty}f_{X}(w)f_{Y}(z - w)dw\\)\n【예제 ③】 독립인 코쉬 확률변수 \\(U \\sim \\text{Cauchy}(0,\\sigma),V \\sim \\text{Cauchy}(0,\\tau)\\)가 있을 때 \\(U + V\\)의 확률밀도함수는 \\(\\text{Cauchy}(0,\\sigma + \\tau)\\)이다.\n\\[f_{U}(u) = \\frac{1}{\\pi\\sigma} \\cdot \\frac{1}{1 + (u/\\sigma)^{2}},f_{V}(v) = \\frac{1}{\\pi\\tau} \\cdot \\frac{1}{1 + (v/\\tau)^{2}}, - \\infty &lt; u,v &lt; \\infty\\]\n\\[f_{Z}(z) = \\int_{- \\infty}^{\\infty}\\frac{1}{\\pi\\sigma} \\cdot \\frac{1}{1 + (w/\\sigma)^{2}} \\cdot \\frac{1}{\\pi\\tau} \\cdot \\frac{1}{1 + \\left( \\frac{z - w}{\\tau} \\right)^{2}}dw\\]\n\\[f_{Z}(z) = \\frac{1}{\\pi(\\sigma + \\tau)} \\cdot \\frac{1}{1 + \\left( \\frac{z}{\\sigma + \\tau} \\right)^{2}}, - \\infty &lt; z &lt; \\infty\\]\n위치-척도 분포 \\(X_{i} \\sim \\frac{1}{\\sigma}f\\left( \\frac{x - \\mu}{\\sigma} \\right)\\) 가족으로부터 추출된 확률표본 합의 샘플링 분포는 동일 확률밀도함수를 가지며 위치 파라미터는 유지되고, 척도 파라미터는 단순히 더해진다. 코쉬분포의 합, 정규분포의 합, 감바분포의 합이 대표적인 예이다.\n【정리】 확률표본 \\(X_{1},\\ldots,X_{n}\\)가 다음의 지수족 모집단에서 추출되었다고 하자. \\(f(x \\mid \\theta) = h(x)c(\\theta)\\exp\\left( \\overset{k}{\\sum_{i = 1}}w_{i}(\\theta)t_{i}(x) \\right)\\), 이때, 통계량 \\(T_{1},\\ldots,T_{k}\\)를 \\(T_{i}(X_{1},\\ldots,X_{n}) = \\overset{n}{\\sum_{j = 1}}t_{i}(X_{j}),i = 1,\\ldots,k\\)와 같이 정의한다면, 이 통계량들의 결합분포는 다음과 같은 형태의 지수족이 된다.\\(f_{T}(u_{1},\\ldots,u_{k} \\mid \\theta) = H(u_{1},\\ldots,u_{k})\\lbrack c(\\theta)\\rbrack^{n}\\exp\\left( \\overset{k}{\\sum_{i = 1}}w_{i}(\\theta)u_{i} \\right)\\)\n이 정리는, 지수족으로부터 추출된 표본의 충분통계량 역시 지수족의 형태를 갖는 분포를 따른다는 중요한 결과를 제시한다.\n【예제 ④】 확률표본 \\(X_{1},\\ldots,X_{n}\\)이 \\(Bernoulli(p)\\)분포에서 추출되었다고 하자. \\(T_{1}(X_{1},\\ldots,X_{n}) = \\overset{n}{\\sum_{i = 1}}X_{i}\\)는 충분통계량이다.\\(k = 1,c(p) = 1 - p,w_{1}(p) = \\log\\left( \\frac{p}{1 - p} \\right),t_{1}(x) = x\\)\n\n\n\nchapter 4. 정규분포로부터의 확률표본\n정규분포를 따르는 모집단은 여전히 가장 널리 사용되는 통계 모형 중 하나이며, 이로부터 표본을 추출하면 표본 통계량의 유용한 성질뿐 아니라, 잘 알려진 여러 셈플링 분포도 함께 유도된다. 통계 모형에 대한 추론은 정규분포를 가정하여 발전한 것과 무관하지 않다.\n\n1. 표본평균과 표본분산의 샘플링 분포\n【정리】 확률표본 \\(X_{1},\\ldots,X_{n}\\)이 \\(\\mathcal{N}(\\mu,\\sigma^{2})\\)분포에서 추출되었다고 하자. 표본평균 \\(\\overline{X} = \\frac{1}{n}\\overset{n}{\\sum_{i = 1}}X_{i}\\), 표본분산 \\(S^{2} = \\frac{1}{n - 1}\\overset{n}{\\sum_{i = 1}}(X_{i} - \\overline{X})^{2}\\)에 대하여 다음이 성립한다.\n\n\\(\\overline{X}\\)와 \\(S^{2}\\)는 서로 독립인 확률변수이다.\n\\(\\overline{X} \\sim \\mathcal{N}(\\mu,\\sigma^{2}/n)\\)\n\\((n - 1)S^{2}/\\sigma^{2} \\sim \\chi^{2}(n - 1)\\)\n\n\n\n2. 카이제곱 확률변수에 대한 성질\n【보조정리】\n\n만약 \\(Z\\)가 표준정규분포 \\(\\mathcal{N}(0,1)\\)를 따른다면, \\(Z^{2}\\)는 자유도 1인 카이제곱 분포를 따른다. \\(Z^{2} \\sim \\chi_{1}^{2}\\)\n독립인 카이제곱 확률변수 \\(X_{1},\\ldots,X_{n}\\)가 각각 자유도 \\(p_{1},\\ldots,p_{n}\\)을 갖는다고 하자. 확률표본의 합은 \\(X_{1} + \\cdots + X_{n} \\sim \\chi_{p_{1} + \\cdots + p_{n}}^{2}\\)을 따른다.\n\n\n\n3. 정규 변수들의 선형결합 간의 독립성 판별\n【보조정리】 \\(X_{j} \\sim \\mathcal{N}(\\mu_{j},\\sigma_{j}^{2}),j = 1,\\ldots,n\\)이 서로 독립이라고 하자. 상수 \\(a_{ij},b_{rj}\\)에 대해 \\(i = 1,\\ldots,k,r = 1,\\ldots,m,j = 1,\\ldots,n\\), 단 \\(k + m \\leq n\\)이라 하자. 다음과 같이 선형결합을 정의한다.\n\\(U_{i} = \\overset{n}{\\sum_{j = 1}}a_{ij}X_{j},i = 1,\\ldots,k\\), \\(V_{r} = \\overset{n}{\\sum_{j = 1}}b_{rj}X_{j},r = 1,\\ldots,m\\)\n\n두 확률변수 \\(U_{i}\\)와 \\(V_{r}\\)는 독립일 필요충분조건은 공분산이 0일 때이다. \\(U_{i}\\bot V_{r} \\Longleftrightarrow \\text{Cov}(U_{i},V_{r}) = 0\\). 공분산은 다음과 같이 계산된다. \\(\\text{Cov}(U_{i},V_{r}) = \\overset{n}{\\sum_{j = 1}}a_{ij}b_{rj}\\sigma_{j}^{2}\\)\n벡터 \\((U_{1},\\ldots,U_{k})\\)와 \\((V_{1},\\ldots,V_{m})\\)이 상호 독립이기 위한 필요충분조건은 모든 \\(i = 1,\\ldots k,\\)와 \\(r = 1,\\ldots,m\\)에 대하여 \\(U_{i}\\bot V_{r}\\)이다. 즉, 각 쌍의 선형결합이 모두 독립일 때, 전체 벡터도 독립이다.\n\n\n표본평균 \\(\\overline{X}\\)와 표본분산 \\(S^{2}\\)의 독립성 증명\n분산분석(ANOVA)에서 독립성 증명: 집단 간 제곱합(SSB)과 집단 내 제곱합(SSW)이 독립임을 보일 때 유용하다. 즉, F-분포의 분자와 분모가 독립이라는 성질을 확인할 때 자주 등장한다.\n회귀분석에서 잔차(residual)와 추정량의 독립성: 예측값과 오차항이 독립이라는 성질을 정규 가정하에서 보장할 때 사용된다.\n고급 통계 이론 및 추정량의 분산 계산: 선형 추정량의 분산 공식을 도출하거나, 선형 회귀에서 BLUE(최선선형불편추정량) 조건을 분석할 때도 활용된다.\n다변량 정규분포의 성질 분석: 독립성 조건을 갖는 여러 선형변환 결과들이 서로 독립이 되는지 판단할 때.\n\n\n\n4. 정규분포로부터 유도된 분포\n\n(1) t-분포\n위에서 유도한 분포들은, 정규성을 가정한 통계 분석의 출발점으로 매우 중요하다. 실제로, 대부분의 실제 통계 분석 상황에서는 모분산 \\(\\sigma^{2}\\)의 값을 알지 못한다. 따라서, 모평균 \\(\\mu\\)에 대한 추론을 위해 \\(\\overline{X}\\)의 변동성을 이해하려면 우선 \\(\\sigma^{2}\\)를 추정해야 한다. 이 주제는 1900년대 초, W. S. Gosset에 의해 처음으로 연구되었으며, 그는 Student라는 필명으로 논문을 발표하였다. Student의 업적은 오늘날 우리가 t 분포라 부르는 Student의 t 분포로 이어졌다.\n정규분포 \\(N(\\mu,\\sigma^{2})\\)로부터의 확률표본 평균 \\(\\overline{X}\\)에 대하여 다음이 성립한다. \\(\\frac{\\overline{X} - \\mu}{\\frac{\\sigma}{\\sqrt{n}}} \\sim N(0,1)\\). 여기서 모집단 표준편차 \\(\\sigma\\) 값을 안다면 모평균 \\(\\mu\\)에 대한 추론이 가능하다. 그러나 만약 \\(\\sigma\\) 값을 모른다면 가정하에 t-분포가 유도되었다.\n\\[\\frac{\\overline{X} - \\mu}{\\frac{S}{\\sqrt{n}}} = \\frac{\\left( \\overline{X} - \\mu \\right)/(\\sigma/\\sqrt{n})}{\\sqrt{S^{2}/\\sigma^{2}}} = \\frac{N(0,1)}{\\sqrt{\\chi_{n - 1}^{2}/(n - 1)}} \\sim t(n - 1)\\]\n\n\n(2) F-분포\n통계적 추론에서 우리는 종종 두 개 이상의 모집단을 비교해야 하는 상황에 놓이게 된다. 특히, 여러 집단 간 평균이 서로 동일한지 검정하고자 할 때, 단순히 두 집단만을 비교하는 t-검정만으로는 충분하지 않다. 예를 들어, 세 가지 종류의 약물이 혈압에 미치는 영향을 비교하고자 한다면, 각 약물 그룹의 평균 혈압을 비교해야 하고, 이를 위해 분산분석(ANOVA)이 사용된다.\nF-분포는 영국의 통계학자 Ronald A. Fisher에 의해 도입되었다. 그는 1920년대 농업 실험을 설계하면서, 여러 가지 비료의 효과를 비교하는 문제에 직면하였고, 이를 해결하기 위해 집단 간 변동과 집단 내 변동의 비율을 비교하는 통계량이 필요했다. 이렇게 하여 F-검정과 분산분석 기법이 탄생하게 되었고, 그 기반이 되는 확률분포가 F-분포이다. 분산분석의 핵심은 전체 변동(총제곱합)을 집단 간 변동(처리 제곱합)과 집단 내 변동(오차 제곱합)으로 분해하고, 이들 제곱합을 자유도로 나눈 평균제곱(mean square)의 비율을 통해 유의성을 검정하는 데 있다. 이때 사용되는 검정통계량이 바로 F-분포를 따르는 통계량이다.\n【정의】 두 독립적인 정규 모집단에서 각각 다음과 같은 표본을 추출한다고 하자.\n\\(X_{1},\\ldots,X_{n} \\sim \\mathcal{N}(\\mu_{X},\\sigma_{X}^{2})\\), \\(Y_{1},\\ldots,Y_{m} \\sim \\mathcal{N}(\\mu_{Y},\\sigma_{Y}^{2})\\)\n이때, 표본분산들 \\(S_{X}^{2},S_{Y}^{2}\\)를 사용하여 다음과 같은 통계량을 정의한다.\n\\(F = \\frac{(S_{X}^{2}/\\sigma_{X}^{2})}{(S_{Y}^{2}/\\sigma_{Y}^{2})}\\). 이때 이 \\(F\\)는 자유도 \\(n - 1\\)과 \\(m - 1\\)을 갖는 Snedecor의 \\(F\\)-분포를 따른다.\n【정리】\n\n만약 \\(X \\sim F_{p,q}\\) 라면, 그 역수인 \\(1/X \\sim F_{q,p}\\)를 따른다.\n만약 \\(X \\sim t_{q}\\)이면, \\(X^{2} \\sim F_{1,q}\\)이다.\n만약 \\(X \\sim F_{p,q}\\)이면, \\(\\frac{(p/q)X}{1 + (p/q)X} \\sim \\text{Beta}\\left( \\frac{p}{2},\\frac{q}{2} \\right)\\)이다.\n\n\n\n\n\nchapter 5. 순서통계량\n확률표본에서 가장 작거나 큰 값, 또는 중앙값과 같은 관측값들은 단순한 평균이나 분산 외에도 중요한 요약 정보를 제공할 수 있다. 예를 들어, 지난 50년간 기록된 최대 홍수 수위나 최저 기온은 향후 재난 예방과 대비에 유용한 기준이 될 수 있으며, 지난달 주택의 중위 가격은 생활비나 주거비용을 추정하는 데 실질적인 참고 자료가 된다.\n평균과 중앙값은 서로 관련되어 있지만, 측정하는 특성과 해석의 관점에서는 분명한 차이가 있다. 이를 이해하기 위해 서울 아파트 가격을 예로 들어보자.\n일부 부동산 정책 입안자들은 “서울 아파트의 평균 매매 가격이 14억 원이므로 현재의 금융 조건이나 세금 정책은 타당하다”고 주장한다. 그러나 많은 실수요자들은 “실제로는 절반 이상의 거래가 9억 원 이하에서 이뤄지고 있으며, 15억 원 이상의 고가 아파트는 강남권 등 특정 지역에만 집중되어 있다”고 말한다. 실제로 몇 채의 초고가 아파트만으로도 전체 평균 가격이 크게 상승할 수 있다.\n이처럼 평균은 극단값(outliers)에 민감하게 반응하여 대표값으로서 왜곡될 수 있으며, 중앙값은 보다 전형적인 거래 수준을 반영해 실생활과 정책 판단에 더 유용한 지표가 될 수 있다.\n\n1. 순서통계량 정의\n【정의】 이러한 값들은 모두 순서통계량 order statistics 이라 불리며, 자료의 크기 순서에 따라 결정되는 통계량이다. 모집단 \\(f(x),x \\in S = \\left\\{ x;a &lt; x &lt; b \\right\\}\\)으로부터의 확률표본 \\((X_{1},X_{2},\\ldots,X_{n})\\) 크기 순으로 정렬하고 \\(X_{(1)} \\leq X_{(2)} \\leq \\ldots \\leq X_{(n)}\\) 이를 순서 통계량이라 한다.\n\n최소값 minimum value : \\(X_{(1)} = \\min_{i}{(x_{1},x_{2},\\ldots,x_{n})}\\)\n최대값 maximum value : \\(X_{(n)} = \\max_{i}{(x_{1},x_{2},\\ldots,x_{n})}\\)\n범위 range : \\(R = X_{(n)} - X_{(1)}\\)\n중앙값 median : \\(P(X \\leq {\\overset{˜}{x}}_{m}) = \\frac{1}{2}\\)을 만족하는 \\({\\overset{˜}{x}}_{m}\\)이다.\n표본 데이터 중앙값 : \\(M = \\{\\begin{matrix}\nX_{((n + 1)/2)} & \\text{if}n\\text{is odd} \\\\\n& \\\\\n\\frac{1}{2}\\left( X_{(n/2)} + X_{(n/2 + 1)} \\right) & \\text{if}n\\text{is even}\n\\end{matrix}\\)\n\n【정의】 백분위값 percentile은 주어진 값보다 작은 관측값의 비율을 나타내는 중요한 지표이다. 0과 1 사이의 어떤 수 \\(p\\)에 대해, 샘플에서의 \\((100p)\\%\\)백분위는 전체 표본 중 약 \\(np\\)개의 관측값이 그 값보다 작고, 약 \\(n(1 - p)\\)개의 관측값이 그 값보다 큰 위치에 있는 값으로 정의된다. \\(Q_{p} = F^{- 1}(p)\\)\n샘플에서의 \\(100p\\)번째 백분위는 다음 관측값으로 정의된다.\n\\(p\\)가 \\(\\frac{1}{2n} &lt; p &lt; 0.5\\) 범위: \\(X_{(np)}\\)\n\n\\(0.5 &lt; p &lt; 1 - \\frac{1}{2n}\\) 범위 : \\(X_{(n + 1 - n(1 - p))}\\)\n\n표본 백분위\n표본 데이터의 \\(p\\)th백분위수는 \\(k = (n + 1)p\\)가 정수이면 \\(X_{(k)}\\)이나 실수인 경우에는 다음 방법으로 구한다. \\((n + 1)p = k + r,0 &lt; r &lt; 1\\), \\(r\\)은 소수 부분으로 가중치로 사용된다. \\(p\\)th백분위수 : \\((1 - r)X_{(k)} + rX_{(k + 1)}\\)\n\n\n2. 순서 통계량 결합, 주변 확률밀도함수\n순서통계량 \\(X_{(1)},X_{(2)},\\ldots,X_{(n)}\\)의 결합 확률밀도함수는 다음과 같다.\n【정리】 \\[f_{X_{(1)},\\ldots,X_{(n)}}(x_{1},\\ldots,x_{n}) = \\{\\begin{matrix}\nn! \\cdot f_{X}(x_{1})\\cdots f_{X}(x_{n}), & \\text{if} - \\infty &lt; x_{1} &lt; \\cdots &lt; x_{n} &lt; \\infty \\\\\n0, & \\text{otherwise}\n\\end{matrix}\\]\n【정리】 \\(X_{(1)},\\ldots,X_{(n)}\\)을 확률표본 \\(X_{1},\\ldots,X_{n}\\)의 순위통계량이라 하자. 이 확률표본은 누적분포함수\\(F_{X}(x)\\), 확률밀도함수 \\(f_{X}(x)\\)를 갖는 연속 분포에서 추출된 것이다. 이때, \\(X_{(j)}\\)의 확률밀도함수는 다음과 같다.\n\\[f_{X_{(j)}}(x) = \\frac{n!}{(j - 1)!(n - j)!}f_{X}(x)\\lbrack F_{X}(x)\\rbrack^{j - 1}\\lbrack 1 - F_{X}(x)\\rbrack^{n - j}\\]\n\n최대값 확률밀도함수 : \\(f_{X_{(n)}}(x) = nF(x)^{n - 1}f(x)\\)\n최소값 확률밀도함수 : \\(f_{X_{(1)}}(x) = n(1 - F{(x))}^{n - 1}f(x)\\)\n\n\\(1 \\leq i &lt; j \\leq n\\)일 때, 순위통계량 \\(X_{(i)},X_{(j)}\\)의 결합 확률밀도함수는 다음과 같다.\n\\[f_{X_{(i)},X_{(j)}}(u,v) = \\frac{n!}{(i - 1)!(j - 1 - i)!(n - j)!}f_{X}(u)f_{X}(v)\\lbrack F_{X}(u)\\rbrack^{i - 1}\\lbrack F_{X}(v) - F_{X}(u)\\rbrack^{j - 1 - i}\\lbrack 1 - F_{X}(v)\\rbrack^{n - j}\\]\n\n\n3. 상자수염그림\n사분위 quartile\n표본 데이터의 \\(p = 25,50,75\\)인 순서 통계량을 1사분위수 first quartile(\\(Q_{1}\\)), 2사분위수 second quartile(\\(Q_{2}\\)), 3사분위수 third quartile(\\(Q_{13}\\))이라 한다. 2사분위수를 중앙값 median이라 한다.\n\n사분위 범위 inter-quartile range : \\({IQR = Q}_{3} - Q_{1}\\)\n\n상자 수염 그림 box whisker plot\n\n\n\n\n\n\n5개 요약 통계량 표현 : (최소값, 1사분위, 중앙값, 3사분위, 최대값)\n하한 펜스 lower fence : \\(Q_{1} - 1.5IQR\\)\n상한 펜스 upper fence : \\(Q_{3} + 1.5IQR\\)\n상한, 하한 펜스를 벗어나는 값들을 이상치 outlier 라 한다. 1.5 대신 3을 사용하면 극심한 severe 이상치라 진단한다.\n\n\n\n\nchapter 6. 수렴개념\n\n1. 확률수렴\n표본크기가 무한대(이론적이고 가상적인 개념)로 접근하는 경우 표본 통계량의 변화에 대한 내용을 다룬다.\n【확률수렴】 \\(X_{n}\\overset{P}{\\rightarrow}X\\)\n\\(\\{ X_{n}\\}\\) 확률변수 시퀀스, \\(X\\) 확률변수일 때 다음을 만족하면 \\(X_{n}\\)은 \\(X\\)에 확률 수렴한다고(\\(X_{n}\\) convergence in probability) 정의한다.\n\\(\\lim_{n \\rightarrow \\infty}{P\\left( \\left| X_{n} - X \\right| \\geq \\epsilon \\right) = 0forall\\epsilon &gt; 0}\\) ⬄\n\\(\\lim_{n \\rightarrow \\infty}{P\\left( \\left| X_{n} - X \\right| &lt; \\epsilon \\right) = 1forall\\epsilon &gt; 0}\\)\n(WLLN weak law of large numbers) \\(\\{ X_{n}\\}\\)은 평균 \\(\\mu\\), 분산 \\(\\sigma^{2} &lt; \\infty\\)을 갖는iid 확률변수 시퀀스라 하면 표본평균(\\({\\overset{¯}{X}}_{n}\\))은 평균(\\(\\mu\\))에 확률 수렴한다. \\({\\overset{¯}{X}}_{n}\\overset{P}{\\rightarrow}\\mu\\)\n\\(E\\left( {\\overset{¯}{X}}_{n} \\right) = \\mu,V\\left( {\\overset{¯}{X}}_{n} \\right) = \\frac{\\sigma^{2}}{n}\\)이므로 Chebyshev inequality 정리 \\(P\\left( |X - \\mu| &gt; k\\sigma \\right) \\leq \\frac{1}{k^{2}}\\)에 의해 다음이 성립한다.\n\\(P\\left( \\left| {\\overset{¯}{X}}_{n} - \\mu \\right| \\geq \\epsilon \\right) = P\\left( \\left| \\overset{¯}{X_{n}} - \\mu \\right| \\geq (\\epsilon\\sqrt{n}/\\sigma)(\\sigma/\\sqrt{}n) \\right) \\leq \\frac{\\sigma^{2}}{n\\epsilon^{2}} \\rightarrow 0\\)\n【정리】 \\(X_{1},X_{2},\\ldots\\)이 확률변수 \\(X\\)에 확률 수렴 하면 연속함수 \\(g\\)에 대하여 \\({g(X}_{1}),g(X_{2}),\\ldots\\)은 \\(g(X)\\)에 확률 수렴한다. \\(X_{n}\\overset{P}{\\rightarrow}X\\) 이면 \\({g(X}_{n})\\overset{P}{\\rightarrow}g(X)wheregiscontinuousfunction\\).\n【정리】 \\(X_{n}\\overset{P}{\\rightarrow}X,Y_{n}\\overset{P}{\\rightarrow}Y\\) 이면 \\({X_{n}Y}_{n}\\overset{P}{\\rightarrow}XY\\)\n【정의】 확률분포함수 \\(F(x;\\theta)\\)를 갖는 모집단으로부터 확률표본 \\(X_{1},X_{2},\\ldots,X_{n}\\)의 통계량 \\(T_{n} = t(x_{1},x_{2},\\ldots,x_{n})\\)이 다음을 만족하면 모수 \\(\\theta\\)의 일치 추정량이라 한다.\n\\(T_{n}\\overset{P}{\\rightarrow}\\theta\\) ⬄ 표본평균은 모평균의 일치 추정량이다.\n【예제 ①】 \\(\\{ X_{n}\\}\\)은 평균 \\(\\mu\\), 분산 \\(\\sigma^{2} &lt; \\infty\\)을 갖는 확률표본 시퀀스라 하자. 표본분산 \\(S_{n}^{2} = \\frac{1}{n - 1}\\sum_{i}^{n}{(X_{i} - {\\overset{¯}{X}}_{n})}\\)은 모집단 분산 \\(\\sigma^{2}\\)의 일치 추정량 이다. \\(S_{n}^{2}\\overset{P}{\\rightarrow}\\sigma^{2}\\)\n\\(S_{n}^{2} = \\frac{1}{n - 1}\\sum_{i}^{n}{\\left( X_{i} - {\\overset{¯}{X}}_{n} \\right)^{2} = \\frac{n}{n - 1}(\\frac{1}{n}\\sum_{i}^{n}{X_{i}^{2} - {\\overset{¯}{X}}_{n}^{2}})\\overset{P}{\\rightarrow}1\\left( E\\left( X^{2} \\right) - \\mu^{2} \\right) = \\sigma^{2}}\\) 표본분산은 모분산 일치 추정량 이다.\n\n\n2. 거의 확실한 수렴 ALL sure convergence\n【정의】 \\(\\{ X_{n}\\}\\) 확률변수 시퀀스, \\(X\\) 확률변수일 때 다음을 만족하면 \\(X_{n}\\)은 \\(X\\)에 거의 확실한 수렴을 한다고 정의한다. \\(P\\left( \\underset{n \\rightarrow \\infty}{lim}\\left| X_{n} - X \\right| &lt; \\epsilon \\right) = 1forall\\epsilon &gt; 0\\)\n【정의】 평균 \\(\\mu\\), 분산 \\(\\sigma^{2} &lt; \\infty\\)을 갖는 확률표본 평균 \\({\\overset{¯}{X}}_{n}\\)은 평균에 거의 확실히 수렴한다. (강한 대수의 법칙)\n\\(P\\left( \\underset{n \\rightarrow \\infty}{lim}\\left| {\\overset{¯}{X}}_{n} - \\mu \\right| &lt; \\epsilon \\right) = 1forall\\epsilon &gt; 0\\)\n\n\n3. 분포 수렴 convergence in distribution\n【정의】 \\(\\{ X_{n}\\}\\) 확률변수 시퀀스, \\(X\\) 확률변수이고 \\(F_{X_{n}},F_{X}\\) 각각 \\(X_{n},X\\) 확률분포함수일 때 다음을 만족하면 \\(X_{n}\\)은 \\(X\\)에 분포 수렴한다고 정의한다. \\(X_{n}\\overset{D}{\\rightarrow}X\\)\n\\(\\lim_{n \\rightarrow \\infty}{F_{X}}_{n}(x) = F_{X}(x)forallx \\in C(F_{X})\\).\n확률변수 \\(X\\)의 분포를 \\(X_{n}\\)의 한계 limiting asymptotic 분포라 한다.\n【정리】 \\(X_{n}\\) 확률변수 시퀀스가 \\(X\\) 확률변수에 확률 수렴하면 \\(X_{n}\\)은 \\(X\\)에 분포 수렴한다. \\(ifX_{n}\\overset{P}{\\rightarrow}X,thenX_{n}\\overset{D}{\\rightarrow}X\\).\n【예제】 \\(X_{1},X_{2},\\ldots\\)은 \\(U(0,1)\\)으로부터 확률표본이다. 최대값 \\(X_{(n)}\\)의 극한 분포를 구하라.\n\\(U(0,1)\\) 로부터 확률표본 이므로 \\(n \\rightarrow \\infty\\)로 가면 \\(X_{(n)}\\)은 1에 근사해야 한다.\n\\[P\\left( \\left| X_{(n)} - 1 \\right| \\geq \\epsilon \\right) = P\\left( X_{(n)} \\geq 1 + \\epsilon \\right) + P\\left( X_{(n)} \\leq 1 - \\epsilon \\right) = P\\left( X_{(n)} \\leq 1 - \\epsilon \\right)\\]\n확률표본 이므로 \\(P\\left( X_{(n)} \\leq 1 - \\epsilon \\right) = \\Pi_{i}^{n}P\\left( X_{i} \\leq 1 - \\epsilon \\right) = (1 - \\epsilon)^{n}\\)\n만약 \\(\\epsilon = \\frac{t}{n}\\)라 놓으면 \\(P\\left( X_{(n)} \\leq 1 - \\frac{t}{n} \\right) = \\left( 1 - \\frac{t}{n} \\right)^{n} \\rightarrow e^{- t}\\) 이다.\n\\(P\\left( {n(1 - X}_{(n)}) \\leq t \\right) \\rightarrow e^{- t}\\) 이므로 \\({n(1 - X}_{(n)})\\) 극한 분포는 \\(\\beta = 1\\)인 지수분포이다.\n【극한 확률분포함수】\n\\(X_{n}\\) 의 확률밀도함수를 이용하여 극한 확률분포함수를 구하는 것은 쉽지 않아 누적 확률분포함수를 이용하는 것이 편리하다.\n\\(X_{n}\\) 확률밀도함수를 \\(p_{n}(x) = \\left\\{ \\begin{array}{r}\n1,x = 2 + \\frac{1}{n} \\\\\n0,otherwise\n\\end{array} \\right.\\ \\)이라 하자.\n극한 확률밀도함수는 \\(\\lim_{n \\rightarrow \\infty}{p_{n}(x) = 0}forallx\\), 즉 극한 확률밀도함수가 존재하지 않는다. \\(X_{n}\\) 누적 확률분포함수는 \\(F_{n}(x) = \\left\\{ \\begin{array}{r}\n0,x &lt; 2 + \\frac{1}{n} \\\\\n1,x \\leq 2 + \\frac{1}{n}\n\\end{array} \\right.\\ \\)이므로 극한 확률분포함수는 \\(F(x) = \\left\\{ \\begin{array}{r}\n0,x &lt; 2 \\\\\n1,x \\leq 2\n\\end{array} \\right.\\ \\) 이다.\n【중심극한정리 central limit theorem】\n\\(X_{1},X_{2},\\ldots\\)은 평균, 분산이 \\(E\\left( X_{i} \\right) = \\mu &lt; \\infty,V\\left( X_{i} \\right) = \\sigma^{2} &lt; \\infty\\) 로부터 확률표본이다. \\(\\frac{\\sqrt{n}({\\overset{¯}{X}}_{n} - \\mu)}{\\sigma}\\)의 극한분포는 표준정규분포이다.\n【예제】 \\(X_{1},X_{2},\\ldots\\)은 평균, 분산이 \\(E\\left( X_{i} \\right) = p,V\\left( X_{i} \\right) = p(1 - p)\\)인 베르누이분포 로부터 확률표본이다. 표본비율 \\(\\overset{\\hat{}}{p} = \\frac{\\sum x_{i}}{n}\\)의 극한 분포는 중심극한 정리에 의해 \\(\\frac{\\sqrt{n}\\left( \\overset{\\hat{}}{p} - p \\right)}{p(1 - p)}\\overset{D}{\\rightarrow}N(0,1)\\) 이다.\n【예제】 \\(X_{1},X_{2},\\ldots,X_{n}\\)은 \\(NB(r,p)\\)의 확률표본이라 하면 \\(E\\left( X_{i} \\right) = \\frac{r(1 - p)}{p},V\\left( X_{i} \\right) = \\frac{r(1 - p)}{p^{2}}\\) 이므로\n중심극한정리에 의해 \\(\\frac{\\sqrt{n}({\\overset{¯}{X}}_{n} - r(1 - p)/p)}{\\sqrt{r(1 - p)/p^{2}}}\\) 극한분포는 정규분포에 근사한다.\n【\\(\\Delta\\)-method theorem】\n\\(X_{n}\\) 시퀀스에 대하여 \\(\\sqrt{n}\\left( X_{n} - \\theta \\right)\\overset{D}{\\rightarrow}N(0,\\sigma^{2})\\) 이며 \\(g(x)\\)가 미분 가능한 연속형 함수이면 \\(g(X_{n})\\) 시퀀스에 대하여 \\(\\sqrt{n}\\left( {g(X}_{n}) - g(\\theta) \\right)\\overset{D}{\\rightarrow}N(0,\\sigma^{2}\\left( g'(\\theta) \\right)^{2})\\)이 성립한다.\n【예제】 \\(X_{n}\\)은 평균, 분산이 \\(E\\left( X_{i} \\right) = p,V\\left( X_{i} \\right) = p(1 - p)\\)인 베르누이분포로부터 확률표본이다.\n\\(X_{n} = \\overset{\\hat{}}{p} = \\frac{\\sum x_{i}}{n}\\)이므로 \\(\\frac{\\sqrt{n}\\left( \\overset{\\hat{}}{p} - p \\right)}{p(1 - p)}\\overset{D}{\\rightarrow}N(0,1)\\)\n오즈비 \\(\\frac{p}{1 - p}\\) 추정한다고 하자. \\(X_{n} = \\frac{\\overset{\\hat{}}{p}}{1 - \\overset{\\hat{}}{p}}\\)이고 \\(g(p) = \\frac{p}{1 - p}\\)이다. \\({E(X}_{n}) = \\frac{p}{1 - p}\\), \\(V\\left( \\frac{\\overset{\\hat{}}{p}}{1 - \\overset{\\hat{}}{p}} \\right) \\approx \\left\\lbrack g'(p) \\right\\rbrack^{2}V\\left( \\overset{\\hat{}}{p} \\right) = \\left\\lbrack \\frac{1}{(1 - p)^{2}} \\right\\rbrack^{2}\\frac{p(1 - p)}{n} = \\frac{p}{n(1 - p)^{3}}\\)\n\\(\\sqrt{n}\\left( X_{n} = \\frac{\\overset{\\hat{}}{p}}{1 - \\overset{\\hat{}}{p}} - g(p) \\right)\\overset{D}{\\rightarrow}N(0,\\frac{p}{n(1 - p)^{3}})\\).\n\n\n\nchapter 7. 확률표본 생성 : 시뮬레이션\n확률변수의 특성을 설명하기 위한 다양한 방법들—변환, 분포, 적률 계산, 극한 정리를 다루었다.이러한 확률변수들은 실제 현상을 설명하고 모형화하는 데 사용되며, 이 변수들에 대한 관측값이 우리가 수집하는 데이터가 된다.\n\n1. 개념\n정의\n관심 대상인 사회현상이나 자연현상에 대한 통찰력을 얻기 위한 시스템 모델링, 실체적 혹은 추상적인 시스템(프로세스)의 주요 특성을 정형화하여 임의 공간에서 모방 실현하는 것을 의미한다.\n필요성\n수학 모형의 해를 대수적 처리 불가능, 근사 해, 현실에서 실행이 불가능할 때, 새로운 정책 효과 측정, 우주 비행 시 발생 문제 예측, 효율성: 비용과 시간 측면, 모의 비행, 교통 신호 체계 변환, 콜 센터/은행 창구 적정 서비스 직원 수\n\n\n\n\n\n난수생성과 시뮬레이션\n난수 생성과 시뮬레이션은 매우 밀접한 관계를 가지고 있다. 시뮬레이션을 수행할 때 현실 세계의 복잡한 현상을 모델링하고 예측하기 위해 무작위한 요소를 포함하는 것이 중요한데 이것이 바로 난수 생성이 시뮬레이션에 사용되는 주된 이유 중 하나이다.\n\n불확실성 모델링: 현실 세계의 많은 요소들은 불확실성을 내포하고 있는데 이러한 불확실성을 모델링하기 위해 난수가 사용될 수 있다. 날씨 예보 시뮬레이션에서는 다양한 요인들에 따라 다른 결과가 발생할 수 있으며 이를 모델링하기 위해 난수를 사용할 수 있다.\n테스트 및 검증: 복잡한 모델이나 알고리즘을 시뮬레이션하고 검증할 때, 난수는 다양한 상황을 시뮬레이션하는 데 사용될 수 있다. 자율 주행 자동차의 시뮬레이션에서는 다양한 운전 상황을 난수를 사용하여 시험할 수 있다.\n결과의 변동성 도입: 난수를 사용하여 모델의 결과에 변동성을 도입할 수 있는데 재무 모델링에서는 주식 시장의 랜덤한 움직임을 모델링하기 위해 난수를 사용할 수 있다.\n몬테카를로 시뮬레이션: 몬테카를로 시뮬레이션은 무작위 샘플링을 사용하여 복잡한 문제의 근사해를 찾는 데 사용하는데 시뮬레이션에서는 난수가 필수적으로 사용된다.\n\n난수 생성은 이러한 시뮬레이션 과정에서 중요한 요소입니다. 올바른 난수 생성은 시뮬레이션 결과의 정확성과 신뢰성을 보장하는 데 도움이 되므로 신뢰할 수 있는 난수 생성 알고리즘과 적절한 난수 분포 선택이 필요하다.\n시뮬레이션을 통한 근사 계산 (법칙을 활용한 검증)\n통계 추론은 우리는 모집단 분포 \\(f(x|\\theta)\\)로부터 확률변수 \\(X_{1},\\ldots,X_{n}\\)을 관측하고 그 분포의 특성을 이용하여 이 변수들의 행동을 설명하고자 한다. 여기서는 그와 반대로, 주어진 분포 \\(f(x|\\theta)\\)로부터 무작위 확률표본 \\(X_{1},\\ldots,X_{n}\\)을 생성하는 방법에 초점을 둔다.\n【예제】 전구 수명이 지수분포 \\(\\text{Exponential}(\\lambda)\\)를 따르는다고 하자. 생산된 전구 \\(c\\)개의 부품 중 적어도 \\(t\\)개 이상이 \\(h\\)시간 이상 작동할 확률을 구한다고 하자.\n\\[\\begin{matrix}\np_{1} & = P(\\text{부품이}h\\text{시간 이상 작동}) \\\\\n& = P(X \\geq h \\mid \\lambda)\n\\end{matrix}\\]\n\\[\\begin{matrix}\np_{2} & = P(\\text{적어도}t\\text{개 이상의 부품이}h\\text{시간 이상 작동}) \\\\\n& = \\overset{c}{\\sum_{k = t}}\\binom{c}{k}p_{1}^{k}(1 - p_{1})^{c - k}\n\\end{matrix}\\]\n\\(p_{1} = \\int_{h}^{\\infty}\\frac{1}{\\lambda}e^{- x/\\lambda}dx = e^{- h/\\lambda}\\)이 계산 가능하다면 \\(p_{2}\\) 확률 계산도 용이하다.\n위의 예제에서 \\(p_{2}\\) 계산이 불가능한 경우 원하는 분포를 따르는 확률변수들을 시뮬레이션을 통해 생성한 후, 대수의 법칙을 이용하여 그 시뮬레이션의 타당성을 검증하는 것이다.\n\n어떤 확률 변수 \\(X\\)에 대해 함수 \\(h(Y)\\)의 기대값을 알고 싶은데,이론적으로 직접 계산하기 어렵다면\n동일한 분포를 따르는 \\(Y_{1},\\ldots,Y_{n}\\) 을 컴퓨터로 많이 만들어서 (시뮬레이션)\n이들의 평균 \\(\\frac{1}{n}\\overset{n}{\\sum_{i = 1}}h(Y_{i})\\)를 구하면, 이 평균은 \\(E\\lbrack h(Y)\\rbrack\\)에 수렴한다는 것이다. 바로 이것이 대수의 법칙의 핵심입니다.\n\n\\(Y_{i},i = 1,\\ldots,n\\)이 서로 독립이고 동일한 분포를 따른다고 가정하면(가정이 충족된다는 전제 하에) \\(\\frac{1}{n}\\overset{n}{\\sum_{i = 1}}h(Y_{i})\\overset{P}{\\rightarrow}E\\lbrack h(Y)\\)이다.\n【예제】 확률 \\(p_{2}\\)는 다음 절차를 통해 시뮬레이션으로 근사할 수 있다.\n\\(j = 1,\\ldots,n\\)에 대해 다음 단계를 수행한다.\n\n\\(X_{1},\\ldots,X_{c}\\)를 서로 독립적으로 생성하되, 각각은 지수분포 \\(\\text{Exponential}(\\lambda)\\)를 따른다.\n\\(X_{i}\\) 중에서 \\(h\\)시간 이상 작동하는 부품이 \\(t\\)개 이상이면\\(Y_{j} = 1\\), 그렇지 않으면 \\(Y_{j} = 0\\)으로 설정한다.\n이렇게 정의된 \\(Y_{j}\\)는 베르누이분포 \\(\\text{Bernoulli}(p_{2})\\)를 따르고, 기대값은 \\(\\mathbb{E}\\lbrack Y_{j}\\rbrack = p_{2}\\)이므로 대수의 법칙에 따라 다음이 성립한다.\n\n\\[\\frac{1}{n}\\overset{n}{\\sum_{j = 1}}Y_{j}\\overset{P}{\\rightarrow}p_{2}\\text{(as}n \\rightarrow \\infty)\\]\n난수 생성 시작\n첫째, 필요한 확률변수를 어떻게 생성할 것인가에 대한 문제이고, 둘째, 그 생성값을 바탕으로 한 근사 결과가 타당한지를 확인하기 위해 대수의 법칙의 한 형태를 사용하는 것이다. 우리는 일단 어딘가에서 출발해야 하므로 독립적이고 동일한 분포(iid)를 따르는 균등분포 난수 \\(U_{1},\\ldots,U_{m}\\)를 생성할 수 있다고 가정한다.\n이러한 균등분포 난수 생성 문제는 컴퓨터 과학자들에 의해 이미 성공적으로 다루어져 왔고 대부분의 통계 소프트웨어 패키지에 우수한 유사 pseudo 난수 생성 알고리즘이 포함되어 있다.\n우리는 이미 균등분포 난수를 생성할 수 있다고 가정하고 있으므로 목표 확률변수를 직접 생성하는 것이 아니라, 균등난수를 원하는 분포로 변환 하는 것이다.\n\n\n2. 난수생성 방법\n\n(1) 선형합동생성기\n연속형 일양분포를 생성하는 이론적인 방법은 기본적으로 균등하게 분포된 확률을 갖는 숫자를 생성하는 것이다. 예를 들어, [0, 1] 범위 내에서의 연속형 일양분포를 생성한다면, 0과 1 사이의 어떤 숫자든 균등하게 선택되어야 한다.\n선형합동생성기는 재귀적 관계를 이용하여 난수를 생성한다.\n\\[X_{n} = (aX_{n - 1} + c)modm\\]\n\n\\(X_{n - 1}\\) : 이전 값, \\(X_{n}\\) : 새로운 값\n\\(m\\) : 모듈러 연산에 사용되는 상수, 선형 합동 생성기는 주기적으로 난수를 반복한다. 하여, m은 주기의 길이를 결정하며 충분히 긴 주기를 가져야 하므로 큰 소수를 사용한다.\n\\(a\\) : 곱셈 상수, 이 값은 주기의 길이와 난수의 분포에 영향을 줍니다. 보통은 m과 서로소인 값으로 선택하는 것이 좋습니다. 또한 m의 약수가 아닌 소수를 선택하는 것이 바람직하다.\n\\(c\\) : 덧셈 상수, 덧셈 상수는 순환 주기를 변화시키는 역할을 합니다. 적절한 값은 일반적으로 작은 수로 선택되며, m과 서로소이면서 \\((c - 1)\\)의 배수이어야 합니다.\n\n\n\n(2) 역변환 방법\n이론적 확률분포함수 \\(f(x)\\)의 누적확률밀도함수 \\(F(x)\\)가 알려진 경우 사용하는 방법이다.\n\n\\(F(x) = U(0,1)\\) : 누적확률밀도함수와 난수생성 일양균일분포 같다고 한다.\n\\(x = F^{- 1}(u)\\) 선형합동생성기에 의하여 생성한 난수(\\((0,1)\\) 균일분포)를 이용하여 \\(x \\sim f(x)\\)을 생성한다.\n\n【예제】 지수분포(\\(\\beta\\))을 따르는 확률변수를 생성하시오.\n\n\\(X \\sim exponential(\\lambda)\\) 이므로 \\(F_{X}(x) = \\int_{0}^{x}{\\frac{1}{\\beta}e^{- \\frac{x}{\\beta}}dx = 1 - e^{- \\frac{x}{\\beta}}}\\)\n\\(U = 1 - e^{- x/\\beta}\\) 변환을 하자. \\(U\\)의 영역은 \\(0 \\leq U \\leq 1\\)이고 \\(X = - \\beta ln(1 - U)\\)\n\\(F_{X}^{- 1}(U) = - \\beta ln(1 - U)\\)는 지수분포 \\((\\beta)\\)을 갖는다.\n\n【지수분포와 함수관계】\n카이제곱분포는 지수분포를 이용하여 생성한다.\n\\(- 2ln(U) \\sim \\chi^{2}(df = 1)\\)을 따른다.\n\n\n(3) convolution 방법\n확률표본의 합(가법성)의 분포를 알 수 있을 때 사용한다.\n\n지수분포의 합은 감마분포를 따른다.\n감마분포(\\(\\alpha = 1,\\beta = 2\\)) ⬄ 자유도 1인 카이제곱이고 자유도 1인 카이제곱은 표준정규분포(\\(X\\))를 따른다. \\(\\mu + \\sigma X\\)는 평균이 \\(\\mu\\), 표준편차 \\(\\sigma\\)인 정규분포를 따른다.\n\n【예제】 형상모수 \\(\\alpha\\), 척도모수 \\(\\beta\\)인 감마분포를 따르는 확률변수(데이터)를 생성하시오.\n\n일양분포로부터 지수분포(\\(\\beta\\))을 따르는 확률변수를 생성한다.\n생성된 확률변수들의 \\(\\alpha\\)개 합을 구하면 이 확률변수가 감마분포를 따른다.\n\n\n\n(4) 기각 채택방법\n이산형인 경우\n이항분포(\\(n = 3,p = 0.75\\))을 따르는 확률변수를 생성하자.\\(P(X = 0|X \\sim B(3,0.75)) = 0.016,P(X = 1) = 0.141,P(X = 2) = P(X = 3) = 0.422\\)\\(U \\sim U(0,1)\\)을 생성하고 \\(X = \\left\\{ \\begin{array}{r}\n0,0 &lt; U \\leq 0.016 \\\\\n1,0.016 &lt; U \\leq 0.156 \\\\\n2,0.156 &lt; U \\leq 0.578 \\\\\n3,0.578 &lt; U \\leq 1.\n\\end{array} \\right.\\ \\)\n연속형인 경우\n\\(F(x)\\)을 구할 수 없는 경우 생성하기 쉬운 확률변수를 이용하여 사용하는 방법이다. 이 방법의 핵심은 상위 경계함수를 설정하는 것이다.\n\n상위 경계함수(majorant functon) \\(e(x)\\) : 확률변수 모든 영역 \\(x\\)에 대하여 \\(e(x) \\geq f(x)\\) 만족하는 함수 \\(e(x)\\)가 존재한다고 하자.\n다음 조건을 만족하는 함수 \\(e(x)\\)는 효율적이다. ① \\(e(x)\\)와 \\(f(x)\\)는 확률변수 \\(X\\)의 영역에서 유사하다. ② 생성하기 쉬운 확률밀도함수 \\(g(x)\\)에 대하여 \\(e(x) = ag(x)\\)이다.\n\\(ag(x)\\)을 생성하여 \\(f(x)\\)보다 작으면 채택, 크면 기각하여 \\(f(x)\\)을 따르는 확률변수를 생성하게 된다.\n\n\n\n\n\n\n【예제】 감마분포(\\(\\alpha,\\beta\\))을 생성하자.\n\\[f(x) = \\frac{1}{\\Gamma(\\alpha)}x^{\\alpha - 1}\\beta^{- \\alpha}e^{- x/\\beta},0 &lt; x &lt; - \\infty\\]\n\n지수분포를 베이스 분포로 하자. \\(g(x) = \\frac{1}{\\beta}e^{- x/\\beta}\\)\n상위 경계함수 \\(e(x) = \\frac{a}{\\alpha}e^{- x/\\alpha}\\), \\(a = \\frac{\\alpha^{\\alpha}e^{1 - \\alpha}}{\\Gamma(\\alpha)}\\)이면, \\(e(x) \\geq f(x)foreveryx\\) 만족한다.\n\n【예제】 베타분포(\\(\\alpha = 2,\\beta = 4\\))을 생성하자.\n\\[f(x) = 20x(1 - x)^{3},0 &lt; x &lt; 1\\]\n\n\\(U(0,1)\\)을 베이스 분포로 하자. \\(g(x) = 1,0 &lt; x &lt; 1\\)\n\\(\\frac{f(x)}{g(x)} \\leq a\\)을 만족하는 \\(a\\)을 구하자. \\(\\frac{f(x)}{g(x)} = 20x(1 - x)^{3}\\)는 \\(x = 1/4\\)에서 최대값을 가지므로 \\(a = 20(1/4)(1 - 1/4)^{3} = 135/64\\)\n그러므로 \\(\\frac{f(x)}{ag(x)} = \\frac{256}{27}x(1 - x)^{3}\\)이다.\n균일분포를 따르는 \\(U(0,1)_{1},U(0,1)_{2}\\)을 생성하고 만약 \\(U_{2} \\leq \\frac{256}{27}U_{1}(1 - U_{1})^{3}\\)이면 \\(X = U_{1}\\)을 채택한다.\n\n【예제】 부품 수명이 지수분포(\\(\\beta\\))을 따른다고 하자. 품질 관리자는 \\(c\\)개의 부품 중 적어도 \\(t\\)개가 \\(h\\) 시간 이상일 확률을 구하고 싶다고 하자.\n1개 부품 수명이 \\(h\\) 이상일 확률 : \\(p_{1} = P\\left( X \\geq h \\middle| X \\sim exponential(\\beta) \\right) = \\int_{h}^{\\infty}{\\frac{1}{\\beta}e^{- h/\\beta}dx = e^{- h/\\beta}}\\)【방법】 모수 \\(\\beta\\)인 지수분포 확률변수 \\(X_{1},X_{2},\\ldots,X_{c}\\) 데이터를 생성한다(역변환 방법).\n\\(c\\)개 중 수명이 \\(h\\) 이상인 부품 개수가 적어도 \\(t\\)개 일 확률 :\n\\[p_{2} = \\overset{c}{\\sum_{k = t}}\\binom{c}{k}p_{1}^{k}(1 - p_{1})^{c - k}\\]\n【계산】 만약 생성한 \\(c\\) 개 데이터 중 \\(h\\) 이상인 데이터가 적어도 \\(t\\)개 이면 \\(Y_{j} = 1\\) 그렇지 않으면 \\(Y_{j} = 0\\)이라 놓는다. 이 과정을 무한 반복하여 \\(Y_{j}\\)의 평균을 구하면 \\(c\\)개의 부품 중 적어도 \\(t\\)개가 \\(h\\) 시간 이상일 확률이다.\n\n\n(5) Metropolis Algorithm\n목표 확률밀도함수가 두꺼운 꼬리를 가질 경우, 유한한 M 값을 갖는 후보 확률밀도함수를 찾는 것이 어려울 수 있다. 이러한 경우에는 수용/기각 알고리즘을 적용할 수 없으며, 대신 마코프 연쇄 몬테카를로 Markov Chain Monte Carlo 방법이라는 또 다른 범주의 기법을 사용하게 된다. 대표적인 특수 사례로는 깁스 샘플링(Gibbs Sampler) 과 메트로폴리스 알고리즘(Metropolis Algorithm)이 있다.\n메트로폴리스 알고리즘 (Metropolis Algorithm)\n확률변수 \\(Y \\sim f_{Y}(y),V \\sim f_{V}(v)\\)라 하자. 이때 \\(f_{Y}\\)와 \\(f_{V}\\)는 공통 정의역을 가진다고 가정한다. \\(Y \\sim f_{Y}\\)를 생성하기 위한 절차는 다음과 같다.\n\n초기화: \\(V \\sim f_{V}\\)에서 생성하고, \\(Z_{0} = V\\)로 설정한다.\n반복 \\(fori = 1,2,\\ldots\\) :\n\n다음을 생성한다: \\(U_{i} \\sim \\text{Uniform}(0,1)\\), \\(V_{i} \\sim f_{V}\\)\n그리고 다음을 계산한다:\n\\(\\rho_{i} = \\min\\left\\{ \\frac{f_{Y}(V_{i})}{f_{V}(V_{i})} \\cdot \\frac{f_{V}(Z_{i - 1})}{f_{Y}(Z_{i - 1})},1 \\right\\}\\)\n\n다음과 같이 \\(Z_{i}\\)를 결정한다.\n\n\\(Z_{i} = \\{\\begin{matrix}\nV_{i} & \\text{if}U_{i} \\leq \\rho_{i} \\\\\nZ_{i - 1} & \\text{if}U_{i} &gt; \\rho_{i}\n\\end{matrix}\\)\n\n반복이 충분히 커지면(\\(i \\rightarrow \\infty\\)), \\(Z_{i}\\overset{d}{\\rightarrow}Y\\).\n\n# Metropolis Algorithm\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\n\n# 목표 분포: 표준 정규분포\ndef target_density(x):\n    return norm.pdf(x, loc=0, scale=1)  # f_Y(x)\n\n# Metropolis 알고리즘\ndef metropolis(n_samples, proposal_std=1.0):\n    samples = np.zeros(n_samples)\n    current = 0  # 초기값\n\n    for i in range(1, n_samples):\n        proposal = np.random.normal(current, proposal_std)  # f_V\n        acceptance_ratio = target_density(proposal) / target_density(current)\n        rho = min(1, acceptance_ratio)\n\n        if np.random.rand() &lt; rho:\n            samples[i] = proposal\n            current = proposal\n        else:\n            samples[i] = current\n    return samples\n\n# 샘플 생성\nnp.random.seed(0)\nsamples = metropolis(10000)\n\n# 결과 시각화\nx = np.linspace(-4, 4, 1000)\nplt.hist(samples, bins=50, density=True, alpha=0.6, label='Metropolis samples')\nplt.plot(x, norm.pdf(x), 'r--', label='Target N(0,1)')\nplt.title(\"Metropolis Algorithm: Sampling from N(0,1)\")\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\nGibbs Sampler 알고리즘\n결합분포 \\(f(x,y)\\)에서 직접 샘플링하기 어려울 때, 조건부 분포 \\(f(x|y),f(y|x)\\)를 통해 순차적으로 샘플을 생성하여 \\((x,y) \\sim f(x,y)\\)를 근사적으로 얻는다.\n\n초기값 선택: \\(x^{(0)},y^{(0)}\\)을 초기값으로 설정한다.\n반복 \\(fort = 1,2,\\ldots,N\\): \\(\\begin{matrix}\nx^{(t)} & \\sim f(x \\mid y^{(t - 1)}) \\\\\ny^{(t)} & \\sim f(y \\mid x^{(t)})\n\\end{matrix}\\)\n\n즉, 이전 단계에서의 \\(y\\)를 고정하고 \\(x\\)를 샘플링한 다음, 새로 얻은 \\(x\\)를 고정하고 \\(y\\)를 샘플링한다.\n\n충분히 반복하면, \\((x^{(t)},y^{(t)})\\overset{d}{\\rightarrow}f(x,y)\\)\n\n#Gibbs Sampler Algorithm\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# 설정: 평균, 표준편차, 상관계수\nmu_x, mu_y = 0, 0\nsigma_x, sigma_y = 1, 1\nrho = 0.8\nn_samples = 10000\n\n# 조건부 분포의 파라미터\ndef sample_y_given_x(x):\n    mu = mu_y + rho * (sigma_y / sigma_x) * (x - mu_x)\n    std = np.sqrt(1 - rho**2) * sigma_y\n    return np.random.normal(mu, std)\n\ndef sample_x_given_y(y):\n    mu = mu_x + rho * (sigma_x / sigma_y) * (y - mu_y)\n    std = np.sqrt(1 - rho**2) * sigma_x\n    return np.random.normal(mu, std)\n\n# 깁스 샘플링\nx_samples = np.zeros(n_samples)\ny_samples = np.zeros(n_samples)\nx, y = 0, 0  # 초기값\n\nfor i in range(n_samples):\n    x = sample_x_given_y(y)\n    y = sample_y_given_x(x)\n    x_samples[i] = x\n    y_samples[i] = y\n\n# 시각화\nplt.figure(figsize=(6, 6))\nplt.plot(x_samples, y_samples, 'o', alpha=0.2, markersize=2)\nplt.title(\"Gibbs Sampler: Bivariate Normal\")\nplt.xlabel(\"X\")\nplt.ylabel(\"Y\")\nplt.axis('equal')\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n(6) BOX-Muller algorithm\n\\(U(0,1)\\)에서 서로 독립인 \\(U_{1},U_{2}\\)을 생성하고 \\(Z_{1},Z_{2}\\)을 다음과 같이 정의하면 서로 독립이고 \\(N(0,1)\\)을 따른다.\n\\(Z_{1} = \\sqrt{- 2lnU_{1}}cos(2\\pi U_{2})\\) \\(Z_{2} = \\sqrt{- 2lnU_{1}}sin(2\\pi U_{2})\\)\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef box_muller_bivariate(n, mu, Sigma):\n    # Step 1: Generate standard normal samples using Box-Muller\n    u1 = np.random.rand(n)\n    u2 = np.random.rand(n)\n    z1 = np.sqrt(-2 * np.log(u1)) * np.cos(2 * np.pi * u2)\n    z2 = np.sqrt(-2 * np.log(u1)) * np.sin(2 * np.pi * u2)\n    \n    Z = np.vstack((z1, z2))  # Shape: (2, n)\n    \n    # Step 2: Cholesky decomposition of Sigma\n    L = np.linalg.cholesky(Sigma)  # Shape: (2, 2)\n\n    # Step 3: Transform standard normals to bivariate normal\n    X = (L @ Z).T + mu  # Shape: (n, 2)\n\n    return X\n\n# Parameters\nmu = np.array([3, 5])  # Mean vector\nSigma = np.array([[4, 2], [2, 3]])  # Covariance matrix\n\n# Generate samples\nsamples = box_muller_bivariate(5000, mu, Sigma)\n\n# Plot\nplt.figure(figsize=(6,6))\nplt.scatter(samples[:,0], samples[:,1], s=5, alpha=0.4)\nplt.title(\"Bivariate Normal Distribution via Box-Muller\")\nplt.xlabel(\"X\")\nplt.ylabel(\"Y\")\nplt.axis('equal')\nplt.grid(True)\nplt.show()"
  },
  {
    "objectID": "notes/math_stat/index.html",
    "href": "notes/math_stat/index.html",
    "title": "수리통계",
    "section": "",
    "text": "수리통계는 확률을 기반으로 한 통계적 추론의 이론적 토대를 다룬다.\n확률변수와 분포, 표본추출, 추정과 가설검정은\n이후 회귀분석과 다변량 분석을 이해하기 위한 핵심 기반이 된다.\n왼쪽 메뉴에서 주제를 선택하시오."
  },
  {
    "objectID": "notes/math_stat/famous_distribution.html",
    "href": "notes/math_stat/famous_distribution.html",
    "title": "수리 통계 3. 유명한 분포",
    "section": "",
    "text": "chapter 1. 확률변수와 확률분포함수\n통계적 분포는 모집단의 특성을 수학적으로 모델링하고 해석하는 데 사용되는 중요한 도구이다. 실제 분석에서는 하나의 고정된 분포보다는, 특정한 형태를 유지하면서 하나 이상의 모수를 통해 다양하게 조정 가능한 분포의 집합, 즉 분포의 가족을 다루는 경우가 일반적이다. 이러한 모수는 분포의 위치, 척도, 형태 등의 특성을 조절하는 역할을 하며, 이를 통해 다양한 모집단 상황을 유연하게 설명할 수 있다.\n예를 들어, 정규분포는 많은 자연적·사회적 현상을 설명하는 데 자주 사용되는 분포 형태지만, 실제로는 모집단의 평균이나 분산을 정확히 알 수 없는 경우가 많다. 이럴 때는 평균과 분산이라는 두 개의 모수를 갖는 정규분포의 모수적 가족을 고려하게 된다. 즉, 평균이 μ이고 분산이 σ²인 정규분포는 상황에 따라 달라질 수 있는 분포의 집합을 표현하며, 이러한 모수들을 추정하는 과정은 통계적 추론에서 매우 중요한 부분이다.\n이와 같은 모수적 접근은 특정한 분포 형태를 가정하고 데이터를 분석하는 데 효과적인 방법이지만, 분포의 형태 자체를 정확히 알 수 없는 경우에는 비모수적 접근이 더 적절할 수 있다. 비모수적 방법은 분포의 형태에 대한 전제가 없거나 느슨하기 때문에, 보다 일반적인 상황에 적용할 수 있는 장점이 있다.\n따라서 다양한 확률분포의 성질과 활용법을 이해하는 것은 데이터 분석, 신뢰구간 추정, 가설 검정 등 통계학 전반에서 핵심적인 역할을 하며, 분석의 타당성과 해석력을 높이는 데 기초가 된다.\n\n1. 이산형 균일분포 \\(X \\sim U(1,K)\\)\n확률질량함수\n\\(p_{X}(x) = \\frac{1}{K},x = 1,2,...,K\\), \\(K\\)는 1보다 큰 정수이다.\n평균과 분산\n\\[E(X) = \\frac{K + 1}{2},V(X) = \\frac{(K + 1)(K - 1)}{12}\\]\n【활용】\n유한한 개수의 이산적인 값들이 동일한 확률로 발생하는 경우를 나타내는 분포를 이산균등분포라고 한다. 이 분포에서는 모든 가능한 값이 동일한 가능성으로 발생하며, 보통 정수 구간 [a, b] 내에 정의된 값들에 대해 사용된다.\n즉, 확률변수가 [a, b] 구간 내의 정수값만을 취하고, 그 각각이 동일한 확률로 나타날 경우, 해당 분포는 이산균등분포를 따른다고 할 수 있다. 이러한 분포는 특정 값에 대한 편향 없이 균일한 무작위 선택을 표현할 때 유용하게 사용된다. 예를 들어, 공정한 주사위의 눈(1부터 6까지)은 이산균등분포의 대표적인 예이다.\n\n주사위(동전) 게임 : 가장 대표적인 사례로 6면체 주사위를 던졌을 때 나올 수 있는 눈의 수는 1부터 6까지이며 각 눈의 확률은 1/6이다. 동전은 1(앞면), 2(뒷면)이고 각 면의 확률은 1/2이다.\n난수 생성 : 컴퓨터 과학에서 난수 생성기는 이산형 균일분포를 활용하여 난수를 생성하는데 특정 범위 내의 정수값을 균일하게 생성하는 데 사용될 수 있다.\n설문 조사 결과 분석 : 설문 조사에서 여러 선택지 중에서 랜덤으로 선택하는 경우 이산형 균일분포를 활용하여 각 선택지의 확률을 계산할 수 있다.\n게임 이벤트 확률 설정: 컴퓨터 게임이나 보드 게임에서 특정 이벤트가 발생할 확률을 설정할 때 이산형 균일분포를 사용할 수 있다. 예를 들어, 아이템 드롭 확률이나 특정 상황에서의 성공 확률을 균일하게 설정할 수 있다.\n임의 샘플링: 임의로 선택된 항목을 조사하거나 분석하는 경우 이산형 균일분포를 사용하여 샘플을 생성할 수 있는데 로또와 같은 추첨 이벤트에서 각 번호의 등장 확률을 나타낼 때 이산형 균일분포를 활용할 수 있다.\n\n\n\n2. 초기하분포 \\(X \\sim HG(N,M,n)\\)\n초기하 분포는 유한한 모집단에서 복원 없이 표본을 추출하는 상황에서 사용되는 확률분포이다. 모집단의 크기가 N이고, 이 중 관심 있는 그룹(예: 성공 또는 특정 특성 보유 집단)이 M개, 나머지 그룹이 N - M개라고 하자. 이 모집단에서 크기 K의 표본을 무작위로 추출할 때, 관심 그룹에서 선택된 원소의 개수를 확률변수 X라고 정의하면, 이 X는 초기하 분포를 따른다.\n초기하 분포는 이항분포와 유사하지만, 표본을 복원하지 않고 추출한다는 점에서 차이가 있다. 이 분포는 모집단이 고정되어 있고, 표본 추출 과정에서 선택된 항목이 다시 모집단으로 돌아가지 않는 경우에 적절하다. 예를 들어, 한 상자에 불량품이 포함된 부품들이 섞여 있을 때, 무작위로 일부를 검사하여 불량품의 개수를 파악하는 상황에 적용할 수 있다.\n이 분포는 이산형 확률변수의 대표적인 예 중 하나이며, 품질관리, 검정 문제, 선택 및 선발 과정 등 실제 다양한 분야에서 활용된다.\n확률질량함수\n\\[P(X = x|N,M,K) = \\frac{\\binom{M}{x}\\binom{N - M}{K - x}}{\\binom{N}{K}},x = 0,1,\\ldots,min(K,M)\\]\n평균과 분산\n\\[E(X) = \\frac{KM}{N},V(X) = \\frac{KM}{N}(\\frac{(N - M)(N - K)}{N(N - 1)})\\]\n【포획-재포획(capture-recapture)】\n포획-재포획 문제는 알려지지 않은 모집단의 크기를 추정하기 위한 고전적인 통계 기법이다. 생태학, 인구 추정, 질병 감시 등 다양한 분야에서 활용된다.\n지리산에 서식하는 멧돼지의 총 개체 수를 N이라 하자. 먼저 각 지역에서 무작위로 M = 10마리의 멧돼지를 포획한 뒤, 표식을 한 후 다시 자연에 방사한다. 일정 기간이 지난 뒤 동일한 지역에서 다시 K = 20마리를 포획하였고, 그 중 X = 3마리에 표식이 있었다고 하자.\n이 상황은 모집단에서 복원 없이 두 번의 표본을 추출하는 과정이므로, 확률변수 X는 초기하 분포(hypergeometric distribution)를 따른다. 이때 N이 주어졌을 때, X = 3이 될 확률은 다음과 같다.\n\\(P(X = 3 \\mid N, M = 10, K = 20) = \\frac{\\binom{10}{3} \\binom{N - 10}{17}}{\\binom{N}{20}}\\)\n이 식은 포획된 20마리 중 표식된 3마리가 나올 확률을 의미하며, 이 확률을 N에 대해 최대화하면 관측된 데이터 하에서 가장 가능성 높은 모집단 크기를 추정할 수 있다. 이때의 N이 바로 최대우도추정값이다.\n실제로는 \\(N \\geq 30\\) 을 만족하는 자연수 값들을 대상으로 위 확률식을 계산하여, 그 중 확률이 가장 큰 N을 선택한다. 이를 통해 알려지지 않은 모집단의 규모를 경험적 데이터 기반으로 합리적으로 추정할 수 있다.\n【성질】\n\n초기하분포 \\(X \\sim HG(N,M,K = 1)\\)는 베르누이 분포이다.\n이항분포는 복원 표본 추출이고 초기하분포는 비복원 추출이다.\n모집단 크기 \\(N\\)이 커지면 초기하분포 \\(X \\sim HG(N,M,K)\\)는 이항분포 \\(B(K,p = M/N)\\)에 근사한다.\n\n【예제】 Acceptance sampling\n한 묶음에 25개의 기계 부품이 포함되어 있으며, 각 부품은 검사 기준을 통과해야 정상으로 간주된다고 가정하자. 10개의 부품을 무작위로 샘플링하여, 모두 정상(불량품이 없음)이라고 판정되었다면, 만약 해당 묶음에 총 6개의 불량품이 포함되어 있다면 이 사건이 발생할 확률은 얼마일까? 초기하분포를 적용하여, \\(N = 25,M = 6,K = 10\\)일 때, 다음과 같이 계산할 수 있다.\n\\(P(X = 0) = \\frac{\\binom{6}{0}\\binom{19}{10}}{\\binom{25}{10}} = 0.028\\). 이는 만약 해당 묶음에 6개 이상의 불량품이 포함되어 있다면, 우리가 관찰한 사건(불량품 없음)이 발생할 확률이 매우 낮다는 것을 보여준다.\n【예제】 포획-재포획 문제\n초원에 서식하는 얼룩말 수(\\(N\\))를 추정하려고 한다. \\(M = 4\\)마리 얼룩말을 잡아 표식을 붙이고 놓아 주었다. 일정 기간이 지난 후 \\(K = 3\\)마리 얼룩말을 잡아 표식 여부를 확인하였다. 표식이 있는 얼룩말의 수를 확률변수 \\(X\\)라 정의하자.\n표식 있는 얼룩말이 1마리일 확률을 구하라. \\(P(X = 1) = \\frac{\\binom{N}{3}\\binom{4}{1}}{\\binom{N + 4}{3}}\\)\n위의 확률을 최대화 하는 N을 구해보자. 지리산 멧돼지 수의 최대우도추정량은 7마리 혹은 8마리이다.\n\n\n\n\n\n\n\n\n\n\n\n\n\nN\n4\n5\n6\n7\n8\n9 1\n0\n\n\nP(X=1|HG(N+4, 4,3))\n0.429\n0.476\n0.500\n0.509\n0.509\n0.503\n0.495\n\n\n\n\n\n3. 베르누이분포 \\(X \\sim B(1,p)\\)\n베르누이 시행\n베르누이 시행은 통계학에서 널리 사용되는 확률 실험의 한 형태로, 두 가지 가능한 결과 중 하나만이 나타나는 실험을 의미한다. 이 실험은 매우 단순하지만, 다양한 확률 모형의 기본 단위로 사용될 만큼 중요한 개념이다. 대표적인 예로는 동전 던지기(앞면 또는 뒷면), 시험의 합격 여부, 로또 당첨 여부 등이 있다.\n베르누이 시행은 다음과 같은 성질을 가진다.\n\n결과는 두 가지 가능한 값 중 하나로 나타난다. 일반적으로 이 두 결과는 “성공”과 “실패”로 구분되며, 성공은 1, 실패는 0으로 수치화한다.\n각 시행은 서로 독립적이다. 이는 한 시행의 결과가 다른 시행에 영향을 미치지 않음을 의미한다. 예를 들어, 여러 번 동전을 던지는 경우 각각의 던지기는 이전 결과와 관계없이 독립적으로 동일한 확률 구조를 갖는다.\n모든 시행에서 성공 확률이 일정하게 유지된다. 즉, 성공 확률을 p, 실패 확률을 1 - p라고 할 때, 모든 시행에서 이 확률 값은 변하지 않는다.\n\n베르누이 시행은 이항분포, 기하분포, 음이항분포 등의 기반이 되며, 불확실한 사건에 대한 모델링과 해석에 있어 가장 기본적인 확률 구조를 제공한다.\n확률질량함수 \\[P(X = x|p) = p^{x}(1 - p)^{1 - x},x = 0,1\\]\n평균과 분산\n\\[E(X) = p,V(X) = p(1 - p)\\]\n【활용】\n베르누이 분포는 결과가 두 가지(예: 성공과 실패)로 나타나는 확률 실험에서 사용되는 이산형 확률분포로, 이진형 결과를 다루는 문제에 널리 활용된다. 머신러닝에서는 감염 여부, 시험 합격 여부, 이메일의 스팸 여부와 같이 결과가 0 또는 1로 표현되는 이진 분류 문제에서 베르누이 분포를 기반으로 한 확률 모델이 학습에 활용된다.\n이항 분포는 베르누이 시행을 독립적으로 n번 반복했을 때, 그 중 성공한 횟수를 확률변수로 정의한 분포이다. 즉, 동일한 성공 확률을 갖는 베르누이 시행이 여러 번 반복되는 상황에서 전체 성공 횟수의 확률분포를 기술한다.\n베르누이 시행이 일정한 시간 간격으로 발생하는 경우, 이를 베르누이 프로세스라고 한다. 이 프로세스에서 단위 시간 내 성공(또는 사건 발생)이 일정한 확률로 발생하며, 이러한 시행이 매우 짧은 시간 간격으로 반복되면, 전체 발생 횟수는 포아송 분포로 근사할 수 있다. 이로 인해 베르누이 시행은 대기 행렬 이론, 고객 도착 모델, 네트워크 트래픽 분석 등 시간적 이벤트가 발생하는 다양한 확률 모델링 상황에 활용된다.\n【성질】\n\n3개 이상의 범주를 갖는 베르누이 분포의 일반 모형은 다항분포이다. \\((X_{1},X_{2},\\ldots,X_{k}) \\sim \\text{Multinomial}(n;p_{1},p_{2},\\ldots,p_{k})\\)\n\n\\[P(X_{1} = x_{1},X_{2} = x_{2},\\ldots,X_{k} = x_{k}) = \\frac{n!}{x_{1}!x_{2}!\\cdots x_{k}!}p_{1}^{x_{1}}p_{2}^{x_{2}}\\cdots p_{k}^{x_{k}}\\]\n\n\\(X \\sim B(0.5)\\)이면 \\(2X - 1\\)은 Rademacher 분포(Bootstrapping 기법에 사용되는 변수)를 따른다.\n\n\n\n4. 이항분포 \\(X \\sim B(n,p)\\)\n이항분포는 동일한 성공 확률을 갖는 독립적인 베르누이 시행을 n번 반복했을 때, 이 중 성공한 횟수를 확률적으로 설명하는 이산형 확률분포이다. 각 시행은 성공(1) 또는 실패(0)의 두 가지 결과만을 가지며, 모든 시행에서 성공 확률 p는 일정하게 유지된다.\n확률질량함수 \\[P(X = x|n,p) = \\binom{n}{x}p^{x}(1 - p)^{n - x},x = 0,1,2,...,n\\]\n평균과 분산\n\\[E(X) = np,V(X) = np(1 - p)\\]\n【활용】\n이항분포는 결과가 성공 또는 실패로 구분되는 이진형 사건에 대한 반복 실험에서 성공 횟수를 확률적으로 설명하는 데 적합한 모델로, 다양한 분야에서 실용적으로 활용된다. 주요 활용 사례는 다음과 같다.\n마케팅 및 광고 분석: 광고 클릭 여부, 이메일 오픈 여부, 고객 구매 여부 등과 같은 이진 결과를 다루는 마케팅 활동에서 이항분포는 핵심적인 도구이다. 예를 들어, 광고를 본 고객 100명 중 실제로 클릭한 사람이 몇 명인지 분석할 때, 이항분포를 이용하여 클릭률을 예측하고 마케팅 전략의 효과를 평가할 수 있다.\n품질 관리: 제조업에서는 제품이 결함이 있는지 없는지와 같은 이진 결과를 기반으로 공정의 품질을 평가한다. 일정 수의 제품을 검사하여 그 중 불량품 수를 기록하고, 이를 이항분포로 모델링함으로써 공정의 안정성과 결함률을 추정할 수 있다.\n의료 통계학 및 임상시험: 약물의 효과 여부, 치료 성공률, 질병 발생 여부와 같은 이진 결과를 다루는 임상연구에서도 이항분포는 중요한 역할을 한다. 예를 들어, 200명의 환자에게 신약을 투여하고 그 중 150명이 효과를 보였을 때, 성공률을 분석하거나 두 치료법 간의 효과 차이를 비교할 수 있다.\n금융 및 보험: 금융 분야에서는 옵션 가격 모델이나 디폴트 위험 분석, 보험 산업에서는 청구 발생 여부나 보상 지급 비율을 분석할 때 이항분포를 활용한다. 반복되는 사건 중에서 손해가 발생하는 비율을 분석하고 이를 리스크 관리에 반영할 수 있다.\n사이버 보안: 침입 탐지 시스템에서는 각각의 접속 시도가 정상 또는 비정상으로 분류되는 이진 결과를 발생시킨다. 이러한 상황에서 이항분포를 활용하여 일정 기간 동안의 침입 가능성을 예측하거나, 보안 시스템의 민감도와 정확도를 평가할 수 있다.\n이처럼 이항분포는 실험의 결과가 두 가지 중 하나로 나타나며 독립적으로 반복되는 상황에 매우 적합하며, 다양한 응용 분야에서 데이터 기반 의사결정과 추론에 활용된다.\n【성질】\n\n\\(X \\sim B(n,p)\\)이면 \\(n - X \\sim B(n,1 - p)\\)이다.\n\\(X \\sim B(n,p),Y \\sim B(m,p)\\)이고 서로 독립이면\n\\(X + Y \\sim B(n + m,p)\\)이다.\n\\(X_{i} \\sim B\\left( n_{i},p \\right)\\) 서로 독립이면 \\(\\sum_{i = 1}^{}X_{i} \\sim B\\left( \\sum_{i}^{}n_{i},p \\right)\\) 이다.\n\\(X \\sim B(n,p)\\)는 포아송 분포의 특수한 형태이다. \\(B(n,p)\\underset{n \\rightarrow \\infty}{\\rightarrow}P(\\lambda = np)\\)\n서로 독립인 \\(X \\sim B\\left( n,p_{1} \\right),Y \\sim B(m,p_{2})\\)의 비 \\(log(T) = log(\\frac{\\frac{X}{n}}{\\frac{Y}{m}}) \\sim (app)N(\\log\\left( \\frac{p_{1}}{p_{2}} \\right),\\frac{\\frac{1}{p_{1}} - 1}{n} - \\frac{\\frac{1}{p_{2}} - 1}{m})\\) 근사한다.\n\n【예제】\n학생들 중 감기 환자가 20%이다. 학생 3명을 임의로 선택하였을 때 확률변수 X을 감기 걸린 학생 수라 정의할 때 X의 확률밀도함수를 구하라. \\(X \\sim B(n = 3,p = 0.2)\\)\n\n\n\n\n\n\n\n\\[X\\]\n\\[P(X = x)\\]\n\n\n0\n0.512\n\n\n1\n0.384\n\n\n2\n0.096\n\n\n3\n0.008\n\n\n\n학생을 15명 임의 선택하였을 때 그 중 적어도 한 명이 환자일 확률을 구하라. \\(P(X \\geq 1) = 1 - P(X = 0) = 0.965\\)\n【예제】 Group Testing\n감염률이 10%인 상황에서 5명을 대상으로 진단검사를 시행할 때, 두 가지 방식인 개별 검사와 그룹 검사의 효율성을 비교할 수 있다.\n개별 검사는 5명을 각각 따로 검사하는 방식으로, 어떤 경우에도 정확히 5번의 검사가 필요하다. 반면, 그룹 검사는 5명의 검체를 하나로 묶어 한 번에 검사한 뒤, 결과에 따라 추가 검사를 진행하는 방식이다. 만약 그룹 검사에서 음성이 나오면 1회의 검사로 모든 사람이 음성임을 확인할 수 있지만, 양성이 나오면 5명을 각각 다시 검사해야 하므로 총 6번의 검사가 필요하다.\n이때 중요한 판단 기준은 그룹 검사에서 양성이 나올 확률이다. 감염률이 10%일 때, 5명 모두 음성일 확률은 약 59.0%이며, 따라서 그룹 검사에서 양성이 나올 확률은 약 41.0%이다. 이를 바탕으로 그룹 검사 방식의 평균 검사 횟수를 계산하면, 약 3.05회로 나타난다. 이는 개별 검사 방식의 5회보다 적은 횟수이며, 검사 자원의 약 40% 절약 효과를 의미한다.\n결론적으로, 감염률이 낮은 상황에서는 그룹 검사가 검사 횟수를 줄일 수 있어 더 효율적인 방법이 된다. 단, 이러한 효율성은 감염률이 높아질수록 감소하며, 감염률이 일정 수준 이상으로 높아지면 오히려 개별 검사가 유리해질 수 있다. 본 사례에서는 감염률이 10%로 낮은 편이기 때문에 그룹 검사가 더 효과적인 방법이라 할 수 있다.\n5명 한꺼번에 조사 했을 때 5명 모두 양성이 나올 확률 = \\(P\\left( X = 0 \\middle| X \\sim B(5,0.1) \\right) = 0.59\\)\n그러므로 5명 중 적어도 한 명이 양성이어서 5개 모두 검사 회수 기대값은 \\(E(X) = 1*0.59 + 6*0.41 = 3.05\\)\n5명을 개별 검사하면 5회 검사이므로 그룹 검사 방법이 검사 회수 측면에세 더 효과적이다.\n\n\n5. 포아송분포 \\(X \\sim Poisson(\\lambda)\\)\n포아송 분포(Poisson distribution)는 단위 시간 또는 단위 공간 내에서 발생하는 사건의 수를 설명하는 데 사용되는 대표적인 이산형 확률분포이다. 이 분포는 사건이 비교적 드물게 발생하지만, 일정한 평균 발생률을 가진다고 가정할 수 있는 현상을 모델링할 때 적합하다.\n예를 들어, 일정 시간 동안 버스를 기다리는 상황에서 단위 시간 내 버스가 도착하는 횟수, 또는 은행 창구에 고객이 도착하는 수는 포아송 분포를 따르는 것으로 설명할 수 있다. 이때 핵심 가정 중 하나는 짧은 시간 간격 내에서 사건이 발생할 확률이 그 시간의 길이에 비례한다는 점이다. 즉, 시간이 길어질수록 사건이 발생할 가능성도 커진다는 전제가 성립한다.\n포아송 분포는 시간뿐 아니라 공간에 대해서도 적용이 가능하다. 예를 들어, 특정 지역에 떨어지는 폭탄의 수, 또는 호수의 일정 면적 내에 서식하는 물고기의 수 등도 포아송 분포로 모델링할 수 있다. 이러한 경우, 사건들이 서로 독립적으로 발생하고 단위 공간 또는 시간당 평균 발생률이 일정하다고 가정할 수 있다면, 포아송 분포는 현실적인 모델이 될 수 있다.\n확률질량함수\n\\[P(X = x|\\lambda) = \\frac{e^{- \\lambda}\\lambda^{x}}{x!},x = 0,1,2,...\\]\n평균과 분산\n\\[E(X) = \\lambda,V(X) = \\lambda\\]\n【활용】 포아송 분포는 단위 시간 또는 단위 공간 내에서 발생하는 사건의 수를 모델링하는 데 유용한 확률분포로, 다양한 실세계 현상에서 널리 활용된다.\n첫째, 도착 패턴 분석에 자주 사용된다. 예를 들어, 일정 시간 동안 도로를 통과하는 차량 수, 고객이 매장에 도착하는 빈도, 콜센터에 걸려오는 전화의 수 등을 포아송 분포로 설명할 수 있다. 이때 각 사건은 독립적으로 발생하며, 단위 시간 내 평균 발생률이 일정하다는 가정이 필요하다.\n둘째, 기계 고장 발생과 같은 신뢰성 분석에서도 포아송 분포가 적용된다. 특정 장비가 고장을 일으키는 평균 발생률이 알려져 있다면, 일정 기간 동안 발생하는 고장의 횟수를 확률적으로 예측할 수 있다. 이는 예방 정비, 부품 수급 계획 등 실무적인 의사결정에 활용된다.\n셋째, 흡수나 희석 작용이 일어나는 물리적 또는 화학적 과정도 포아송 분포로 모델링할 수 있다. 대표적인 예로는 방사성 물질에서 단위 시간당 방출되는 입자의 수를 들 수 있으며, 이는 시간 간격에 따라 입자 발생 수가 일정한 평균을 가지고 독립적으로 발생하는 것으로 가정할 수 있다.\n마지막으로, 머신러닝에서의 이상 탐지에서도 포아송 분포가 활용된다. 정상적인 사건 발생 패턴을 포아송 분포로 모델링한 뒤, 이 분포에서 크게 벗어나는 사건들을 이상(anomaly)으로 간주하여 탐지하는 방식이다. 이 접근은 특히 네트워크 보안, 서버 요청 패턴 분석, 금융 이상 거래 탐지 등에 응용된다.\n이처럼 포아송 분포는 발생 빈도가 낮고 독립적인 사건들을 모델링할 수 있는 강력한 도구로, 다양한 분야에서 확률적 사고와 데이터 분석의 기반이 된다.\n【포아송 프로세스】\n설명한 다섯 가지 조건을 만족하는 확률과정 N_t는 포아송 분포를 따르는 포아송 프로세스라고 한다. 이 프로세스는 일정한 평균 도착률 \\(\\lambda\\) 를 가지고, 시간에 따라 사건이 발생하는 패턴을 모델링하는 데 사용된다. 각 조건의 의미는 다음과 같다.\n\n초기 조건: \\(N_0 = 0\\) 이다. 시간 0에서 출발하므로, 처음에는 사건이 아무것도 발생하지 않은 상태이다.\n독립성: 임의의 시간 s &lt; t에 대해 \\(N_s\\) 와 \\(N_t - N_s\\) 는 서로 독립이다. 즉, 분리된 시간 구간에서 발생한 사건 수는 서로 영향을 미치지 않는다.\n동일한 분포: 임의의 \\(s \\geq 0, t \\geq 0\\) 에 대해 \\(N_s\\) 와 \\(N_{t+s} - N_t\\) 는 동일한 분포를 따른다. 이는 도착한 사건 수가 절대적인 시간 위치가 아닌 시간 간격의 길이에만 의존함을 의미한다.\n도착 확률: \\(\\lim_{t \\to 0} \\frac{P(N_t = 1)}{t} = \\lambda\\) 이다. 아주 짧은 시간 간격에서 정확히 한 건의 사건이 발생할 확률은 시간 길이에 비례하며, 비례 상수는 \\(\\lambda\\) 이다. 이는 평균적으로 단위 시간당 \\(\\lambda\\) 건의 사건이 발생함을 뜻한다.\n동시 도착 없음: \\(\\lim_{t \\to 0} \\frac{P(N_t &gt; 1)}{t} = 0\\) 이다. 아주 짧은 시간 간격에서는 두 건 이상의 사건이 동시에 발생할 가능성이 없다. 이는 한 순간에 한 사건씩만 발생한다는 조건이다.\n\n포아송 분포 유도\n시간이나 면적과 같은 연속적인 영역을 동일한 크기의 n개 구간으로 나눈다고 하자. 이때 각 구간에서 사건이 발생할 확률을 p라고 하면, 사건이 발생하지 않을 확률은 (1 - p)가 된다. 또한 각 구간에서의 사건 발생 여부는 독립적으로 이루어진다고 가정할 수 있다.\n이러한 구조에서는 각 구간에서의 사건 발생 여부를 두 가지 결과 중 하나(발생 또는 미발생)로 구분할 수 있으므로, 각 구간은 베르누이 시행과 동일한 형태의 실험이 된다. 즉, 전체 영역은 n회의 독립적인 베르누이 시행으로 구성되며, 특정 시간이나 면적 내에 발생하는 전체 사건 수는 이항 분포를 따르게 된다.\n포아송 분포는 이러한 이항 분포에서 \\(n \\to \\infty, p \\to 0\\), 단 \\(np = \\lambda\\) 가 일정하게 유지되는 극한 과정에서 유도될 수 있다. 이를 통해 드물게 발생하는 사건이 넓은 시간이나 공간에 걸쳐 어떻게 분포하는지를 분석할 수 있다.\n\n\n\n\n\n\\(e^{- \\lambda} = \\lim_{n \\rightarrow \\infty}\\left( 1 - \\frac{\\lambda}{n} \\right)^{n}\\) 사실을 이용하여 다음을 증명할 수 있다. 모수 \\((n,p)\\)인 이항분포는 모수 \\(\\lambda = np\\)인 포아송 분포에 근사한다.\n\\[{\\lim_{n \\rightarrow \\infty}\\binom{n}{x}p^{x}(1 - p)^{n - x} = \\lim_{n \\rightarrow \\infty}\\binom{n}{x}\\left( \\frac{\\lambda}{n} \\right)^{x}}\\left( 1 - \\frac{\\lambda}{n} \\right)^{n - x}\\]\n\\[= \\frac{\\lambda^{x}}{x!}\\lim_{n \\rightarrow \\infty}{\\left( 1 - \\frac{\\lambda}{n} \\right)^{n}\\left( 1 - \\frac{\\lambda}{n} \\right)^{- x}\\frac{n(n - 1)(n - x + 1)}{n^{x}}} = \\frac{\\lambda^{x}}{x!}e^{- \\lambda}\\]\n\\[{\\because\\lim_{n \\rightarrow \\infty}}{\\left( 1 - \\frac{\\lambda}{n} \\right)^{- x}\\frac{n(n - 1)(n - x + 1)}{n^{x}} = 1}\\]\n【성질】\n\n재귀 관계식: \\(P(X = x) = \\frac{\\lambda}{x}P(X = x - 1),x = 1,2,\\ldots\\)\n단위 구간 평균 발생 회수는 구간의 크기에 비례한다. \\(X\\sim P(\\lambda) \\leftrightarrow kX\\sim P(k\\lambda)\\)\n분포의 가법성: 서로 독립인 포아송 분포 \\(X_{i} \\sim P(\\lambda_{i})\\) 합은 포아송 분포 \\(\\sum_{i}^{n}X_{i} \\sim P(\\sum_{i}^{n}\\lambda_{i})\\) 이다. 가법성을 갖는 연속형 확률변수는 감마분포(*)와 정규분포이다.\n\n【예제】 기다리는 시간\n대기 중인 사건이 발생하는 경우를 모델링하는 예제로, 평균적으로 3분마다 5통의 전화를 처리하는 전화 교환원을 고려해보자. ① 다음 1분 동안 전화가 전혀 오지 않을 확률은 얼마인가? ② 다음 1분간 최소한 2통 이상의 전화가 올 확률은 얼마인가?\n1분당 전화가 걸려오는 횟수를 나타내는 확률변수를 \\(X\\)라고 정의하면 \\(X \\sim Poisson(\\lambda = 5/3)\\)이다.\n① \\(P(X = 0) = \\frac{e^{- 5/3}\\left( \\frac{5}{3} \\right)^{0}}{0!} = 0.189\\)\n② \\(P(X \\geq 2) = 1 - P(X = 0) - P(X = 1)\\)\n【예제】\n초원에 서식하는 얼룩말의 수는 1 에이커 당 평균 5마리이고 포아송 분포를 따른다고 하자.\n0.5에이커를 무작위 조사하였을 때 얼룩말을 하나도 보지 못할 확률은? \\(P(X = 0|X \\sim P(\\lambda = 2.5)) = 0.6065\\)\n2에이커를 조사했을 때 13마리 이상 볼 확률은? \\(P(X \\geq 13|X \\sim P(\\lambda = 10)) = 0.2084\\)\n\n\n6. 음이항분포 \\(X \\sim NB(r,p)\\)\n이항 분포는 고정된 횟수 n의 독립적인 베르누이 시행에서 성공이 몇 번 발생하는지를 나타내는 확률분포이다. 반면, 음이항 분포는 고정된 횟수의 성공 r을 얻기 위해 필요한 전체 시행 수, 또는 그 과정에서 발생하는 실패의 횟수를 세는 데 사용된다.\n보다 구체적으로, 음이항 분포는 다음과 같은 상황을 모델링한다. 독립적인 베르누이 시행을 반복하여 수행하면서, 성공 확률이 p로 일정하다고 하자. 이때, r번째 성공이 발생하기 전까지 관찰된 실패의 수를 확률변수 X라고 정의하면, X는 음이항 분포를 따른다. 즉, 이 분포는 “몇 번 실패한 후에 r번째 성공이 발생하는가”에 대한 확률을 나타낸다.\n확률질량함수 \\[P(X = x|r,p) = \\binom{r + x - 1}{x}p^{r}(1 - p)^{x},x = 0,1,2,\\ldots\\]\n평균과 분산 \\[E(X) = \\frac{r(1 - p)}{p},V(X) = \\frac{r(1 - p)}{p^{2}}\\]\n【활용】\n\n고객 서비스 센터의 상담 전화 모델링: 한 상담원이 고객과의 통화 중 특정 유형의 문제를 해결하는 것을 “성공”이라고 정의할 경우, r번째 문제 해결이 이루어지기 전까지 실패한 시도(즉, 해결되지 않은 통화 수)를 모델링하는 데 음이항 분포를 사용할 수 있다.\n생물학 및 의학 연구: 예를 들어 한 환자가 치료에 반응하는 것을 “성공”이라 하면, r번째 성공적인 치료 반응이 나타나기 전까지의 치료 시도 횟수를 분석할 수 있다. 백신 연구에서는 r명의 감염자를 찾기까지 필요한 검사 횟수를 예측하는 데에도 음이항 분포가 적용된다.\n마케팅 및 광고 효과 분석: 특정 광고가 소비자로부터 “구매”라는 반응을 유도할 경우, r번째 구매가 발생하기 전까지의 시도 횟수(즉, 광고 노출 수)를 음이항 분포로 모델링할 수 있다.\n온라인 시스템 및 네트워크 트래픽 모델링: 웹사이트에서 특정 페이지에 대해 r번째 클릭이 발생하기까지의 총 방문자 수를 예측하거나, 데이터 패킷 전송 과정에서 r번째 성공적인 전송이 이루어지기 전까지 손실된 패킷 수를 분석하는 데 음이항 분포가 활용된다.\n\n이처럼 음이항 분포는 고정된 성공 횟수에 도달하기 전까지의 실패 횟수 또는 총 시도 횟수를 모델링해야 하는 다양한 실제 문제에 적용할 수 있다.\n【성질】\n\n독립성: 음이항 분포에서 각 시행은 독립적인 베르누이 시행으로 이루어져 있으며, 이전 시행의 결과가 이후 시행에 영향을 미치지 않는다.\n이항 분포는 시행 횟수가 고정되고 성공 횟수가 변하는 모델이며, 음이항 분포는 성공 횟수가 고정되고 시행 횟수가 변하는 모델이다.\n합의 분포: 만약 두 개의 독립적인 음이항 분포가 다음과 같이 주어진다면, \\(X_{1} \\sim NB(r_{1},p),X_{2} \\sim NB(r_{2},p)\\) 그 합은 다시 음이항 분포를 따른다. \\(X_{1} + X_{2} \\sim NB(r_{1} + r_{2},p)\\) 즉, 동일한 성공 확률 p 을 갖는 두 개의 음이항 분포는 성공 횟수를 합하면 다시 음이항 분포를 형성한다.\n포아송 분포와의 관계: 음이항 분포는 포아송 분포의 가변형으로 해석될 수도 있다. 즉, 포아송 분포의 분산이 평균보다 큰 경우, 음이항 분포를 대체 사용할 수 있다. 특히, 감마 분포가 포아송 분포의 평균을 따른다면, 결과적으로 음이항 분포를 얻게 됩니다. 즉, 만약 \\(\\lambda \\sim \\text{Gamma}(r,\\theta)\\) 이고 \\(X|\\lambda \\sim \\text{Poisson}(\\lambda)\\)이면, X 는 음이항 분포를 따른다.\n음이항 분포의 포아송 근사: 성공 확률 p 이 작고, 목표 성공 횟수 r 이 크다면 음이항 분포는 포아송 분포에 근사할 수 있습니다. 즉, \\(r \\rightarrow \\infty,p \\rightarrow 0\\) 그리고 \\(\\lambda = rp\\)을 일정하게 유지하면 \\(\\text{NB}(r,p) \\approx \\text{Poisson}(\\lambda)\\)\n\n【예제】\n지질학 연구 결과 석유 탐사 중 석유 발견 확률은 0.2이다.\n\n3번째 탐사에서 처음 석유가 발견될 확률을 계산하라.\n7번째 탐사에서 3번째 석유가 발견될 확률을 계산하라.\n3번째 석유 발견을 위하여 적어도 20번 이상 탐사할 확률을 계산하라.\n석유가 3개 발견될 때까지 시추를 계속하기로 하였다. 몇 번 정도 시추해야 하나?\n\n【풀이】 0.128 / 0.049 / 0.237 / \\(\\frac{2*0.8}{0.2} = 12\\)+3=15번 시추\n【예제】 역 이항표본 추출\n역 이항 표본 추출(inverse binomial sampling)은 생물학적 개체군을 표본 조사할 때 유용한 방법이다. 이 기법은 특정 특성을 가진 개체의 비율이 p일 때, 해당 특성을 가진 개체를 r개 발견할 때까지 표본을 계속 추출하는 방식이다. 이 과정에서 표본으로 선택된 총 개체 수는 확률변수로 간주되며, 이는 음이항 분포를 따른다.\n즉, 고정된 성공 횟수 r에 도달하기 위해 필요한 전체 시도 횟수(표본 수)를 모델링하는 것으로, 실험이나 조사 중 관찰된 특성의 발생률을 추정하거나 샘플 크기를 결정하는 데 활용될 수 있다.\n예를 들어, 잠자리 개체군에서 고추 잠자리 비율에 관심이 있다고 가정하자. 우리는 50마리의 고추잠자리를 찾을 때까지 표본을 추출하기로 결정한다. 이때, 최소한 \\(N\\) 마리의 잠자리를 조사해야 할 확률은 다음과 같이 계산된다.\n\\(P(X \\geq N) = \\overset{\\infty}{\\sum_{x = N}}\\binom{x - 1}{49}p^{50}(1 - p)^{x - 50}\\). 주어진 \\(p\\) 와 \\(N\\) 값에 대해, 위 식을 계산하여 몇 마리의 잠자리를 조사해야 하는지를 추정할 수 있다. (단, 이러한 계산은 복잡할 수 있으며, 재귀 관계를 활용하면 계산 속도를 높일 수 있다.) 이 예제는 음이항 분포가 생물학적 표본 추출에서 어떻게 활용될 수 있는지를 보여준다.\n\n\n7.기하분포 \\(X \\sim Geo(p)\\)\n기하 분포는 가장 단순한 대기 시간 분포 중 하나이며, 음이항 분포의 특수한 경우이다.\n확률질량함수 \\[P(X = x|p) = p(1 - p)^{x - 1}y = 0,1,2,\\ldots\\]\n평균과 분산 \\[E(X) = \\frac{1}{p},V(X) = \\frac{1 - p}{p^{2}}\\]\n【활용】\n\n사건 발생까지 걸리는 시간 모델링: 어떤 사건이 확률 p로 발생한다고 할 때, 해당 사건이 처음으로 발생할 때까지 걸리는 시행 횟수를 기하분포로 모델링할 수 있다. 예를 들어, 고장 확률이 p인 기계가 처음 고장날 때까지의 시간을 나타낼 수 있다.\n머신러닝에서의 훈련 수렴 시간: 반복적인 학습을 통해 모델이 수렴할 때까지 필요한 시행 횟수를 이산형 대기 시간으로 간주하여 기하분포로 모델링할 수 있다. 이는 최적화 과정에서 특정 조건이 만족되기까지 걸리는 시행 횟수를 예측하는 데 활용된다.\n웹 사용자 행동 분석: 사용자가 웹사이트에서 특정 행동(예: 클릭, 구매 등)을 하기까지 걸리는 시간을 기하분포로 모델링함으로써 사용자 행동 패턴을 분석하고, 효과적인 마케팅 전략을 수립할 수 있다.\n서비스 대기 시간 분석: 고객이 서비스를 요청한 후 실제로 서비스를 받기까지의 시간을 분석할 때 기하분포가 활용된다. 예를 들어, 콜센터에 전화한 고객이 상담을 받기까지 기다리는 시간은 기하분포적 특성을 가질 수 있다.\n마케팅 캠페인 성과 평가: 고객이 첫 구매를 하기까지 걸리는 시간을 기하분포로 모델링하면 마케팅 활동의 효율성을 분석할 수 있다. 이를 통해 고객 유치 속도를 추정하고, 마케팅 자원 배분의 최적화를 도모할 수 있다.\n\n【성질】\n\n기하분포 \\(X\\sim G(p)\\)는 음이항 분포 \\(X\\sim NB(r = 1,p)\\)의 특수한 분포이다.\n독립인 \\(r\\)개 기하분포 \\(X_{i} \\sim G(p)\\)의 합 \\(\\sum_{k = 1}^{r}X_{i}\\) 확률변수는 음이항분포 \\(NB(r,p)\\)를 따른다.\n\\(X_{i} \\sim G\\left( p_{i} \\right)\\)을 갖고 서로 독립이 경우 \\(W = \\min_{i = 1,2,\\ldots,m}X_{i} \\sim G(1 - \\prod_{i}^{}{(1 - p_{i})})\\)분포이다.\n기하분포는 사건 발생 시간에 대한 이산형 분포 함수이므로 연속형 사건 발생 시간에 대한 지수분포와 다음의 관계가 있다. \\(X \\sim expential(\\lambda):Y = ⎣X⎦ \\sim G(p = 1 - e^{- \\lambda} = - \\ln(1 - p))\\), \\(\\lfloor x\\rfloor\\)함수는 floor 함수 \\(\\lfloor 3.17\\rfloor = 3\\).\n\\(X \\sim G(p = \\frac{1}{n})\\)이면 \\(\\frac{X}{n} \\rightarrow exponential(1)\\)이다.\n\n【예제】 무기억성\n기하 분포는 ”무기억성(memoryless)” 속성으로 알려진 흥미로운 성질을 갖는다. 정수 \\(s &gt; t\\)에 대해 다음이 성립한다.\n\\(P(X &gt; s|X &gt; t) = P(X &gt; s - t);\\)즉, 기하 분포는 이전에 발생한 사건을 ”잊어버린다”. 이미 \\(t\\)개의 실패를 관찰한 후 추가적으로 \\(s - t\\) 개의 실패가 발생할 확률은, 시퀀스의 시작에서 \\(s - t\\) 개의 실패가 발생할 확률과 동일하다. 다시 말해, 연속적인 실패가 발생할 확률은 위치에 의존하지 않고 오직 연속된 실패의 길이에만 의존한다.\n\\(P(X &gt; n) = P(\\text{n번 시행까지 성공 없음}) = (1 - p)^{n}\\)이므로\n\\[\\begin{matrix}\nP(X &gt; s|X &gt; t) & = \\frac{P(X &gt; s\\text{and}X &gt; t)}{P(X &gt; t)} \\\\\n& = \\frac{P(X &gt; s)}{P(X &gt; t)} = (1 - p)^{s - t} = P(X &gt; s - t)\n\\end{matrix}\\]\n【예제】 고장 시간\n기하 분포는 종종 부품의 수명 또는 고장까지의 시간을 모델링하는데 사용된다. 예를 들어, 어떤 전구가 하루 동안 고장 날 확률이 0.001이라면, 해당 전구가 최소 30일 이상 지속될 확률은\\(P(X &gt; 30) = \\overset{\\infty}{\\sum_{x = 31}}0.001(1 - 0.001)^{x - 1} = (0.999)^{30} \\approx 0.970\\)\n\n\n8. 이산형 분포 관계도\n\n\n\n\n\n\n\n\nchapter 2. 연속형 분포\n확률변수 \\(X\\)가 가질 수 있는 서로 다른 값이 유한이거나 셀 수 있는 이산형 확률변수와 달리 확률실험 결과 서로 다른 값이 무한히 발생할 수 있는 연속형 확률변수에 대해 다룰 것이다. 연속형에서는 아무리 작은 구간을 설정하더라도 적어도 하나 이상의 값이 관측된다.\n이산형인 경우 확률밀도함수(확률변수가 갖는 값에 대해 확률을 할당하는 식, 표, 그래프)를 얻는 것은 어렵지 않다. 그러나 아무리 작은 구간이더라도 많은 값들이 관측될 수 있는 연속형의 경우 확률밀도함수를 얻는 것은 불가능하다. 그리하여 관측된 데이터로부터 히스토그램을 얻고, 이것이 알려진 확률밀도함수 중 어느 것과 가장 유사한지 판단하여 이론적 분포를 얻게 된다.\n\n1. 균일분포 \\(X \\sim U(a,b)\\)\n확률질량함수\n\\(f(x|a,b) = \\frac{1}{b - a},a \\leq x \\leq b\\).\n평균과 분산\n\\[E(X) = \\frac{(a + b)}{2},V(X) = \\frac{(b - a)^{2}}{12}\\]\n【활용】\n연속형 균일분포는 다양한 통계적 및 계산적 기법에서 활용되는 기본적인 분포이다. 특히 난수 생성에서 중심적인 역할을 한다. 확률변수 \\(X \\sim U(0,1)\\) 는 [0,1] 구간에서 동일한 확률로 값을 가지며, 이는 시뮬레이션, 무작위 추출, 몬테카를로 방법 등의 기반이 된다.\n또한 확률밀도함수를 이용하여 특정 구간 내에서 사건이 발생할 확률을 직접 계산하거나 추정하는 데 활용할 수 있다. 예를 들어,$ X U(a,b)$ 인 경우, 어떤 구간 [c,d]에 대해 X가 그 구간에 속할 확률은 단순한 면적 계산으로 구할 수 있다.\n균일분포는 다른 확률분포와 결합하여 복합적인 모델을 구성하는 데에도 사용된다. 예를 들어, 정규분포와 균일분포를 혼합하여 이상값이 포함된 자료를 모델링할 수 있으며, 이는 혼합분포(mixture distribution)의 대표적인 예시다. 이러한 모델은 비정상적이거나 다양한 패턴을 보이는 데이터를 설명하는 데 유용하다.\n【성질】\n\n\\(a = 0,b = 1\\)인 균일분포를 표준 균일분포라 한다.\n표준 균일분포 \\(X\\sim U(0,1)\\)의 역함수 \\(Y = - \\lambda ln(X)\\) 는 지수분포 \\(exponential(\\lambda)\\) 따른다.\n표준 균일분포 \\(X\\sim U(0,1)\\)의 \\(Y = X^{n}\\)은 베타분포 \\(Beta(\\frac{1}{n},1)\\) 따른다.\n표준 균일분포 \\(X\\sim U(0,1)\\)는 베타분포 \\(Beta(1,1)\\) 특수한 형태이다.\n두 균일분포의 합과 차이의 분포는 삼각형 분포를 triangle distribution 따른다.\n\n\n\n\n\n\n【예제】\n만약 낙하산이 marker A와 B 사이에 임의의 지점에 떨어진다고 하자.\n1) 낙하산이 B 보다 A 지점에 더 가까이 떨어질 확률을 구하라. (답) 지점 (A, B) 가운데 지점부터 B지점에 떨어져야 하므로 확률은 1/2이다.\n2) 낙하산 떨어진 지점에서 지점 A까지의 거리가 지점 B까지의 거리의 3배 이상일 확률을 구하라. (답) 동일한 논리로 1/4 지점이 근거이므로 확률은 1/4이다.\n3) 3개의 낙하산 중 정확하게 한 개만 지점 B에 가까이 떨어질 확률을 구하라. (답) 낙하산 1개 B지점, 2개 A지점에 가까울 확률은 1/8이므로 3개의 경우가 발생하므로 3/8이다.\n\n\n2. 감마분포 \\(X \\sim Gamma(\\alpha, \\beta)\\)\n우로 치우친 분포 감마분포(Gamma distribution)는 매개변수 (\\(\\alpha,\\beta = \\frac{1}{\\lambda}\\))를 갖는다.\n감마함수 gamma function \\(\\Gamma(x)\\)\n\n\\(\\Gamma(\\alpha) = \\int_{0}^{\\infty}t^{\\alpha - 1}e^{- t}dt.\\).\n정수 \\(n\\)에 대하여 \\(\\Gamma(n) = (n - 1)\\Gamma(n - 1)\\).\n정수 \\(n\\)에 대하여 \\(\\Gamma(n) = (n - 1)!\\).\n\\(\\Gamma(1/2) = \\sqrt{\\pi}\\)\n삼각함수 관련성 : 복소수 \\(i\\)에 대해 \\(\\Gamma(i)\\Gamma(1 - i) = \\frac{\\pi}{sin(i\\pi)}\\)\n\n감마함수는 실수 또는 복소수 입력에 대해 정의된 함수로, 팩토리얼 함수의 연속 확장이라 볼 수 있다. 양의 정수 n에 대해 감마함수 \\(\\Gamma(n)\\) 는 (n - 1)!과 같다. 이 특성으로 인해 통계학, 특히 연속 확률분포의 정의와 계산에서 중요한 역할을 한다.\n통계학에서는 감마함수가 베타함수의 정의에 포함되며, 베타분포와 감마분포의 확률밀도함수를 정규화할 때 사용된다. 예를 들어, 베타분포의 밀도함수에서 분모에 있는 베타함수는 두 감마함수의 비로 표현된다.\n수학 물리학에서는 감마함수가 여러 미분방정식의 해, 복소수 영역에서의 해석 함수 이론, 적분 변환 등의 계산에 사용되며, 복소해석학에서는 극점, 유수 계산 등에서 중요한 도구가 된다.\n또한 감마함수는 지수적 성장, 로그함수와의 관계, 오일러 적분 표현 등을 통해 다양한 수학적 등식의 유도나 해석에 활용된다. 이러한 성질 덕분에 감마함수는 순수 수학과 응용 수학 모두에서 핵심적인 특수함수로 간주된다.\n확률질량함수\n\\[f(x|\\alpha,\\beta) = \\frac{1}{\\Gamma(\\alpha)\\beta^{\\alpha}}x^{\\alpha - 1}e^{- x/\\beta},0 &lt; x &lt; \\infty,\\alpha &gt; 0,\\beta &gt; 0\\]\n매개변수 \\(\\alpha\\) 는 감마분포에서 형상 매개변수로 불리며, 분포의 모양을 결정하는 데 핵심적인 역할을 한다. 특히, 값에 따라 분포가 단봉, 지수형, 또는 좌우로 비대칭한 형태를 띨 수 있다. 예를 들어, \\(\\alpha = 1\\) 일 경우 감마분포는 지수분포와 같아지며,$ &gt; 1$ 일수록 분포는 뾰족한 단봉형에 가까워진다. 이는 분포의 첨도 및 비대칭성에 큰 영향을 미친다.\n반면, \\(\\beta\\) 는 척도 매개변수로, 분포의 수평 방향 확장 또는 압축에 관여한다. \\(\\beta\\) 가 클수록 분포는 넓게 퍼지며, 작을수록 분포는 좁고 집중된 형태가 된다. 이는 분포의 산포, 즉 퍼짐 정도(분산)에 영향을 주며, 단위의 크기를 조정하는 효과가 있다.\n따라서 감마분포는 \\(\\alpha\\) 와 \\(\\beta\\) 의 결합에 따라 다양한 형태를 취할 수 있어, 대기 시간, 생존 분석, 신뢰도 분석 등에서 유연하게 사용된다.\n평균과 분산\n\\[E(X) = \\alpha\\beta,V(X) = \\alpha\\beta^{2}\\]\n【포아송 분포와 지수분포 관계】\n\n\n\n\n\n감마 분포와 포아송 분포 사이에는 흥미로운 관계가 있다. 만약 \\(X\\)가 감마 분포 \\(\\text{gamma}(\\alpha,\\beta)\\)를 따르는 확률 변수이고, \\(\\alpha\\)가 정수라면, 임의의 \\(x\\)에 대해 다음이 성립한다. \\(P(X \\leq x) = P(Y \\geq \\alpha)\\), 여기서 \\(Y \\sim \\text{Poisson}(x/\\beta)\\)이다.\n\\(\\alpha\\)가 정수이므로 감마 함수의 정의에서 \\(\\Gamma(\\alpha) = (\\alpha - 1)!\\)이므로 \\(P(X \\leq x) = \\frac{1}{(\\alpha - 1)!\\beta^{\\alpha}}\\int_{0}^{x}t^{\\alpha - 1}e^{- t/\\beta}dt\\)\n\\(= \\frac{1}{(\\alpha - 1)!\\beta^{\\alpha}}\\left\\lbrack - t^{\\alpha - 1}\\beta e^{- t/\\beta}|_{0}^{x} + \\int_{0}^{x}(\\alpha - 1)t^{\\alpha - 2}\\beta e^{- t/\\beta}dt \\right\\rbrack\\)부분 적분(\\(u = t^{\\alpha - 1}\\), \\(dv = e^{- t/\\beta}dt\\))를 치환하여 계산을 이어가면,\n\\[P(X \\leq x) = \\frac{- 1}{(\\alpha - 1)!\\beta^{\\alpha - 1}}x^{\\alpha - 1}e^{- x/\\beta} + \\frac{1}{(\\alpha - 2)!\\beta^{\\alpha - 1}}\\int_{0}^{x}t^{\\alpha - 2}e^{- t/\\beta}dt\\]\n\\(= \\frac{1}{(\\alpha - 2)!\\beta^{\\alpha - 1}}\\int_{0}^{x}t^{\\alpha - 2}e^{- t/\\beta}dt - P(Y = \\alpha - 1)\\), 여기서 \\(Y \\sim \\text{Poisson}(x/\\beta)\\)이다. 이를 반복 실행하면 \\(P(X \\leq x) = P(Y \\geq \\alpha)\\)이 성립한다.\n【감마분포의 특수한 형태 ①】\n만약 \\(\\alpha = r/2\\), 여기서 \\(r\\)는 정수이고 \\(\\beta = 2\\)라면, 감마 확률 밀도 함수는 다음과 같이 된다. \\(f(x|p) = \\frac{1}{\\Gamma(p/2)2^{\\frac{p}{2}}}x^{(p/2) - 1}e^{- x/2},0 &lt; x &lt; \\infty\\),\n이는 자유도 \\(r\\)를 가지는 카이제곱 확률 밀도 함수이다. 카이제곱 분포의 평균, 분산은 앞서 도출한 감마 분포의 공식을 사용하여 모두 계산할 수 있다. 카이제곱 분포는 특히 정규 분포에서 표본을 추출할 때 통계적 추론에서 중요한 역할을 한다.\n【감마분포의 특수한 형태 ②】\n또 다른 감마 분포의 중요한 특수한 경우는 \\(\\alpha = 1\\)인 경우이다. 이때 확률 밀도 함수는 다음과 같이 된다.\n\\(f(x|\\beta) = \\frac{1}{\\beta}e^{- x/\\beta},0 &lt; x &lt; \\infty\\). 이는 척도 매개변수 \\(\\beta\\)를 가진 지수 분포(exponential)이다.\n【지수분포와 기하분포】\n\\(X \\sim exponential(\\beta)\\)이고 확률변수 \\(Y\\)을 다음과 같이 정의할 때 포아송 분포를 따르는 것을 증명하라.\n\\(Y = kifk - 1 \\leq X &lt; k,k = 1,2,\\ldots\\).\n【증명】 \\(X_{t} &gt; x \\equiv (Y_{t} = Y_{t + x})\\), \\(X_{t}\\): 어떤 사람이 t시각에 도착한 후 그 다음 사람이 도착할 때까지 걸리는 시간, \\(Y_{t}\\): t 시간 동안 도착한 사람 수\\(\\sim\\)모수가 \\(\\lambda\\)인 포아송분포\\({P(X}_{t} \\leq x) = 1 - P(X_{t} &gt; x) = 1 - P(Y_{t + x} - Y_{t} = 0)\\)이므로\\(P\\left( Y_{t + x} - Y_{t} = 0 \\right) = P\\left( Y_{x} = 0 \\right) = \\frac{(\\lambda x)^{0}e^{- \\lambda x}}{0!} = e^{- \\lambda x}\\). 그러므로 X는 지수분포를 따른다.\n【지수분포의 무기억성】\n지수 분포는 기하 분포와 같은 “무기억성(memoryless)” 속성을 가진다. 만약 \\(X \\sim \\text{exponential}(\\beta)\\)이면, \\(s &gt; t \\geq 0\\)에 대해 다음이 성립한다. \\(P(X &gt; s|X &gt; t) = P(X &gt; s - t)\\).\n\\[P(X &gt; s|X &gt; t) = \\frac{P(X &gt; s,X &gt; t)}{P(X &gt; t)} = \\frac{\\int_{s}^{\\infty}\\frac{1}{\\beta}e^{- x/\\beta}dx}{\\int_{t}^{\\infty}\\frac{1}{\\beta}e^{- x/\\beta}dx}\\]\n\\[= \\frac{e^{- s/\\beta}}{e^{- t/\\beta}} = e^{- (s - t)/\\beta} = P(X &gt; s - t)\\]\n【지수분포, 기하분포, 그리고 와이블분포】\n지수 분포와 감마 분포 모두와 관련된 또 다른 분포는 와이블분포(Weibull distribution)이다. 만약 \\(X \\sim \\text{exponential}(\\beta)\\)이면, \\(Y = X^{\\frac{1}{\\gamma}} \\sim Weibull(\\gamma,\\beta)\\)분포를 따른다.\n\\[f_{Y}(y|\\gamma,\\beta) = \\frac{\\gamma}{\\beta}y^{\\gamma - 1}e^{- y^{\\gamma}/\\beta},0 &lt; y &lt; \\infty,\\gamma &gt; 0,\\beta &gt; 0\\]\n와이블분포는 무기억성을 지닌 지수분포의 한계로 적용할 수 없는 고장 시간 데이터 분석, 위험 함수 모델링에 매우 유용하게 사용된다.\n【활용】\n\n신뢰도 분석: 감마분포는 제품이나 시스템의 수명을 모델링하는 데 적합하며, 고장이 발생하기까지의 시간이나 시스템의 신뢰도 수준을 추정하는 데 사용된다. 예를 들어, 전자 부품의 수명 분포를 모델링하거나 유지보수 주기를 계획할 때 유용하다.\n금융 분야: 옵션 가치 평가나 포트폴리오 리스크 관리 등에서 감마분포가 사용된다. 특히, 극단적인 손실이나 수익률 분포의 꼬리 부분을 분석할 때 활용되며, 옵션 가격 결정 모델의 확률적 요소를 구성하는 데 기여한다.\n생명과학 및 의학 연구: 암 환자의 생존 시간, 치료 반응 시간, 약물의 흡수 및 대사 속도와 같은 생명현상을 시간 기반의 분포로 모델링할 때 감마분포가 사용된다. 비정규적인 생존시간 분포를 설명할 수 있어 생존 분석에 효과적이다.\n통신공학: 무선 통신에서 신호의 세기가 다양한 환경 요인에 따라 변동하는 현상을 페이딩(fading)이라고 하는데, 이때 감마분포는 신호 강도 변동을 모델링하는 데 사용된다. 특히, 레일리 분포나 나카가미 분포의 일반화된 형태로써 감마분포가 쓰이기도 한다.\n\n이처럼 감마분포는 다양한 실험적·산업적 환경에서 시간, 강도, 수명 등의 연속적인 변수를 모델링하는 데 널리 활용된다.\n【성질】\n\n감마분포함수  \\(X \\sim \\Gamma(\\alpha,\\beta)\\)이면 상수 \\(c\\)에 대하여 \\(cX \\sim \\Gamma(\\alpha,\\frac{\\beta}{c})\\)이다.\n지수분포 \\(X \\sim exponential(\\beta)\\) 의 확률표본 \\((X_{1},X_{2},\\ldots,X_{r})\\)의 합 \\(\\sum_{i}^{r}X_{i}\\sim\\Gamma(r,\\beta)\\) 감마분포를 갖는다.\n감마분포함수 \\(X \\sim \\Gamma(\\alpha,\\beta)\\)의 확률표본 \\((X_{1},X_{2},\\ldots,X_{n})\\)의 합 \\(\\sum_{i}^{n}X_{i}\\sim\\Gamma(n\\alpha,\\beta)\\) 감마분포를 갖는다. 【가법성】\n\n\n\n3. 정규분포 \\(X \\sim N(\\mu,\\sigma)\\)\n정규 분포(또는 가우시안 분포)는 통계학에서 매우 중심적인 역할을 하는 확률분포로, 그 중요성은 다음 세 가지 주요 이유에서 비롯된다.\n첫째, 수학적으로 다루기 쉬운 구조를 가지고 있다. 정규 분포는 평균을 중심으로 좌우 대칭이며, 확률밀도함수가 연속적이고 미분 가능하여 다양한 통계적 분석에서 수식 전개가 용이하다. 표준편차에 따라 그래프의 형태가 결정되며, 표준편차가 작으면 뾰족하고, 크면 평평한 종 모양을 갖는다.\n둘째, 실제 현상을 잘 설명하는 형태를 가지고 있다. 정규 분포는 자연 현상이나 사회 현상 등 다양한 실제 데이터에서 자주 관측되는 분포 형태이며, 평균값을 중심으로 데이터가 몰려 있고 극단값은 드물게 나타나는 특징이 있다. 다른 종 모양의 분포들도 존재하지만, 정규 분포만큼 분석적으로 편리한 경우는 드물다.\n셋째, 중심극한정리(Central Limit Theorem)의 기반이 되는 분포이다. 중심극한정리에 따르면, 원래의 모집단 분포가 정규분포가 아니더라도, 충분히 큰 표본의 평균은 정규분포에 가까워지게 된다. 이는 다양한 통계적 추론 기법이 정규분포를 전제로 할 수 있도록 해주는 이론적 근거가 된다.\n이와 같은 특성 덕분에 정규분포는 회귀분석, 가설검정, 신뢰구간 추정 등 대부분의 통계 분석 기법에서 기본적인 가정으로 자리 잡고 있다.\n확률질량함수\n\\[f(x|\\mu,\\sigma^{2}) = \\frac{1}{\\sqrt{2\\pi}\\sigma}e^{- (x - \\mu)^{2}/(2\\sigma^{2})}, - \\infty &lt; x &lt; \\infty\\]\n정규 분포는 2개의 매개변수가 있는데 위치 모수인 평균 \\(\\mu\\)과 척도 모수인 분산 \\(\\sigma^{2}\\)이다.\n평균과 분산\n\\[E(X) = \\mu,V(X) = \\sigma^{2}\\]\n【표준정규분포】\n만약 \\(X \\sim n(\\mu,\\sigma^{2})\\)이면, 새로운 확률 변수 \\(Z = \\frac{X - \\mu}{\\sigma}\\)는 표준 정규분포 \\(N(0,1)\\)를 따른다.\n【활용】\n정규분포는 통계학의 여러 분야에서 핵심적인 역할을 하며, 다음과 같은 다양한 응용 사례를 통해 실질적인 분석과 추론의 도구로 사용된다.\n첫째, 통계적 추론에서 정규분포는 가설 검정과 신뢰구간 추정에 활용된다. 데이터가 정규분포를 따른다는 가정하에, 표본 평균이나 두 집단 간의 평균 차이를 비교하는 등의 통계적 추론이 가능해진다. 이때, 정규분포의 특성을 이용하여 검정통계량을 정의하고, 유의확률(p-value)을 계산한다.\n둘째, 자연과학 및 공학 분야에서는 실험 오차나 측정값의 정밀도를 모델링할 때 정규분포가 사용된다. 예를 들어, 물리적 실험에서 측정값이 반복될수록 평균값을 중심으로 분포되는 경향이 있으며, 이는 정규분포의 형태와 일치한다.\n셋째, 금융 분석에서는 자산 수익률의 분포를 정규분포로 근사하여 포트폴리오 이론, 옵션 가격 결정, 리스크 측정 등에 활용된다. 비록 실제 데이터는 정규분포에서 벗어나는 경우도 있지만, 이론적 모델링에서는 정규분포가 기본 가정으로 자주 사용된다.\n넷째, 데이터 분석 및 예측 모델링에서도 정규분포는 잔차의 분포 가정에 사용된다. 선형 회귀나 로지스틱 회귀, 시계열 예측 등에서 모델의 적합성과 예측력을 평가할 때 잔차가 정규분포를 따른다고 가정한다.\n마지막으로, 중심극한정리(Central Limit Theorem)에 따르면, 개별 데이터의 분포와 관계없이 충분히 큰 확률표본의 합이나 평균은 정규분포에 근사하게 된다. 이를 통해 모집단의 평균이나 비율에 대한 추론이 가능해지며, 많은 통계적 방법이 이 정리를 기반으로 하고 있다.\n【역사】\nCarl Friedrich Gauss : 가우스는 정규분포에 대한 초기 연구를 수행한 과학자 중 하나로 알려져 있다. 그는 천체 관측 오차의 분포를 분석하면서 정규분포의 개념을 처음으로 제시하였는데 최소자승법과 함께 정규분포의 특성과 확률론적인 속성에 대해 연구하였다.\nAdrien-Marie Legendre : 아들라드는 정규분포와 최소자승법의 개념을 독립적으로 개발하였다. 그는 1805년에 최소제곱법에 대한 연구를 발표하였고 이를 정규분포와 관련된 추정 방법론으로 사용되었습니다.\nJakob Bernoulli : 18세기 초기에 베르누이는 이항분포의 극한으로서 정규분포에 대한 아이디어를 제시하였다. 이항분포는 독립적인 베르누이 시행의 합으로서 정의되며 시행 횟수가 증가할수록 성공회수인 이항분포가 정규분포에 근사됨을 발견하였다.\n【특성】 실증적 법칙\n정규 분포는 두 개의 매개변수, 평균와 분산 만으로 그 모양과 위치를 완전히 결정할 수 있다는 점에서 특별한 성질을 가진다. 이러한 성질은 정규 분포만의 고유한 특징이 아니라, 위치-척도 가족으로 알려진 확률밀도함수 계열에서도 공유된다.\n\n\\(P(|X - \\mu| \\leq \\sigma) = P(|Z| \\leq 1) = 0.6826\\),\n\\(P(|X - \\mu| \\leq 2\\sigma) = P(|Z| \\leq 2) = 0.9544\\),\n\\(P(|X - \\mu| \\leq 3\\sigma) = P(|Z| \\leq 3) = 0.9974\\). 여기서 \\(X \\sim N(\\mu,\\sigma^{2}),Z \\sim N(0,1)\\) 이다.\n\n【특성】 정규분포 근사\n이항 분포가 정규 분포로 근사될 때 연속성 수정(Continuity Correction)을적용해야 한다. 이항 분포는 이산형 분포이므로, 연속형인 정규 분포로 근사할 때 작은 보정이 필요합니다.\n\n\\(P(X \\leq x) \\approx P(Y \\leq x + 0.5)\\)\n\\(P(X \\geq x) \\approx P(Y \\geq x - 0.5)\\)\n\n【성질】\n\n정규분포는 좌우 대칭 종모양 분포로 평균, 중앙값, 최대값 모두 동일하다.\n중심극한 정리: 모집단의 분포와 상관없이 표본크기가 충분히 큰 (\\(n \\geq 20\\sim 30\\)) 경우 확률표본의 표본 합과 표본 평균의 분포는 정규분포에 근사한다.\n만약 \\(n\\)이 충분히 크고 \\(p\\)가 0, 1에 아주 가까운 값이 아니면 \\(B(n,p) \\rightarrow N(np,npq)\\) 근사한다.\n만약 \\(\\lambda\\)이 충분히 크면 \\(P(\\lambda) \\rightarrow N(\\lambda,\\lambda)\\) 근사한다.\n만약 \\(r\\)이 충분히 크면 \\(\\chi^{2}(r) \\rightarrow N(r,2r)\\) 근사한다.\n\\(X \\sim N(\\mu,\\sigma^{2})\\) 이면 \\(a + bX \\sim N(a + b\\mu,b^{2}\\sigma^{2})\\) 정규분포를 따른다. 단, \\(a,b\\)는 상수이다.\n\\(X \\sim N(\\mu,\\sigma^{2})\\) 이면 \\(e^{X} \\sim lnN(\\mu,\\sigma^{2})\\) 로그 정규분포를 따른다.\n\\(Z \\sim N(0,1)\\)이면 \\(Z^{2} \\sim \\chi^{2}(1)\\) 자유도 1인 카이제곱분포를 따른다.\n만약 \\(X \\sim N\\left( \\mu_{1},\\sigma_{1}^{2} \\right),Y \\sim N(\\mu_{2},\\sigma_{2}^{2})\\) 이고 서로 독립이면 \\(X \\pm Y \\sim N(\\mu_{1} \\pm \\mu_{2},\\sigma_{1}^{2} + \\sigma_{2}^{2})\\)이다.\n만약 \\(X \\sim N\\left( \\mu_{1},\\sigma_{1}^{2} \\right),Y \\sim N(\\mu_{2},\\sigma_{2}^{2})\\) 이고 서로 독립이 아니면 \\(X \\pm Y \\sim N(\\mu_{1} \\pm \\mu_{2},\\sigma_{1}^{2} + \\sigma_{2}^{2} \\pm 2cov(X,Y))\\)이다.\n\n【분포 가법성】 \\(X_{i} \\sim N\\left( \\mu_{i},\\sigma_{i}^{2} \\right)\\) 이고 서로 독립이면 \\(\\sum_{i}^{}{X_{i} \\sim N(\\sum_{i}^{}\\mu_{i},\\sum_{i}^{}\\sigma_{i}^{2})}\\) 이다.\n【예제】 학생들의 SAT 점수는 평균 75, 표준편차 10인 정규분포를 따른다고 하자. (1) 60점~80점 사이 학생의 비율은? (2) 상위 10% 학생의 점수는 몇 점인가? 【풀이】 (1) 0.625 (2), 87.8점\n【예제】 생산하는 볼트 지름의 크기는 평균 950mm, 표준편차 10mm 인 정규분포를 따른다고 한다.\n1. 볼트를 하나 선택했을 때 그것의 지름이 947~958mm일 확률을 계산하라.\n\n\n\n\n\n2. 볼트지름이 상수 \\(k\\) 보다 적을 확률이 0.9일 경우 상수 \\(k\\) ?\n\n\n\n\n\n\n\n4. 베타분포 \\(X \\sim B(\\alpha,\\beta)\\)\n0과 1 사이의 값을 갖는 연속형 확률변수에 대한 분포는 베타분포이다. 이 분포는 두 개의 양의 실수 매개변수 (, )를 가지며, 이들 형상(shape) 매개변수에 따라 분포의 형태가 다양하게 조정된다. 특징은 다음과 같다:\n\n\\((\\alpha, \\beta)\\) = (1,1)인 경우는 모든 값에 대해 균일한 확률을 가지는 균일분포와 동일하다.\n\\(\\alpha = \\beta\\) 이면 분포는 좌우 대칭을 이룬다.\n\\(\\alpha &gt; \\beta\\) 인 경우에는 왼쪽으로 치우친 분포(우측에 봉우리가 있음)를 가지며, 값이 1에 가까운 쪽에서 더 큰 밀도를 가진다.\n\\(\\alpha &lt; \\beta\\) 인 경우에는 오른쪽으로 치우친 분포(좌측에 봉우리가 있음)를 가지며, 값이 0에 가까운 쪽에서 더 큰 밀도를 가진다.\n\n베타분포는 다음과 같은 분야에서 널리 활용된다.\n\n베르누이 및 이항 분포의 모수(p)에 대한 사전 분포(prior distribution)로 자주 사용된다. 이는 베이지안 추론에서 중요한 역할을 한다.\n감마분포의 비율로부터 유도될 수 있으며, 감마분포와 수학적으로 밀접한 관련이 있다.\n\n이처럼 베타분포는 단순한 형태에서부터 매우 다양한 분포 형태를 표현할 수 있어, 불확실성을 표현하는 데 매우 유용한 도구이다.\n베타함수\n베타 함수는 적분과 관련된 특수 함수로서, 주로 베타 분포의 확률밀도함수를 정규화하는 데 사용된다. 이 함수는 두 개의 양의 실수 값을 입력으로 받아 정의되며, 입력값이 양수일 때에만 정의된다. 베타 함수의 가장 중요한 역할은 베타 분포의 확률밀도함수가 전체 구간에서 적분했을 때 1이 되도록 만드는 정규화 상수로 작용하는 것이다. 그러나 베타 함수의 값을 직접 계산하는 것은 복잡할 수 있기 때문에, 일반적으로 수치적 계산 방법이나 감마 함수와의 관계를 활용하여 값을 구하는 방식이 널리 사용된다. 이처럼 베타 함수는 통계학, 확률론, 베이지안 추론 등 다양한 분야에서 핵심적인 도구로 활용된다.\n\\[ B(\\alpha,\\beta) = \\frac{\\Gamma(\\alpha)\\Gamma(\\beta)}{\\Gamma(\\alpha + \\beta)} \\]\n확률질량함수\n\\[f(x|\\alpha\\beta) = \\frac{1}{B(\\alpha,\\beta)}x^{\\alpha - 1}(1 - x)^{1 - \\beta},0 &lt; x &lt; 1,\\alpha &gt; 1,\\beta &gt; 1\\]\n베타 분포는 2개의 매개변수가 있는데 위치 모수인 \\(\\alpha\\)과 형상 모수인 \\(\\beta\\)이다.\n평균과 분산\n\\(E\\lbrack X\\rbrack = \\frac{\\alpha}{\\alpha + \\beta}\\), \\(Var(X) = \\frac{\\alpha\\beta}{(\\alpha + \\beta)^{2}(\\alpha + \\beta + 1)}\\)\n【활용】\n베타 분포는 다양한 분야에서 활용되며 특히 이진형 결과를 다루는 베르누이 분포의 모수 추정에 있어 중요한 역할을 한다.\n우선, 베르누이 분포의 사전 분포로 베타 분포를 사용하는 경우가 대표적이다. 베르누이 분포는 성공과 실패처럼 두 가지 결과를 갖는 확률 실험을 모델링할 때 쓰이며, 이때 성공 확률인 모수 p에 대한 불확실성을 반영하기 위해 베타 분포를 사전 분포로 설정한다. 이후 관측된 데이터를 바탕으로 사후 분포를 계산함으로써 p에 대한 추론을 수행할 수 있다. 이와 같은 방식은 베르누이 분포의 일반화인 이항 분포에도 동일하게 적용된다.\nA/B 테스트와 같은 실험 설계에서도 베타 분포는 널리 사용된다. 예를 들어, 웹사이트의 두 버전(A와 B)에 대해 사용자 반응을 비교하고자 할 때, 각 버전에 대한 성공률을 베타 분포로 모델링한다. 실험을 통해 얻은 성공 횟수와 실패 횟수를 이용하여 베타 분포의 매개변수를 업데이트하면, 두 버전의 효과 차이를 정량적으로 비교할 수 있다.\n베이지안 추론의 틀 안에서도 베타 분포는 중요한 사전 분포로 자리잡고 있다. 특히 모수가 0과 1 사이의 확률 값일 때, 베타 분포는 유연하게 다양한 형태의 사전 신념을 표현할 수 있어 유용하다. 관측 데이터를 통해 사후 분포를 업데이트하면서 베이지안 방식의 추정이 가능해진다.\n의료 및 생물학 분야에서도 베타 분포는 생존률, 회복률, 치료 성공률 등 이진형 결과를 확률적으로 모델링할 때 자주 사용된다. 예를 들어 임상시험에서 특정 치료의 효과를 판단할 때, 환자의 회복 여부에 대한 확률을 베타 분포로 표현함으로써 불확실성을 고려한 해석이 가능해진다.\n【성질】\n\n매개변수 \\(\\alpha,\\beta\\)가 변함에 따라 베타분포는 다양한 형태를 가질 수 있다.\n\n\n단조 증가: \\(\\alpha &gt; 1,\\beta = 1\\),\n\n\n단조 감소: \\(\\alpha = 1,\\beta &gt; 1\\), U자형 분포: \\(\\alpha &lt; 1,\\beta &lt; 1\\),\n\n\n단봉형 분포: \\(\\alpha &gt; 1,\\beta &gt; 1\\),\n\n\n그리고 \\(\\frac{1}{2}\\)을 중심으로 대칭: \\(\\alpha = \\beta\\)\n\n\n\\(\\alpha = \\beta = 1\\) 베타 분포는 균등 분포(Uniform(0,1))가 된다.\n\\(X \\sim \\Gamma(\\alpha,\\lambda),Y \\sim \\Gamma(\\beta,\\lambda)\\)이고 서로 독립이면 \\(\\frac{X}{X + Y} \\sim B(\\alpha,\\beta)\\)\n\\(X \\sim B(\\alpha,\\beta)\\)이면 \\(1 - X \\sim B(\\beta,\\alpha)\\)\n\\(X \\sim B(\\alpha = 1,\\beta)\\)이면 \\(- \\ln(X) \\sim exponential(\\beta)\\)\n\n【Frequentist vs. bayesian】\n\n\n\n\n\n어느 지역의 감기 환자 비율(\\(p\\))을 알아보기 위하여 표본크기 \\(n\\)의 확률표본을 \\((x_{1},x_{2},\\ldots,x_{n})\\)을 추출하였다고 하자.\n【Frequentists】 \\(\\sum_{i}^{n}{X_{i} \\sim B(n,p)}\\)이므로  \\(\\overset{\\hat{}}{p} = \\frac{\\sum_{i}^{n}X_{i}}{n}\\)이고 샘플링 분포 \\(\\overset{\\hat{}}{p} \\sim N(np,np(1 - p))\\)을 이용하여 신뢰구간과 가설검정을 한다.\n【Bayesian】 (1) 모수 \\(p\\)에 대한 사전확률을 \\(\\pi(p) \\sim U(0,1)\\)을 가정하자. (2) 모수 \\(p\\)에 대한 사후확률 \\(\\pi\\left( p|x_{1},x_{2},\\ldots,x_{n} \\right) \\sim B(\\sum X_{i} + 1,n - \\sum X_{i} + 1)\\)이다. (3) 모수에 대한 베이지안 추정치는 제곱오차 손실함수 방법을 적용할 시 사후확률 평균 \\(\\frac{\\sum X_{i} + 1}{n + 2}\\)이다.\n\n\n5. 코시 분포 \\(X \\sim Cauchy(\\theta)\\)\n코시 분포(Cauchy distribution)는 \\(( - \\infty,\\infty)\\)에서 정의된 대칭적인 종 모양의 분포이다. \\(\\theta\\)는 위치모수이다.\n확률밀도함수\n\\[f(x|\\theta) = \\frac{1}{\\pi}\\frac{1}{1 + (x - \\theta)^{2}}, - \\infty &lt; x &lt; \\infty, - \\infty &lt; \\theta &lt; \\infty\\]\n평균과 분산 존재하지 않음\n【코시 분포의 특성】\n\n기대값과 분산이 존재하지 않는다.\n중앙 위치를 나타내는 모수: \\(\\theta\\)는 분포의 중심을 나타내며, 코시 분포의 중앙값이다. \\(P(X \\geq \\theta) = \\frac{1}{2}\\)\n정규 분포와의 차이점: 코시 분포는 정규 분포와 비슷한 대칭적인 종 모양을 가지지만, 꼬리가 훨씬 두껍다.\n\n【활용】\n로버스트 회귀 분석: 코시 분포는 극단적인 이상치(outliers)가 존재하는 데이터에 강건한(robust) 성질을 제공하기 때문에 로버스트 회귀 분석에서 활용된다. 일반 선형 회귀(OLS)에서는 정규 분포를 가정하지만, 이상치가 많으면 회귀 계수가 크게 왜곡될 수 있다. 코시 분포를 활용한 M-추정 기법은 이상치의 영향을 줄이는 데 유용하다.\n두 개의 독립적인 표준 정규 분포를 따르는 변수의 비율: 만약 X와 Y가 서로 독립이고 표준 정규 분포 N(0,1)을 따른다면, \\(Z = \\frac{X}{Y}\\) 는 표준 코시 분포, \\(Cauchy(0,1)\\)를 따른다. 특히, 키와 몸무게의 비율을 고려할 때 비정상적인 분포 형태가 나타날 수 있는 경우 활용된다.\n\n\n6. 로그정규분포 \\(Log(X) \\sim N(\\mu,\\sigma^{2})\\)\n만약 확률 변수 \\(X\\)의 로그 값이 정규 분포를 따른다면 \\(\\log X \\sim N(\\mu,\\sigma^{2})\\) 이때, X는 로그 정규 분포를 가진다고 한다.\n확률밀도함수\n\\[f(x|\\mu,\\sigma^{2}) = \\frac{1}{\\sqrt{2\\pi}\\sigma}\\frac{1}{x}e^{- (\\log x - \\mu)^{2}/(2\\sigma^{2})},0 &lt; x &lt; \\infty, - \\infty &lt; \\mu &lt; \\infty,\\sigma &gt; 0.\\]\n평균과 분산\n\\(E(X) = e^{\\mu + (\\sigma^{2}/2)}\\), \\(Var(X) = e^{2(\\mu + \\sigma^{2})} - e^{2\\mu + \\sigma^{2}}\\)\n【로그 정규 분포와 감마 분포의 유사성】\n로그 정규 분포는 확률 변수가 양의 실수 범위에 있고 오른쪽으로 치우쳐 있는 경우에 적합한 확률 분포로, 감마 분포와 유사한 비대칭 형태를 가진다. 이 분포는 확률 변수의 로그를 취했을 때 그 값이 정규 분포를 따르는 경우를 의미한다.\n대표적인 예로 소득 분포를 들 수 있다. 현실에서 소득은 대부분의 사람이 중간 정도의 값을 가지며, 소수의 사람이 매우 높은 소득을 가지는 구조를 보여 오른쪽으로 긴 꼬리를 갖는 분포가 된다. 이 경우 원자료를 그대로 정규 분포로 가정하기에는 적절하지 않으며, 로그 변환을 통해 분포의 비대칭성을 줄이고 정규성 가정을 충족시킬 수 있다. 이처럼 로그 정규 분포는 비대칭적 데이터를 정규화하여 통계적 분석이나 추론을 가능하게 하며, 경제, 생물학, 환경 분야 등 다양한 영역에서 활용된다.\n【활용】\n로그 정규 분포는 다양한 분야에서 자연스럽게 나타나는 비대칭적이고 양의 값만을 갖는 데이터를 설명하는 데 유용하게 사용된다. 다음과 같은 사례들이 대표적이다.\n\n자산 가격 및 주가 변동: 금융 분야에서 주식이나 자산의 로그 수익률이 정규 분포를 따른다고 가정하면, 원래 자산 가격은 로그 정규 분포를 따른다. 이는 블랙-숄즈 옵션 가격 모형(Black-Scholes Model)의 핵심 가정 중 하나로, 주식 가격이 시간에 따라 지수적으로 변동한다는 점을 설명하는 데 매우 유용하다.\n병원체 성장 및 전파 모델링: 박테리아, 바이러스와 같은 병원체의 증식 속도는 초기에는 기하급수적으로 증가한 후 일정 수준에서 포화된다. 이때 각 개체의 성장량이나 전파 시간 등의 분포는 로그 정규 분포로 모델링할 수 있다. 감염병의 전파 속도, 감염 후 잠복기 분포 등도 이 분포로 설명될 수 있다.\n지진 강도 및 환경 데이터: 지진의 에너지 방출량 또는 강도는 로그 정규 분포를 따를 수 있으며, 이는 소수의 매우 강한 지진이 존재하고 대부분은 약한 지진이라는 현상을 반영한다. 마찬가지로 대기 중의 오염 물질 농도(예: 미세먼지, 중금속 등)도 로그 정규 분포를 따르는 경향이 있으며, 환경 분야에서 자주 활용된다.\n\n이처럼 로그 정규 분포는 다양한 분야에서 실측 데이터를 보다 잘 설명하기 위한 실용적인 확률 분포로 자리 잡고 있다.\n\n\n7. 이중지수분포 \\(X \\sim DE(\\mu,\\sigma)\\)\n이중 지수 분포(double exponential distribution)는 지수 분포를 확장하여 평균을 중심으로 대칭적인 형태를 갖도록 만든 분포이다. 이 분포는 중심값을 기준으로 좌우가 대칭이지만, 정규 분포와 달리 중심점에서 뾰족한 꼭짓점을 갖는 특징이 있다. 이는 분포가 \\(x = \\mu\\) 에서 미분 가능하지 않음을 의미한다.\n이중 지수 분포는 정규 분포보다 두꺼운 꼬리를 가지므로 극단값(outlier)에 더 민감하다. 이러한 특성 덕분에 이상값에 강건한 회귀 분석, 신호처리, 베이지안 추론 등 다양한 통계적 모델에서 활용된다. 특히, 모든 모멘트가 존재한다는 점에서 수학적으로 다루기 용이한 특성을 지니며, 종 모양은 아니지만 분포의 중심을 기준으로 대칭성과 꼬리 두꺼움을 동시에 고려해야 할 경우 유용하다.\n확률밀도함수\n\\[f(x|\\mu,\\sigma) = \\frac{1}{2\\sigma}e^{- |x - \\mu|/\\sigma}, - \\infty &lt; x &lt; \\infty, - \\infty &lt; \\mu &lt; \\infty,\\sigma &gt; 0\\]\n평균과 분산\n\\(E(X) = \\mu\\), \\(Var(X) = 2\\sigma^{2}\\)\n【활용】\n이중 지수 분포는 꼬리가 두껍고 중심에서 뾰족한 형태의 분포로, 이상치에 강건한 특성 덕분에 다음과 같은 분야에서 널리 활용된다.\n1. 로버스트 회귀 분석: 이상치에 덜 민감한 회귀 분석 기법을 개발하기 위해 이중 지수 분포가 활용된다. 일반적인 최소자승법(OLS)은 오차항이 정규분포를 따른다는 가정하에 작동하지만, 이 가정은 이상치가 존재할 경우 쉽게 깨진다. 반면, 오차항이 이중 지수 분포를 따른다고 가정하면, 평균 절대 오차(Mean Absolute Error)를 최소화하는 L1 손실 함수를 사용하는 회귀 분석이 가능하며 이는 이상치에 더 강건하다.\n2. 신호 처리 및 이미지 처리: 신호의 잡음을 제거하거나 이미지를 보정하는 과정에서 이상치나 급격한 변화에 민감하지 않은 필터링이 중요하다. 이중 지수 분포를 기반으로 한 라플라스 필터링 기법은 정규분포 기반 방법보다 이상값에 덜 민감하므로, 음성 신호나 이미지의 노이즈 제거에 효과적이다.\n3. 금융 자산 가격 변동 모델링: 금융 시장에서는 자산 가격이 급격하게 변동하는 경우가 많으며, 이러한 극단값은 정규 분포로는 설명하기 어렵다. 이중 지수 분포는 꼬리가 두꺼운 특성을 지녀, 주가 수익률이나 환율의 급격한 변화 등 비정상적이고 큰 변동을 더 현실적으로 반영할 수 있다. 이로 인해 리스크 관리나 옵션 가격 모델링 등에서 유용하게 활용된다.\n\n\n8. t-분포 \\(X \\sim t(df)\\)\nt-분포는 영국의 통계학자 스튜던트(Student, 본명 William Sealy Gosset)가 작은 표본의 평균에 대한 통계적 추론을 위해 개발한 분포이다. 당시 그는 기네스 양조장에서 실험 양조사로 근무하던 중, 작은 표본에서 표준 정규 분포를 그대로 사용하는 데 한계가 있다는 점을 인식하였다.\n특히, 모집단의 분산을 알 수 없고 이를 표본으로부터 추정해야 하는 경우, 표본 평균의 분포는 정규 분포가 아니라 보다 분산이 큰 새로운 분포를 따른다는 점에 주목하였다. 이 문제의 해결책으로 1908년에 발표한 논문 「The Probable Error of a Mean」에서 t-분포를 제시하였고, 이를 통해 작은 표본에서의 평균에 대한 신뢰구간과 가설 검정 방법을 제시하였다.\n이후 피셔(Ronald A. Fisher)는 스튜던트의 연구를 확장하여 t-분포를 다양한 통계적 추론 문제에 적용하였으며, 특히 표본 크기와 모집단 분산의 추정 여부에 따라 t-분포의 형태가 달라진다는 점을 체계화하였다.\nt-분포는 표본 수가 작을수록 꼬리가 두꺼운 형태를 가지며, 이상치에 대해 보다 강건한 특성을 지닌다. 이로 인해 작은 표본이나 이상치가 존재할 수 있는 데이터에서 평균에 대한 추론을 수행할 때 유용하게 활용된다.\n【분포 유도】\n\\(Z \\sim N(0,1)\\), \\(W \\sim \\chi^{2}(n)\\)\\(W \\sim \\chi^{2}(\\nu)\\)이고 독립이면 \\(\\frac{Z^{2}}{\\sqrt{W/\\nu}} \\sim t(\\nu)\\)이다. 모수 \\(\\nu\\)는 자유도이다.\n확률밀도함수\n\\[f(x) = \\frac{\\Gamma\\left( \\frac{\\nu + 1}{2} \\right)}{\\sqrt{\\nu\\pi}\\Gamma\\left( \\frac{\\nu}{2} \\right)}\\left( 1 + \\frac{x^{2}}{\\nu} \\right)^{- \\frac{\\nu + 1}{2}}, - \\infty &lt; x &lt; \\infty,\\nu &gt; 0.\\]\n평균과 분산\n\\[E(X) = 0,V(X) = \\frac{n}{n - 2}\\]\n【활용】\nt-분포는 다음과 같은 상황에서 유용하게 사용된다.\n\n소표본에서의 평균 추론: 표본 크기가 작을 때(일반적으로 n &lt; 30), 표본 평균에 대한 신뢰구간을 계산하거나 가설 검정을 할 때 정규 분포 대신 t-분포를 사용한다. 이는 소표본일수록 표본 평균의 변동성이 커지기 때문이다.\n모집단 분산을 모를 때: 모집단의 분산을 알 수 없고, 이를 표본 분산으로 추정해야 하는 경우, 표본 평균의 분포는 정규 분포가 아니라 t-분포를 따른다. 이때 자유도는 (표본 크기 – 1)로 설정된다.\nt-검정의 활용: 평균에 대한 통계적 가설 검정에서 t-분포를 기반으로 한 다양한 검정이 사용된다. 대표적으로는\n\n\n1. 단일 표본 t-검정: 한 집단의 평균이 특정 값과 다른지를 검정  2. 독립 표본 t-검정: 두 독립된 집단의 평균 차이를 검정  3. 대응 표본 t-검정: 동일한 집단에 대해 두 조건(예: 전후 변화)의 평균 차이를 검정 4. 회귀계수 유의성 검정: 선형모형의 회귀계수의 유의성을 검정\n\n이처럼 t-분포는 불확실한 분산 추정과 작은 표본의 상황에서 평균 추론을 수행할 수 있도록 돕는 중요한 통계적 도구이다.\n【성질】\n\n자유도 \\(n\\)이 커지면 \\(X \\sim t(n)\\)은 표준정규분포 \\(N(0,1)\\)에 근사한다.\n\\(\\nu = 1\\): 코시 분포와 동일하다.\n\n\n\n8. F-분포 \\(X \\sim F(df_{1},df_{2})\\)\nF-분포는 두 모집단의 분산을 비교하거나 분산 분석(ANOVA)에서 그룹 간 차이를 검정할 때 사용하는 확률 분포이다. 이 분포는 비대칭적이며 오른쪽으로 꼬리가 긴 형태를 가지며, 특히 큰 값에서의 확률이 상대적으로 높다. F-분포의 형태는 분자와 분모의 자유도(\\(n_1\\), \\(n_2\\))에 따라 달라지며, 자유도가 커질수록 F-분포는 정규 분포에 가까워지는 성질을 가진다.\n통계 분석에서는 주로 다음과 같은 상황에서 F-분포를 사용한다. 두 분산 추정량의 비율을 비교하여 모집단의 분산이 동일한지를 검정하거나, 세 개 이상의 집단 평균 차이를 검정하기 위해 분산 분석을 수행할 때 그룹 간 변동과 그룹 내 변동의 비율을 계산하여 이 비율이 F-분포를 따르는지 판단한다.\nF-분포는 영국의 통계학자 로널드 A. 피셔(Ronald A. Fisher)에 의해 제안되었다. 그는 실험 데이터를 분석할 때 그룹 간 차이와 그룹 내 변동을 동시에 고려할 수 있는 통계량이 필요하다고 보았고, 이 두 변동의 제곱합 비율이 특정 확률 분포를 따른다는 점을 발견했다. 1924년, 피셔는 이 이론을 바탕으로 F-분포에 대한 논문을 발표하였고, 이후 F-분포는 실험 설계와 분산 분석 분야에서 핵심적인 도구로 자리 잡게 되었다.\n【F분포 유도】\n\\(X \\sim \\chi_{(d1)}^{2}\\), \\(Y \\sim \\chi_{(d2)}^{2}\\)이고 서로 독립이면 \\(\\frac{\\frac{X}{d_{1}}}{\\frac{Y}{d_{2}}} \\sim F(d_{1},d_{2})\\)\\(\\frac{\\frac{X}{n_{1}}}{\\frac{Y}{n_{2}}} \\sim F(n_{1},n_{2})\\)\n확률밀도함수\n\\[f(x) = \\frac{\\Gamma\\left( \\frac{d_{1} + d_{2}}{2} \\right)}{\\Gamma\\left( \\frac{d_{1}}{2} \\right)\\Gamma\\left( \\frac{d_{2}}{2} \\right)} \\cdot \\left( \\frac{d_{1}}{d_{2}} \\right)^{\\frac{d_{1}}{2}} \\cdot x^{\\frac{d_{1}}{2} - 1} \\cdot \\left( 1 + \\frac{d_{1}}{d_{2}}x \\right)^{- \\frac{d_{1} + d_{2}}{2}},x &gt; 0\\]\n여기서 \\(d_{1}\\)은 분자 자유도, \\(d_{2}\\)은 분모 자유도이다.\n평균과 분산\n\\[E(X) = \\frac{n_{2}}{n_{2} - 2},V(X) = complicated\\]\n【활용】\n\n독립 두 모집단 분산 차이 검정\n분산분석에서 모형, 요인들의 유의성 검정\n\n【성질】\n\n\\(X \\sim B(\\frac{n_{1}}{2},\\frac{n_{2}}{2})\\) 이면 \\(\\frac{n_{2}X}{n_{1}(1 - X)} \\sim F(n_{1},{n}_{2})\\) 이다.\n\\(X\\sim F\\left( n_{1},n_{2} \\right)\\) 이면 \\(\\lim_{n_{2} \\rightarrow \\infty}{n_{1}X} \\sim \\chi^{2}(n_{1})\\) 이다.\n\\(X\\sim F\\left( n_{1},n_{2} \\right)\\) 이면 \\(\\frac{1}{X}\\sim F\\left( n_{2},n_{1} \\right)\\) 이다.\n\\(X\\sim t(n)\\) 이면 \\(X^{2}\\sim F(1,n)\\) 이다.\n\n\n\n9. 연속형분포 관계도\n\n\n\n\n\n\n\n\nchapter 3. 지수족, 확률 관련 등식, 부등식\n\n1. 지수족 정의\n확률밀도함수 또는 확률질량함수의 한 가족을 지수족(exponential family)이라고 하며, 다음과 같은 형태로 표현될 수 있다.\n【지수족 정의】 \\(f(x|\\theta) = h(x)c(\\theta)\\exp\\left( \\overset{k}{\\sum_{i = 1}}w_{i}(\\theta)t_{i}(x) \\right)\\). 여기서,\n\n\\(h(x) \\geq 0\\) 및 \\(t_{1}(x),\\ldots,t_{k}(x)\\)는 관측값 \\(x\\)의 실수값 함수이며, 이들은 \\(\\theta\\)에 의존하지 않는다.\n\\(c(\\theta) \\geq 0\\) 및 \\(w_{1}(\\theta),\\ldots,w_{k}(\\theta)\\)는 매개변수 \\(\\theta\\)의 실수값 함수이며, 이들은 \\(x\\)에 의존하지 않는다.\n\n확률밀도함수 또는 확률질량함수가 지수 가족에 속하는지 확인하려면 함수 \\(h(x),c(\\theta),w_{i}(\\theta),t_{i}(x)\\)을 식별하고 이들이 위의 형태를 만족하는지 보이면 된다. 이에 포함되는 연속형 분포로는 정규분포, 감마분포, 베타분포가 있으며, 이산형 분포로는 이항분포, 포아송분포, 음이항분포가 있다.\n이항분포 지수족\n\\(n\\)을 양의 정수라고 하고 이항 분포 \\(\\text{Binomial}(n,p)\\)를 고려하자. 여기서 \\(0 &lt; p &lt; 1\\)이다. 이 분포의 확률질량함수는 다음과 같다.\n\\[\\begin{matrix}\nf(x|p) & = \\binom{n}{x}p^{x}(1 - p)^{n - x} = \\binom{n}{x}(1 - p)^{n}\\left( \\frac{p}{1 - p} \\right)^{x} \\\\\n& = \\binom{n}{x}(1 - p)^{n}\\exp\\left( \\log\\left( \\frac{p}{1 - p} \\right)x \\right).\n\\end{matrix}\\]\n\\(\\begin{matrix}\n& h(x) = \\{\\begin{matrix}\n\\binom{n}{x}, & x = 0,\\ldots,n \\\\\n0, & \\text{그 외}\n\\end{matrix}c(p) = (1 - p)^{n},0 &lt; p &lt; 1, \\\\\n& w_{1}(p) = \\log\\left( \\frac{p}{1 - p} \\right),0 &lt; p &lt; 1,\\text{그리고}t_{1}(x) = x\n\\end{matrix}\\)이므로 \\(f(x|p) = h(x)c(p)\\exp\\lbrack w_{1}(p)t_{1}(x)\\rbrack\\)이 성립한다.\n특히, \\(h(x) &gt; 0\\)는 오직 \\(x = 0,\\ldots,n\\)인 경우에만 성립하며 \\(c(p)\\)는 \\(0 &lt; p &lt; 1\\)에서만 정의된다. \\(p = 0\\) 및 \\(p = 1\\)도 이항분포에 포함될 수 있지만 지수족은 아니다.\n지시(indicator) 함수\n집합 \\(A\\)의 지시 함수는 보통 \\(I_{A}(x)\\)로 표기되며, 다음과 같이 정의된다.\n\\(I_{A}(x) = \\{\\begin{matrix}\n1 & x \\in A \\\\\n0 & x \\notin A.\n\\end{matrix}\\) 또 다른 표기법으로는 \\(I(x \\in A)\\)가 있다.\n정규분포의 지수족\n\\[f(x|\\mu,\\sigma^{2}) = h(x)c(\\mu,\\sigma)\\exp\\left\\lbrack w_{1}(\\mu,\\sigma)t_{1}(x) + w_{2}(\\mu,\\sigma)t_{2}(x) \\right\\rbrack I_{( - \\infty,\\infty)}(x)\\]\n\n\n2. 지수족 정리\n만약 X 가 확률밀도함수 또는 확률질량함수가 다음과 같은 형태를 가지는 확률변수라면 다음이 성립한다.\n정리\n\\[E\\left( \\overset{k}{\\sum_{i = 1}}\\frac{\\partial w_{i}(\\theta)}{\\partial\\theta_{j}}t_{i}(X) \\right) = - \\frac{\\partial}{\\partial\\theta_{j}}\\log c(\\theta)\\]\n\\[\\text{Var}\\left( \\overset{k}{\\sum_{i = 1}}\\frac{\\partial w_{i}(\\theta)}{\\partial\\theta_{j}}t_{i}(X) \\right) = - \\frac{\\partial^{2}}{\\partial\\theta_{j}^{2}}\\log c(\\theta) - E\\left( \\overset{k}{\\sum_{i = 1}}\\frac{\\partial^{2}w_{i}(\\theta)}{\\partial\\theta_{j}^{2}}t_{i}(X) \\right)\\]\n비록 이러한 식들이 복잡해 보일 수 있지만, 특정 경우에 적용하면 상당히 깔끔하게 정리될 수 있다.이 식들의 장점은, 적분이나 합산을 미분으로 대체할 수 있다는 점이며, 더 직관적인 접근 방식이 된다.\n이항분포의 평균\n\\[\\frac{d}{dp}w_{1}(p) = \\frac{d}{dp}\\log\\frac{p}{1 - p} = \\frac{1}{p(1 - p)}\\]\n\\[\\frac{d}{dp}\\log c(p) = \\frac{d}{dp}n\\log(1 - p) = \\frac{- n}{1 - p}\\]\n지수족 정리로부터 \\(E\\left( \\frac{1}{p(1 - p)}X \\right) = \\frac{n}{1 - p}\\)이므로 \\(E(X) = np\\)이다.\n\n\n3. 위치 및 척도 가족\n확률분포 가족을 구성하는 대표적인 세 가지 기법은 위치 가족(Location Families), 척도 가족(Scale Families), 그리고 위치-척도 가족(Location-Scale Families)이다. 이들 분포 가족은 물리적으로 해석이 가능하며, 모델링에 유용하고 수학적으로도 편리한 성질을 갖는다.\n각 분포 가족은 하나의 표준 확률밀도함수(standard probability density function)를 기준으로 정의된다. 가족 내의 다른 모든 확률밀도함수는 이 표준 함수를 일정한 규칙(예: 위치 이동, 척도 조정 등)에 따라 변형하여 생성된다.\n【정리】 \\(f(x)\\)가 임의의 확률밀도함수이고 \\(\\mu\\)와 \\(\\sigma &gt; 0\\)가 임의의 상수라고 하자. 그러면 함수 \\(g(x|\\mu,\\sigma) = \\frac{1}{\\sigma}f\\left( \\frac{x - \\mu}{\\sigma} \\right)\\) 는 확률밀도함수이다.\n【정의】 \\(f(x)\\) 가 임의의 확률밀도함수라고 하자. 그러면 매개변수 \\(\\mu( - \\infty &lt; \\mu &lt; \\infty)\\) 에 의해 변환된 확률밀도함수 \\(f(x - \\mu)\\) 의 집합을 표준 확률밀도함수 \\(f(x)\\) 를 갖는 위치 가족이라고 하며, \\(\\mu\\) 를 이 가족의 위치 매개변수(location parameter) 라고 한다.\n만약 \\(X\\)가 확률밀도함수 \\(f(x - \\mu)\\)를 가지는 확률변수라면 \\(X\\)는 \\(X = Z + \\mu\\)로 표현될 수 있으며 여기서 \\(Z\\)는 확률밀도함수 \\(f(z)\\)를 가지는 확률변수이다. 어떤 실험이 물리적 상수 \\(\\mu\\)(예: 용액의 온도)를 측정하도록 설계되었다고 가정하자. 그러나 관측 과정에서 측정 오차가 발생할 수 있다. 따라서 실제 관측된 값 \\(X\\)는 \\(X = Z + \\mu\\)로 표현되며, 여기서 \\(Z\\)는 측정 오차를 나타낸다. 만약 이 분포의 확률밀도함수가 \\(f(z)\\)라면, 관측된 값 \\(X\\)의 확률밀도함수는 \\(f(x - \\mu)\\)가 된다.\n【정의】 \\(f(x)\\)가 임의의 확률밀도함수라고 하자. 그러면 임의의 \\(\\sigma &gt; 0\\) 에 대해, 매개변수 \\(\\sigma\\)에 의해 변환된 확률밀도함수 \\((1/\\sigma)f(x/\\sigma)\\)의 집합을 표준 확률밀도함수 \\(f(x)\\) 를 갖는 척도 가족이라고 하며, \\(\\sigma\\) 를 이 가족의 척도 매개변수(scale parameter) 라고 한다.\n척도 매개변수를 도입하면, 확률밀도함수를 늘이거나(\\(\\sigma &gt; 1\\)) 줄이는효과를 가지며, 그래프의 기본적인 형태는 유지된다. 일반적으로 척도 매개변수가 사용될 때 \\(f(x)\\)는 0에 대해 대칭이거나, \\(x &gt; 0\\)인 경우에만 양수를 갖는다. 감마분포는 \\(\\beta\\), 정규분포는 \\(\\sigma\\), 이중지수분포 \\(\\sigma\\)는 척도 매개변수이고, 만약 척도 매개변수를 1로 설정하면 표준(감마분포 \\(\\alpha\\) 고정, 이중지수분포와 정규분포는 \\(\\mu = 0\\)) 확률밀도함수를 얻을 수 있다.\n【정의】 \\(f(x)\\)가 임의의 확률밀도함수라고 하자. 그러면 임의의 \\(\\mu( - \\infty &lt; \\mu &lt; \\infty)\\) 및 \\(\\sigma &gt; 0\\)에 대해, 매개변수 \\((\\mu,\\sigma)\\)에 의해 색변환된 확률밀도함수 \\(\\frac{1}{\\sigma}f\\left( \\frac{x - \\mu}{\\sigma} \\right)\\)의 집합을 표준 확률밀도함수 \\(f(x)\\)를 갖는 위치-척도 가족이라고 하며, \\(\\mu\\)를 위치 매개변수, \\(\\sigma\\)를 척도 매개변수라고 한다.\n위치 및 척도 매개변수를 도입하면, 확률밀도를 늘이거나 축소한 후, 그래프를 이동시켜 원래 0 위에 있던 점이 이제 \\(\\mu\\)위에 오도록 한다. 정규 분포, 코시분포, 이중 지수분포는 위치-척도 가족의 예시이다.\n【정리】 \\(f( \\cdot )\\)가 임의의 확률밀도함수라고 하자. \\(\\mu\\)가 임의의 실수이고 \\(\\sigma\\)가 임의의 양의 실수라고 하자. 그러면 \\(X\\)가 확률밀도함수 \\(\\frac{1}{\\sigma}f\\left( \\frac{x - \\mu}{\\sigma} \\right)\\)를 가지는 확률변수일 필요충분조건은, 확률밀도함수 \\(f(z)\\)를 가지는 어떤 확률변수 \\(Z\\)가 존재하여 \\(X = \\sigma Z + \\mu\\)를 만족하는 것이다.\n위의 정리에서 \\(\\sigma = 1\\)로 설정하면 위치 가족의 결과를 얻을 수 있으며, \\(\\mu = 0\\)으로 설정하면 척도 가족의 결과를 얻을 수 있다.\n【정리】 \\(Z\\)가 확률밀도함수 \\(f(z)\\)를 가지는 확률변수라고 하자. 또한 \\(E(Z)\\)와 \\(V(Z)\\)가 존재한다고 가정하자. 만약 \\(X\\)가 확률밀도함수 \\(\\frac{1}{\\sigma}f\\left( \\frac{x - \\mu}{\\sigma} \\right)\\)를 가지는 확률변수라면 \\(E(X) = \\sigma E(Z) + \\mu,V(X) = \\sigma^{2}V(Z)\\)\n\n\n4. 확률분포함수 관련 부등식 및 등식\n\n(1) 체비셰프 부등식(Chebyshev’s Inequality)\n\\(X\\)를 확률변수라고 하고 \\(g(x)\\)를 음이 아닌 함수라고 하자. 그러면, 임의의 \\(r &gt; 0\\)에 대해 \\(P(g(X) \\geq r) \\leq \\frac{E(g(X))}{r}\\)이다.\n체비셰프 부등식의 가장 널리 사용되는 형태는 평균과 분산을 포함한다. \\(g(x) = \\frac{(x - \\mu)^{2}}{\\sigma^{2}}\\)라고 하자. 여기서 \\(\\mu = E(X)\\), \\(\\sigma^{2} = V(X)\\) 이다. 편의상 \\(r = t^{2}\\)라고 두면, \\(P\\left( \\frac{(X - \\mu)^{2}}{\\sigma^{2}} \\geq t^{2} \\right) \\leq \\frac{1}{t^{2}}\\mathbb{E}\\left( \\frac{(X - \\mu)^{2}}{\\sigma^{2}} \\right) = \\frac{1}{t^{2}}\\).\n\\(P(|X - \\mu| \\geq t\\sigma) \\leq \\frac{1}{t^{2}}\\) ⇔ \\(P(|X - \\mu| &lt; t\\sigma) \\geq 1 - \\frac{1}{t^{2}}\\)\n이 부등식은 표준편차 \\(\\sigma\\)를 기준으로 한 \\(|X - \\mu|\\)의 편차에 대한 보편적인 상한을 제공한다. 예를 들어, \\(t = 2\\)를 대입하면,\n\\(P(|X - \\mu| \\geq 2\\sigma) \\leq \\frac{1}{2^{2}} = 0.25\\). 즉, 어떤 확률변수의 값이 평균에서 2배 표준편차 이내에 있을 확률이 적어도 75% 임을 보장한다.이 결과는 확률변수 X 의 분포 형태와 무관하게 항상 성립한다.\n체비셰프 부등식 적률생성함수 버전\n\\(P(X \\geq a) = e^{- at}M_{X}(t)\\), \\(M_{X}(t)\\)는 확률변수 \\(X\\)의 적률생성함수이다.\n\n\n(2) 항등식: 포아송 분포의 재귀 관계(recursion relation)\n\\(P(X = x + 1) = \\frac{\\lambda}{x + 1}P(X = x)\\).\n\\(X\\)를 감마분포 \\(\\text{Gamma}(\\alpha,\\beta)\\)를 따르는 확률변수라고 하자. 여기서 확률밀도함수는 \\(f(x|\\alpha,\\beta)\\)이며, \\(\\alpha &gt; 1\\)이라고 가정한다. 그러면 임의의 상수 \\((a,b)\\)에 대해,\n【정리】 \\[P(a &lt; X_{\\alpha,\\beta} &lt; b) = \\beta\\left( f(a|\\alpha,\\beta) - f(b|\\alpha,\\beta) \\right) + P(a &lt; X_{\\alpha - 1,\\beta} &lt; b)\\]\n만약 \\(\\alpha\\)가 정수라면, 위를 반복적으로 사용하면 결국 적분이 발생하며, 이를 해석적으로 계산할 수 있다. (특히 \\(\\alpha = 1\\)인 경우, 이는 지수 분포가 된다.) 따라서 우리는 이러한 감마 확률을 쉽게 계산할 수 있다.\n\n\n(3) Stein’s Lemma\n\\(X \\sim N(\\theta,\\sigma^{2})\\)라고 하자. 또한, \\(g\\)가 미분 가능한 함수이고, \\(\\mathbb{E}|g'(X)| &lt; \\infty\\)를 만족한다고 가정하자. 그러면 다음이 성립한다. \\(\\mathbb{E}\\lbrack g(X)(X - \\theta)\\rbrack = \\sigma^{2}\\mathbb{E}g'(X)\\).\n【보조정리】 Stein의 보조정리(Stein’s Lemma)를 사용하면 고차 모멘트 계산이 상당히 쉬워진다. 예를 들어, \\(X \\sim N(\\theta,\\sigma^{2})\\)이면,\n\\(\\begin{matrix}\n\\mathbb{E}X^{3} = \\mathbb{E}X^{2}(X - \\theta + \\theta) = \\mathbb{E}X^{2}(X - \\theta) + \\theta\\mathbb{E}X^{2} \\\\\n= 2\\sigma^{2}\\mathbb{E}X + \\theta\\mathbb{E}X^{2} = 2\\sigma^{2}\\theta + \\theta(\\sigma^{2} + \\theta^{2}) = 3\\theta\\sigma^{2} + \\theta^{3}\n\\end{matrix}\\)"
  },
  {
    "objectID": "notes/linear_model/lm_concept.html",
    "href": "notes/linear_model/lm_concept.html",
    "title": "회귀분석 1. 개념&추정",
    "section": "",
    "text": "chapter 1. 회귀분석 개념\n\n1. 역사\n영국의 통계학자이자 유전학자인 Francis Galton(1822–1911) 은 식물 실험, 특히 완두콩(sweet pea) 씨앗 무게의 유전을 관찰하면서 ”평균으로의 회귀(regression toward the mean)” 현상에 주목하였다.\n큰 씨앗에서 나온 후대 씨앗은 크지만 부모만큼 극단적으로 크지 않고 평균에 더 가까워지는 경향을 보이며, 작은 씨앗에서 나온 후대 씨앗 역시 극단적으로 작지 않고 평균 쪽으로 이동하는 경향을 보인다.\nGalton은 이러한 관찰을 인간의 키(height) 연구로 확장하였다. 그는 수천 쌍의 부모–자녀 키 자료를 수집하였고, 이 과정에서 ”부모 평균키”를 다음과 같이 정의하였다.\n\\(\\frac{FatherHeight + 1.08 \\times MotherHeight}{2}\\), 이는 어머니가 아버지보다 평균적으로 키가 작다는 점을 보정하기 위하여, 어머니 키에 1.08배 보정 계수를 곱한 것이다.\n\n부모 평균키와 자녀 키를 교차표와 산점도로 정리한 결과, 다음과 같은 패턴이 나타났다.\n부모 키가 평균 수준에 가까운 경우, 자녀 키는 직선적 비례 관계를 보인다.\n부모 키가 매우 크거나 작은 경우, 자녀 키는 부모만큼 극단적이지 않고 전체 평균 신장 쪽으로 더 가까워진다.\n\nGalton은 이 현상을 회귀(regression)라고 명명하였다. 이는 원래 평균으로 되돌아가는 경향을 의미하며, 여기에서 오늘날의 회귀분석 용어가 비롯되었다.\n\n\n\n\n\n\n\n2. 회귀모형\n함수관계\n측정형 변수들 사이의 함수적 관계에 관심을 갖는다. 함수관계 중에서도 해석이 가장 용이한 형태는 직선 함수관계이다. 상관분석은 변수들 간의 직선적 연관성을 파악하지만, 원인과 결과의 방향성을 고려하지 않는다.\n반면, 회귀분석은 변수들 간의 관계를 설명할 때 방향성을 명확히 한다. 즉, 어떤 변수를 독립변수(independent variable, input) 로 두어 결과에 영향을 주는 원인으로 간주하고, 이에 대응하는 결과 변수를 종속변수(dependent variable, output, 또는 목표변수 target variable) 로 설정한다. 회귀분석은 이러한 독립변수와 종속변수 간의 함수적 관계를 모형화하고 해석하는 방법이다.\n함수 형태 중 선형을 가장 선호\n\\(y = a + b_{1}X_{1} + b_{2}X_{2} + ... + b_{p}X_{p} + e\\)\n회귀분석에서는 여러 함수 형태 중에서 선형 함수(linear function) 를 가장 선호한다. 선형 모형이 선호되는 이유는 해석이 용이하기 때문이다. 회귀계수는 함수의 기울기에 해당하므로, 예측변수가 한 단위 증가(또는 감소)할 때 목표변수가 얼마나 변화하는지를 단위로써 직접적으로 해석할 수 있다.\n비선형 함수 형태의 모형이라 하더라도, 적절한 변환 을 통해 선형 함수로 변환할 수 있다. 예를 들어, 로그 변환은 대표적인 방법이다.\n\nCobb-Douglas 생산함수 \\(Q = \\alpha L^{\\beta}L^{\\lambda}u\\) (\\(Q\\)=생산량, \\(k\\)=자본, \\(L\\)=투입노동, \\(\\alpha,\\beta,\\lambda\\) 모수, \\(u\\)오차항) - 양변을 로그 변환하면 \\(ln(Q) = ln(\\alpha) + \\beta ln(L) + \\lambda ln(K) + ln(u)\\)\n인구성장모형 \\(P = \\alpha e^{\\beta T}u\\) (\\(P\\)= 총인구수, \\(T\\)=시간, \\(\\alpha,\\beta\\) 모수) 모두 양변에 로그를 취하여 선형함수로 만들 수 있음 \\(ln(P) = ln(\\alpha) + \\beta T + ln(u)\\)\n\n따라서 회귀분석에서는 선형 모형을 기본 틀로 두고, 필요할 경우 비선형 모형도 적절한 변환을 통해 선형 모형으로 분석하는 것이 가능하다.\n목표변수(\\(Y\\))와 분포 가정\n회귀분석에서 목표변수(종속변수) 는 하나이며, 원칙적으로 정규분포를 따르는 측정형 변수이다. 목표변수가 정규분포를 따르지 않는 경우에는 연결함수를 이용하여 종속변수를 정규분포로 변환한 후 선형 회귀모형을 적용한다. 특히, 종속변수가 이산형인 경우에는 적합한 연결함수를 사용한다.\n\n예를 들어, 종속변수가 성공/실패 와 같은 이분형 변수인 경우, 로짓 함수(logit function) 를 연결함수로 사용한다.\n\\(\\text{logit}(p) = \\ln\\left( \\frac{p}{1 - p} \\right)\\). 여기서 \\(p = P(Y = 1)\\)이며, Y는 성공(1) 또는 실패(0) 값을 갖는 이항 변수이다.\n\n이 경우 모형은 다음과 같이 표현된다.\n\\[\\ln\\left( \\frac{p}{1 - p} \\right) = \\beta_{0} + \\beta_{1}X_{1} + \\cdots + \\beta_{k}X_{k}\\]\n즉, 성공 확률 p 자체를 직접 모형화하는 대신, 성공 확률의 로그 오즈(log-odds)를 선형함수로 모형화하는 방식이다.\n회귀분석과 일반화 선형모형(GLM)\n회귀분석은 두 변수 간의 단순한 상관관계를 넘어, 원인과 결과의 방향성을 고려하여 함수적 관계를 설정하는 방법이다. 가장 기본적인 회귀모형은 선형 회귀모형으로, 종속변수가 정규분포를 따른다고 가정한다.\n그러나 현실에서 다루는 종속변수가 항상 정규분포를 따르는 것은 아니다. 예를 들어, 환자의 치료 성공 여부(성공/실패), 소비자의 구매 여부(구매/비구매)와 같이 두 가지 값만을 가지는 이항형 변수는 정규분포 가정과 맞지 않는다. 교통수단 선택과 같은 범주형 자료, 혹은 일정 기간 동안 발생한 사고 건수나 방문 횟수처럼 계수형 자료 역시 마찬가지이다. 이러한 경우 기존의 선형 회귀모형을 그대로 적용하면 예측 확률이 0과 1 사이를 벗어나거나, 분산 구조가 잘못 추정되는 등의 문제가 생긴다.\n이러한 한계를 극복하기 위해 도입된 개념이 연결함수이다. 연결함수는 종속변수를 직접 선형식으로 표현하지 않고, 종속변수의 기댓값을 적절히 변환한 후 이를 선형예측자와 연결한다. 예를 들어, 종속변수가 이항형인 경우에는 성공 확률 p 자체를 선형식으로 모형화할 수 없다. 대신, 성공 확률을 변환한 로짓 함수 \\(\\text{logit}(p) = \\ln\\left( \\frac{p}{1 - p} \\right)\\)를 사용하면, 변환된 값은 실수 전체 범위를 가지므로 선형예측자와 자연스럽게 연결할 수 있다. 따라서 이항 로지스틱 회귀모형은 \\(\\ln\\left( \\frac{p}{1 - p} \\right) = \\beta_{0} + \\beta_{1}X_{1} + \\cdots + \\beta_{k}X_{k}\\)의 형태를 갖는다. 이 경우 회귀계수는 독립변수가 한 단위 변할 때 성공 확률의 로그 오즈가 얼마나 변하는지를 의미한다.\n비슷한 방식으로, 종속변수가 어떤 사건의 발생 횟수처럼 계수형 자료라면 포아송 분포를 가정하고 로그 함수를 연결함수로 사용한다. 예를 들어, 평균 발생 건수를 \\(\\mu\\)라고 할 때, \\(\\ln(\\mu) = \\beta_{0} + \\beta_{1}X_{1} + \\cdots + \\beta_{k}X_{k}\\)로 모형화할 수 있다. 이 경우 회귀계수는 독립변수가 한 단위 변할 때 평균 발생 건수의 로그 값이 얼마나 변하는지를 나타낸다.\n이처럼 종속변수의 분포에 맞는 확률분포와 연결함수를 선택하여 선형모형의 틀을 확장한 것이 바로 일반화 선형모형(GLM, Generalized Linear Model) 이다. GLM은 크게 세 가지 구성요소를 가진다. 첫째, 종속변수는 정규분포, 이항분포, 포아송분포 등 지수분포족에 속한다고 가정한다. 둘째, 독립변수들의 선형 결합을 나타내는 선형예측자(linear predictor)를 정의한다. 셋째, 종속변수의 기댓값과 선형예측자를 연결하는 연결함수를 지정한다.\n이러한 체계를 통해 회귀분석은 정규분포를 따르는 연속형 종속변수에 국한되지 않고, 이산형 자료나 비정규분포 자료까지 분석할 수 있는 범용적인 모형으로 확장된다. 따라서 고전적 회귀분석은 GLM의 한 특별한 경우라 할 수 있으며, GLM은 회귀분석의 현대적 일반화라고 이해할 수 있다.\n예측변수(\\(X's\\))는 다수이고 역시 측정형변수를 원칙으로 한다.\n회귀분석에서 예측변수(독립변수) 는 다수일 수 있으며, 원칙적으로 측정형 변수를 사용한다. 예측변수가 하나인 경우를 단순회귀분석이라 하고, 두 개 이상인 경우를 다중회귀분석이라 한다.\n회귀분석은 함수관계에 기반하므로, 범주형 변수는 직접 사용할 수 없다. 예를 들어, 성별(남/여), 지역(서울/대전/부산)과 같은 범주형 변수는 숫자 연속값으로 정의되어 있지 않기 때문이다. 이러한 경우 회귀분석에서는 지시변수 또는 더미변수로 변환하여 사용한다.\n범주형 변수가 k개의 수준(level)을 가진다면, 회귀모형에는 k-1개의 더미변수를 포함시켜야 한다. 각 더미변수는 특정 범주에 속하면 1, 그렇지 않으면 0의 값을 갖는 이진형(binary) 변수이다. 예를 들어, 지역 변수가 세 수준(서울, 대전, 부산)을 가진다면 두 개의 더미변수를 생성하여, 각각 특정 지역 여부를 0과 1로 표시한다.\n이 방식은 기준 범주를 하나 설정하고, 나머지 범주들을 해당 기준과 비교하는 형태로 해석할 수 있게 한다. 따라서 범주형 변수 역시 측정형 변수와 함께 회귀모형에 포함될 수 있다.\n\n목표변수 \\(Y\\) : 수능성적, 측정형 예측변수 \\(X\\) : 일주일 공부시간, 더미변수 성별 \\(D = 0male,1female\\)이라 하자.\n\\(Y = a + b_{1}D + b_{2}X + b_{3}(X*D) + e\\)\n\n함수관계와 인과관계\n회귀분석은 변수들 사이의 함수적 관계를 밝히는 방법이지, 인과관계를 증명하는 방법은 아니다. 회귀분석의 목적은 단지 관심의 대상이 되는 목표변수에 직선적 함수 관계를 갖는 예측변수를 찾는 데 있다. 따라서 회귀분석 결과만으로 ”A가 B를 원인적으로 변화시킨다”고 단정할 수 없다.\n인과관계는 철저한 실험설계를 통해서만 확인할 수 있다. 예를 들어, ”공부 시간과 수능 성적 사이에 인과관계가 존재하는가?“라는 질문에 답하기 위해서는, 사회경제적 환경이 동일하고 학습 능력이 동일한 고등학교 3학년 학생 집단을 무작위로 선택한 뒤, 1년 동안 주당 공부 시간을 다르게 할당하여 관리하고, 이후 수능 성적을 측정해야 한다. 이렇게 외생적 요인을 통제하고 실험적으로 조작된 데이터를 수집해야만 두 변수 간의 인과관계를 명확히 밝힐 수 있다.\n반면, 실제 사회과학이나 경영·경제학 연구에서 다루는 자료는 대부분 횡단면 데이터 이거나 관측 자료이다. 이런 경우에는 연구자가 직접 변수를 조작할 수 없으므로, 인과관계를 단정하기 어렵다. 대신, 다른 요인들의 영향을 최대한 통제한 상태에서 예측변수와 목표변수 사이의 함수적 관계를 추정해야 한다. 이 과정에서 중요한 역할을 하는 변수가 있다.\n\n통제변수(control variable) : 다른 요인의 영향을 제거하기 위해 모형에 포함하는 변수\n매개변수(mediator variable) : 독립변수가 종속변수에 영향을 주는 경로를 매개하는 변수\n교락변수(confounding variable) : 독립변수와 종속변수 모두에 영향을 주어 관계를 왜곡시키는 변수\n\n따라서 회귀분석을 활용할 때에는 결과를 단순히 인과적 해석으로 확대하지 않고, 함수적 관계와 통제변수의 역할을 명확히 이해하는 것이 필요하다.\n\n\n\n\n\n회귀분석에서 중요한 개념은 교락(confounding) 이다. 교락변수는 독립변수와 종속변수 모두에 영향을 주어, 두 변수 사이의 함수관계를 왜곡할 수 있다.\n운동능력(X)과 체중 증가(Y)의 관계를 알고자 할 때, 연령(Z) 이 X와 Y 모두에 영향을 주면 Z는 교락변수이다. 일반적으로 연령이 높을수록 운동능력은 낮아지고, 체중 증가는 커질 가능성이 높다. 이때 연령을 모형에 포함하지 않으면, X와 Y 사이의 순수한 함수관계가 왜곡된다. 따라서 연령을 통제변수로 포함하여 동일 연령에서의 \\(X \\rightarrow Y\\)관계를 추정하는 것이 필요하다.\n\n\n3. 통제변수 매개변수 교락변수\n생략변수편의란 실제로는 종속변수 Y에 영향을 주는 중요한 변수를 회귀모형에서 누락시킴으로써, 포함된 예측변수의 회귀계수가 편의(bias) 를 가지게 되는 현상을 말한다. X는 예측변수(처치, 원인), Y는 목표변수(결과)이다.\n\n(1) 통제변수(control variable)\n모형에 포함하여 원하지 않는 변동을 제거하거나 편의를 줄이기 위해 사용하는 변수이다. 통제변수라는 말은 용도가 넓다. 실무에서는 크게 두 부류가 있다.\n교락변수(진짜 통제 대상): 편의를 없애려는 목적, \\(Z \\rightarrow X\\)이고 \\(Z \\rightarrow Y\\)이다.\n정밀도 변수: Y에만 강하게 연관되어 분산을 줄이는 변수(교락은 아님), \\(Z \\rightarrow Y\\)이고 \\(Z \\nrightarrow X\\)이다.\n통제 효과\n\n교락이면 포함해야 편의가 줄어든다.\n정밀도 변수면 포함해도 편의는 변하지 않지만 추정의 표준오차가 줄어들 수 있다.\n\n모형\n시험점수 예측에서 ”이전 학업성취도(중간고사 점수)“는 최종점수 Y의 변동을 설명하므로 정밀도를 높이는 통제변수이다.\n통제 전: \\(Y = \\alpha_{0} + \\alpha_{1}X + e\\)\n통제 후: \\(Y = \\beta_{0} + \\beta_{1}X + \\beta_{2}Z + u\\)\n여기서 \\(\\beta_{1}\\)은 연령 Z를 동일하게 고정했을 때 운동능력 X의 효과로 해석된다.\n통제 전: 운동능력과 체중 증가 사이의 혼합된 효과, \\(\\alpha_{1}\\).\n통제 후: 같은 연령에서 운동능력의 순수 효과, \\(\\beta_{1}\\).\n\n\n(2) 매개변수(mediator)\nX가 Y에 영향을 미치는 경로 중간에 위치하는 변수이다. \\(X \\rightarrow M \\rightarrow Y\\) 구조이다. ”효과가 어떻게 전달되는가?“를 설명한다.\n운동량 X → ”칼로리 소모” M → 체중변화 Y. 칼로리 소모는 매개이므로 총효과 추정에서는 포함하지 않는다.\n매개변수가 존재하면, 독립변수의 효과는 직접효과(매개변수를 거치지 않는 영향) 와 간접효과(매개변수를 통해 전달되는 영향) 로 분해할 수 있으며, 이 둘의 합이 총효과가 된다.\n직접효과 (Direct Effect) \\(X \\rightarrow Y\\)\nM을 거치지 않고 바로 Y에 영향을 미치는 효과로 회귀식으로 표현하면, \\(Y = \\beta_{0} + \\beta_{1}X + \\beta_{2}M + u\\), 여기서 \\(\\beta_{1}\\)이 바로 직접효과이다.\n간접효과 (Indirect Effect)\n\\(X \\rightarrow M \\rightarrow Y\\)경로를 따라 전달되는 효과이다.\n\n매개변수 방정식: \\(M = \\alpha_{0} + \\alpha_{1}X + v\\)\n종속변수 방정식: \\(Y = \\beta_{0} + \\beta_{1}X + \\beta_{2}M + u\\)\n\n이때 간접효과는 \\(\\alpha_{1} \\times \\beta_{2}\\)이다. 즉, X가 M에 영향을 주고, 그 M이 다시 Y에 영향을 주는 효과의 곱이다.\n총효과 (Total Effect)\n총효과는 직접효과 + 간접효과 이다. \\(\\beta_{1} + (\\alpha_{1} \\times \\beta_{2})\\)\nX: 운동능력, M: 칼로리 소모량, Y: 체중 변화\n운동능력이 직접적으로 체중 변화에 영향을 미친다면 → 직접효과\n운동능력이 칼로리 소모를 증가시키고, 그 칼로리 소모가 체중 변화로 이어진다면 → 간접효과\n\n\n(3) 교락변수(confounder)\nX와 Y에 동시에 원인으로 작용하여 \\(X \\rightarrow Y\\) 관계를 왜곡하는 변수이다. 교락을 통제하지 않으면 생략변수편의가 발생한다.\n\\(Z \\rightarrow X,Z \\rightarrow Y\\), 또는 인과적으로 동등한 구조(공통원인).\n모형: \\(Y = \\beta_{0} + \\beta_{1}X + \\beta_{2}Z + u,E(u \\mid X,Z) = 0\\)\nZ를 포함하면 백도어 경로가 차단되어 \\(X \\rightarrow Y\\)의 순수한 조건부 평균 효과를 추정할 수 있다.\n(참고) 백도어 경로: 관심 경로 \\(X \\rightarrow Y\\) 이외에, \\(X \\leftarrow Z \\rightarrow Y\\)처럼 뒤에서 돌아오는 경로를 의미하며, 이 경로는 교락을 유발하여, X와 Y 사이에 인과가 없는 ”가짜 상관”을 만들어낼 수 있다.\n모형에서 Z를 생략한 기울기 편의: \\(\\text{Bias}({\\widehat{\\alpha}}_{1}) = \\beta_{2} \\cdot \\frac{Cov(X,Z)}{Var(X)}\\)\n운동능력 X–체중증가 Y에서 연령 Z. 연령은 활동에도, 체중에도 영향 → 교락. 반드시 통제한다.\n\n\n(4) 비교\n운동능력 X–체중증가 Y에서 연령\n연령 Z : 활동 X와 체중증가 Y의 교락 → 통제한다.\n칼로리 소모 M : X의 매개 → 총효과면 제외, 직접효과면 포함.\n기저체지방률 W : Y의 강한 예측자이나 X의 원인은 아님 → 정밀도 통제로 포함 가능.\n병원 방문 여부 C : 활동과 연령의 공통 결과라면 콜라이더 → 통제하지 않는다.\n실무 판별 체크리스트\n시간 순서: 원인은 결과보다 앞서야 한다. Z가 X 이후에 발생하면 교락이 아니다(매개일 가능성).\n원인성: Z가 X와 Y의 공통 원인인가? 그렇다면 교락이다.\n경로 목적: 총효과를 원하면 매개를 빼고, 직접효과를 원하면 매개를 포함한다.\n정밀도 변수: Z가 Y 예측력을 높이지만 X에는 원인으로 작용하지 않는가? 그렇다면 포함해도 편의는 변하지 않으나 효율은 좋아진다.\n콜라이더 경계: 공통 결과나 선택 변수(병원 내 환자만 분석 등)를 통제하지 않는다.\n콜라이더 collider\n\\(X \\rightarrow C \\leftarrow Z\\), 여기서 \\(C\\)가 콜라이더다. 원래는 X와 Z가 독립일 수 있으나 C를 조건부로 통제(회귀모형에 포함, 표본을 C 값으로 제한)하면, X와 Z 사이에 가짜 상관이 생긴다. 이 현상을 콜라이더 편향 또는 선택편의라고 한다.\n\n\n\n4. 회귀분석 절차\n\n(1) 모형 설정\n첫 단계는 연구자가 관심을 두는 목표변수(종속변수) 와 그것에 영향을 미칠 것으로 가정되는 예측변수(독립변수) 를 명확히 설정하는 것이다. 이 과정은 단순히 통계적 기법을 적용하는 수준을 넘어, 연구 주제에 대한 이론적 근거와 선행연구를 바탕으로 이루어진다. 즉, ”어떤 변수가 결과에 영향을 줄 것인가?“를 합리적으로 규정해야 한다.\n\n\n(2) 데이터 전처리\n자료를 확보한 후에는 산점도를 그려 변수 간의 기초적 관계를 탐색하고, 필요할 경우 자료를 변환한다. 예를 들어, 분포가 비대칭인 경우 로그 변환을 통해 정규성 가정을 충족시키거나, 비선형 관계가 의심되면 변수 변환이나 다항항 추가를 고려한다. 또한 결측치 처리, 이상치 확인, 단위 일관성 확보 등의 기본 정제 작업도 이 단계에서 수행된다.\n\n\n(3) 모형 추정\n자료 준비가 완료되면 회귀모형을 실제로 적합한다. 가장 기본적인 방법은 최소제곱법(OLS, Ordinary Least Squares) 으로, 이는 잔차제곱합 \\(\\sum(y_{i} - \\widehat{y_{i}})^{2}\\)을 최소화하는 방식이다. 이 과정에서 회귀계수의 추정치가 얻어진다.\n\n\n(4) 유의 변수 선택\n추정된 모형 내에서 모든 변수가 실제로 유의한 것은 아니다. 일부 변수는 목표변수에 거의 영향을 주지 않을 수 있다. 따라서 회귀계수의 통계적 유의성 검정(t-검정 등)을 통해 목표변수에 유의미한 영향을 주는 변수를 식별한다. 필요할 경우 변수선택 기법(예: 단계적 회귀, LASSO 등)을 활용하여 최적의 설명변수 집합을 구성한다.\n\n\n(5) 다중공선성 진단\n예측변수들 사이에 지나치게 높은 상관관계가 존재하면, 회귀계수의 추정이 불안정해지고 해석이 왜곡될 수 있다. 이를 다중공선성(multicollinearity) 문제라고 한다. VIF(Variance Inflation Factor) 등 진단지표를 이용하여 다중공선성을 확인하고, 필요하다면 변수를 제거하거나 결합하여 문제를 완화한다.\n\n\n(6) 모형 진단 및 활용\n마지막 단계는 적합된 모형이 회귀분석의 가정을 충족하는지 점검하는 것이다.\n잔차 분석을 통해 선형성, 등분산성, 독립성, 정규성 가정을 확인한다.\n이상치와 영향치를 식별하여 분석 결과에 과도한 영향을 주는 자료가 없는지 검토한다.\n모형이 타당하다면, 각 변수의 효과 크기를 비교하여 중요도를 평가하고, 결과를 이용해 예측구간과 신뢰구간을 도출한다.\n이러한 일련의 절차를 통해 회귀분석은 단순한 통계 기법을 넘어, 연구자의 가설 검증과 정책적·실무적 의사결정을 지원하는 유용한 도구로 활용될 수 있다.\n\n\n\n\nchapter 2. 회귀모형\n\n1. 모형\n\\[Y_{i} = a + b_{1}X_{i1} + b_{2}X_{i2} + \\ldots + + b_{p}X_{ip} + e_{i}\\]\n\\[\\left\\lbrack \\begin{array}{r}\ny_{1} \\\\\ny_{2} \\\\\n... \\\\\ny_{n}\n\\end{array} \\right\\rbrack = \\begin{bmatrix}\n1 & x_{11} & ... & x_{1p} \\\\\n1 & x_{21} & ... & x_{2p} \\\\\n... & & & \\\\\n1 & x_{n1} & ... & x_{np}\n\\end{bmatrix}\\left\\lbrack \\begin{array}{r}\na \\\\\nb_{1} \\\\\n... \\\\\nb_{p}\n\\end{array} \\right\\rbrack + \\left\\lbrack \\begin{array}{r}\ne_{1} \\\\\ne_{2} \\\\\n... \\\\\ne_{n}\n\\end{array} \\right\\rbrack\\]\\[\\underset{¯}{y} = X\\underset{¯}{b} + \\underset{¯}{e}\\]\n\n첨자 \\(i\\)는 표본 데이터를 구별을 위한 것으로. \\(i = 1,2,...,n\\)이고 \\(n\\)은 총 표본의 크기이다.\n\\(Y\\) : 종속 dependent 변수, 반응 response 변수, 내생 endogeous 변수, 목표 target 변수 등으로 불리고 목표변수는 확률변수이다.\n\\(X's\\) : 독립 independent 변수, 설명 exploratory 변수, 외생 exogenous 변수, 예측 predictor, feature 변수 등으로 불리고 예측변수는 결정변수로 확률변수가 아니므로 분포함수를 갖지 않으며 모형 내에서 결정된다.\n\\(p\\) : 모형 내 고려되는 예측변수 개수\n\\(\\alpha\\) : 절편(intercept), 모든 예측변수 \\(X\\) 값들이 0 일때 목표변수 \\(Y\\)의 값\n\\(\\beta_{k}\\) : 예측변수 \\(X_{k}\\)의 회귀계수 - 예측변수 \\(X_{k}\\) 가 한 단위 증가할 때마다 목표변수 \\(Y\\)의 증가량(편미분 계수)\n\\(e_{i}\\) : 오차항(백색잡음)으로 정규분포를 가정한다.\n\n\n\n2. 가정\n\n(1) 선형성 linearity\n회귀분석은 목표변수(종속변수) 와 예측변수(독립변수) 사이의 관계가 직선 형태를 따른다고 가정한다. 즉, 예측변수 \\(X_{1},X_{2},\\ldots,X_{k}\\)가 있을 때, 종속변수 Y의 조건부 평균은 다음과 같은 선형 함수로 표현된다고 전제한다.\n\\[E(Y \\mid X_{1},X_{2},\\ldots,X_{k}) = \\beta_{0} + \\beta_{1}X_{1} + \\beta_{2}X_{2} + \\cdots + \\beta_{k}X_{k}\\]\n여기서 ”선형”이란 회귀계수가 변수에 곱해져 더해지는 형태라는 뜻이지, 반드시 직선 그래프라는 의미만은 아니다. 예를 들면 \\(X^{2}\\)같은 항을 포함하면 변수 변환을 통해 곡선 관계도 선형회귀의 틀 안에서 다룰 수 있다.\n전체 모형의 선형성 검정\n전체 회귀모형의 선형성은 분산분석 F-검정을 통해 확인한다.\n\n귀무가설: 모든 회귀계수 \\(\\beta_{1} = \\beta_{2} = \\cdots = \\beta_{k} = 0\\)\n대립가설: 적어도 하나의 회귀계수 \\(\\beta_{j} \\neq 0\\)이다.\n\nF-검정에서 유의하면, 목표변수와 예측변수 전체가 집합적으로 직선적 함수관계를 가진다고 해석할 수 있다.\n개별 예측변수의 선형성 검정\n각 예측변수의 선형성은 해당 회귀계수의 t-검정을 통해 확인한다.\n귀무가설: \\(\\beta_{j} = 0\\), 대립가설: \\(\\beta_{j} \\neq 0\\)\nt-검정에서 유의하면, 해당 예측변수 \\(X_{j}\\)가 목표변수 Y와 유의미한 선형적 관계를 가진다고 해석한다.\n선형성 위배 시 대처\n만약 선형성이 충족되지 않는다면, 다음과 같은 방법을 고려할 수 있다.\n변수 변환(log, 제곱근, Box-Cox 변환 등)\n다항항 추가(\\(X^{2},X^{3}\\))\n곡선이나 비선형 모형(예: 로지스틱 회귀, 지수 회귀)\n스플라인 회귀(spline regression)와 같은 유연한 함수형\n선형성 사전 진단도구\n산점도 행렬 : 목표변수와 예측변수 간 개별 산점도 - 직선관계 발견\n\n\n\n\n\n단순회귀모형에서의 선형변환\n단순회귀모형에서 목표변수 Y, 예측변수 X에 대한 선형변환, 즉 \\(Y^{*} = a + bx^{*}(b \\neq 0)\\)를 취하더라도 모형의 구조는 그대로 유지된다. 단순히 회귀계수 \\beta_0, \\beta_1의 값만 달라질 뿐, 변수 간의 선형성 자체는 변하지 않는다. 따라서 단순회귀에서는 목표변수의 선형변환이 허용된다.\n다중회귀모형에서의 선형변환 문제\n\\[Y = \\beta_{0} + \\beta_{1}X_{1} + \\beta_{2}X_{2} + \\cdots + \\beta_{k}X_{k} + \\varepsilon\\]\n만약 Y에 대해 임의의 선형변환을 가하면, 특정 예측변수 X_j와의 선형성을 보정하려는 의도가 있더라도, 동시에 다른 예측변수들과의 함수관계까지 함께 변형되므로 전체 모형 해석이 왜곡된다.\n즉, 다중회귀에서는 목표변수의 변환으로 특정 변수와의 선형성을 보정할 수 없다는 한계가 있다. 따라서 다중모형에서는 선형성 보정이 필요하다면 목표변수보다는 예측변수의 변환(log, 제곱근, 다항항 추가 등) 에 국한해야 한다.\n빅데이터·고차원 데이터의 경우\n예측변수가 많거나 빅데이터 환경에서는 개별 예측변수와 목표변수 사이의 선형성 진단이 사실상 불가능하거나 불필요하다.\n많은 예측변수가 동시에 포함되면, 각 변수의 선형성보다 모형의 예측력과 일반화 성능이 더 중요하다.\n이 경우 회귀분석보다는 기계학습적 접근(regularization, ensemble methods) 으로 넘어가는 경우가 많다.\n따라서 전통적 회귀분석에서 강조하는 선형성 진단은 고차원 데이터에서는 상대적으로 중요도가 낮아진다.\n\n\n(2) 오차 가정 \\(\\underset{¯}{e} \\sim MNV(\\underset{¯}{0},\\sigma^{2}I)\\)\n회귀모형에서 오차항은 예측변수들에 의해 설명되지 않는 부분이다.\n만약 오차항이 전혀 없다면, 종속변수의 값을 예측변수들로 완벽히 설명할 수 있으며 이는 더 이상 통계모형이 아니라 수학적 결정론적 함수가 된다.\n실제 자료에서는 항상 설명되지 않는 변동이 존재하므로, 회귀모형은 설명된 변동(회귀식으로 설명 가능한 부분) 과 설명되지 않는 변동(오차항) 으로 나뉜다.\n이 두 변동의 비율을 이용하여 회귀모형 전체의 유의성을 검정할 수 있으며, 이것이 바로 분산분석 F-검정이다.\n독립성(independent)\n오차항은 서로 독립이어야 한다. 즉, 어떤 관측치의 오차가 다른 관측치의 오차에 영향을 주어서는 안 된다.\n독립성 가정은 특히 시계열 자료(시간적 순서가 있는 데이터)에서 중요하다. 시계열 자료가 아닌 횡단자료에서는 독립성 가정이 필요없다.\n예를 들어, 오늘의 주가 오차가 내일 주가 오차와 연관된다면 독립성이 위배된 것이다. 이 경우에는 자기상관(autocorrelation)을 점검해야 하며, 더 적합한 시계열모형(AR, ARIMA 등)을 사용해야 한다.\n정규성(normality)\n회귀분석에서 가장 중요한 오차항 가정 가운데 하나는 정규성이다. 오차항이 정규분포를 따른다고 가정하는 이유는 통계적 검정을 정당화하기 위함이다. 구체적으로, 회귀모형 전체의 적합성을 검정하는 분산분석 F-검정이나, 각 회귀계수가 유의한지를 판단하는 t-검정은 모두 오차항이 정규분포라는 전제를 필요로 한다.\n오차항이 정규분포를 따르면 종속변수 또한 정규분포를 따르게 된다. 이는 종속변수가 회귀식으로 설명되는 부분과 오차항으로 설명되지 않는 부분의 합으로 이루어지기 때문이다. 따라서 실제 데이터 점들은 추정된 회귀직선 위에 정확히 놓이지 않고, 정규분포를 따르는 오차항으로 인해 직선 주변에 일정한 규칙을 가지고 흩어진다. 이 벗어나는 정도는 모든 예측변수 값에서 동일하다고 가정하며, 이는 정규분포의 확률적 성질을 따른다.\n또한 정규성 가정은 이상치 진단에도 중요한 역할을 한다. 잔차가 정규분포를 따른다고 가정할 때, 정규분포의 규칙에서 크게 벗어난 값은 발생 가능성이 매우 낮으므로 이상치로 간주할 수 있다. 따라서 정규성 가정은 단순히 수학적 전제가 아니라, 모형의 추론 타당성과 데이터 진단을 동시에 가능하게 하는 핵심 요소이다.\n회귀분석에서 오차항의 정규성 가정은 분산분석 F-검정과 회귀계수의 t-검정을 수행하기 위한 수학적 전제이다. 그러나 실제 연구에서는 정규성이 완벽히 만족되지 않더라도, 표본의 크기가 충분히 크면 큰 문제가 되지 않는 경우가 많다.\n그 이유는 중심극한정리(Central Limit Theorem, CLT) 때문이다. 표본 크기가 커질수록, 오차항의 분포가 정규분포가 아니더라도 회귀계수 추정량의 분포는 점점 정규분포에 가까워진다. 즉, 표본이 충분히 크면 t-검정과 F-검정이 근사적으로 타당성을 유지할 수 있다. 따라서 정규성 가정은 소표본 분석(small sample analysis) 에서 특히 중요하며, 대규모 표본에서는 상대적으로 덜 엄격하다.\n물론 표본이 크다고 해서 정규성 검정을 무시할 수 있는 것은 아니다. 오차항이 극단적으로 치우친 분포를 가진다면, 회귀계수 추정치는 여전히 왜곡될 수 있고, 이상치가 많을 경우 결과는 심각하게 흔들릴 수 있다. 이런 상황에서는 로그 변환이나 제곱근 변환 같은 변수 변환을 고려하거나, 정규성 가정을 완화한 비모수적 방법이나 강건 회귀(robust regression), 부트스트랩 추론과 같은 대안을 사용하는 것이 바람직하다.\n등분산성(Homoscedasticity)\n등분산성은 모든 예측변수 값에서 오차항의 분산이 동일하다는 가정이다. 다시 말해, 특정한 X 값에서 관측된 종속변수 Y의 분산은 다른 X 값에서의 분산과 같아야 한다. 이 가정이 위배되면 표준오차 추정이 왜곡되고, 관측값이 이상치인지 정상적인 값인지조차 명확히 구분하기 어렵다. 따라서 잔차분석을 통해 등분산성을 점검하는 과정은 회귀분석에서 반드시 필요하다.\n만약 등분산성이 만족되지 않고, 오차항의 분산이 예측변수 값에 따라 달라진다면 이를 이분산성(heteroscedasticity) 이라고 한다. 이 경우 비록 회귀직선 자체는 여전히 적절할 수 있으나, 회귀계수의 표준오차가 잘못 추정되어 검정과 신뢰구간의 타당성이 훼손된다. 또한 잔차 산점도에서 관측치들이 직선 주변에 일정한 폭으로 모이지 않고, 특정 구간에서는 크게 퍼지거나 좁게 모이는 현상이 나타난다.\n등분산 가정이 충족되지 않으면 관측치의 해석도 모호해진다. 예를 들어, 어떤 관측치가 회귀직선에서 크게 벗어나 있는 경우, 등분산 가정이 만족된다면 이는 이상치로 간주될 수 있다. 그러나 특정 구간에서 본래 오차 분산이 크다면, 동일한 관측치도 실제 발생 가능한 정상적 값으로 해석될 수 있다. 즉, 같은 점이라도 등분산성이 성립하는지 여부에 따라 이상치인지 정상치인지의 판단이 달라질 수 있다.\n\n\n\n3. 수리적 접근(상관계수 접근)\n두 측정형 변수 \\((X,Y)\\)는 평균으로 \\(\\mu_{x},\\mu_{y}\\), 분산은 \\(\\sigma_{x}^{2},\\sigma_{y}^{2}\\)를 따르고 상관계수를 \\(\\rho\\)을 가질 때, 조건부 기대값은 선형 함수로 표현된다.\n\\(m(x) = E(Y \\mid X = x) = \\mu_{y} + \\rho\\frac{\\sigma_{y}}{\\sigma_{x}}(x - \\mu_{x}) = a + bx\\), 여기서 절편 \\(a = \\mu_{y} - b\\mu_{x}\\)이고, 기울기는 \\(b = \\rho\\frac{\\sigma_{y}}{\\sigma_{x}}\\)이다. 따라서 \\(m(x)\\)는 X에 대한 회귀함수가 된다.\n다음의 조건부 분산은 X 값에 관계없이 일정하다. 즉, X의 특정 값 x에 대해 예측한 Y의 조건부분산은 항상 일정하며, 두 변수의 상관계수 크기에 의해 결정된다. \\(v(x) = Var(Y \\mid X = x) = \\sigma_{y}^{2}(1 - \\rho^{2})\\)\n이 두 식은 다음을 말해 준다. X로 Y를 선형적으로 예측할 수 있으며, 예측식은 회귀직선 \\(m(x) = a + bx\\)이다. 예측이 아무리 좋아도 여전히 일정한 오차 분산이 남으며, 그 크기는 \\(\\sigma_{y}^{2}(1 - \\rho^{2})\\)이다.\n따라서 상관계수 \\(\\rho\\)는 단순한 상관의 크기를 넘어, 예측직선의 기울기 \\(b\\)를 결정하고, 설명된 변동과 설명되지 않은 변동의 비율까지 규정한다.\n\n\n4. 범주형 예측변수 처리\n회귀분석은 기본적으로 측정형 변수를 전제로 한다. 따라서 범주형 변수는 그대로 사용할 수 없고, 수치형으로 변환해야 한다. 이때 가장 널리 쓰이는 방법이 더미변수 dummy, 지시변수 indicator 변환이다.\n더미변수의 정의\n범주형 변수가 k개의 수준(level)을 가질 때, 회귀모형에 직접 넣을 수 없으므로 k-1개의 이진(dummy) 변수를 만들어 사용한다. 예를 들어, 성별 변수가 남자/여자 두 수준이라면, 하나의 더미변수를 만들어 \\(D = \\{\\begin{matrix}\n1 & \\text{male} \\\\\n0 & \\text{female}\n\\end{matrix}\\) 라고 정의한다.\n기준범주(reference category)\n범주형 변수의 한 수준은 기준범주로 남기고, 나머지 범주들을 더미변수로 만든다. 기준범주는 회귀식 절편에 포함된다. 다른 범주 계수는 기준범주 대비 효과로 해석된다.\n예를 들면 학력 변수가 ”고졸, 대졸, 대학원졸” 3개 수준이라면 기준범주: 고졸, 더미1: 대졸(대졸이면 1, 아니면 0), 더미2: 대학원졸(대학원졸이면 1, 아니면 0) \\(Y = \\beta_{0} + \\beta_{1}D_{\\text{대졸}} + \\beta_{2}D_{\\text{대학원}} + u\\)\n해석상의 유의점\n더미변수 계수는 차이(difference) 의 의미를 갖는다. 기준범주에 따라 계수 해석이 달라지므로, 해석 의도를 고려해 기준을 설정하는 것이 중요하다. 더미변수와 다른 연속형 변수의 상호작용항을 넣으면, 범주별로 기울기가 다른 모형을 추정할 수도 있다.\n\\[Y = \\beta_{0} + \\beta_{1}X + \\beta_{2}D + \\beta_{3}(X \\times D) + u.\\]\n\\[D = 0:Y = \\beta_{0} + \\beta_{1}X + u\\]\n\\[D = 1:Y = \\beta_{0} + \\beta_{2} + (\\beta_{1} + \\beta_{3})X + u\\]\n빅데이터에서 전통적 더미 처리의 한계 범주형 변수를 회귀모형에 투입하려면 보통 더미 변수 처리를 한다. 수준이 k개라면 기준 범주를 하나 정해 두고 나머지 k-1개에 대한 더미를 만든다. 그런데 빅데이터 환경에서는 이 방식이 곧바로 한계에 부딪힌다. 범주의 종류가 수천 개에 이르는 경우라면 열(column)도 수천 개가 추가되면서 차원이 급격히 불어난다. 그 결과 계산 속도가 느려지고 메모리 사용량이 커지며, 변수들 사이의 상관관계가 복잡하게 얽히면서 다중공선성의 위험까지 높아진다.\n또 다른 문제는 드물게 나타나는 범주 수준이다. 특정 수준이 데이터에 몇 번밖에 관측되지 않는다면, 그 범주에 대응하는 회귀계수는 극도로 불안정해지고 추정치의 신뢰성이 떨어진다. 결국 전통적인 더미 처리 방식은 범주가 많거나 희귀 수준이 많은 빅데이터 상황에서는 적합하지 않으며, 이를 보완할 다른 차원 축소 기법이나 범주 병합, 임베딩 방법 등이 필요해진다.\n빅데이터에서의 대안적 처리 방법\n(1) 차원 축소 기반 방법: 희귀 수준 통합: 빈도가 낮은 범주를 ”기타(Other)” 범주로 묶는다. 빈도/순위 인코딩: 범주를 등장 빈도나 순위로 치환해 수치형 변수로 사용한다.\n(2) 목표값(target) 기반 인코딩: 타깃 인코딩(Target / Mean Encoding): 각 범주를 종속변수의 평균값으로 대체한다. 예를 들면, 고객 ID별 평균 구매율을 사용한다. 차원 폭발을 방지할 수 있으나, 데이터 누설(leakage) 가능 → 반드시 교차검증이나 홀드아웃 기반으로 추정한다.\n(3) 해싱 인코딩(Feature Hashing): 각 범주 수준을 해시 함수로 고정된 크기의 벡터 공간으로 매핑. 차원이 제한되므로 효율적이나 충돌이 생겨 서로 다른 범주가 같은 벡터로 인코딩될 수 있다.\n(4) 임베딩(embedding): 딥러닝 접근: 범주형 변수를 밀집 벡터(dense vector) 로 학습한다. 예를 들면 Word2Vec, Entity Embedding → 범주 간의 유사성을 벡터 공간에서 반영한다. 빅데이터에서 강력하며, 추천 시스템·클릭 예측 모델에서 표준으로 쓰인다.\n빅데이터 환경에서의 선택 기준\n데이터 규모: 수준이 10~20개 정도면 여전히 더미 변수가 적절.\n수준 수백 이상: target encoding, hash encoding, frequency encoding 활용.\n수천 이상: 해싱 + 임베딩 접근이 현실적.\n해석이 목표라면 → 더미(희귀 수준 통합).\n예측이 목표라면 → target encoding, 임베딩, 해싱이 유리.\n빅데이터에서는 범주형 변수의 수준이 많아 단순 더미변수 변환이 차원 폭발을 일으킨다. 이 경우 빈도 기반 코딩, 타깃 인코딩, 해싱, 임베딩과 같은 기법을 사용하여 차원을 줄이고 계산 효율성을 확보한다. 해석이 중요한 전통적 통계 모형에서는 더미변수가 여전히 유용하지만, 예측력이 중요한 빅데이터 환경에서는 해석보다는 차원 축소와 일반화 성능에 중점을 두는 방식이 더 적합하다.\n\n\n\nchapter 3. 회귀모형 추정\n\n1. 회귀계수 추정 OLS 방법\n회귀분석에서 가장 중요한 절차는 주어진 데이터로부터 회귀계수(절편과 기울기) 를 추정하는 것이다. 이때 사용되는 가장 기본적이고 널리 쓰이는 방법이 바로 최소자승법(OLS, Ordinary Least Squares) 이다.\n역사적으로 이 방법은 Karl Pearson(1857–1936) 에 의해 체계적으로 제안되었다. 그는 피어슨 상관계수의 창시자로도 잘 알려져 있으며, 초기 연구에서 아버지와 성인 아들의 키 데이터를 사용하여 두 변수 사이의 직선적 관계를 도출하는 과정에서 최소자승법을 활용하였다.\n\n\n\n\n\n관측점들을 가장 대표하는 직선 (best fit)을 어떻게 구할 것인가?\n데이터 \\((x_{i},y_{i})\\)를 활용하여 점들에 가장 적합한 직선의 회귀계수 \\((a,b)\\) 를 추정하고 이를 이용하여 목표변수 추정값을 \\({\\widehat{y}}_{i} = \\widehat{a} + \\widehat{b}x_{i}\\)(fitted value) 구한다. 오차항에 대한 추정값으로 \\({\\widehat{e}}_{i} = y_{i} - {\\widehat{y}}_{i}\\)을 사용하여 직선의 유의성을 검증한다.\n\\[\\begin{array}{r}\nmin_{a,b}\n\\end{array}\\sum(e_{i})^{2} = \\begin{array}{r}\nmin_{a,b}\n\\end{array}\\sum(y_{i} - {\\widehat{y}}_{i})^{2}\\]\n수평, 수직 편차 중 왜 수직 편차인가?\n수평편차는 예측변수의 변동에 대한 척도이고 수직편차는 목표변수의 변동을 척도이다. 선형모형은 목표변수의 변동(값)을 예측하는 것이 목표이므로 수직 편차를 최소화 하는 것이 점들을 대표하는 회귀식(적합식)이다.\n왜 제곱인가?\n잔차 \\((y_{i} - {\\widehat{y}}_{i})\\)를 그대로 합하면 0이 되므로 오차 크기를 반영할 수 없다. 절대값을 사용할 수도 있지만 수학적으로 다루기 불편하다(오차의 크기를 모두 양수로 만들 수 있지만, 절대값 함수는 미분이 불가능한 점(0에서의 뾰족함)이 있어 수학적으로 다루기 어렵다).\n제곱은 항상 양수이고, 미분 가능하며, 오차가 클수록 더 큰 페널티를 주기 때문에 회귀분석에서 잔차 제곱합을 최소화하는 방법을 사용한다.\n\n\n2. 단순회귀모형 추정\n모형: \\(y_{i} = a + bx_{i} + e_{i},e_{i} \\sim N(0,\\sigma^{2})\\)\n추정 방정식: \\(\\min_{a,b}\\overset{n}{\\sum_{i = 1}}e_{i}^{2} = \\min_{a,b}\\overset{n}{\\sum_{i = 1}}(y_{i} - a - bx_{i})^{2}\\)\n정규방정식: \\(\\frac{\\partial}{\\partial a}\\sum(y_{i} - a - bx_{i})^{2} = 0,\\frac{\\partial}{\\partial b}\\sum(y_{i} - a - bx_{i})^{2} = 0\\)\nOLS 추정치: \\(\\widehat{b} = \\frac{\\sum(x_{i} - \\overline{x})(y_{i} - \\overline{y})}{\\sum(x_{i} - \\overline{x})^{2}},\\widehat{a} = \\overline{y} - \\widehat{b}\\overline{x}\\)\n상관계수와 관계: \\(r = b\\frac{\\sqrt{\\sum(x_{i} - \\overline{x})^{2}}}{\\sqrt{\\sum(y_{i} - \\overline{y})^{2}}}\\)\n최대우도추정량 Maximum Likelihood Estimator\n\\(y_{i} \\sim iidN(a + bx_{i},\\sigma^{2})\\)이므로 우도함수는 다음과 같으므로 우도함수 최대화 = OLS 추정과 동일하다.\n\\[L(a,b;x_{1},x_{2},...,x_{n}) = (\\frac{1}{\\sqrt{2\\pi}\\sigma})^{n}exp( - \\frac{\\sum(y_{i} - a - bx_{i})^{2}}{2\\sigma^{2}})\\]\n회귀계수에 대한 OLS 추정치는 BLUE(Best Linear Unbiased Estimator)이다. 즉 모든 선형, 불편 추정량 중 최소 분산(minimum variance)를 갖는다. 분포함수가 지수족이므로 MLE는 CSS이고 불편추정량이므로 Rao-Blackwell 정리에 의해 MVUE이다.\n잔차 = 오차의 추정량\n\n\\(r_{i} = {\\widehat{e}}_{i} = y_{i} - {\\widehat{y}}_{i}\\)\n\\(\\sum r_{i} = 0\\) : 잔차의 합은 0이다.\n\\(\\sum x_{i}r_{i} = 0\\) : 관측치 \\(x_{i}\\)을 가중치로 계산한 가중평균은 0이다\n\\(\\sum{\\widehat{y}}_{i}r_{i} = 0\\) : 예측치 \\({\\widehat{y}}_{i}\\)을 가중치로 계산한 가중평균은 0이다\n적합된 회귀직선은 \\((\\overline{x},\\overline{y})\\)을 지난다.\n\n\n\n3. 다중회귀모형 추정\n행렬모형 : \\(\\underset{¯}{y} = X\\underset{¯}{b} + \\underset{¯}{e}\\), (가정) \\(\\underset{¯}{e} \\sim N(\\underset{¯}{0},\\sigma^{2}I_{n})\\)\n\\(\\underset{¯}{y} = \\left( \\begin{array}{r}\ny_{1} \\\\\ny_{2} \\\\\n\\ldots \\\\\ny_{n}\n\\end{array} \\right)_{n \\times 1}\\), \\(\\underset{¯}{e} = \\left( \\begin{array}{r}\ne_{1} \\\\\ne_{2} \\\\\n\\ldots \\\\\ne_{n}\n\\end{array} \\right)_{n \\times 1}\\), \\(\\underset{¯}{b} = \\left( \\begin{array}{r}\n\\alpha \\\\\nb_{1} \\\\\n\\\\\nb_{2} \\\\\n\\ldots \\\\\nb_{p}\n\\end{array} \\right)_{(p + 1) \\times 1}\\), \\(X = \\begin{pmatrix}\n1 & x_{11} & x_{12} & \\ldots & x_{1p} \\\\\n1 & x_{21} & x_{22} & \\ldots & x_{2p} \\\\\n\\ldots & & & & \\\\\n1 & x_{n1} & x_{n2} & \\ldots & x_{np}\n\\end{pmatrix}_{n \\times p}\\)\n\\(\\begin{array}{r}\nmin \\\\\na,b_{1},b_{2},\\ldots\n\\end{array}\\sum(e_{i})^{2}\\) ⇥ \\(\\begin{array}{r}\nmin \\\\\n\\underset{¯}{b}\n\\end{array}{\\underset{¯}{e}}'\\underset{¯}{e} = \\begin{array}{r}\nmin \\\\\n\\underset{¯}{\\beta}\n\\end{array}(\\underset{¯}{y} - X\\underset{¯}{b})'(\\underset{¯}{y} - X\\underset{¯}{b})\\)\n\\[Q = (\\underset{¯}{y} - X\\underset{¯}{b})'(\\underset{¯}{y} - X\\underset{¯}{b}) = ({\\underset{¯}{y}}' - {\\underset{¯}{b}}'X')(\\underset{¯}{y} - X\\underset{¯}{b}) = {\\underset{¯}{y}}'\\underset{¯}{y} - {\\underset{¯}{y}}'X\\underset{¯}{b} - {\\underset{¯}{b}}'X'\\underset{¯}{y} + {\\underset{¯}{b}}'X'X\\underset{¯}{b}\\]\n\\(Q\\)를 최소화 하는 회귀계수(모수) \\(\\underset{¯}{b}\\)를 찾기 위한 정규방정식 normal equation은 다음과 같다. \\(\\frac{\\partial Q}{\\partial\\underset{¯}{b}} = - X'\\underset{¯}{y} - X'\\underset{¯}{y} + 2X'X\\widehat{\\underset{¯}{b}} = 0\\), 이를 정리하면 \\(\\widehat{\\underset{¯}{b}} = (X'X)^{- 1}(X'\\underset{¯}{y})\\)는 OLS이다.\nOLS 추정치 성질 : GAUSS-MARKOV Theorem\n회귀계수에 대한 OLS 추정치는 BLUE(Best Linear Unbiased Estimator)이다. 즉 모든 선형, 불편 추정량 중 최소 분산(minimum variance)를 갖는다. 분포함수가 지수족이므로 MLE는 CSS이고 불편추정량이므로 Rao-Blackwell 정리에 의해 MVUE이다.\nOLS 평균과 (추정)분산\nOLS는 불편추정량이다. \\(E(\\widehat{\\underset{¯}{b}}) = E((X'X)^{- 1}X'\\underset{¯}{y}) = (X'X)^{- 1}X'E(\\underset{¯}{y}) = (X'X)^{- 1}X'X\\underset{¯}{b} = \\underset{¯}{b}\\)\nOLS 추정분산\\(V(\\widehat{\\underset{¯}{b}}) = V((X'X)^{- 1}X'\\underset{¯}{y}) = (X'X)^{- 1}X')\\sigma^{2}I((X'X)^{- 1}X')' = \\sigma^{2}(X'X)^{- 1}\\)\n추정분산 추정치: \\(\\widehat{V(\\widehat{\\underset{¯}{b}})} = \\widehat{\\sigma^{2}}(X'X)^{- 1} = MSE(X'X)^{- 1}\\)\n예측변수 \\(X^{k}\\) 추정오차 \\(s({\\widehat{b}}_{k})\\) : \\(MSE(X'X)^{- 1}\\)의 \\(k\\)번째 대각원소 양의 제곱근\nMLE 추정\n오차항의 가정 \\(\\underset{¯}{e} \\sim N(\\underset{¯}{0},\\sigma^{2}I_{n})\\)이므로 \\(\\underset{¯}{y} \\sim N(\\underset{¯}{X}\\underset{¯}{b},\\sigma^{2}I_{n})\\)이다. 우도함수는 \\(L(\\underset{¯}{y};X,\\underset{¯}{b},\\sigma^{2}) \\propto exp( - \\frac{1}{2\\sigma^{2}}(\\underset{¯}{y} - X\\underset{¯}{b})'(\\underset{¯}{y} - X\\underset{¯}{b}))\\)이므로 로그 우도함수를 최대화 하는 \\(MLE\\) 추정치는 \\(min_{\\underset{¯}{\\beta}}(\\underset{¯}{y} - X\\underset{¯}{b})'(\\underset{¯}{y} - X\\underset{¯}{b})\\)을 최소화 하는 OLS 추정치와 동일하다. 그리고 MLE는 MVUE이므로 가장 좋은 추정치이다.\n적합치 \\({\\widehat{y}}_{i}\\)\n\\(\\widehat{\\underset{¯}{y}} = X\\widehat{\\underset{¯}{b}} = X(X'X)^{- 1}X'\\underset{¯}{y} = H\\underset{¯}{y}\\), Hat 행렬 \\(H = X(X'X)^{- 1}X'\\)\n\nH 행렬은 대칭행렬(\\(H' = H\\))이며 멱등행렬(\\(HH = H\\))이다.\n행렬 \\((I - H)\\)도 대칭행렬이며 멱등행렬이다.\n만약 \\(AA = A\\)가 성립하는 행렬 \\(A\\)를 멱등행렬(idempodent)이라 한다. A가 멱등행렬이면, \\(A^{n} = A\\)(n은 2보다 큰 정수)가 성립한다.\n\n잔차 residual \\(r_{i} = y_{i} - {\\widehat{y}}_{i}\\)\n관측치와 적합치의 차이로 오차항에 대한 추정치이다.\n\\[\\underset{¯}{r} = \\underset{¯}{y} - \\widehat{\\underset{¯}{y}} = \\underset{¯}{y} - H\\underset{¯}{y} = (I - H)\\underset{¯}{y}\\]\n\n잔차는 오차의 추정치이다. \\({\\widehat{e}}_{i} = r_{i}\\)\n잔차의 평균은 0이다.\n잔차 분산 : \\(V(\\underset{¯}{r}) = V((I - H)\\underset{¯}{y}) = \\sigma^{2}(I - H)\\)\n\n\n\n4. 회귀모형 검정\n\n(1) 분산분석적 접근\n목표변수의 분산(총변동 )을 모형이 설명변동과 설명하지 못하는 오차변동으로 이분화 하고 설명변동이 오차변동에 비해 충분히 크다면 회귀모형이 적절하다고(유의하다고) 판단한다.\n\n귀무가설 : 모든 회귀계수는 유의하지 않다. \\(\\left( \\begin{array}{r}\n\\\\\nb_{1} \\\\\n\\\\\nb_{2} \\\\\n\\ldots \\\\\nb_{p}\n\\end{array} \\right)_{p \\times 1} = \\left( \\begin{array}{r}\n\\\\\n0 \\\\\n0 \\\\\n\\ldots \\\\\n0\n\\end{array} \\right)_{p \\times 1}\\)\n대립가설 : 적어도 하나의 회귀계수는 유의하다. 적어도 \\(b_{k} \\neq 0\\)이다.\n\n\n\n(2) 변동분할 variation decomposition\n회귀분석은 단순히 회귀계수를 추정하는 과정이 아니라, 종속변수의 변동을 모형이 얼마나 설명하는가를 평가하는 과정이다. 이를 위해 목표변수의 변동을 세 부분으로 분할한다.\n총변동 Total Sum of Square\n목표변수의 값들의 변동에 대한 척도로 예측변수들이 목표변수를 잘 설명한다는 것은 변화를 예측하는 것이다. 목표변수의 실제 관측값은 전체 평균 주위에서 퍼져 있는데, 이 퍼짐 정도가 바로 총변동(Total Sum of Squares, SST) 이다. 총변동은 목표변수의 분산과 동일한 개념으로, 모형이 설명해야 하는 전체 변동량을 의미한다.\n\\(SST = \\sum(y_{i} - \\overline{y})^{2} = (\\underset{¯}{y} - \\overline{y}1)'(\\underset{¯}{y} - \\overline{y}1) = {\\underset{¯}{y}}'M\\underset{¯}{y}\\), 여기서 \\(M = I - \\frac{1}{n}11' = I - \\frac{1}{n}J\\)\n모형변동 Regression (Model) SS\n설정된 회귀모형 목표변수 총변동 중 설정된 모형에 의한 설명 부분이다. 총변동에 가까울수록 회귀모형 설정이 적절함을 의미한다.\n\\[SSR = \\sum({\\widehat{y}}_{i} - \\overline{y})^{2} = {\\underset{¯}{y}}'(H - (1/n)J)\\underset{¯}{y}\\]\n오차변동 Error SS\n회귀모형으로도 설명되지 않는 부분, 즉 오차변동(Error Sum of Squares, SSE) 이다. 이는 실제값과 예측값 사이의 차이, 즉 잔차(residual)에서 비롯된 변동으로, 모형의 한계와 측정 불확실성을 반영한다.\n\\[SSE = \\sum(y_{i} - {\\widehat{y}}_{i})^{2} = (\\underset{¯}{y} - H\\underset{¯}{y})'(\\underset{¯}{y} - H\\underset{¯}{y}) = {\\underset{¯}{y}}'(I - H)\\underset{¯}{y}\\]\n\n\n(3) 이차형식\n【정의】 확률변수 벡터 \\(\\underset{¯}{y}\\), 상수행렬 \\(A\\)에 대하여 \\(\\underset{¯}{y}'A\\underset{¯}{y}\\)을 이차형식이라 한다.\n【이차형식 분포 정리】 만약 \\(\\underset{¯}{y} \\sim MVN(\\underset{¯}{\\mu} = \\underset{¯}{0},\\Sigma = I)\\)(표준정규 다변량분포를 따른다면), 이차형식 \\(Q = \\underset{¯}{y}'A\\underset{¯}{y}\\)은 자유도가 \\(rank(A)\\) 인 \\(\\chi^{2}\\)분포를 따른다.\n\n\n(4) 변동분포\n총변동 분포 \\(SST \\sim \\chi^{2}(n - 1)\\)\n\n\\(SST = {\\underset{¯}{y}}'(I - (1/n)J)\\underset{¯}{y}\\) 는 목표변수 \\(\\underline y\\) 의 이차형식이다.\n오차항의 정규성 가정에 의하여 \\(\\underset{¯}{y} \\sim MVN(X\\underset{¯}{b},\\sigma^{2}I)\\) 정규분포를 따르므로 이차형식인 \\(SST\\)는 자유도 \\(rank(I - J/n) = n - 1\\)인 \\(\\chi^{2}\\)분포를 따른다.\n\n오차변동 분포 \\(SSE \\sim \\chi^{2}(n - p - 1)\\)\n\n\\(SSE = {\\underset{¯}{y}}'(I - H)\\underset{¯}{y}\\)는 목표변수 \\(\\underline y\\) 의 이차형식이다.\n오차항의 정규성 가정에 의하여 \\(\\underset{¯}{y} \\sim MVN(X\\underset{¯}{b},\\sigma^{2}I)\\)정규분포를 따르므로 이차형식인 \\(SSE\\)는 자유도 \\(rank(I - H) = n - p - 1\\)인 \\(\\chi^{2}\\)분포를 따른다.\n\n자유도 분할: Cochran 정리\n\n총변동의 자유도(관측치 중 자유로운 개수, 관측치 하나 하나는 독립적이고 정보를 갖고 있다)는 평균이 추정되었으므로 (\\(n - 1\\))이다. &lt;=&gt; 이차형식 결과와 동일\n\\(SSE\\)의 자유도는 \\((n - p - 1)\\)인 \\(\\chi^{2}\\)분포를 따른다.\n\\(SST \\sim \\chi^{2}(n - 1)\\)는 서로 독립인 \\(SSE \\sim \\chi^{2}(n - p - 1)\\)와 \\(SSR\\)로 나뉘고 \\(SSR\\)도 \\(\\underset{¯}{y}\\)의 이차형식 이므로 카이제곱 분포를 따르고 Cochran 정리에 의해 자유도는 \\((n - 1) - (n - p - 1) = p\\)(예측변수 개수)이다.\n그러므로 \\(SSR \\sim \\chi^{2}(p)\\)\n\n\n\n(5) 평균변동및 기대평균변동 Expected MSE\n평균오차변동\n\\(MSE = \\frac{SSE}{n - p - 1}\\) : 오차분산 \\(\\sigma^{2}\\)의 추정값 - \\(\\widehat{\\sigma^{2}} = MSE\\)\n평균오차변동 기대값 : \\(E(\\widehat{\\sigma^{2}}) = E(MSE) = \\sigma^{2}\\)\n평균회귀변동\n\\(MSR = \\frac{SSR}{p}\\) : 평균 회귀변동 : \\(E(MSR) = \\sigma^{2} + b^{2}\\sum(x_{i} - \\overline{x})^{2}\\) (단순회귀 경우)\n회귀변동 값은 회귀계수가 0과 차이가 많거나 예측변수 값의 차이가 큰 경우 회귀변동 값은 커진다.\n회귀변동이 커진다는 것은 회귀계수가 유의하든가 예측변수의 차이가 많다는 것을 의미한다.\n\n\n(6) 분산분석표, F - 검정 : 설정 회귀모형 유의성 검정\n\n귀무가설 : 설정한 회귀모형은 유의하지 않다. &lt;=&gt; 모든 예측변수의 회귀계수 값은 0이다. &lt;=&gt; \\(\\underset{¯}{y} = X\\underset{¯}{b}\\) 적절하지 않다. &lt;=&gt; 목표변수는 설정된 모든 예측변수로 설명되지 않는다.\n대립가설 : 설정한 회귀모형 \\(\\underset{¯}{y} = X\\underset{¯}{b}\\)은 유의하다. &lt;=&gt; 회귀계수 벡터 \\(\\underline b\\) 중 유의한 회귀계수가 적어도 하나가 있다.\n검정통계량 : \\(TS = \\frac{MSR}{MSE} \\sim F(p,n - p - 1)\\)\n\n그러므로 F-검정 결과 귀무가설이 기각되면 유의한 설명 변수가 하나 이상 있다는 것이므로 각 설명 변수에 대한 유의성을 t-검정을 이용하여 알아 보면 된다.\n\n\n\n\n\n\n\n\n\n\n변동(source)\n자승합\nSum of Sqaures\n자유도\nDf\n평균자승합\nMean SS\nF-통계량\n\n\n\n\n회귀변동\n\\[SSR = {\\underset{¯}{y}}'(H - (1/n)J)\\underset{¯}{y}\\]\n\\[p\\]\n\\[MSR = \\frac{SSR}{p}\\]\n\\[TS = \\frac{MSR}{MSE}\\]\n\\[\\sim F(p,n - p - 1)\\]\n\n\n오차변동\n\\[SSE = {\\underset{¯}{y}}'(I - H)\\underset{¯}{y}\\]\n\\[n - p - 1\\]\n\\[MSE = \\frac{SSE}{n - p - 1}\\]\n\n\n총변동\n\\[SST = {\\underset{¯}{y}}'(I - (1/n)J)\\underset{¯}{y}\\]\n\\[n - 1\\]\n\n\n\n\n\n\n\n(7) \\(t -\\)검정 : 개별 예측변수 유의성 검정 \\(H_{0}:b_{k} = 0\\)\n\n귀무가설 : 예측변수 \\(X_{k}\\)는 유의하지 않다. \\(H_{0}:b_{k} = 0\\)\n대립가설 : 예측변수 \\(X_{k}\\)는 유의하다. \\(H_{0}:b_{k} \\neq 0\\)\n검정통계량 : \\(TS = \\frac{{\\widehat{b}}_{k}}{s({\\widehat{b}}_{k})} \\sim t(n - p - 1)\\)\n\n예측변수 \\(X^{k}\\) 추정오차 \\(s({\\widehat{b}}_{k})\\) : \\(MSE(X'X)^{- 1}\\)의 \\(k\\)번째 대각원소 양의 제곱근\n두 개 이상 예측변수의 유의성 동시 검정\nFull Model 예측변수 개수 \\(p\\)\n\\(Y_{i} = a + b_{1}X_{i1} + b_{2}X_{i2} + \\ldots + b_{p}X_{ip} + e_{i}\\)\nReduced Model\n귀무가설 : 일부 예측변수가 유의하지 않다. 유의하지 않은 예측변수 개수 = \\(k\\)\n만약 첫 2개 예측변수 \\((X_{1},X_{2})\\)가 유의하지 않다면 귀무가설은 \\(H_{0}:b_{1} = b_{2} = 0\\)이 된다. \\(k = 2\\)\nReduced model : \\(Y_{i} = a + b_{3}X_{i1} + b_{4}X_{i2} + \\ldots + + b_{p}X_{ip} + e_{i}\\)\n추가 자승합 Extra Sum of Sqaures  정의\n\n\\(SSR(X_{1},X_{2},...,X_{p})\\) : \\(Y_{i} = a + b_{1}X_{i1} + b_{2}X_{i2} + \\ldots + b_{p}X_{ip} + e_{i}\\) 완전모형의 회귀(모형)변동\n\\(SSE(X_{1},X_{2},...,X_{p})\\) : \\(Y_{i} = a + b_{1}X_{i1} + b_{2}X_{i2} + \\ldots + b_{p}X_{ip} + e_{i}\\) 완전모형의 오차변동\n\\[SSR(X_{i}|X_{j}) = SSR(X_{i},X_{j}) - SSR(X_{i})\\]\n\n검정통계량 \\(H_{0}:b_{j} = 0,forj = 1,2,...,k\\)\n\\(TS = \\frac{(SSR_{F} - SSR_{R})/k}{MSE_{F}} \\sim F(k,n - p - 1)\\), \\(k =\\)귀무가설에서 설정된 회귀계수 0인 예측변수 개수이다.\nSequential 순차 자승합 SS, Type I\n회귀모형 삽입 예측모형 순서 대로 자승합이 출력된다.\n회귀모형 : \\(Y_{i} = a + b_{1}X_{i1} + b_{2}X_{i2} + \\ldots + b_{p}X_{ip} + e_{i}\\)\n\n삽입 순서에 따른 설명변동의 차이가 있음\n\\(X_{1}\\) : \\(SSR(X_{1}|\\mu)\\) - \\(\\mu\\)는 절편항을 의미한다.\n\\(X_{2}\\) : \\(SSR(X_{2}|X_{1},\\mu)\\), …\n\\(X_{p}\\) : \\(SSR(X_{p}|X_{1},X_{2},...,X_{p - 1},\\mu)\\)\n통제변수 모형에 주로 사용된다.\n\nType II: hierarchical or partially sequential 부분 자승합\n다른 모든 예측변수가 고정인 경우 해당 변수의 변동\n\n\\[SSR(X_{k}|X_{1},...,X_{k - 1},X_{k + 1},...,X_{p},\\mu)\\]\n유의 변수 선택에 사용된다.\n교차항이 있을 경우에는 Type III와 상이하다.\n\nType III: marginal or orthogonal , partial\n타입 2 자승합과 동일하다. 부분자승합 개념이나 교차항이 있는 경우 상이하다.\n\n\n\n\nchapter 4. 사례 데이터\n보스턴 주택가격\n# Boston Housing 데이터 불러오기 및 회귀분석 사례\nimport pandas as pd\nimport statsmodels.api as sm\nfrom sklearn.datasets import fetch_openml\n\n# 1. 데이터 불러오기 (OpenML에서 Boston Housing)\nboston = fetch_openml(name=\"boston\", version=1, as_frame=True)\ndf = boston.frame\ndf.info()\nCRIM: per capita crime rate by town → 타운별 1인당 범죄율.  ZN: proportion of residential land zoned for lots over 25,000 sq.ft. → 25,000 제곱피트(약 695평) 이상의 부지로 구획된 주거용 토지 비율.  INDUS: proportion of non-retail business acres per town → 타운별 비소매(non-retail) 상업용 토지 비율.  CHAS: Charles River dummy variable (= 1 if tract bounds river; 0 otherwise) → Charles River 더미 변수, 강을 경계로 하면 1, 아니면 0.  NOX: nitric oxides concentration (parts per 10 million, ppm × 10) → 산화질소(NOₓ) 농도, 단위: 10ppm.  RM: average number of rooms per dwelling → 주택당 평균 방의 수.  AGE: proportion of owner-occupied units built prior to 1940 → 1940년 이전에 건축된 자가 주택 비율.  DIS: weighted distances to five Boston employment centres → 보스턴 고용 중심지 5곳까지의 가중 평균 거리.  RAD: index of accessibility to radial highways → 방사형(radial) 고속도로 접근성 지수.  TAX: full-value property-tax rate per $10,000 → $10,000당 재산세율.  PTRATIO: pupil-teacher ratio by town → 타운별 학생-교사 비율.  B: 1000(Bk − 0.63)², where Bk is the proportion of Black residents by town → 인종 관련 변수: B = 1000(Bk − 0.63)^2. 여기서 Bk는 흑인 비율.  LSTAT: % lower status of the population → 저소득·사회적 지위가 낮은 계층의 비율(%).  MEDV: median value of owner-occupied homes in $1000’s → 자가 주택의 중앙값 가격 (단위: $1000).\n산점도 행렬\n2개의 측정형 변수 데이터\\((x_{i},y_{i})\\)를 2차원 공간에 표현하여 두 변수의 함수 관계를 시각적으로 예상한다.\n\nX- 축 : 결정(원인) 요인으로 예측변수, 독립변수, 설명변수 등을 불리는 변수를 사용한다.\nY-축 : 결과로 목표변수, 종속변수, 반응변수로 불리는 변수를 사용한다. 그러므로 산점도는 두 변수의 인과 관계를 시각적으로 표현한다.\n\n진단내용(1) 함수관계\n산점도는 두 변수 간의 함수 관계를 본다. 일반적으로 해석이 용이하고 상관계수의 척도인 직선 관계의 정도를 파악하는데 사용된다. 두 변수의 함수 관계는 수집된 데이터 범위 내에서만 해석이 가능하다. 관측된 데이터 범위를 벗어나는 경우 두 변수의 함수관계는 수집된 데이터 범위 내의 패턴(함수 관계)과 다를 수 있기 때문이다.\n진단내용(2) 이상치\n두 변수의 직선 관계에서 특이한 패턴을 보이는 관측치를 시각적으로 진단한다.\n\n선형 함수 관계에서 적합한 직선을 임의의 \\(x\\)값에서 다른 관측치에 비해 \\(y\\)값이 직선을 많이 벗어난 관측치를 이상값(그림 상에서 붉은 점)이라 한다.\n이상치 진단은 반드시 데이터 범위 내에 있는 관측값들에 적용된다.\n(진단) 오차의 추정치인 (표준화) 잔차가 \\(\\pm 2\\) 값을 벗어나면 이를 이상치로 진단한다. 즉 이상치는 직선의 관계를 나타내는 척도를 낮추어 예측모형의 예측 정도를 저평가 한다.\n(해결) 고전적 통계분석에서는 이상치는 삭제하는 것을 원칙으로 한다. 그러나 대용량 데이터에서 이상치는 정상적인 패턴을 벗어나는 관측치여서 향후 시스템 개발에 주요한 정보를 제공하게 된다.\n\n진단내용(3) 영향치\nX-축(예측변수) 데이터의 범위를 벗어난 관측치를 영향치 influential observation 라 한다. 영향치에는 다음 2 종류의 영향치가 존재한다.\n\n순수 영향치 : 회귀 직선 추정 식 상에 있어 함수관계(기울기 변동)에는 영향을 주지 않으나 결정계수 높여 예측변수의 설명 능력을 과다하게 높은 것으로 판단하게 하는 결과를 왜곡한다.\n이상 영향치 : 추정 회귀 직선을 벗어난 관측치로 이상치 개념이다.\n(진단) 예측모형 잔차분석의 Hat 통계량(Leverage 값)을 활용하여 판단한다.\n(해결) 영향치 주변의 관측값을 추가 수집 후 분석, 영향치 값이 실제 발생 가능하지 않은 경우 제외\n\n#산점도 행렬 그리기\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsns.pairplot(boston.iloc[:,1:15],kind='reg',diag_kind='kde')\nplt.show()\nMEDV 목표변수(제일 마지막 대각 KDE - 커널추정 확률밀도함수)의 분포는 좌우 대칭을 보이므로 정규변환 필요는 없어 보인다. 예측변수가 많은 경우 모든 예측변수의 정규성 검정은 의미가 없으므로 목표변수의 정규성만 파악하면 된다. 사실 목표변수의 정규성 사전 진단도 의미가 없는 것은 오차항의 정규분포 가정이기 때문이다. 오차항의 정규성 검정은 모형 진단 과정에서 최종 검증하게 된다.\n다중회귀모형에서 영향치, 이상치 사전 진단 또한 의미가 없고 모형 최종 진단 과정에서 이루어진다.\n\n\n\n\n\n추정\n이진형변수 chas가 (1=주변지역, 0=아닌 지역)으로 입력되어 있어 그대로 사용해도 되지만 만약 문자로 입력되었다면 (1,0)으로 변환하여 추정해야 한다.\npd.get_dummies(boston['chas'],drop_first=True)\n#Using Pearson Correlation\ndf_cor=df.iloc[:,1:15].corr(method ='pearson')\n# 스타일 지정: 소숫점 2자리 + 색상맵\ndf_cor.style.format(\"{:.2f}\").background_gradient(cmap='coolwarm')\n\n\n\n\n\n종속변수와 상관관계(상관계수 크기)가 가장 높은 예측변수는 lstat(-0.74) &gt; rm(0.695) &gt; ptratio(0.51) 순이다.\nimport pandas as pd\nimport statsmodels.api as sm\n\n# 1) 종속/설명변수 분리\ny = pd.to_numeric(df['MEDV'], errors='coerce')             # 숫자화\nX = df.drop(columns=['MEDV']).copy()\n\n# 2) 설명변수 전부 숫자형으로 강제 변환\nfor col in X.columns:\n    # category/object이면 숫자로 변환 (필요시 coerce로 NaN 처리)\n    if X[col].dtype.name in ['category', 'object']:\n        X[col] = pd.to_numeric(X[col], errors='coerce')\n    else:\n        # float/int 등은 그대로 두되, numpy형 보장\n        X[col] = X[col].astype(float)\n\n# 3) 결측치 제거(숫자화 과정에서 생겼을 수 있는 NaN)\nmask = y.notna() & X.notna().all(axis=1)\ny = y[mask]\nX = X[mask]\n\n# 4) 절편 추가 후 적합\nX = sm.add_constant(X, has_constant='add')\nfit = sm.OLS(y, X).fit()\n\nprint(fit.summary())\n\n\n\n\n\nage 예측변수를 제외하고는 모든 예측변수는 유의하다. 모형의 결정계수는 74.1%로 설명력이 매우 높다. 분석결과 아래 Notes [2]에 보면, 예측변수 간 다중공선성 문제 발생하였다는 경고가 출력되어 있다."
  },
  {
    "objectID": "notes/linear_model/lm_logistic.html",
    "href": "notes/linear_model/lm_logistic.html",
    "title": "회귀분석 5. 로지스틱회귀",
    "section": "",
    "text": "chapter 1. 일반화 선형모형\n\n1. 일반화 선형모형 개념\n전통적인 선형회귀모형은 목표변수가 연속형이고 정규분포를 따른다는 가정을 전제로 한다. 하지만 실제 자료에서는 목표변수가 이진형, 계수형, 범주형 등 다양한 형태로 나타난다. 예를 들어 구매 여부(예/아니오), 교통사고 건수, 고객의 만족도 척도 등은 정규분포 가정을 따르지 않는다. 이러한 경우에도 회귀분석의 틀을 적용할 수 있도록 확장한 것이 일반화선형모형(GLM)이다.\nGLM은 세 가지 요소로 구성된다.\n\n확률분포 가정: 목표변수 Y는 지수족에 속하는 분포를 따른다고 가정한다. 여기에는 정규분포, 이항분포, 포아송분포, 감마분포 등이 포함된다.\n선형예측자: 설명변수들의 선형 결합 \\(\\eta = \\beta_{0} + \\beta_{1}X_{1} + \\cdots + \\beta_{p}X_{p}\\)를 사용한다.\n연결함수: 기대값 \\(\\mu = E(Y|X)\\)와 선형예측자 \\(\\eta\\)를 연결하는 함수 \\(g(\\mu) = \\eta\\)를 정의한다. 이 함수는 분포의 제약(예: 확률은 0과 1 사이, 기대값은 양수 등)을 해소해준다.\n\n따라서 이산형 목표변수의 분포 특성에 맞는 회귀모형이 필요하다.\n이진형(binary): 두 가지 결과(예: 성공/실패, 합격/불합격, 구매/비구매)만 가질 수 있는 경우이므로 베르누이 분포에 따르므로 로지스틱 회귀, 프로빗 회귀 등을 사용한다.\n이산형 정수형(count data): 사건 발생 횟수를 나타내는 변수 (예: 교통사고 건수, 병원 방문 횟수, 감염 환자 수)로 포아송 분포 또는 음이항 분포를 따르므로 포아송 회귀를 사용한다.\n순서형(ordinal): 순서 정보는 있으나 간격이 일정하지 않은 변수 (예: 학점 A–B–C–D–F, 만족도 리커트 척도)는 다항 분포 기반이므로 순서형 로지스틱 회귀분석을 이용한다.\n명목형(nominal): 범주에 단순한 구분만 존재하고 순서가 없는 변수 (예: 직업, 거주지역, 혈액형) 다항 분포 기반인 다항 로지스틱 회귀분석을 이용한다.\n\n\n2. 왜 OLS를 사용할 수 없나?\n이진형 종속변수(예: 합격=1, 불합격=0)를 설명변수 X로 회귀한다고 하자. 이때 단순히 선형회귀를 적용하면 여러 가지 문제가 발생한다.\n첫째, 예측값의 범위 문제가 있다. 선형회귀식 \\(Y = \\beta_{0} + \\beta_{1}X + \\varepsilon\\)을 그대로 적용하면, 예측값 \\(\\widehat{Y}\\)는 이론적으로 모든 실수 값을 가질 수 있다. 그러나 이진형 변수는 성공 확률 p로 해석되어야 하므로, 예측값은 반드시 [0,1] 범위 안에 있어야 하나 실제로는 시험 점수 X=30일 때 \\(\\widehat{Y} = - 0.3\\)처럼 음수가 나오거나, X=90일 때 \\(\\widehat{Y} = 1.2\\)처럼 1을 초과할 수 있어 확률 해석이 불가능하다. 따라서 선형회귀식은 확률모형으로 적합하지 않다.\n둘째, 분산 불일치 문제가 있다. 이진형 변수는 베르누이 확률변수이므로, 기대값은 \\(E(Y|X) = p,\\)분산은 \\(Var(Y|X) = p(1 - p)\\)의 형태를 가진다. 즉, 분산이 확률 p의 크기에 따라 달라진다. 그러나 선형회귀에서는 오차항의 분산이 일정하다는 가정, 즉 \\(Var(\\varepsilon) = \\sigma^{2}\\)를 전제로 한다. 이 가정이 깨지므로 추정량의 효율성과 검정의 타당성에 문제가 생긴다.\n셋째, 선형성 가정이 불합리하다는 점이다. 현실에서 사건 발생 확률은 설명변수가 증가한다고 직선적으로 증가하지 않는다. 보통은 S자 모양의 곡선을 따른다. 즉, 낮은 구간에서는 거의 0에 수렴하고, 일정 구간에서 급격히 증가하며, 높은 구간에서는 다시 1에 가까워지면서 평탄해진다. 하지만 선형회귀는 직선 관계만을 가정하기 때문에 확률의 비선형적 구조를 설명하지 못한다.\n이러한 한계를 해결하기 위해 등장한 것이 로지스틱 회귀분석이다. 로지스틱 회귀에서는 확률 p를 직접 선형식으로 표현하지 않고, 로짓 변환을 사용한다. 로짓은 \\(\\text{logit}(p) = \\log\\frac{p}{1 - p}\\)로 정의되며, 이는 \\(- \\infty\\)부터 \\(+ \\infty\\)까지 모든 실수 값을 가질 수 있다. 따라서 이를 \\(\\beta_{0} + \\beta_{1}X\\)와 같은 선형식으로 표현할 수 있다. 이후 역변환을 취하면, 확률 p는 항상 0과 1 사이에 위치하는 로지스틱 함수 형태가 된다. 이런 함수를 연결함수라 한다.\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nX = np.linspace(0, 100, 400)\nbeta0, beta1 = -25, 0.5\nlogit = beta0 + beta1 * X\np = 1 / (1 + np.exp(-logit))\n\nfig, ax1 = plt.subplots(figsize=(8,5))\n\n# 왼쪽 y축: 확률 (0~1)\nax1.plot(X, p, label=\"Logistic (probability)\", linewidth=2)\nax1.set_ylim(-0.05, 1.05)\nax1.set_ylabel(\"Probability p\")\nax1.axhline(0, color=\"gray\", linestyle=\":\")\nax1.axhline(1, color=\"gray\", linestyle=\":\")\nax1.axvline(-beta0/beta1, color=\"gray\", linestyle=\"--\", alpha=0.7)\n\n# 오른쪽 y축: 선형 예측치\nax2 = ax1.twinx()\nax2.plot(X, logit, color=\"red\", linestyle=\"--\", label=\"Linear prediction\")\nax2.set_ylabel(\"Linear prediction\")\n\n# 범례 합치기\nh1, l1 = ax1.get_legend_handles_labels()\nh2, l2 = ax2.get_legend_handles_labels()\nax1.legend(h1+h2, l1+l2, loc=\"lower right\")\n\nax1.set_title(\"Linear vs Logistic (dual y-axes)\")\nax1.set_xlabel(\"Score (0–100)\")\nax1.grid(True)\nplt.show()\n\n\n\n\n\n\n\n3. 연결 LINK 함수\n일반화선형모형에서는 종속변수가 반드시 정규분포를 따를 필요가 없으며, 베르누이 분포, 포아송 분포, 다항 분포 등 다양한 분포를 따를 수 있다. 그러나 이들 분포에서 종속변수의 기대값 \\(\\mu = E(Y|X)\\)는 특정한 범위 제약을 갖는다. 예를 들어, 이진형 변수에서는 확률이므로 \\(0 &lt; \\mu &lt; 1\\), 포아송 분포에서는 \\(\\mu &gt; 0\\)이어야 한다.\n반면, 설명변수들의 선형 결합으로 만들어지는 선형예측자 \\(\\eta = \\beta_{0} + \\beta_{1}X_{1} + \\cdots + \\beta_{p}X_{p}\\)는 이론적으로 \\(( - \\infty, + \\infty)\\)까지 모든 값을 가질 수 있다. 따라서 기대값 \\(\\mu\\)와 선형예측자 \\(\\eta\\)를 직접 연결할 경우, 서로의 값의 범위가 맞지 않는 문제가 발생한다.\n이때 필요한 것이 바로 연결함수이다. 연결함수는 기대값 \\(\\mu\\)를 변환하여 \\(\\eta\\)와 같은 스케일로 만들어주는 함수이다. 즉, \\(g(\\mu) = \\eta\\) 형태를 가지며, 이를 통해 \\(\\mu\\)와 \\(\\eta\\) 사이의 불일치를 해소한다. 예를 들어, 로지스틱 회귀에서는 \\(\\mu = p\\)가 확률이므로 [0,1] 범위를 갖는다. 이를 선형예측자와 연결하기 위해 로짓함수 \\(g(p) = \\log\\frac{p}{1 - p}\\)를 사용한다. 로짓은 확률을 \\(( - \\infty, + \\infty)\\) 범위로 변환하므로, 선형식과 자연스럽게 결합할 수 있다.\n따라서 연결함수는 단순히 수학적 편의가 아니라, 확률모형으로서의 타당성을 확보하고 다양한 분포를 회귀분석에 활용할 수 있게 해주는 핵심 장치라고 할 수 있다.\n목표변수는 지수족이어야 한다.\nGLM의 기본 구조 \\(g(\\mu) = \\eta = X\\beta,\\mu = E(Y|X)\\)이며 이에 필요한 전제는 다음과 같습니다. 분포의 평균과 분산이 간단한 형태로 표현되어야 한다. 그래야 기대값과 분산를 쉽게 다룰 수 있다. 지수족 분포는 평균과 분산이 모수(특히 자연모수)에 의해 단순하게 표현된다. 그리고 우도함수가 지수형태로 정리될 수 있어야 한다. 이렇게 해야 최대우도추정(MLE)이 수학적으로 해석 가능해지기 때문이다. 지수족은 정의에 의해 우도함수로 표현된다.\n\n\n\n\n\n\n\n\n목표변수 분포\n연결함수\n평균함수\n\n\n정규분포\n항등함수 \\(g(\\mu) = \\mu\\)\n\\[\\mu = X\\underset{¯}{b}\\]\n\n\n지수분포 감마분포\n음의 역함수 \\(g(\\mu) = - \\mu^{- 1}\\)\n\\[\\mu = - X{\\underset{¯}{b}}^{- 1}\\]\n\n\n포아송분포\n로그함수 \\(g(\\mu) = ln(\\mu)\\)\n\\[\\mu = exp(X\\underset{¯}{b})\\]\n\n\n베르누이분포\n로짓함수 \\(g(\\mu) = ln(\\frac{\\mu}{1 - \\mu})\\)\n\\[\\mu = \\frac{1}{1 + exp( - X\\underset{¯}{b})}\\]\n\n\n이항분포\n로짓함수 \\(g(\\mu) = ln(\\frac{\\mu}{n - \\mu})\\)\n\\[\\mu = \\frac{1}{1 + exp( - X\\underset{¯}{b})}\\]\n\n\n범주형분포\n다항분포\n로짓함수 \\(g(\\mu) = ln(\\frac{\\mu}{1 - \\mu})\\)\n\\[\\mu = \\frac{1}{1 + exp( - X\\underset{¯}{b})}\\]\n\n\n\n\n\n4. 일반화 선형모형 추정\n모형 \\(g(E(y)) = Xb + e,e \\sim N(0,\\sigma^{2})\\)\n\n이진형 분포 \\(P(y_{i} = 1) = p,p = e^{( - \\theta x)}\\) =&gt; \\(g(p_{x}) = ln(p_{x}) = - \\theta x\\)\nPoisson \\(E(y) = \\lambda,\\lambda = ne^{\\theta},g(\\lambda_{i}) = ln(n_{i}) + \\theta*i\\)\n\n지수족 exponential family\n\\(f(y;\\theta) = h(x)exp(\\eta(\\theta)T(x) - A(\\theta))\\)로 표현되는 확률변수 y는 지수족이다. \\(\\theta\\)는 모수, \\(T(x)\\)는 충분통계량이다.\n\n(성질) 지수족의 T(x)는 완비통계량이며 T(x)의 함수 중 불편성을 만족하는 통계량이 MVUE이다.\n(성질) 로그우도함수 \\(ln(f(y;\\theta))\\)는 다음 성질을 갖는다. (1) score 함수 \\(U = \\frac{dln(f(y;\\theta))}{d\\theta}\\)의 기대값은 \\(E(U) = 0\\) 이다. (2) U의 분산을 Information이라 정의한다. \\(V(U) = J\\) (3) 스코어 함수의 \\(\\theta\\) 1차 미분의 기대값은 \\(E(\\frac{dU}{d\\theta}) = - V(U) = - J\\) 관계식을 갖는다.\n\n정규분포, 감마분포, 포아송분포, 이항분포, 베타분포 등 대부분의 유명한 분포는 지수족이다. 스코어 함수 \\(U = 0\\)을 N-R 방식으로 풀면 \\(\\theta\\)의 MVUE 추정치를 얻는다. \\({\\widehat{\\theta}}^{(m)} = {\\widehat{\\theta}}^{(m - 1)} + \\frac{U^{(m - 1)}}{J^{(m - 1)}}\\)\n추정량의 샘플링분포 \\(\\widehat{\\theta} \\sim ?\\)\n대표본 이론에 의해 [이차형식] \\(U'J^{- 1}U \\sim \\chi^{2}(p)\\),\\(p\\)=모수의 개수\nMLE의 공분산 : \\(E\\lbrack(\\widehat{\\theta} - \\theta)'(\\widehat{\\theta} - \\theta)\\rbrack = J(\\widehat{\\theta})\\)\nWald 검정통계량 \\((\\widehat{\\theta} - \\theta)'J^{- 1}((\\widehat{\\theta})(\\widehat{\\theta} - \\theta) \\sim \\chi^{2}(p)\\)\nLR 우도비 검정 \\(H_{0}:\\theta = 0\\)\n\\[\\lambda = \\frac{L(y;\\widehat{\\theta}underH_{0})}{L(y;\\widehat{\\theta})} \\sim 2ln\\lambda \\sim \\chi^{2}(1)\\]\n\n\n\nchapter 2. 이진형 종속변수 : 로지스틱 회귀\n\n1. 개념\n로짓변환은 목표변수가 이진형 변수이고 모수 \\(p\\)(성공확률)를 추정하기 위한 연결함수이다. 이진형 변수를 따르는 확률실험의 성공의 회수는 개별 관측치 \\(z_{i}B(\\theta = \\pi)\\)이므로 \\(i -\\)구간의 성공 회수는 \\(y_{i} \\sim B(n,\\pi)\\)이므로 \\(E(Y) = np\\)이다.\n\n\n\n\n\n이항분포를 따르는 경우에도 마찬가지로, 개별 관측치 수준에서는 이항 데이터를 0과 1의 이진형 데이터로 변환하여 로지스틱 회귀를 적용할 수 있다. 예를 들어 ”10명 중 7명이 성공”이라는 집계 데이터를 7개의 성공(=1)과 3개의 실패(=0)로 풀어쓰면, 베르누이 시행들의 합이 이항분포가 된다는 점을 활용하는 것이다. 이렇게 변환하면 로지스틱 회귀는 이항분포와 베르누이분포를 모두 포괄할 수 있다.\n\n\n2. 링크함수 \\(g(\\pi_{i}) = Xb\\)\n회귀모형 적용을 위해서는 링크함수의 범위는 반드시 \\(( - \\infty,\\infty)\\)이어야 한다.\n로짓(link: logit)\n이진형 목표변수를 다룰 때 가장 널리 쓰이는 연결함수는 로짓이다. 로짓 함수는 확률 p를 오즈(odds)의 로그로 변환하여 선형예측자와 연결한다. 즉, \\(g(p) = \\log\\frac{p}{1 - p}\\)의 형태를 가지며, 이를 통해 확률이 0과 1 사이에 있다는 제약을 풀어내고 선형식 \\(\\eta = X\\beta\\)와 자연스럽게 결합시킨다. 로짓 모형의 큰 장점은 해석이 직관적이라는 점이다. 추정된 회귀계수 \\(\\beta_{j}\\)는 해당 설명변수가 1 단위 증가할 때 오즈비가 \\(e^{\\beta_{j}}\\)배 커진다는 의미를 갖는다. 이 때문에 로지스틱 회귀는 의학, 사회과학, 정책연구 등에서 사실상 표준처럼 사용되고 있으며, 특히 역학 연구에서 위험요인을 설명할 때 필수적으로 쓰인다.\n프로빗(link: probit)\n프로빗 함수는 확률을 표준정규분포의 누적분포함수(CDF)의 역함수를 통해 변환한다. 즉, \\(g(p) = \\Phi^{- 1}(p)\\)로 정의된다. 이는 곧 잠재변수 모형과 연결되는데, 보이지 않는 잠재변수 \\(Y^{*} = X\\beta + \\varepsilon\\)를 가정하고, 오차항 \\(\\varepsilon\\)이 표준정규분포를 따른다고 할 때 Y=1일 확률이 \\(\\Phi(X\\beta)\\)로 표현된다는 아이디어다. 프로빗 모형은 로짓과 형태가 거의 유사한 S자 곡선을 그리지만, 계수의 크기와 해석이 조금 다르다. 계수는 곧바로 오즈비로 해석하기는 어렵고, z-점수 단위의 변화량을 의미한다. 따라서 실제 보고에서는 한계효과를 계산해 해석하는 경우가 많다. 프로빗은 특히 경제학, 계량경제학 분야에서 널리 쓰이며, 효용모형이나 선택모형처럼 정규분포 오차가 자연스럽게 가정되는 상황에서 많이 활용된다.\n콤플리멘터리 로그–로그(link: cloglog)\n로짓이나 프로빗과 달리 비대칭적인 S자 곡선을 만든다. 정의는 \\(g(p) = \\log( - \\log(1 - p))\\)이며, 역변환을 하면 \\(p = 1 - \\exp( - \\exp(\\eta))\\)의 형태를 갖는다. 이 함수는 특히 확률이 0에 가까운 영역에서 변화가 천천히 일어나다가 일정 시점 이후 급격히 1에 가까워지는 모양을 보인다. 따라서 희귀사건이나 불균형 데이터에서 사건 발생 확률을 모형화할 때 적합하다. 또한 이산 시간 생존모형서도 널리 쓰이는데, 이 경우 cloglog 링크를 사용하면 회귀계수의 지수형태가 위험률과 직접적으로 연결되기 때문이다. 다만 계수를 바로 오즈비로 해석하기는 어렵고, 누적위험이나 한계확률 효과를 중심으로 해석해야 한다.\n즉, 세 가지 연결함수는 모두 이진형 목표변수에 적용할 수 있지만, 데이터 특성과 해석 목적에 따라 선택이 달라진다. 로짓은 오즈비 해석이 직관적이라 가장 보편적으로 쓰이고, 프로빗은 정규분포 오차를 가정하는 선택모형 이론과 잘 어울리며, cloglog는 비대칭 확률 구조와 위험률 해석이 필요한 상황에서 빛을 발한다.\n3가지 방법의 장점\n로짓 함수는 이진형 목표변수를 다룰 때 가장 널리 사용되는 방법이다. 그 이유는 해석이 매우 직관적이기 때문이다. 회귀계수 \\beta_j는 설명변수 X_j가 한 단위 증가할 때 오즈비(odds ratio)가 e^{\\beta_j}배 변한다는 의미를 갖는다. 오즈비는 상대위험이나 사건 발생 가능성을 직관적으로 보여주기 때문에 의학, 사회과학, 정책분야 등에서 널리 채택된다. 또한 로짓은 이항분포의 **정준 연결함수(canonical link)**로서 수학적으로도 우수한 성질을 지니며, 최대우도추정(MLE)에서 안정적인 결과를 준다.\n프로빗 함수의 장점은 이론적 기반에 있다. 확률을 표준정규분포의 누적분포함수(CDF)의 역함수로 변환하기 때문에, 잠재변수 모형(latent variable model)과 자연스럽게 연결된다. 설명변수가 잠재효용이나 심리적 임계값을 통해 결과에 영향을 준다고 생각할 때 프로빗은 합리적인 선택이 된다. 경제학의 이산선택모형이나 심리측정 연구에서 주로 사용되는 이유가 여기에 있다. 또한 로짓과 결과가 크게 다르지 않으면서, 정규분포 기반의 해석틀을 선호하는 분야에서는 이론적 정합성이 강한 장점으로 작용한다.\ncloglog 함수의 장점은 비대칭성을 표현할 수 있다는 점이다. 로짓이나 프로빗은 모두 대칭적인 S자 곡선을 가지지만, cloglog는 낮은 확률 영역에서는 매우 천천히 증가하다가 어느 시점 이후 급격히 1에 가까워진다. 이런 특성은 희귀사건(rare events)이나 불균형 데이터에서 유리하다. 또한 이산 시간 생존분석(discrete-time survival analysis)에서 hazard function과 직접 연결되기 때문에, 사건 발생의 위험률을 해석하는 데 특히 적합하다. 따라서 의료 통계, 생존분석, 공학적 신뢰도 분석 등에서 강점을 가진다.\n로짓, 프로빗, cloglog 세 가지 방법은 모두 이진형 목표변수를 설명할 수 있으며, 실제 분석 결과에서 예측확률 자체는 크게 다르지 않은 경우가 많다. 따라서 어떤 연결함수를 선택하느냐는 ”데이터의 특성”과 ”연구 맥락”에 달려 있다.\n실제로 로짓은 오즈비 해석이 직관적이기 때문에 기본값처럼 사용된다. 연구자나 독자가 ”위험이 몇 배 늘어난다”와 같은 해석을 선호한다면 로짓이 가장 설득력이 있다. 반면, 잠재효용이나 임계값 같은 개념을 강조하는 경제학적 분석에서는 프로빗이 더 자연스럽다. 또 사건이 희귀하거나 생존시간 분석처럼 위험률을 직접 다루는 맥락에서는 cloglog가 적합하다.\n즉, 세 가지 방법의 선택은 절대적인 우열이 아니라 데이터의 구조와 연구 목적을 기준으로 판단해야 한다. 중요한 점은 연결함수의 차이에 매몰되기보다는, 설명변수와 목표변수의 관계를 충실히 모형화했는지, 그리고 결과를 어떻게 해석할 것인지가 더 본질적이라는 것이다.\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\n\n# η 값 범위\neta = np.linspace(-4, 4, 400)\n\n# 로짓\nlogit_p = 1 / (1 + np.exp(-eta))\n\n# 프로빗\nprobit_p = norm.cdf(eta)\n\n# cloglog\ncloglog_p = 1 - np.exp(-np.exp(eta))\n\n# 그래프\nplt.figure(figsize=(8,5))\nplt.plot(eta, logit_p, label=\"Logit\", color=\"blue\", linewidth=2)\nplt.plot(eta, probit_p, label=\"Probit\", color=\"green\", linestyle=\"--\", linewidth=2)\nplt.plot(eta, cloglog_p, label=\"Cloglog\", color=\"red\", linestyle=\"-.\", linewidth=2)\n\n# 기준선\nplt.axvline(0, color=\"gray\", linestyle=\":\")\nplt.axhline(0.5, color=\"gray\", linestyle=\":\")\n\nplt.title(\"Logit vs Probit vs Cloglog\")\nplt.xlabel(\"Linear predictor η\")\nplt.ylabel(\"Predicted probability\")\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n3. 오즈비\n오즈비(odds ratio)는 두 가지 사건의 발생 확률을 비교하는 통계적 지표로, 주로 의학 연구나 사회과학 연구에서 두 집단 간의 관계를 평가할 때 사용됩니다. 오즈비는 특정 사건이 한 집단에서 발생할 오즈(odds)와 다른 집단에서 발생할 오즈의 비율을 의미한다.\n예를 들어, 특정 약물이 질병을 예방하는지 평가하는 연구에서 다음과 같은 결과를 얻었다고 가정합시다:\n\n(a) 약물을 복용한 그룹 (노출 있음)에서 질병이 발생한 사람 수: 20명\n(b) 약물을 복용한 그룹 (노출 있음)에서 질병이 발생하지 않은 사람 수: 80명\n(c) 약물을 복용하지 않은 그룹 (노출 없음)에서 질병이 발생한 사람 수: 30명\n(d) 약물을 복용하지 않은 그룹 (노출 없음)에서 질병이 발생하지 않은 사람 수: 70명\n오즈비 : \\(\\frac{ad}{bc} = 0.5833\\)\n오즈비가 1보다 작으면 노출된 그룹이 노출되지 않은 그룹보다 사건 발생 확률이 낮다는 것을 의미하고, 오즈비가 1보다 크면 그 반대를 의미합니다. 위의 예시에서 오즈비가 0.5833이므로, 약물을 복용한 그룹이 질병에 걸릴 확률이 약물을 복용하지 않은 그룹보다 낮다고 해석할 수 있습니다.\n\n\n\n4. 추정\n앞에서 설명하였듯이 모수 \\(b\\)의 추정량은 MLE 방법, N_R 방법으로 추정한다. 종속변수의 관측치는 (0, 1)의 이진형 값이나 추정 회귀계수에 의해 계산된 적합치는 (0, 1) 사이의 확률 값이다. 다음 예제는 약물 사용량에 따라 무당벌레 죽는 여부를 측정한 자료이다. 65마리 무당벌레에 1.69 용량을 살포했을 때 6마리는 죽고 나머지 59마리는 생존하였다.\n\n\n\n\n\n\\(logit(\\pi_{i}) = log(\\frac{\\pi_{i}}{1 - \\pi_{i}}) = a + b(Dose) + e\\), \\(\\pi_{i} = P(Success|Dose = x_{i})\\)\n\n오즈비 \\(\\frac{\\pi}{1 - \\pi}\\) : Dose=X 1단위 증가하면 오즈비는 \\(e^{b}\\)만큼 증가한다.\n모수 모형 : \\(\\pi = \\frac{1}{1 - exp(a + b(Dose))}\\), 회귀계수의 부호가 양수이고 값이 커지면 (성공: \\(\\pi_{i}\\) , event)가 커지므로 성공 확률이 높아지고 부호가 음수=&gt; 절대값이 커지면 \\(\\pi_{i}\\) 가 작아지므로 성공 확률이 낮아진다.\n\n로지스틱 모형\n\\(logit(\\pi) = ln\\frac{\\pi}{1 - \\pi} = X\\underset{¯}{b} + \\underset{¯}{e},\\underset{¯}{e} \\sim N(\\underset{¯}{0},\\sigma^{2}I)\\), \\(\\pi = P(Y = 1)\\)\n\\[P(y_{i} = 1) = \\pi_{i} = \\frac{1}{1 + exp( - \\alpha - \\beta_{1}x_{1i} - ... - \\beta_{p}x_{pi})} + e_{i}\\]\n\n목표변수 : (목표변수=1(관심사건 발생)) 확률이므로 적합값은 사후확률이다.\n오차 가정 : \\(e_{i} \\sim N(0,\\sigma^{2})\\) (정규성, 독립성, 등분산성)\n추정방법은 OLS 방법을 적용한다.\n\n모형평가\n혼동 행렬(Confusion Matrix)은 분류 모델의 성능을 평가할 때 사용되는 도구입니다. 혼동 행렬은 실제 값과 모델의 예측 값 간의 비교를 통해 모델의 예측 성능을 시각적으로 표현한다.\n질병 진단에서 질병이 없는 사람이 대부분인 경우, 정확도만으로 모델의 성능을 평가하는 것은 적절하지 않을 수 있습니다. 이 경우 정밀도, 재현율, F1 스코어 등의 지표를 함께 고려하는 것이 중요합니다.\n\nTrue Positive (TP, 진양성): 실제로 성공(양성)인 샘플을 성공으로 정확하게 예측한 경우의 수.\nTrue Negative (TN, 진음성): 실제로 실패(음성)인 샘플을 성공으로 정확하게 예측한 경우의 수.\nFalse Positive (FP, 위양성): 실제로 실패인 샘플을 성공으로 잘못 예측한 경우의 수. 흔히 Type I Error(1형 오류)라고 합니다.\nFalse Negative (FN, 위음성): 실제로 성공인 샘플을 실패으로 잘못 예측한 경우의 수. 흔히 Type II Error(2형 오류)라고 합니다.\n\n\n\n\n\n\n\n\n실제. 예측-&gt;\n성공\n실패\n\n\n\n\n성공\nTP\nFP\n\n\n실패\nFN\nTN\n\n\n\n정확도 accuracy 전체 예측 중 맞은 비율 \\(\\frac{TP + TN}{TP + TN + FP + FN}\\): 전체 예측 중에서 맞게 분류한 비율을 말한다. 즉, 모델이 전체 데이터에서 얼마나 많이 정답을 맞췄는지를 보여주는 가장 직관적인 지표이다. 하지만 정확도는 데이터가 불균형할 때(예: 사망자가 훨씬 많고 생존자가 적은 경우) 실제 성능을 과대평가할 수 있다는 한계가 있다.\n정밀도 precison 양성 예측 중 실제 양성의 비율 \\(\\frac{TP}{TP + FP}\\): 정밀도는 모델이 ”양성(생존)“이라고 예측한 것 중에서 실제로 양성인 비율을 의미한다. 즉, 생존이라고 판정한 사람 중에서 실제로 생존한 사람의 비율이다. 정밀도가 높다는 것은 ”거짓 양성”을 줄였다는 의미로, 잘못된 긍정을 피하는 데 강점이 있다.\n재현율 recall 실제 양성 중 양성으로 예측한 비율 (TPRate) \\(\\frac{TP}{TP + FN}\\) 민감도 sensitivity 라 한다. 재현율은 실제 양성 중에서 모델이 양성으로 제대로 맞춘 비율을 말한다. 즉, 실제 생존자 중에서 모델이 생존으로 판정한 비율이다. 재현율이 높다는 것은 ”거짓 음성(False Negative)“을 줄였다는 의미로, 놓치는 사례가 적다는 뜻이다. 특히 의료 진단처럼 놓치면 안 되는 문제에서 중요하다.\n특이도 specificity 실제 음성 중 음성으로 예측한 비율 \\(\\frac{TN}{TN + FP}\\): 특이도는 실제 음성(negative)인 대상 중에서 모델이 음성으로 올바르게 분류한 비율을 말한다. 즉, 사망자 중 사망이라고 맞춘 비율을 의미한다. 특이도가 높을수록, 모델이 실제 음성을 잘 걸러낸다는 의미다.\nF1 스코어 : \\(F1 = \\frac{2 \\times Precision \\times Recall}{Precision + Recall}\\) 정밀도와 재현율의 조화 평균이다. F1은 Precision과 Recall 중 하나가 낮으면 값도 낮아지므로, 두 지표 사이의 균형을 평가하는 데 적합하다. 불균형 데이터 상황에서 Accuracy보다 훨씬 의미 있는 지표가 된다.\n\n\n\n\nchapter 3. 로지스틱 회귀 사례분석\n\n1. 타니타닉 데이터\n\n목표변수 : survived(생존=1, 사망=0)\n측정형 예측변수 : 나이 age, 요금 fare, 동반 탑승한 형제자매, 배우자수 aibsp, 탑승한 부모자녀 수 parch\n범주형 예측변수 : 성별 sex(이진형), 출항항구 embarked_town\n(제외) 탑승권 등급 pclass, 객실등급 class는 요금과 상관계수가 높아 제외하였음\n\n# ===============================\n# 1. 데이터 불러오기\n# ===============================\nimport pandas as pd\nimport seaborn as sns\n\n# seaborn 패키지에 내장된 타이타닉 데이터셋 불러오기\ntitanic = sns.load_dataset('titanic')\n\n# 데이터 구조 확인\n# - 변수명, 자료형, 결측치 개수 등을 출력\ntitanic.info()\npclass: 탑승권 등급, 1(upper), 2(middle), 3(lower)\nsurvived: 생존여부, 1=생존, 0=사망 •name: 탑승자 이름\nsex: 성별, male/female\nage: 탑승자 나이\nsibsp: 동반 탑승한 형제자매, 배우자 수 •parch: 탑승한 부모자녀 수\nfare: 탑승권 지불요금\nembarked_town: 출항항구, C = Cherbourg, Q=Queenstown, S=Southampton\nclass : 객실등급 first, second, third\nadult_male : 성인남자 여부\ndeck : 'C', 'E', 'G', 'D', 'A', 'B', 'F'\nalive: 생존여부\nalone : 혼자 탑승여부\n\n\n2. 데이터 전처리\n범주형 변수를 이진형(0, 1) 변수로 만들어 측정형으로 만든다. 성별의 수준이 2개이므로 1개 이진형 변수(male 변수, 1=남자, 0=여자), 출발항구는 3곳이므로 2개 이진형 변수(Queenstown=0, Southampton=0이면 Cherbough 출항 승객)를 만들었다.\n# ===============================\n# 2. 데이터 전처리\n# ===============================\n\n# (1) 성별(sex) 변수 → 더미(dummy) 변수 변환\n# get_dummies(): 범주형 변수를 0/1 더미변수로 변환\n# drop_first=True → 기준 범주('female')를 제거하고 'male'만 생성\n# astype(int) → True/False를 1/0 정수형으로 변환\ntitanic['male'] = pd.get_dummies(titanic['sex'], drop_first=True).astype(int)\n\n# (2) 출항항구(embark_town) 변수 → 더미(dummy) 변수 변환\n# embark_town 범주는 ['Cherbourg', 'Queenstown', 'Southampton']\n# drop_first=True → 기준 범주 'Cherbourg' 제거\n# 결과적으로 'Queenstown', 'Southampton' 두 개의 더미변수 생성\ntitanic[['Queenstown', 'Southampton']] = pd.get_dummies(\n    titanic['embark_town'], drop_first=True\n).astype(int)\n\n\n3. 모형 추정\n# ===============================\n# 1. 로지스틱 회귀모형 추정\n# ===============================\nimport statsmodels.api as sm\n\n# 사용할 변수 지정\nvars = ['survived', 'age', 'sibsp', 'parch', 'fare', 'male', 'Queenstown', 'Southampton']\n\n# 결측치 제거\ntitanic_clean = titanic[vars].dropna()\n\n# y와 X 재정의\ny = titanic_clean['survived']\nX = titanic_clean[['age', 'sibsp', 'parch', 'fare', 'male', 'Queenstown', 'Southampton']]\nX = sm.add_constant(X)\n\n# 로지스틱 회귀 적합 및 출력\nlogit_model = sm.Logit(y, X)\nresult = logit_model.fit()\nprint(result.summary2())\n\n\n\n\n\n1. 모형 적합도 요약\n\nPseudo R² = 0.288 → 설명력이 나쁘지 않은 수준 (의학·사회 데이터에서는 0.2~0.4도 꽤 괜찮음).\nLLR p-value ≈ 3.9e-56 → 모형 전체가 유의함.\n\n2. 회귀계수 부호 해석\n\nage (-0.0214, p&lt;0.01): 나이가 많을수록 생존 확률이 감소한다. 즉, 나이 1살 증가 시 생존 오즈가 감소한다.\nsibsp (-0.3911, p&lt;0.01): 동반한 형제자매·배우자 수가 많을수록 생존 확률이 낮다. 탑승 인원이 많으면 생존 가능성이 떨어지는 경향.\nparch (-0.2167, p=0.067): 부모·자녀 동반 인원은 음(-)의 계수를 가지지만, 유의수준 5%에서는 통계적으로 유의하지 않다.\nfare (0.0151, p&lt;0.001): 운임이 비쌀수록 생존 확률이 증가한다. 높은 요금은 객실 위치·구조 등과 연결될 가능성이 큼.\nmale (-2.5626, p&lt;0.001): 남성일수록 생존 확률이 크게 감소한다. 여성의 생존율이 높았다는 역사적 사실과 일치한다.\nQueenstown (-1.3843, p&lt;0.05): 출항항구가 Queenstown일 경우, 기준(Cherbourg)보다 생존 확률이 낮다.\nSouthampton (-0.5943, p&lt;0.05): Southampton 출항자 역시 Cherbourg 대비 생존 확률이 낮음.\n\n\n\n4. 오즈비\n# 추정 오즈비 출력\nimport numpy as np\nodds_ratios = np.exp(result.params)\nprint(odds_ratios)\nconst 8.432476  age 0.978865  sibsp 0.676346  parch 0.805146  fare 1.015252  male 0.077106  Queenstown 0.250506  Southampton 0.551975\n오즈비는 통제집단 대비 처리집단의 성공 가능성으로 1보다 높으면 처리집단이 높다는 것이다.\n\nage (0.98): 나이가 한 살 많을수록 생존 오즈가 약 2% 감소한다.\nsibsp (0.68): 동반한 형제·자매·배우자가 1명 늘어나면 생존 오즈가 약 32% 감소한다.\nparch (0.81): 부모·자녀 동반 인원이 많을수록 생존 오즈가 감소하지만, 통계적으로는 뚜렷하지 않음.\nfare (1.02): 운임이 1 단위(파운드) 증가할 때 생존 오즈가 약 2% 증가한다.\nmale (0.08): 남성의 생존 오즈는 여성의 약 8%에 불과하다. (즉, 여성의 생존 확률이 훨씬 높음)\nQueenstown (0.25): Queenstown에서 탑승한 승객은 Cherbourg 승객에 비해 생존 오즈가 75% 낮다.\nSouthampton (0.55): Southampton 탑승자는 Cherbourg 대비 생존 오즈가 약 45% 낮다.\n\n\n\n5. 모형 정확도 Confusion 행렬\nfrom sklearn.metrics import confusion_matrix, classification_report\n\n# 1. 예측확률 추출\ny_pred_prob = result.predict(X)\n\n# 2. 임계값 0.5 기준으로 분류 (생존=1, 사망=0)\ny_pred = (y_pred_prob &gt;= 0.5).astype(int)\n\n# 3. 혼동행렬 생성\ncm = confusion_matrix(y, y_pred)\n\nprint(\"Confusion Matrix:\")\nprint(cm)\n\n# 4. 추가로 정밀도/재현율/정확도 보고서 출력\nprint(\"\\nClassification Report:\")\nprint(classification_report(y, y_pred, digits=3))\nConfusion Matrix:  [[363 61]  [ 93 197]]\nTN (363): 실제 사망(0), 예측도 사망(0) → 맞춘 경우  FP (61): 실제 사망(0), 예측은 생존(1) → 잘못 예측  FN (93): 실제 생존(1), 예측은 사망(0) → 잘못 예측  TP (197): 실제 생존(1), 예측도 생존(1) → 맞춘 경우\n즉, 모델은 사망자 424명 중 363명을 맞췄고, 생존자 290명 중 197명을 정분류하였다.\nClassification Report:  precision recall f1-score support  0 0.796 0.856 0.825 424  1 0.764 0.679 0.719 290  accuracy 0.784 714  macro avg 0.780 0.768 0.772 714  weighted avg 0.783 0.784 0.782 714\nAccuracy (정확도) = 0.784 (78.4%): 전체 예측의 약 78%가 맞았다. Titanic 데이터라는 복잡한 실제 사례에서는 꽤 준수한 성능이다.\nPrecision (정밀도): 사망(0): 0.796 → ”사망이라고 예측한 사람 중 약 79.6%가 실제 사망”, 생존(1): 0.764 → ”생존이라고 예측한 사람 중 약 76.4%가 실제 생존”\nRecall (재현율): 사망(0): 0.856 → ”실제 사망자 중 85.6%를 모델이 사망으로 잘 맞춤”, 생존(1): 0.679 → ”실제 생존자 중 67.9%만 모델이 생존으로 맞춤” → 즉, 사망자 예측은 잘 하지만, 생존자를 놓치는 경우가 많다는 의미.\nF1-score (정밀도와 재현율의 조화 평균): 사망(0): 0.825, 생존(1): 0.719, 생존자 쪽에서 다소 낮다.\n모델은 사망자 분류에 더 강하다 (recall=0.856). 하지만 생존자 분류는 약하다 (recall=0.679). 즉, 실제 생존자를 ”사망”으로 오판하는 경우(FN=93)가 적지 않다. Titanic 데이터의 역사적 사실(여성·아이 생존율 ↑, 남성·3등석 생존율 ↓)을 반영했지만, 단순 변수만으로는 생존자를 완전히 잘 구분하긴 어렵다. 전체 정확도는 78% 수준으로, 단순히 ”다 사망했다고 예측하는 trivial model”보다 훨씬 낫다.\n\n\n6. ROC 커브\nROC (Receiver Operating Characteristic) 커브는 이진 분류 문제에서 모델의 성능을 평가하는 그래프입니다. 이 커브는 y-축 민감도(TPR)와 x-축 (1-특이도=FPR)을 다양한 임계값(threshold)에서 계산하여 나타낸 그래프이다. ROC 커브 해석은 다음과 같다.\n\n커브의 좌상단에 가까울수록 좋은 모델: 민감도 높고 (1-특이도) 낮음.\n대각선 (랜덤 분류기): (0, 0)에서 (1, 1)로 가는 대각선은 랜덤 추측을 의미합니다. 모델의 성능이 이 대각선에 가까울수록 무작위 추측과 다를 바 없습니다.\n\nAUC (Area Under the Curve): ROC 곡선 아래의 면적을 의미하며, 모델의 분류 능력을 종합적으로 평가하는 지표입니다. AUC 값은 0.5에서 1 사이의 값을 가집니다.\n\nAUC = 0.5: 랜덤 추측\nAUC = 1: 완벽한 분류기\nAUC &gt; 0.8: 좋은 분류기\nAUC &lt; 0.5: 모델이 오히려 역으로 예측하고 있음\n\nYouden’s J score\nROC 커브에서 최적의 임계값(threshold)을 결정하는 데 사용하는 지표 중 하나입니다. 이 지표는 분류 모델의 성능을 평가할 때 민감도(sensitivity)와 특이도(specificity)를 동시에 고려합니다.\nYouden's J score = 민감도 + 특이도 -1\nYouden's J score는 -1에서 1 사이의 값을 가지며, 1에 가까울수록 모델의 성능이 좋다는 것을 의미합니다. 0일 경우에는 무작위 추측(random guessing)과 동일한 성능을 나타냅니다. Youden's J score의 해석은 다음과 같다.\n\nJ = 1: 완벽한 분류기 (모든 양성 샘플을 정확히 양성으로, 모든 음성 샘플을 정확히 음성으로 분류함)\nJ = 0: 모델이 무작위로 추측하는 것과 동일한 성능\nJ &lt; 0: 모델이 오히려 반대로 예측하고 있음\n\nYouden's J score를 사용하여 최적의 임계값 찾기 : 모델의 예측 결과를 기반으로 다양한 임계값에 대해 TPR과 FPR을 계산하고, 각 임계값에서의 Youden's J score를 구하여 최적의 임계값을 찾을 수 있습니다. 이는 Youden's J score가 최대가 되는 지점입니다.\n# ===============================\n# ROC Curve & AUC (for statsmodels Logit result)\n# ===============================\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import roc_curve, roc_auc_score, confusion_matrix, classification_report\n\n# 1) 예측확률\ny_prob = result.predict(X)      # P(Y=1|X)\ny_true = y.values if hasattr(y, \"values\") else y  # 정답 벡터\n\n# 2) ROC 좌표와 AUC\nfpr, tpr, thresholds = roc_curve(y_true, y_prob)\nauc = roc_auc_score(y_true, y_prob)\nprint(f\"AUC = {auc:.3f}\")\n\n# 3) ROC 그리기\nplt.figure(figsize=(6,5))\nplt.plot(fpr, tpr, lw=2, label=f\"ROC (AUC = {auc:.3f})\")\nplt.plot([0,1], [0,1], linestyle=\"--\", color=\"gray\", label=\"Random guess\")\nplt.xlim(0,1); plt.ylim(0,1)\nplt.xlabel(\"False Positive Rate (1 - Specificity)\")\nplt.ylabel(\"True Positive Rate (Sensitivity / Recall)\")\nplt.title(\"ROC Curve\")\nplt.legend(loc=\"lower right\")\nplt.grid(True)\nplt.show()\n\n# 4) (선택) 최적 임계값 선택: Youden's J = TPR - FPR 최대화\nyouden_idx = np.argmax(tpr - fpr)\nbest_thresh = thresholds[youden_idx]\nprint(f\"Best threshold by Youden's J = {best_thresh:.3f}\")\n\n# 5) (선택) 해당 임계값으로 성능 요약\ny_pred_best = (y_prob &gt;= best_thresh).astype(int)\ncm = confusion_matrix(y_true, y_pred_best)\nprint(\"Confusion matrix at best threshold:\\n\", cm)\nprint(\"\\nClassification report at best threshold:\")\nprint(classification_report(y_true, y_pred_best, digits=3))\n\n\n\n\n\nBest threshold by Youden's J = 0.287\nConfusion matrix at best threshold:\n[[333 91]  [ 65 225]]\nClassification report at best threshold:  precision recall f1-score support  0 0.837 0.785 0.810 424  1 0.712 0.776 0.743 290  accuracy 0.782 714  macro avg 0.774 0.781 0.776 714  weighted avg 0.786 0.782 0.783 714\n성공 예측 기준값을 0.5 사용 대신 0.287을 사용하면\nAccuracy (정확도): 0.5 기준: 0.784, 0.287 기준: 0.782 → 전체 정확도는 거의 차이가 없음.\nPrecision (정밀도, class=1): 0.5 기준: 0.764, 0.287 기준: 0.712\n→ 생존자라고 예측했을 때 실제 생존일 확률은 조금 낮아짐.\nRecall (재현율, class=1): 0.5 기준: 0.679, 0.287 기준: 0.776 → 실제 생존자를 잡아내는 비율이 크게 향상됨.\nF1-score (class=1): 0.5 기준: 0.719, 0.287 기준: 0.743 → 정밀도와 재현율의 균형 지표가 좋아짐.\n임계값을 0.5로 두는 경우는 다소 보수적인 기준이다. 이때 모델은 ”생존”이라고 판단하기보다는 ”사망” 쪽으로 예측하는 경향이 강하다. 그 결과, 실제 생존자임에도 불구하고 사망으로 분류되는 사례가 많아져 재현율(Recall)이 낮아진다. 그러나 일단 ”생존”이라고 예측한 경우에는 실제로 생존일 가능성이 높아 정밀도(Precision)는 상대적으로 좋아진다. 즉, 잘못된 긍정(False Positive)을 줄이는 데 강점이 있는 방식이다.\n반대로 ROC 분석에서 제시된 Youden’s J 최적 임계값(약 0.287)을 적용한 경우, 모델은 더 많은 사람을 ”생존”으로 분류한다. 이로 인해 생존자를 놓치는 경우(False Negative)가 줄어들어 재현율이 크게 향상된다. 다만, ”생존”이라고 예측했지만 실제로는 사망인 경우가 늘어나므로 정밀도는 다소 떨어진다. 즉, 실제 생존자를 더 많이 잡아내는 대신, 예측의 순도는 조금 희생하는 결과가 된다.\n정리하면, 임계값 0.5는 정밀도를 중시하는 보수적 접근이고, 임계값 0.287은 재현율을 중시하여 놓치는 생존자를 줄이는 접근이라고 할 수 있다.\n\n\n\nchapter 4. 다중 로짓 회귀\n\n(1) 개념\n다중 로짓모형은 종속변수가 세 개 이상의 범주(명목형)를 가질 때 사용하는 확장된 로지스틱 회귀모형이다. 이항 로지스틱은 ”예/아니오(0/1)” 두 가지 경우만 다루나 다중 로짓은 ”A, B, C”처럼 세 가지 이상 범주를 동시에 다룬다. 각 범주에 속할 확률을 추정하며, 특정 범주를 기준(baseline)으로 두고 나머지 범주와의 상대적 오즈를 추정한다.\n\n\n(2) 모형 구조 및 해석\n예를 들어, 종속변수 Y가 K개의 범주를 가진다고 하자. 범주 k에 속할 확률은 \\(P(Y = k|X) = \\frac{\\exp(X\\beta_{k})}{\\sum_{j = 1}^{K}\\exp(X\\beta_{j})},k = 1,\\ldots,K\\) 으로 정의된다.\n보통 하나의 범주(예: K)를 기준범주로 두고, 해당 범주에 대한 계수는 \\(\\beta_{K} = 0\\)으로 고정한다. 따라서 나머지 범주의 계수들은 기준범주 대비 상대적 로그 오즈를 의미한다.\n이항 로짓에서는 계수 \\(\\beta_{j}\\)가 ”X_j가 1 증가할 때 오즈비가 \\(e^{\\beta_{j}}\\)배 변한다”라고 해석된다.\n다중 로짓에서도 동일하게, 각 계수는 기준범주에 비해 특정 범주에 속할 상대적 오즈비로 해석된다. 따라서 해석은 ”기준범주와의 비교”라는 점을 잊지 말아야 한다.\n다중 로짓모형과 이항 로지스틱 회귀의 반복\n명목형 목표변수가 세 개 이상의 범주를 가질 때 가장 정석적인 방법은 다중 로짓모형을 적용하는 것이다. 이 모형은 한 번에 모든 범주를 고려하여 각 선택지에 속할 확률을 추정할 수 있고, 기준범주 대비 상대적 오즈비 해석도 가능하다.\n그러나 실제 연구에서는 다중 로짓모형 대신 이항 로지스틱 회귀를 여러 번 수행하는 방법이 종종 권장되기도 한다. 예를 들어, 목표변수가 세 가지 범주(A, B, C)라면, (A vs 나머지), (B vs 나머지), (C vs 나머지)처럼 이진화(binary coding) 해서 각각 로지스틱 회귀를 적합할 수 있다. 또는 연구의 목적에 따라 (A vs B), (A vs C), (B vs C)처럼 특정 두 범주만 비교하는 쌍대 비교(pairwise comparison) 로 나눌 수도 있다.\n이 방식의 장점은 해석이 단순하고, ”A 범주일 확률이 다른 모든 범주보다 얼마나 높은가?” 또는 ”B와 C 중 어떤 요인이 차이를 만드는가?“와 같이 명확한 질문에 답할 수 있다는 점이다. 특히 표본수가 적거나 변수 구조가 단순한 경우에는 다중 로짓보다 반복된 이항 로지스틱 회귀가 오히려 안정적이고 직관적일 수 있다.\n즉, 다중 로짓모형은 이론적으로 가장 일반화된 접근이지만, 연구 목적에 따라 이항 로지스틱을 여러 번 수행하는 방법이 더 권장될 수도 있다. 실제 분석에서는 ”모든 범주를 한꺼번에 설명하는 것이 필요한가?” 아니면 ”특정 범주 쌍의 비교가 더 의미 있는가?“라는 질문을 먼저 던지고, 그에 따라 적절한 모형을 선택하는 것이 바람직하다.\n\n\n(3) 사례 분석\n데이터명: Travel Mode Choice (modechoice)  1987년 호주 동부 지역(시드니–캔버라–멜번 구간)  비업무(non-business) 목적의 장거리(intercity) 여행 자료  표본 수: 210명 응답자 × 4가지 교통수단, 1 = Air (항공기), 2 = Train (기차), 3 = Bus (버스), 4 = Car (자가용)\n대안 특성(alternative-specific variables)  ttme : terminal waiting time (분) — car = 0  invc : monetary cost ($, 1987 AUD 기준)  invt : in-vehicle travel time (분)  gc : generalized cost = invc + (invt × 시간가치)\n개인 특성(individual-specific variables)  hinc : household income (in $1,000s 단위)  psize: party size (동행 인원 수)\n# ===============================\n# 1. 라이브러리 불러오기\n# ===============================\nimport pandas as pd\nimport numpy as np\nimport statsmodels.api as sm\n\n# ===============================\n# 2. 데이터 로드\n# ===============================\n# Travel Mode Choice (modechoice) 데이터 불러오기\ndata = sm.datasets.modechoice.load_pandas().data\n\n# choice == 1 인 행만 골라 개인별 선택 모드 확인\nchosen = data.loc[data['choice'] == 1, ['individual', 'mode']].copy()\n\n# 모드 숫자코드 → 라벨로 변환\nmode_map = {1: 'air', 2: 'train', 3: 'bus', 4: 'car'}\nchosen['mode'] = chosen['mode'].map(mode_map)\n\n# 종속변수 y: 개인별 선택 모드\ny = chosen.set_index('individual')['mode']\n\n# ===============================\n# 3. long → wide 변환 (대안특성)\n# ===============================\ntmp = data.copy()\ntmp['mode'] = tmp['mode'].map(mode_map)\n\n# 비용, 시간, 대기, 일반화 비용\nwide_invc = tmp.pivot(index='individual', columns='mode', values='invc').add_prefix('invc_')\nwide_invt = tmp.pivot(index='individual', columns='mode', values='invt').add_prefix('invt_')\nwide_ttme = tmp.pivot(index='individual', columns='mode', values='ttme').add_prefix('ttme_')\nwide_gc   = tmp.pivot(index='individual', columns='mode', values='gc').add_prefix('gc_')\n\n# 개인 특성 (소득, 동승자 수)\nindiv = (tmp.drop_duplicates(['individual'])\n           .set_index('individual')[['hinc','psize']])\n\n# ===============================\n# 4. 기준대안(car) 대비 차이변수 생성 함수\n# ===============================\ndef diff_vs_car(df, var):\n    \"\"\"wide 포맷 df에서 기준(car) 대비 대안별 차이 변수 생성\"\"\"\n    cols = [f'{var}_air', f'{var}_train', f'{var}_bus', f'{var}_car']\n    w = df[cols]\n    out = pd.DataFrame(index=w.index)\n    for a in ['air', 'train', 'bus']:   # car는 기준\n        out[f'{var}_{a}_minus_car'] = w[f'{var}_{a}'] - w[f'{var}_car']\n    return out\n\n# 차이변수 생성 (여기서는 일반화비용 gc만 사용)\nX_gc = diff_vs_car(wide_gc, 'gc')\n\n# ===============================\n# 5. 설명변수 설계행렬 구성\n# ===============================\nX = pd.concat([X_gc, indiv], axis=1).loc[y.index]\nX = sm.add_constant(X, has_constant='add')  # 상수항 추가\n\n# ===============================\n# 6. 다중 로짓(MNLogit) 적합\n# ===============================\nmodel = sm.MNLogit(y, X)\nres = model.fit()\nprint(res.summary())\n\n# ===============================\n# 7. 오즈비(odds ratio) 계산\n# ===============================\nodds_ratio = np.exp(res.params)\nprint(\"\\n[Odds Ratios]\\n\", odds_ratio)\n\n# ===============================\n# 8. 예측확률 출력\n# ===============================\npred_prob = res.predict(X)\nprint(\"\\n[First 5 predicted probabilities]\\n\", pred_prob.head())\n\n\n\n\n\n[Odds Ratios]  0 1 2\nconst 2.191713 0.895179 13.068463  gc_air_minus_car 0.992664 0.961226 0.947739  gc_train_minus_car 1.025245 0.980246 0.907682  gc_bus_minus_car 0.946977 1.091743 1.133681  hinc 0.952935 0.989719 0.960261  psize 0.909128 0.842491 0.773353\n[First 5 predicted probabilities]  0 1 2 3\nindividual  1.0 0.286019 0.024696 0.456432 0.232852  2.0 0.159306 0.020636 0.399126 0.420931  3.0 0.343931 0.061799 0.587268 0.007002  4.0 0.250099 0.002585 0.644929 0.102388  5.0 0.207010 0.054901 0.117606 0.620483\n(1) 모형 적합도:\nPseudo R² = 0.3437 → 선택행동 데이터를 설명하는 데 상당히 괜찮은 수준. LLR p-value &lt; 0.001 → 모형 전체가 통계적으로 유의.\n(2) 비용 차이 변수\ngc_air_minus_car: car, train 모두 음수 & 유의 → 항공의 일반화 비용이 커질수록 다른 수단(car, train)을 선택할 확률이 증가. 즉, 항공이 비싸질수록 항공보다 다른 수단 선호.\ngc_train_minus_car: train 방정식에서 음수 & 매우 유의 → 기차의 일반화 비용이 커지면 car 대비 기차 선택 확률이 줄어듦. bus/car 식에서는 방향성이 약하거나 유의성 낮음.\ngc_bus_minus_car: bus와 train 방정식에서 양수 & 유의 → 버스의 일반화 비용이 커질수록 car 대비 bus 선택 가능성이 줄고, car 선택 가능성이 커짐.\n정리: 일반화 비용(gc)이 커질수록 해당 교통수단의 선택 확률이 줄어든다는 경제적 직관과 일치.\n(3) 개인 특성\nhinc (소득): car, train, bus 모두 음수 & 유의 → 소득이 높을수록 car 대비 다른 교통수단 선택 확률이 줄어듦. 즉, 소득이 높을수록 항공을 선호한다는 해석 가능. (air가 baseline이므로 나머지 수단 대비 항공 선택 확률 증가로 읽을 수 있음)\npsize (동행인원): 모든 방정식에서 음수지만, 통계적으로 유의하지 않음. 즉, 동승자 수는 교통수단 선택에 뚜렷한 영향을 주지 않음.\n(4) 오즈비(odds ratios) 해석\n\ngc_air_minus_car: 오즈비 &lt; 1 → 항공비용이 커질수록 상대수단 선택 오즈 증가.\ngc_train_minus_car (train eq.) ≈ 0.91: 기차의 일반화 비용이 1단위 증가할 때, car 대비 기차 선택 오즈가 약 9% 감소.\ngc_bus_minus_car (bus eq.) ≈ 0.95: 버스의 일반화 비용이 1단위 증가하면, car 대비 버스 선택 오즈가 약 5% 감소.\nhinc ≈ 0.95: 소득이 1000$ 증가할 때, car 대비 bus/train 선택 오즈가 약 5% 감소 → 소득이 높을수록 항공 선호.\npsize &lt; 1 (0.77~0.91): 동승 인원 많을수록 car 대비 다른 교통수단 선택 오즈가 줄지만 통계적 유의성은 낮음.\n\n(5) 요약\n대안특성 효과: 각 교통수단의 일반화 비용(gc)이 커질수록 해당 교통수단의 선택 확률은 감소. 경제학적으로 일관된 결과dlek.\n개인특성 효과: 소득(hinc)이 높을수록 car·bus·train보다 항공 선택 가능성이 커짐. 동승자 수(psize)는 뚜렷한 영향 없ek\n전체적 시사점: 교통수단 선택은 비용/시간 같은 대안특성이 주요 결정요인이고, 개인 특성 중에서는 소득이 강한 영향. 이는 ”소득이 높을수록 빠른 교통수단(항공)을 선호한다”는 행동경제학적 직관과 부합한다."
  },
  {
    "objectID": "notes/linear_model/lm_selection.html",
    "href": "notes/linear_model/lm_selection.html",
    "title": "회귀분석 2. 변수선택",
    "section": "",
    "text": "chapter 1. 변수선택 개념\n회귀분석에서 첫 단계는 어떤 예측변수를 모형에 포함할 것인가를 결정하는 것이다. 이는 단순히 통계적 절차로만 이루어지는 것이 아니라, 연구자의 이론적 근거와 사전 지식에 바탕을 두고 목표변수 Y에 영향을 줄 것으로 예상되는 변수를 선택하는 과정이다.\n이렇게 선정한 예측변수를 이용해 선형 회귀모형을 설정하고, 데이터를 수집하여 회귀계수를 최소자승법(OLS) 으로 추정한다. 추정된 모형이 실제로 목표변수의 변동을 설명할 수 있는지를 판단하기 위해서는 검정 절차가 필요하다.\n먼저, 분산분석(ANOVA)의 F-검정을 통해 전체 회귀모형의 유의성을 검정한다.\n\n귀무가설: 모든 회귀계수는 0이다. \\(\\beta_{1} = \\beta_{2} = \\cdots = \\beta_{k} = 0\\)\n대립가설: 적어도 하나의 회귀계수는 0이 아니다.\n\n만약 F-검정에서 귀무가설이 기각되지 않으면, 선택한 예측변수들이 목표변수를 설명하지 못한다는 의미가 된다. 반대로 귀무가설이 기각되면, 모형 전체가 유의하다고 판단할 수 있으며, 이때 개별 예측변수가 유의한지도 살펴보아야 한다.\n개별 예측변수의 효과는 t-검정을 통해 확인한다.\n\n귀무가설: \\(\\beta_{j} = 0\\), 대립가설: \\(\\beta_{j} \\neq 0\\)\n\nt-검정에서 유의하다면 해당 예측변수는 목표변수에 선형적으로 의미 있는 영향을 준다고 해석할 수 있다.\n오컴의 면도날(Occam’s Razor)\n회귀분석에서 모형을 설정할 때, 가능한 한 단순하면서도 충분히 설명력이 있는 모형을 추구하는 것이 원칙이다. 이러한 사고방식은 철학적 개념인 오컴의 면도날(Occam’s Razor) 과 연결된다. 오컴의 면도날은 “모든 것이 동일하다면 가장 간단한 설명이 최선이다”라는 원리를 말하며, 통계학에서는 이를 경제성의 원리(principle of economy) 또는 절약성의 원리(principle of parsimony) 라고 부른다.\n회귀분석에 적용하면, 동일한 설명력을 가진 두 개의 모형이 있다면, 예측변수가 더 적은 모형을 선택하는 것이 바람직하다는 의미가 된다. 즉, 불필요하게 많은 변수를 넣는 것은 해석을 복잡하게 만들고 과적합(overfitting)의 위험까지 초래하기 때문에, 가능한 한 간결한 모형을 유지하는 것이 좋다.\n실제 변수 선택 과정에서는 다음과 같은 절차가 자주 사용된다.\n1. 먼저 모든 후보 예측변수를 포함한 완전모형(full model) 을 적합한다.\n2. 그 후 유의하지 않은 변수를 하나씩 제거해 나간다.\n3. 제거할 변수는 가장 유의하지 않은 변수, 즉 t-검정 통계량 절댓값이 가장 작은 변수(동일하게는 p-value가 가장 큰 변수)를 우선적으로 제외한다.\n4. 변수를 제거할 때마다 다시 모형을 적합하고, 모든 남아 있는 변수가 유의해질 때까지 이 과정을 반복한다.\n다중공선성 분석과 순서 문제\n대부분의 교과서에서는 다중공선성 진단을 먼저 설명하고 있으나 어느 단계를 먼저해도 동일 결과를 얻는 경우가 대부분이므로 간편 작업(변수선택 과정을 먼저 거치면 다중공선성 진단을 하면 진단 변수의 수 줄어듬)을 위하여 변수선택을 먼저하는 것이 적절하다.\n변수선택 반드시 필요한가?\n회귀분석 과정에서 변수 선택(variable selection) 과 다중공선성(multicollinearity) 진단은 모두 필수적인 절차이다. 전통적인 교과서에서는 보통 다중공선성 진단을 먼저 소개하고, 그 후 변수 선택 방법을 설명하는 경우가 많다. 하지만 실제 분석 실무에서는 이 순서를 반드시 고정할 필요는 없다.\n왜냐하면, 변수 선택과 다중공선성 진단의 순서를 어떻게 두더라도 최종적으로 동일한 결과를 얻는 경우가 대부분이기 때문이다. 다만, 효율성을 고려한다면 순서를 바꾸는 것이 더 합리적일 수 있다.\n특히, 모든 후보 변수를 포함한 상태에서 다중공선성을 먼저 진단하면 불필요한 변수까지 함께 고려해야 하므로 계산과 해석이 번거로워진다. 반대로, 먼저 변수 선택 과정을 거쳐 유의하지 않은 예측변수를 제거하고 난 뒤 다중공선성을 진단하면, 분석해야 할 변수가 줄어들어 작업이 간편해진다.\n따라서 실무적으로는 변수 선택 → 다중공선성 진단 순서로 접근하는 것이 적절하다. 이는 분석의 효율성을 높이고, 해석해야 할 변수 집합을 줄여주는 장점이 있다.\n교과서에서 ”다중공선성 진단을 먼저”라고 가르치는 이유도 나름 있습니다. 만약 강한 공선성이 있으면, t-검정 값이 작게 나와 유의하지 않은 것처럼 보일 수 있습니다. 즉, 공선성 때문에 유효한 변수가 잘려 나갈 위험이 있다는 것이죠. =&gt; 이를 방지하기 위하여 목표변수와 상관계수가 일정 값 이상인 예측변수는 변수선택 과정에서 우선 선택한다.\n\n\nchapter 2. 변수선택 고전적 방법\n\n1. 모형 설명력 척도\n결정계수 Determination Coefficient \\(R_{p}^{2} = \\frac{SSR_{p}}{SST_{p}}\\)\n회귀분석에서 결정계수는 모형 내의 예측변수들이 목표변수의 총변동(즉, 목표변수 관측값들의 변동)을 얼마나 잘 설명하는지를 나타내는 수치이므로, 변수 선택의 중요한 지표가 된다. 결정계수는 총변동 중 회귀모형이 설명하는 부분의 비율로 정의되며, 단순회귀의 경우에는 상관계수의 제곱과 동일하다.\n결정계수의 크기가 70%라면 설정된 예측변수의 설명력이 충분하다고 볼 수 있고, 80% 이상이면 매우 충분하다고 평가할 수 있다. 90% 이상이면 거의 완전한 설명력을 가진 것으로, 현재 모형에 포함된 예측변수만으로도 목표변수의 변동을 매우 잘 설명할 수 있음을 의미한다. 다만 이 해석은 학문 분야에 따라 상대적인 차이가 있음을 유념해야 한다. 예를 들어, 물리학·공학 분야에서는 자연 현상이 비교적 규칙적이므로 결정계수가 0.9 이상인 경우가 흔하지만, 사회과학·의학 연구에서는 인간 행위나 복잡한 환경 요인으로 인해 \\(R^{2}\\)가 0.3~0.5 정도만 되어도 상당히 높은 설명력으로 간주된다. 따라서 결정계수 크기의 ”충분하다”는 판단 기준은 절대적인 것이 아니라 연구 맥락에 따라 달라진다.\n결정계수는 예측변수의 개수가 같은 경우, 어떤 변수 집합이 더 큰 설명력을 가지는지를 비교하는 데 유용하다. 그러나 결정계수 자체에는 검정 통계량이 존재하지 않는다는 한계가 있다. 따라서 모형이 유의한지를 확인하기 위해서는 분산분석(ANOVA)을 통한 F-검정을 별도로 실시해야 한다.\n또한 예측변수의 개수가 증가할수록 결정계수는 항상 커지므로, 단순 결정계수를 변수 개수가 다른 모형을 비교하는 데 사용하는 것은 적절하지 않다. 불필요한 변수를 계속 추가해도 \\(R^{2}\\) 값은 인위적으로 증가할 수 있기 때문이다. 이러한 한계를 보완하기 위해서는 수정결정계수를 사용하는 것이 바람직하다. 수정결정계수는 표본 크기와 변수 개수를 함께 고려하여, 유효한 변수가 추가될 때만 값이 증가하며, 불필요한 변수가 들어가면 오히려 감소할 수 있다. 따라서 변수 개수가 다른 모형을 비교할 때는 반드시 수정결정계수를 기준으로 판단해야 한다.\n더 나아가, 실제 분석에서는 결정계수만을 의존하는 것도 위험하다. \\(R^{2}\\)는 훈련 데이터에서의 설명력을 반영할 뿐, 새로운 데이터에 대한 예측 성능을 보장하지 않는다. 변수를 많이 넣으면 설명력은 높아 보이지만 실제로는 과적합(overfitting)이 발생하여 예측력이 떨어질 수 있다. 따라서 교차검증(cross-validation)을 통한 예측력 평가나, AIC·BIC 같은 정보 기준을 함께 사용하는 것이 바람직하다.\n수정결정계수 adjusted \\(R_{adj}^{2} = 1 - \\frac{(1 - R^{2})/(n - 1)}{(n - p - 1)}\\)\n수정결정계수는 결정계수의 문제점, 즉 유의하지 않은 예측변수가 삽입되어도 \\(R^{2}\\) 값이 항상 증가한다는 한계를 보완하기 위해 고안된 척도이다. 수정결정계수는 예측변수의 수가 늘어날 때 감소할 수도 있기 때문에, 불필요한 변수를 넣었을 때 자동적으로 설명력이 과대평가되는 문제를 줄여 준다.\n그러나 이 지표 역시 완벽하지 않다. 예측변수의 개수가 늘어나면 SSE(오차제곱합)는 줄어들지만 동시에 오차 자유도는 감소하므로, 이 두 효과가 상쇄되어 값이 크게 달라지지 않는다. 따라서 수정결정계수를 회귀모형의 절대적인 설명력 지표로 보기는 어렵다. 또한 수정결정계수 자체에는 통계적 유의성을 검정할 수 있는 별도의 검정통계량이 존재하지 않는다. 이 때문에 모형의 유의성은 여전히 분산분석의 F-검정이나 개별 계수의 t-검정을 통해 확인해야 한다.\n수정결정계수의 가장 큰 활용 가치는, 예측변수 집합과 개수가 서로 다른 회귀모형을 비교할 때이다. 단순 결정계수는 변수 개수가 많아질수록 무조건 커지므로 모형 비교에 적절하지 않지만, 수정결정계수는 불필요한 변수가 포함되면 오히려 감소할 수 있어 모형 간의 상대적 비교에 유리하다. 따라서 분석 목적이 목표변수의 예측력을 높이는 최적 모형을 찾는 것이라면, 수정결정계수를 기준으로 모형을 선택하는 방법을 사용할 수 있다.\nMallow \\(C_{p} = \\frac{SSE_{p}}{{\\widehat{\\sigma}}^{2}} - (n - 2p)\\)\nMallow \\(C_{p}\\) 값은 회귀모형 선택 기준 가운데 하나로, 예측변수 개수(절편 포함 \\(p + 1\\))와 \\(C_{p}\\) 값이 근사할 경우 좋은 회귀모형으로 판단한다. 이때 중요한 점은, 이 방법 역시 개별 예측변수의 유의성을 직접 검정하는 절차는 아니라는 것이다.\n이론적 근거는 다음과 같다. Mallow \\(C_{p}\\)는 모형의 추정치와 실제 모형이 주는 기대값 사이의 평균제곱오차(MSE)를 근사하는 척도로, 불편성과 효율성을 동시에 고려한 기준이다. 즉, 만약 모형이 적합하다면 \\(C_{p}\\) 값은 대략적으로 예측변수 개수(\\(p + 1\\))와 유사해져야 한다. 따라서 C_p \\approx p 인 경우, 해당 모형은 설명력이 충분하면서도 불필요한 변수를 포함하지 않는 간명한 모형이라고 해석할 수 있다.\n\n\\(C_{p} \\approx p + 1\\): 적절한 모형 → 편향(bias)과 분산(variance)의 균형이 잘 맞음.\n\\(C_{p} \\gg p + 1\\): 불필요한 변수가 많아 오차항이 커진 모형.\n\\(C_{p} &lt; p + 1\\): 과소적합 가능성(중요 변수를 놓쳤을 수 있음).\n\n여전히 개별 계수의 유의성은 따로 검정하지 않으며, 표본 수가 작거나 예측변수 간 다중공선성이 강하면 안정성이 떨어진다.\n예측잔차자승합(Prediction REsidual Sum of Squares) \\(PRESS = \\sum(y_{i} - \\widehat{y_{(i)}})^{2}\\)\n\\(\\widehat{y_{(i)}}\\) : \\(i\\)-번째 관측치를 제외한 후 모형을 추정한 후 구한 목표변수 \\(y_{i}\\)의 적합값, i-번째 관측치 제외 잔차 \\((y_i - \\hat y_{(i)})\\), 각 관측치에 대해 ”나를 빼고 학습했을 때 나를 얼마나 잘 맞추는가?“를 재는 LOOCV(Leave-One-Out Cross-Validation) 오차의 합으로 값이 작을수록 일반화 예측력이 좋다는 뜻이다.\nAIC, SBC 모두 작을수록 적합도가 높다.\nAIC(Akaike Information Criteria)\n\\(AIC = 2p - 2ln(\\widehat{L})\\), \\(\\widehat{L}\\) : 회귀모델의 최대우도함수\nBIC(Bayesian information criterion)\n\\[BIC = p*ln(n) - 2ln(\\widehat{L})\\]\nComment\n(수정)결정계수, Mallow \\(C_{p}\\), PRESS 통계량은 모두 ”어떤 변수 집합이 좋은 모형인가”를 평가하는 지표일 뿐, 개별 변수의 유의성을 직접 검정하는 절차는 아니다. 따라서 이 방법들로 선택된 모형 내에서도 각 예측변수가 통계적으로 유의한지는 별도의 t-검정이나 F-검정을 통해 확인해야 한다.\n일반적으로는 다음과 같은 절차가 활용된다. 먼저 수정결정계수와 Mallow \\(C_{p}\\) 기준에 따라 설명력이 충분하다고 판단되는 변수 집합을 몇 개 선택한다. 그 다음 각 후보 집합에 대해 PRESS 값을 계산하여 실제 예측력이 가장 좋은 모형을 최종 선택하는 방식이다.\nAIC, BIC, PRESS와 같은 지표는 서로 전혀 다른 변수 집합을 비교할 때 특히 유용하다. 즉, 이미 선택된 변수들이 모두 유의하다고 가정한 뒤, ”어떤 조합이 가장 적합한가”를 평가하는 데 사용된다. 이들은 적합도의 정도를 비교하는 수치로 널리 쓰이며, 특히 AIC와 BIC는 빅데이터 분석에서 예측모형을 비교·선택할 때 표준적으로 사용된다.\n\n수정결정계수: 변수 개수가 다른 모형 비교 시 필수적이지만, 유의성 검정은 제공하지 않는다.\nMallow \\(C_{p}\\): \\(C_{p} \\approx p + 1\\)일 때 좋은 모형으로 간주되며, 잔차분산과 변수 개수 균형을 본다.\nPRESS: LOOCV(Leave-One-Out Cross Validation) 기반 예측력 지표로, 값이 작을수록 일반화 성능이 좋다.\nAIC/BIC: 정보 기준으로, 모형의 적합도와 복잡도를 동시에 고려한다. BIC는 변수 수에 더 큰 페널티를 주므로, 빅데이터 환경에서 과적합 방지에 특히 효과적이다.\n해석 관점: 이 지표들은 모두 ”모형 수준의 비교”를 위한 것이지, ”개별 변수의 유의성 판단”을 대체하지 못한다.\n\n부분 결정계수 Coefficient of Partial Determination\n부분 결정계수는 ”기존 모형에 새로운 변수를 넣었을 때 설명력이 얼마나 더 증가했는가”를 나타낸다. 예를 들어 3개의 예측변수가 존재하는 \\(X_{1},X_{2},X_{3}\\) 모형을 가정하자. \\(X_{1}\\)에 의해 설명되고 남은 예측변수 변동을 두 예측변수(\\(X_{2},X_{3}\\))가 설명하는 변동을 다음과 같이 정의하고 이를 부분 결정계수라 한다.\n\n\n\n\n\n\n분자: \\(X_{2},X_{3}\\)가 \\(X_{1}\\)이 설명하지 못한 부분에서 추가적으로 설명한 변동\n분모: \\(X_{1}\\)만 썼을 때 남아 있던 오차 변동\n값이 0에 가까우면, 새로운 변수(\\(X_{2},X_{3}\\))는 기존 변수(\\(X_{1}\\))가 설명하지 못한 변동을 거의 설명하지 못한다.\n값이 1에 가까우면, 새로운 변수가 기존 모형에 비해 상당한 설명력을 추가했음을 의미한다.\n\n부분결정계수의 활용을 정리하면 다음과 같다.\n1. 변수 선택: 단계적 회귀(stepwise regression)에서 새로운 변수를 추가할지 말지 판단하는 기준. 예: \\(X_{1}\\)이 이미 들어간 상태에서 \\(X_{2},X_{3}\\)를 추가했을 때 부분 \\(R^{2}\\)가 크면 포함할 가치가 있음.\n2. 공헌도 평가: 여러 변수들이 동시에 들어가 있을 때, 특정 변수 집합이 추가적으로 기여하는 설명력 파악. 예: ”인구학적 변수(연령·성별) 통제 후, 사회경제적 변수(소득·학력)가 얼마만큼 설명력을 추가하는가?”\n3. 다중공선성 관련 해석: 어떤 변수가 기존 변수들과 강하게 상관되어 있으면, 부분 결정계수가 낮게 나와 실제로는 중요하지만 모형에서 기여가 작아 보일 수 있음. 따라서 부분 R^2 해석 시 공선성 여부를 함께 고려해야 한다.\n\n\n2. 최종 회귀모형 선택 방법 : 모형비교\n서로 예측변수가 고려된 모형 중 최적 모형을 선택하고자 할 때는 MAE(Mean Absolute Error 오차평균절대), MSE(Mean Sqaured Error 오차평균자승합), RMSE(Root Mean Sqaured Error 자승합 제곱근) 나 수정결정계수 값을 비교한다.\n표본데이터 크기 : \\(n\\)이고 OLS 목표변수 적합치(추정치) : \\({\\widehat{y}}_{i}\\)인 경우\n\n\\(MAE = \\frac{1}{n}\\sum|Y_{i} - {\\widehat{Y}}_{i}|\\)\n\\(MSE = \\frac{1}{n}\\sum(Y_{i} - {\\widehat{Y}}_{i})^{2}\\), \\(RMSE = \\sqrt{MSE}\\)\n\n새로운 자료 (표본크기 \\(n^{*}\\)) 수집하고 모형의 예측력을 다음 방법에 의해 계산하여 각 모형을 비교한다.\n평균자승합예측오차 Mean Square Prediction Error\n\\(MSPE = \\frac{1}{n^{*}}\\sum^{n^{*}}(y_{i} - {\\widehat{y}}_{i})^{2}\\), 새로운 자료로 회귀 모형을 추정하였을 때 이전 자료에서 추정된 회귀 모형과 유사하면 추정된 회귀 모형은 좋다고 판단할 수 있다. 새로운 자료 수집이 불가능한 경우에는 데이터를 splitting하여 모형추정 데이터와 예측 데이터로 나누어 분석한다.\n수정결정계수나 PRESS, AIC, SBC에 의한 최적 회귀모형 선택은 예측변수가 서로 다른 그룹을 비교할 때 사용되는 통계량이다.\n\n\n3. 고전적 변수선택 방법\n\n(1) 후진제거 Backward\n후진제거법은 완전모형에서 시작하여, 가장 유의하지 않은 변수를 하나씩 제거하면서 최종적으로 모든 변수가 유의한 상태의 모형을 선택하는 방법이다. 직관적이고 계산이 간편하다는 장점이 있지만, 다중공선성이나 p-value 의존성으로 인해 변수 선택이 불안정해질 수 있으므로 다른 모형 선택 지표와 함께 활용하는 것이 바람직하다\n1. 우선 고려된 모든 예측변수를 모형에 포함하여 회귀모형을 적합한다.\n2. 추정된 회귀계수들에 대해 유의성 검정을 실시하고, 이 중 가장 유의하지 않은 변수, 즉 유의확률(p-value)이 가장 크거나 F값이 가장 작은 변수를 선택하여 제외한다.\n3. 변수를 제거한 새로운 모형을 다시 적합한 뒤, 동일한 절차를 반복한다.\n4. 최종적으로 모형에 남은 모든 예측변수가 통계적으로 유의할 때 과정이 종료된다.\n장점\n\n모든 변수를 고려한 상태에서 시작하기 때문에 중요한 변수를 놓칠 위험이 상대적으로 적다.\n계산 절차가 비교적 단순하고 직관적이다.\n\n단점\n\n표본 크기가 작거나 예측변수 간 다중공선성이 심할 경우, 변수 제거 과정이 불안정할 수 있다.\np-value 기준으로만 판단하기 때문에, 실제로는 설명력이 있는 변수가 제외될 가능성이 있다.\n변수 제거 순서에 따라 결과 모형이 달라질 수 있다.\n\n후진제거법은 보통 ”단계적 회귀(stepwise regression)“의 한 형태로 사용되며, 수정결정계수나 AIC/BIC, PRESS와 같은 모형 선택 기준을 함께 고려하면 더 안정적인 모형 선택이 가능하다.\n# ==============================================\n# Boston Housing: 후진제거 변수선택\n# ==============================================\n\nimport pandas as pd\nimport numpy as np\nimport statsmodels.api as sm\nfrom sklearn.datasets import fetch_openml\n\n# --- 0) 데이터 로드 ---\nboston = fetch_openml(name=\"boston\", version=1, as_frame=True)\ndf = boston.frame.copy()\n\n# 종속변수 / 설명변수\ny = pd.to_numeric(df[\"MEDV\"], errors=\"coerce\")\nX = df.drop(columns=[\"MEDV\"]).copy()\n\n# 숫자형으로 변환\nfor c in X.columns:\n    if X[c].dtype.name in [\"object\", \"category\"]:\n        X[c] = X[c].astype(str).str.strip()\n        X[c] = pd.to_numeric(X[c], errors=\"coerce\")\n    else:\n        X[c] = X[c].astype(float)\n\n# 결측 제거\nmask = y.notna() & X.notna().all(axis=1)\ny = y[mask].reset_index(drop=True)\nX = X.loc[mask].reset_index(drop=True)\n\n# --- 1) 후진제거 함수 ---\ndef backward_elimination(X, y, alpha=0.05, verbose=True):\n    selected = list(X.columns)   # 모든 변수 포함\n    changed = True\n\n    while changed and len(selected) &gt; 0:\n        changed = False\n        X_try = sm.add_constant(X[selected], has_constant='add')\n        model = sm.OLS(y, X_try).fit()\n\n        # 절편 제외 p-value 확인\n        pvals = model.pvalues.drop(\"const\")\n        worst_feature = pvals.idxmax()\n        worst_pval = pvals.max()\n\n        if worst_pval &gt; alpha:\n            selected.remove(worst_feature)\n            changed = True\n            if verbose:\n                print(f\"[REMOVE] {worst_feature} (p={worst_pval:.4g})\")\n\n    return selected\n\n# --- 2) 실행 ---\nselected_features = backward_elimination(X, y, alpha=0.05, verbose=True)\nprint(\"\\n최종 선택 변수:\", selected_features)\n\n# --- 3) 최종 모형 적합 ---\nX_final = sm.add_constant(X[selected_features], has_constant='add')\nfinal_model = sm.OLS(y, X_final).fit()\nprint(final_model.summary())\n[REMOVE] AGE (p=0.9582)  [REMOVE] INDUS (p=0.738)\n\n\n(2) 전진삽입 Forward\n전진삽입법은 후진제거법과 반대로, 아무 변수도 포함하지 않은 상태에서 시작하여, 하나씩 변수를 추가하면서 최종 모형을 완성하는 방식이다.\n1. 고려된 예측변수들 중에서 목표변수 변동을 가장 많이 설명하는 변수, 즉 결정계수 증가량(\\Delta R^2)이 가장 크거나 유의확률(p-value)이 가장 작은 변수를 찾는다. 그 설명력이 통계적으로 유의하면 이 변수를 첫 번째 예측변수로 선택한다.\n2. 첫 번째 변수가 선택된 이후에는, 이미 선택된 변수들이 설명하고 남은 목표변수의 변동(잔차 부분)을 기준으로 한다. 남은 후보 변수들 중에서 이 잔차 변동을 가장 많이 설명하는 변수(부분 F-통계량이 가장 큰 변수)를 찾고, 그 설명력이 유의하다면 두 번째 변수로 선택한다.\n3. 이 과정을 반복하여, 더 이상 추가할 만한 유의한 변수가 없을 때 멈춘다. 이때 남은 후보 변수들은 기존 변수들과의 상관관계 때문에 추가 설명력이 통계적으로 유의하지 않다고 판단된 경우이다.\n장점\n\n모형에 변수가 점점 추가되는 과정이 직관적이며, 분석자가 단계별 변화를 쉽게 이해할 수 있다.\n불필요한 변수가 처음부터 들어오지 않으므로, 단순한 모형을 우선적으로 고려할 수 있다.\n\n단점\n\n초기에 잘못 선택된 변수가 있으면, 이후 절차에 큰 영향을 미쳐 전체 모형이 왜곡될 수 있다.\n일단 포함된 변수는 나중에 제거되지 않기 때문에, 후진제거법보다 유연성이 떨어진다.\n다중공선성이 강한 경우에는 유의성 판단이 불안정할 수 있다.\n\n전진삽입법은 후진제거법과 함께 단계적 회귀(stepwise regression) 의 기본 틀을 이룬다. 단순히 p-value만 기준으로 삼기보다는 AIC, BIC, PRESS, 교차검증(CV)과 같은 다른 모형 선택 기준을 병행하면 안정적인 결과를 얻을 수 있다.\n전진삽입법은 공변량이 없는 상태에서 시작하여, 설명력이 가장 큰 변수를 하나씩 추가해 나가되, 각 단계에서 통계적으로 유의한 경우에만 포함시키는 방법이다. 단순하고 직관적이라는 장점이 있지만, 초기 선택이 잘못되면 최종 모형이 왜곡될 수 있으며, 다중공선성 문제에 취약하므로 다른 기준과 병행하는 것이 바람직하다.”\n# ==============================================\n# Boston Housing: 전진삽입 변수선택\n# ==============================================\n\nimport numpy as np\nimport statsmodels.api as sm\nfrom sklearn.datasets import fetch_openml\n\n# --- 0) 데이터 로드 ---\nboston = fetch_openml(name=\"boston\", version=1, as_frame=True)\ndf = boston.frame.copy()\n\n# --- 1) 숫자형으로 강제 변환 + 결측 제거 ---\n# 종속변수\ny = pd.to_numeric(df[\"MEDV\"], errors=\"coerce\")\n\n# 설명변수: MEDV 제외\nX = df.drop(columns=[\"MEDV\"]).copy()\n\n# 모든 열 숫자형으로 변환 (object/category -&gt; numeric)\nfor c in X.columns:\n    if X[c].dtype.name in [\"object\", \"category\"]:\n        # 공백/기호가 있을 수 있으므로 문자열이면 strip\n        X[c] = X[c].astype(str).str.strip()\n        X[c] = pd.to_numeric(X[c], errors=\"coerce\")\n    else:\n        X[c] = X[c].astype(float)\n\n# y와 X 모두 결측 없는 행만 사용\nmask = y.notna() & X.notna().all(axis=1)\ny = y[mask]\nX = X.loc[mask].reset_index(drop=True)\ny = y.reset_index(drop=True)\n\n# --- 2) 전진삽입 함수 (p-value 기준, 방어 로직 포함) ---\ndef forward_selection(X, y, alpha=0.05, verbose=True):\n    selected = []\n    remaining = list(X.columns)\n    best_changed = True\n\n    while best_changed and len(remaining) &gt; 0:\n        best_changed = False\n        pvals = []\n\n        for feat in remaining:\n            try:\n                X_try = sm.add_constant(X[selected + [feat]], has_constant='add')\n                model = sm.OLS(y, X_try).fit()\n                # 후보 feat의 p-value만 본다\n                pv = model.pvalues.get(feat, np.nan)\n                pvals.append((feat, pv))\n            except Exception as e:\n                # 특이행렬/수치 문제는 건너뛴다\n                if verbose:\n                    print(f\"[skip] {feat}: {e}\")\n                continue\n\n        if not pvals:\n            break\n\n        # p-value 최소 찾기\n        feat_best, pv_best = sorted(pvals, key=lambda x: (np.isnan(x[1]), x[1]))[0]\n\n        if (not np.isnan(pv_best)) and (pv_best &lt; alpha):\n            selected.append(feat_best)\n            remaining.remove(feat_best)\n            best_changed = True\n            if verbose:\n                print(f\"[ADD] {feat_best} (p={pv_best:.4g})\")\n        else:\n            # 더 이상 유의한 변수 없음\n            break\n\n    return selected\n\n# --- 3) 실행 ---\nselected_features = forward_selection(X, y, alpha=0.05, verbose=True)\nprint(\"\\n최종 선택 변수:\", selected_features)\n\n# --- 4) 최종 모형 적합 ---\nX_final = sm.add_constant(X[selected_features], has_constant='add')\nfinal_model = sm.OLS(y, X_final).fit()\nprint(final_model.summary())\n[ADD] LSTAT (p=5.081e-88) [ADD] RM (p=3.472e-27) [ADD] PTRATIO (p=1.645e-14) [ADD] DIS (p=1.668e-05) [ADD] NOX (p=5.488e-08) [ADD] CHAS (p=0.0002655) [ADD] B (p=0.0007719) [ADD] ZN (p=0.004652) [ADD] CRIM (p=0.04457) [ADD] RAD (p=0.001692) [ADD] TAX (p=0.0005214)  최종 선택 변수: [‘LSTAT’, ‘RM’, ‘PTRATIO’, ‘DIS’, ‘NOX’, ‘CHAS’, ‘B’, ‘ZN’, ‘CRIM’, ‘RAD’, ‘TAX’]\n\n\n(3) 단계삽입 stepwise\n단계삽입 방법은 기본적으로 전진삽입법과 유사하다. 그러나 중요한 차이는, 이미 선택된 변수들도 새롭게 들어온 변수를 기준으로 다시 유의성 검정을 받는다는 점이다. 따라서 불필요해진 변수가 있으면 모형에서 제거될 수 있다.\n1. 첫 단계: 고려된 예측변수들 중에서 목표변수 변동을 가장 많이 설명하는 변수, 즉 유의확률(p-value)이 가장 작은 변수를 선택한다. 설명력이 유의하면 이 변수를 첫 번째로 포함한다. → 전진삽입법과 동일하다.\n2. 두 번째 단계: 첫 번째 변수가 설명한 부분을 제외하고 남은 목표변수의 변동을 기준으로, 남은 변수들 중 가장 설명력이 큰 변수를 선택한다. 설명력이 유의하다면 두 번째 변수로 포함한다. → 역시 전진삽입법과 동일하다.\n3. 선택된 변수 검정: 새로운 변수가 추가되면, 이미 모형에 포함된 기존 변수들이 여전히 유의한지를 다시 검정한다. 만약 기존 변수 중 하나가 더 이상 유의하지 않다면, 그 변수를 제거한다. 즉, 선택과 제거가 반복적으로 일어나는 것이 stepwise의 핵심이다.\n4. 종료 조건: 새로운 변수가 더 이상 추가될 수 없고, 기존 변수들이 모두 유의성을 유지할 때 절차가 멈춘다.\n장점\n\n불필요한 변수를 자연스럽게 걸러낼 수 있고, 전진/후진 방식의 단점을 보완한다.\n자동화된 변수 선택 절차로 계산이 간편하다.\n\n단점\n\n표본이 작거나 다중공선성이 심한 경우, 변수의 포함 여부가 불안정하다.\n모형 선택 기준이 순전히 통계적(p-value, F값)에만 의존하기 때문에, 이론적 타당성을 무시할 위험이 있다.\n데이터가 조금만 달라져도 선택된 변수가 크게 달라질 수 있다.\n\nstepwise 결과를 그대로 ”최종 모형”으로 사용하는 것은 위험하다. 연구에서는 이론적 근거를 기반으로 주요 변수를 미리 지정하고, stepwise는 보조적으로 활용하는 것이 바람직하다. PRESS, AIC, BIC, 교차검증(CV) 같은 다른 모형 비교 지표와 함께 사용하는 것이 좋다.\n”단계삽입(stepwise) 회귀는 전진삽입법과 유사하지만, 새로운 변수가 들어올 때마다 기존 변수를 다시 검정하여 유의하지 않은 경우 제거한다는 점에서 더 유연한 방법이다. 선택과 제거가 반복되면서 최적 모형을 찾아가는 과정이지만, 표본 특성이나 다중공선성에 민감하므로 반드시 이론적 근거 및 다른 모형 선택 지표와 병행하는 것이 필요하다.”\n# ==============================================\n# Boston Housing: 단계삽입 변수선택\n# ==============================================\n\nimport pandas as pd\nimport numpy as np\nimport statsmodels.api as sm\nfrom sklearn.datasets import fetch_openml\n\n# --- 0) 데이터 로드 ---\nboston = fetch_openml(name=\"boston\", version=1, as_frame=True)\ndf = boston.frame.copy()\n\n# 종속변수 / 설명변수\ny = pd.to_numeric(df[\"MEDV\"], errors=\"coerce\")\nX = df.drop(columns=[\"MEDV\"]).copy()\n\n# 숫자형 변환\nfor c in X.columns:\n    if X[c].dtype.name in [\"object\", \"category\"]:\n        X[c] = X[c].astype(str).str.strip()\n        X[c] = pd.to_numeric(X[c], errors=\"coerce\")\n    else:\n        X[c] = X[c].astype(float)\n\n# 결측 제거\nmask = y.notna() & X.notna().all(axis=1)\ny = y[mask].reset_index(drop=True)\nX = X.loc[mask].reset_index(drop=True)\n\n# --- 1) 단계적 회귀 함수 ---\ndef stepwise_selection(X, y, alpha_in=0.05, alpha_out=0.10, verbose=True):\n    \"\"\" 단계적 선택: forward + backward \"\"\"\n    included = []\n    while True:\n        changed = False\n\n        # Forward Step\n        excluded = list(set(X.columns) - set(included))\n        new_pvals = pd.Series(index=excluded, dtype=float)\n        for new_col in excluded:\n            model = sm.OLS(y, sm.add_constant(X[included + [new_col]], has_constant='add')).fit()\n            new_pvals[new_col] = model.pvalues[new_col]\n        if not new_pvals.empty:\n            best_pval = new_pvals.min()\n            if best_pval &lt; alpha_in:\n                best_feature = new_pvals.idxmin()\n                included.append(best_feature)\n                changed = True\n                if verbose:\n                    print(f\"[ADD] {best_feature} (p={best_pval:.4g})\")\n\n        # Backward Step\n        model = sm.OLS(y, sm.add_constant(X[included], has_constant='add')).fit()\n        pvals = model.pvalues.drop(\"const\")\n        worst_pval = pvals.max()\n        if worst_pval &gt; alpha_out:\n            worst_feature = pvals.idxmax()\n            included.remove(worst_feature)\n            changed = True\n            if verbose:\n                print(f\"[REMOVE] {worst_feature} (p={worst_pval:.4g})\")\n\n        if not changed:\n            break\n    return included\n\n# --- 2) 실행 ---\nselected_features = stepwise_selection(X, y, alpha_in=0.05, alpha_out=0.10, verbose=True)\nprint(\"\\n최종 선택 변수:\", selected_features)\n\n# --- 3) 최종 모형 적합 ---\nX_final = sm.add_constant(X[selected_features], has_constant='add')\nfinal_model = sm.OLS(y, X_final).fit()\nprint(final_model.summary())\n[ADD] LSTAT (p=5.081e-88) [ADD] RM (p=3.472e-27) [ADD] PTRATIO (p=1.645e-14) [ADD] DIS (p=1.668e-05) [ADD] NOX (p=5.488e-08) [ADD] CHAS (p=0.0002655) [ADD] B (p=0.0007719) [ADD] ZN (p=0.004652) [ADD] CRIM (p=0.04457) [ADD] RAD (p=0.001692) [ADD] TAX (p=0.0005214)  최종 선택 변수: [‘LSTAT’, ‘RM’, ‘PTRATIO’, ‘DIS’, ‘NOX’, ‘CHAS’, ‘B’, ‘ZN’, ‘CRIM’, ‘RAD’, ‘TAX’]\n\n\n(4) 최적모형 (Adjusted R² 기준)\n# ==============================================\n# Boston Housing: 최적모형: 수정 결정계수 기준\n# ==============================================\n\nimport itertools\nimport pandas as pd\nimport numpy as np\nimport statsmodels.api as sm\nfrom sklearn.datasets import fetch_openml\n\n# 1. 데이터 불러오기\nboston = fetch_openml(name=\"boston\", version=1, as_frame=True)\ndf = boston.frame.copy()\n\n# 종속변수 / 설명변수\ny = pd.to_numeric(df[\"MEDV\"], errors=\"coerce\")\nX = df.drop(columns=[\"MEDV\"]).copy()\n\n# 숫자형 변환\nfor c in X.columns:\n    if X[c].dtype.name in [\"object\", \"category\"]:\n        X[c] = X[c].astype(str).str.strip()\n        X[c] = pd.to_numeric(X[c], errors=\"coerce\")\n    else:\n        X[c] = X[c].astype(float)\n\n# 결측 제거\nmask = y.notna() & X.notna().all(axis=1)\ny = y[mask].reset_index(drop=True)\nX = X.loc[mask].reset_index(drop=True)\n\n# 2. Best Subset 함수\ndef best_subset(X, y, max_features=5, criterion=\"adj_r2\"):\n    results = []\n    n = len(y)\n\n    for k in range(1, max_features+1):  # 변수 개수 1개부터 max_features까지\n        for combo in itertools.combinations(X.columns, k):\n            X_combo = sm.add_constant(X[list(combo)], has_constant=\"add\")\n            model = sm.OLS(y, X_combo).fit()\n\n            if criterion == \"adj_r2\":\n                score = model.rsquared_adj\n            elif criterion == \"aic\":\n                score = -model.aic  # 최소화 기준 → 부호 바꿔서 최대화\n            elif criterion == \"bic\":\n                score = -model.bic\n            else:\n                raise ValueError(\"criterion은 'adj_r2', 'aic', 'bic' 중 선택\")\n\n            results.append({\n                \"num_features\": k,\n                \"features\": combo,\n                \"criterion\": score\n            })\n\n    # 최적 모형 선택\n    best_model = max(results, key=lambda x: x[\"criterion\"])\n    return pd.DataFrame(results), best_model\n\n# 3. 실행 (예: 최대 10개 변수까지 탐색, Adjusted R² 기준)\nall_results, best = best_subset(X, y, max_features=7, criterion=\"adj_r2\")\n\nprint(\"Best Model (Adjusted R² 기준):\")\nprint(\"변수:\", best[\"features\"])\nprint(\"Adj R²:\", best[\"criterion\"])\nBest Model (Adjusted R² 기준): 변수: (‘CHAS’, ‘NOX’, ‘RM’, ‘DIS’, ‘PTRATIO’, ‘B’, ‘LSTAT’)  Adj R²: 0.7182560407158507\n\n\n\n\nchapter 3. 변수선택 빅데이터 방법\n\n1. 과적합과 과소적합\n\n(1) 과적합, 과소적합 개념\n\n\n\n\n\n과적합 overfitting\n과적합은 우리가 추정한 모형이”너무 잘 훈련”되었고 현재는 훈련 데이터 세트를 가장 잘 설명하고 있다는 것을 의미한다. 위의 오른쪽 두 추정 모형을 보면 과적합으로 인하여 데이터에 범용(향후 검증 데이터에 적용 가능한지? 혹은 일반 상황에도 적용 가능한지?) 적합한 모형인지 이 훈련 데이터에만 적합한지 알 수 없다는 사실이다.\n관측치 수에 비해 너무 많은 특징(변수)가 존재하는 경우 발생하며 추정 모형은 훈련 데이터에서 매우 정확하지만 훈련되지 않은 또는 새로운 데이터에서는 정확하지 않을 수 있다.\n이 추정모형의 결과를 일반화하고 다른 데이터, 즉 궁극적으로 수행하려는 작업에 대해 추론 할 수 없음을 의미한다. 기본적으로 과적합이 발생하면 모델은 데이터의 변수 간 실제 관계 대신 학습 데이터의 \"노이즈\"를 학습하거나 설명하게 되므로 이 노이즈가 포함되어 있지 않은 (검증)데이터 세트에는 적용할 수 없게 된다.\n과소적합 underfitting\n과적합과 대조적으로, 모델이 과소적합되면 모델이 훈련 데이터에 적합하지 않으므로 데이터의 추세를 놓치게 되며 또한 추정 모형을 새 데이터로 일반화 할 수 없음을 의미한다.\n이는 매우 간단한 모형 (충분한 예측 변수 / 독립 변수가 아님)의 결과로 선형이 아닌 데이터에 선형 모델 (예 : 선형 회귀)을 적용 할 때도 발생할 수 있다. 이 모델은 예측 능력이 좋지 않을 것이며 훈련 데이터에서 다른 데이터로 일반화 할 수 없음을 알 수 있다.\n\n\n(2) 과적합과 변수선택\n과적합(overfitting)이란 모형이 훈련 데이터의 노이즈까지 학습하여, 새로운 데이터 예측력이 떨어지는 현상으로 주된 원인 중 하나가 불필요한 변수가 너무 많이 포함된 경우이다. 특히 빅데이터(p ≫ n)에서는, 변수가 많아질수록 OLS 회귀는 데이터를 지나치게 잘 맞추고, 일반화는 망가진다.\n변수 선택(variable selection)의 목적은 필요한 설명변수만 남기고, 불필요한 변수는 제거하여 모형을 단순화하고, 과적합을 줄이는 것이다.즉, 변수 선택은 편향–분산 균형에서 분산을 줄이는 전략입니다.\n편향–분산 관점\n변수를 많이 넣으면 → 분산(variance) ↑, 과적합 ↑.\n변수를 줄이면 → 편향(bias) ↑, 그러나 분산 ↓, 일반화 성능 ↑.\n따라서 변수 선택은 ”조금의 편향을 감수하고, 분산을 크게 줄여 과적합을 방지”하는 절차입니다.\n\n\n\n\n\n\n\n\n2. L1 and L2 Regularization\n과적합 문제를 해결하기 위한 전처리 작업에 해당된다.\n\nLasso(L1): 일부 계수를 0으로 만들어 변수 제거 → 불필요한 변수 자동 차단 → 과적합 방지.\nRidge(L2): 계수를 수축해 노이즈에 덜 민감하게 함 → 다중공선성 완화.\nElastic Net: L1+L2 혼합으로 변수 선택 + 안정성.\n트리 기반(Random Forest, XGBoost): 변수 중요도 기반으로 핵심 변수만 반영.\nPCA/PLS: 차원 축소로 고차원 문제 해결.\n\nNorm \\(||X_{p}|| = (\\sum|x_{i}|^{p})^{\\frac{1}{p}}\\)\n벡터의 크기 혹은 두 벡터의 거리를 측정하는 함수이다. p=놈의 차수를 의미하며 p=1이면 \\(L_{1}\\) 놈, p=2이면 \\(L_{2}\\)놈이라 한다.\n【\\(L_{1}\\)놈】 맨하턴(Manhattan) 거리이다. 좌표축을 따라 이동하는 거리로 ”맨해튼 거리”라고 불리는 이유는 뉴욕 맨해튼처럼 격자형 도로만 따라 움직여야 할 때의 거리와 같기 때문이다. 회귀에서 L1 규제(Lasso) 는 이 L1 놈을 이용해 계수들의 크기를 제약. → 많은 계수가 정확히 0이 되어 변수 선택 효과 발생.\n【\\(L_{2}\\) 놈】 유클리디안(Eucleadian) 거리이다. 우리가 익숙한 ”직선 거리”, 즉 피타고라스의 정리로 계산되는 거리이다. 회귀에서 L2 규제(Ridge) 는 이 L2 놈을 이용해 계수들을 제약하여, 계수를 0에 가깝게 줄이되, 정확히 0으로 만들지는 않는다.\nL1 vs L2 규제와의 연결\nL1 (맨해튼) → 축을 따라 sharp한 모서리가 있기 때문에 계수가 정확히 0으로 떨어질 가능성이 높음. → 변수 선택 효과.\nL2 (유클리디안) → 원 모양 제약이라 계수를 부드럽게 shrink. → 안정성은 높지만, 변수 선택은 없다.\n\n\n\n\n\nL1 Regularization: Lasso Regression (Least Absolute Shrinkage and Selection Operator) \n손실함수 \\(OLS(L(y_i, \\hat y_i))+\\lambda \\sum |\\beta_j |)\\)\n\n\\(\\lambda=0\\) 이면 OLS 추정과 동일하고 \\(\\lambda\\) 가 커지면 회귀계수 영향이 작아져 과소적합이 된다.\n목표변수에 영향도가 적은 예측변수의 회귀계수를 크기를 최소화 하고 영향도가 큰 예측변수의 회귀계수 크기를 상대적으로 크게 하여 주요 예측변수 ’feature selection’에 사용된다.\n\nL2 Regularization: Ridge Regression 능형 회귀분석\n손실함수 \\(OLS(L(y_i, \\hat y_i))+\\lambda \\sum \\beta_j^2 )\\)\n\n\\(\\lambda =0\\) 이면 OLS 추정과 동일하고 \\(\\lambda\\) 가 커지면 불편성이 발생하고 MSE를 최소화 한다.\n다중공선성 문제를 해결하는 정규화 방법이다.\n\n\n\n3. Features Enginerring\nFeature 선택\nFeature 순위 또는 Feature 중요도 부여하는 과정으로 중요 변수(주요 신호)의 부분집합을 선택하거나, 불필요한(중요하지 않은) 변수들을 제거하여 변수의 차원(개수)을 줄이는데 그 목적이 있다. 분류모델 중 Decision Tree 같은 경우는 트리의 상단에 있을수록 중요도가 높으므로 이를 반영하여 특징 별로 중요도를 매길 수 있다. 회귀모델의 경우에는 유의 변수선책 알고리즘을 통해 변수 선택이 가능하다.\nFeature 선택기법에는 상관분석과 모델기반 방법인 Lasso Regression, Recursive Feature Elimination, Tree-based Model, (Logistics) Discriminant Analysis 등이 있다\nFeature 추출\n원데이터 변수들의 구조적 관계를 활용하여 새로운 변수를 생성하거나(고차원의 공간을 저차원의 새로운 공간으로 차원축소) 변수들 간의 상관계수를 변수 유사성 척도로 하여 Feature를 탐색한다. PCA, SVD 차원축소 방법도 여기에 해당하며, 상관관계 유의한 변수군을 만드는 Canonical Correlation Analysis(CCA) 등이 Feature 추출 과정의 방법론이다.\n\n\n4. 데이터 분할 Data Split\ntrain and test data\n대용량 빅데이터의 경우에는 데이터 세트를 2분화 하여 모형을 추정하는 데이터(훈련 데이터 train dataset), 추정 모형을 이용하여 예측 정확도를 판단하는데(평가) 활용되는 데이터(검증 test dataset)로 활용한다. 일반적으로 8:2 혹은 7:3으로 나눈다. 이는 예측모형의 과적합과 underfitting 문제를 해결하기 위하여 머신러닝 기법에서 제안된 방법이다. 이방법은 전혀 새롭지 않은 개념이다. 이전 데이터마이닝에서 데이터를 3단계, train, validation, test 데이터로 나눈 것과 유사하다.\n\n\n\n\n\ncross validation 필요 이유\n훈련 데이터와 검증 데이터의 스플릿이 무작위가 아닌 경우 문제가 발생한다. 훈련 데이터가 특정 변인에 의해 왜곡되어 있다면 과적합 문제가 발생하다. 예를 들어 우연히 소득수준이 높은 집단이 훈련 데이터에 많이 포함되어 있다면? 한 번의 데이터 스플릿에 의한 분석은 왜곡된 정보를 준다.\nK-Folds Cross Validation에서 훈련 데이터를 k 개의 다른 부분 집합으로 나눈다. k-1 부분 집합을 사용하여 데이터를 훈련시키고 마지막 부분 집합을 테스트 데이터로 하여 검증하게 된다. 이런 과정을 k번 거쳐 계산된 정확도 평균을 훈련 데이터 정확도로 하게 되며 그런 다음 검증 데이터 세트에 대해 검증한다.\n\n\n\n\n\n\n\n5. 사례분석\n\n(1) Univariate Selection : 전진삽입방법과 동일\n# ==============================================\n# Boston Housing: Univariate Selection\n# ==============================================\n\nimport pandas as pd\nimport statsmodels.api as sm\nfrom sklearn.datasets import fetch_openml\n\n# 1. 데이터 불러오기 (OpenML에서 Boston Housing)\nboston = fetch_openml(name=\"boston\", version=1, as_frame=True)\ndf = boston.frame\n\n#Univariate Selection :  전진삽입방법과 동일\nimport numpy as np\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_regression\ny=df['MEDV']\nX=df.iloc[:,0:13]\nbestfeatures = SelectKBest(score_func=f_regression, k=10) #default k=10\nfit=bestfeatures.fit(X,y)\ndfpvalues = pd.DataFrame(fit.pvalues_)\ndfscores = pd.DataFrame(fit.scores_)\ndfcolumns = pd.DataFrame(X.columns)\n#concat two dataframes for better visualization\nfeatureScores = pd.concat([dfcolumns,dfscores,dfpvalues],axis=1)\nfeatureScores.columns = ['features','score','pvalues']  #naming the dataframe columns\nprint(featureScores.nlargest(7,'score'))  #print best features\nfeatures score pvalues  12 LSTAT 601.617871 5.081103e-88  5 RM 471.846740 2.487229e-74  10 PTRATIO 175.105543 1.609509e-34  2 INDUS 153.954883 4.900260e-31  9 TAX 141.761357 5.637734e-29  4 NOX 112.591480 7.065042e-24  0 CRIM 89.486115 1.173987e-19\n#유의수준 5% 유의한 변수 선택\nselect_vars=featureScores.loc[featureScores['pvalues']&lt;0.05,:]['features']\nprint(select_vars)\n0 CRIM 1 ZN 2 INDUS 3 CHAS 4 NOX 5 RM 6 AGE  7 DIS 8 RAD 9 TAX 10 PTRATIO. 11 B 12 LSTAT\n\n\n(2) 재귀적 변수제거 Recursive Feature Elimination (RFE)\nRFE는 모형 적합 → 중요도 평가 → 가장 덜 중요한 변수 제거 → 반복이라는 순환 절차를 통해 최적 변수 집합을 찾는 방법이다. 이는 변수 선택을 자동화할 수 있는 강력한 도구지만, 계산량이 많고 변수 간 상관관계에 민감하므로 이론적 근거와 병행해 사용하는 것이 바람직하다.\n재귀적 변수 제거(RFE)는 많은 예측변수 중에서 중요하지 않은 변수를 단계적으로 제거하면서 최적의 변수 집합을 찾는 방법이다. 이름 그대로 ”재귀적으로(feature by feature)” 변수를 줄여 나간다는 점이 특징이다.\n1. 우선 모든 예측변수를 포함하여 모형을 적합시킨다. 이때 사용할 수 있는 모형은 선형 회귀, 로지스틱 회귀, 서포트 벡터 머신(SVM), 트리 기반 모형 등 예측력이 있는 알고리즘이면 된다.\n2. 적합된 모형에서 각 변수의 중요도(feature importance 또는 회귀계수 크기)를 평가한다.\n3. 가장 중요도가 낮은 변수를 제거한 후, 남은 변수들로 다시 모형을 적합한다.\n4. 동일한 과정을 반복하면서 변수를 하나씩 줄여 나가고, 각 단계별로 모형의 성능(예: 교차검증 정확도, RMSE, AUC 등)을 기록한다.\n5. 성능이 가장 우수한 시점에서 남아 있는 변수 집합을 최종 선택한다.\n장점\n\n변수 선택 과정을 체계적이고 자동화할 수 있다.\n단순히 p-value 기준으로 변수 제거하는 방법보다, 예측 성능을 직접 기준으로 삼기 때문에 실무 데이터 분석에서 신뢰성이 높다.\n특정 변수들의 상호작용이나 중요도의 상대적 크기를 평가할 수 있다.\n\n단점\n\n변수 개수가 많으면 모든 반복 단계에서 모형을 다시 학습해야 하므로 계산량이 매우 크다.\n변수 간 다중공선성이 심할 경우, 중요도 평가가 왜곡될 수 있다.\n”어떤 변수가 반드시 포함되어야 한다”는 이론적 근거를 반영하지 못하고, 순전히 데이터 기반으로 선택한다는 한계가 있다.\n\n# ==============================================\n# Boston Housing: RFE 방법\n# ==============================================\n\n# 1. 데이터 불러오기 (OpenML에서 Boston Housing)\nboston = fetch_openml(name=\"boston\", version=1, as_frame=True)\ndf = boston.frame\n\n#Recursive Feature Elimination : 후진 제거방법과 동일\ny=df['MEDV']\nX=df.iloc[:,0:13]\nfrom sklearn.feature_selection import RFE\nfrom sklearn.svm import SVR\nestimator = SVR(kernel=\"linear\")\nselector = RFE(estimator,n_features_to_select=7) #최적 7개 변수만 선택\nselector = selector.fit(X, y)\nprint(X.columns,'\\n', selector.support_,'\\n',selector.ranking_)\nIndex(['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', ’TAX', 'PTRATIO', 'B', 'LSTAT'],\n[ True False False True True True False True False False True False True]\n[1 3 2 1 1 1 4 1 6 7 1 5 1]\n#유의한 예측변수\nimport numpy as np\nvar_select=pd.DataFrame(np.transpose([np.array(X.columns),selector.ranking_]))\nvar_select.columns=['var_name','rank']\nprint(var_select.loc[var_select['rank']==1,\"var_name\"])\n0 CRIM 3 CHAS 4 NOX 5 RM 7 DIS 10 PTRATIO. 12 LSTAT\n\n\n(3) LASSO 방법\n데이터 값을 축소(shrinkage) - 중심점 평균으로 중심화 하여(L1 정규화) 목표변수에 영향을 많이 미치는(목표변수 변동을 보다 잘 설명하는) 예측변수를 선택하거나 다중공선성 문제 진단하는데도 사용된다.\n손실함수 \\(OLS(L(y_i, \\hat y_i))+\\lambda \\sum |\\beta_j |)\\) 을 최소화 하는 것은 $ | _j | s $ 제약 조건 하에서 OLS의 SSE를 최소화 하는 추정치를 구하는 것과 동일하다.\n\\(\\lambda\\) 결정\n\n\\(\\lambda = 0\\) : OLS 추정과 동일, 모든 예측변수가 추정된다.\n\\(\\lambda = \\infty\\) : 모든 예측변수의 회귀계수가 0이 되어 모든 예측변수 제거된다.\n\\(\\lambda\\) 가 증가하면 추정 편이는 증가하고 제거되는 예측변수의 개수는 많아진다.\n최적 람다는 AIC, BIC를 최소화 하는 람다를 구한다.\n\n# ==============================================\n# Boston Housing: LASSO 방법\n# ==============================================\n\nfrom sklearn import linear_model\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\n\n# 1. 데이터 불러오기 (OpenML에서 Boston Housing)\nboston = fetch_openml(name=\"boston\", version=1, as_frame=True)\ndf = boston.frame\n\ny=df['MEDV']\nX=df.iloc[:,0:13]\n# Standardize the features\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n# Apply Lasso regression\nlassoReg = linear_model.Lasso(alpha=0.1, fit_intercept=True)\nfit = lassoReg.fit(X_scaled, y)\nplt.title('Reg. Coefficients')\nplt.plot(fit.coef_)\nplt.show()\n\n\n\n\n\n#모든 예측변수 출력\nvar_nm=pd.Series(X.columns,name='Var')\ncoeff=pd.Series(fit.coef_,name='Coeff')\nLASSO=pd.concat([var_nm,coeff],axis=1)\nprint(LASSO[abs(LASSO['Coeff'])&gt;0.8])\nVar Coeff  4 NOX -1.574193  5 RM 2.826269  7 DIS -2.422079  8 RAD 1.195937  9 TAX -0.846468  10 PTRATIO -1.922493  12 LSTAT -3.726184\n# ==============================================\n# Boston Housing: LASSO 방법 최종 회귀모형 \n# ==============================================\n\nimport pandas as pd\nimport statsmodels.api as sm\n\n# 1) LASSO에서 선택된 변수명 목록 만들기 (계수 절댓값 &gt; 1인 변수 6개)\n#    LASSO는 변수명 'Var', 계수 'Coeff' 컬럼을 가진 DataFrame이라고 가정\nselected_vars = LASSO.loc[LASSO['Coeff'].abs() &gt; 1, 'Var'].tolist()\n\n# 2) 종속/설명변수 준비 (반드시 DataFrame df에서 꺼냄)\ny = pd.to_numeric(df['MEDV'], errors='coerce')\n\n# 선택된 변수들만 추출 (숫자형 강제 변환은 상황에 따라 추가)\nX = df[selected_vars].copy()\n\n# 필요시 숫자형 보정 (object/category 방지)\nfor c in X.columns:\n    if X[c].dtype.name in ['object', 'category']:\n        X[c] = pd.to_numeric(X[c], errors='coerce')\n\n# y, X 결측 행 제거(동일 행 유지)\nmask = y.notna() & X.notna().all(axis=1)\ny = y[mask]\nX = X.loc[mask]\n\n# 3) 절편 추가 후 OLS 적합\nX = sm.add_constant(X, has_constant='add')\nfit = sm.OLS(y, X).fit()\n\nprint(selected_vars)      # 선택된 변수명 확인\nprint(fit.summary())\n[‘NOX’, ‘RM’, ‘DIS’, ‘RAD’, ‘PTRATIO’, ‘LSTAT’]\n결정계수는 70.9%로 양호하며, 결과에서도 다중공선성 경고는 나타나지 않았다. 그러나 예측변수 RAD의 상관계수 부호와 회귀계수 부호가 일치하지 않아, 다중공선성 문제가 존재하는 것으로 판단된다(이에 대한 자세한 논의는 다음 장에서 다룬다).\n\n\n\n\n\n\n\n(4) 트리 기반 모형 (Tree-based Model)\n트리 기반 모형은 데이터를 조건에 따라 여러 번 분할하여 나무(tree) 구조 형태로 의사결정을 만들어내는 예측 기법이다. 각 분할은 특정 변수의 값을 기준으로 이루어지며, 최종적으로 분할이 끝난 말단 노드(leaf node)는 예측값이나 분류 집단을 의미한다. 즉, ”조건 → 결과” 의 규칙들을 계층적으로 정리한 방법이 트리 기반 모형이다.\n트리 모형은 크게 두 가지 목적으로 사용된다.\n\n회귀 트리(Regression Tree): 목표변수가 연속형일 때 사용. 각 노드에서 목표변수의 평균값 등을 예측값으로 사용한다.\n분류 트리(Classification Tree): 목표변수가 범주형일 때 사용. 각 노드에서 다수 클래스 또는 확률을 기준으로 분류한다.\n\n트리 모형의 분할 원리\n\n트리는 데이터를 잘게 나누면서 ”비슷한 값끼리 모으는” 것을 목표로 한다.\n분류 문제에서는 지니지수(Gini Index), 엔트로피(Entropy) 같은 불순도(impurity) 기준을 사용한다.\n회귀 문제에서는 잔차제곱합(SSE)을 최소화하는 방향으로 분할한다.\n이렇게 반복적으로 최적의 분할을 찾아 내려가면서 트리 모형이 완성된다.\n\n트리 기반 주요 방법: 1. 의사결정나무(Decision Tree)\n\n하나의 트리를 완성하여 예측에 활용한다.\n해석이 직관적이고 ”규칙(rule)“을 제시할 수 있다는 장점이 있다.\n그러나 트리를 깊게 성장시키면 과적합(overfitting)이 발생하기 쉽다.\n\n【변수선택】 변수를 하나씩 분할 기준으로 사용 → 분할에 자주 등장하는 변수가 중요한 변수이다. 각 노드에서 ”얼마나 불순도를 줄였는가(분산 감소, 지니 감소, 정보이득)“를 통해 변수의 기여도를 평가한다.\n# ==============================================\n# Boston Housing - 의사결정나무 (회귀 / 분류)\n# ==============================================\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error, r2_score, accuracy_score, roc_auc_score\nfrom sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier, plot_tree, export_text\nfrom sklearn.preprocessing import StandardScaler\n\n# -----------------------------\n# 0) 데이터 로드 & 전처리\n# -----------------------------\nboston = fetch_openml(name=\"boston\", version=1, as_frame=True)\ndf = boston.frame.copy()\n\n# 숫자형 강제 변환 및 결측 제거\nfor c in df.columns:\n    if df[c].dtype.name in [\"object\", \"category\"]:\n        df[c] = pd.to_numeric(df[c].astype(str).str.strip(), errors=\"coerce\")\ndf = df.dropna().reset_index(drop=True)\n\n# -----------------------------\n# 1) 회귀 트리 (y = MEDV)\n# -----------------------------\ny_reg = df[\"MEDV\"].astype(float)\nX_reg = df.drop(columns=[\"MEDV\"]).astype(float)\n\nXtr, Xte, ytr, yte = train_test_split(X_reg, y_reg, test_size=0.2, random_state=42)\n\n# 기본 트리\ndt_reg = DecisionTreeRegressor(random_state=42)  # 필요시 max_depth, min_samples_leaf 조정\ndt_reg.fit(Xtr, ytr)\npred = dt_reg.predict(Xte)\nrmse = np.sqrt(mean_squared_error(yte, pred))\nr2 = r2_score(yte, pred)\nprint(\"=== 회귀 트리 ===\")\nprint(f\"RMSE={rmse:.3f}  R^2={r2:.3f}\")\n\n# 변수 중요도\nimp_reg = pd.Series(dt_reg.feature_importances_, index=X_reg.columns).sort_values(ascending=False)\nprint(\"\\n[회귀 트리 변수 중요도 Top 7]\")\nprint(imp_reg.head(7))\n\n# 가지치기(비용-복잡도 프루닝)\npath = dt_reg.cost_complexity_pruning_path(Xtr, ytr)\nccp_alphas = path.ccp_alphas\n\nbest_score = -np.inf\nbest_alpha = None\nfor alpha in ccp_alphas:\n    m = DecisionTreeRegressor(random_state=42, ccp_alpha=alpha)\n    m.fit(Xtr, ytr)\n    s = r2_score(yte, m.predict(Xte))\n    if s &gt; best_score:\n        best_score, best_alpha = s, alpha\n\ndt_reg_pruned = DecisionTreeRegressor(random_state=42, ccp_alpha=best_alpha)\ndt_reg_pruned.fit(Xtr, ytr)\npred_p = dt_reg_pruned.predict(Xte)\nrmse_p = np.sqrt(mean_squared_error(yte, pred_p))\nr2_p = r2_score(yte, pred_p)\nprint(f\"\\n[Pruning] best ccp_alpha={best_alpha:.4g}  →  RMSE={rmse_p:.3f}  R^2={r2_p:.3f}\")\n\n# 트리 시각화 (회귀)\nplt.figure(figsize=(12, 6))\nplot_tree(dt_reg_pruned, feature_names=X_reg.columns, filled=True, max_depth=3, fontsize=8)\nplt.title(\"Decision Tree Regressor (pruned, depth≤3 view)\")\nplt.show()\n\n# 텍스트 규칙\nprint(\"\\n[회귀 트리 규칙 일부]\")\nprint(export_text(dt_reg_pruned, feature_names=list(X_reg.columns), max_depth=3))\n\n# -----------------------------\n# 2) 분류 트리 (MEDV 고가/저가)\n# -----------------------------\nmedian_price = df[\"MEDV\"].median()\ny_clf = (df[\"MEDV\"] &gt;= median_price).astype(int)\nX_clf = df.drop(columns=[\"MEDV\"]).astype(float)\n\nXtr, Xte, ytr, yte = train_test_split(X_clf, y_clf, test_size=0.2, random_state=42, stratify=y_clf)\n\ndt_clf = DecisionTreeClassifier(random_state=42)  # 필요시 max_depth 등 조정\ndt_clf.fit(Xtr, ytr)\nproba = dt_clf.predict_proba(Xte)[:, 1]\npredc = (proba &gt;= 0.5).astype(int)\n\nacc = accuracy_score(yte, predc)\ntry:\n    auc = roc_auc_score(yte, proba)\nexcept Exception:\n    auc = np.nan\n\nprint(\"\\n=== 분류 트리 ===\")\nprint(f\"ACC={acc:.3f}  AUC={auc:.3f}\")\n\n# 변수 중요도\nimp_clf = pd.Series(dt_clf.feature_importances_, index=X_clf.columns).sort_values(ascending=False)\nprint(\"\\n[분류 트리 변수 중요도 Top 7]\")\nprint(imp_clf.head(7))\n\n# 가지치기(분류)\npath_c = dt_clf.cost_complexity_pruning_path(Xtr, ytr)\nbest_alpha_c, best_acc = None, -np.inf\nfor alpha in path_c.ccp_alphas:\n    m = DecisionTreeClassifier(random_state=42, ccp_alpha=alpha)\n    m.fit(Xtr, ytr)\n    acc_tmp = accuracy_score(yte, m.predict(Xte))\n    if acc_tmp &gt; best_acc:\n        best_acc, best_alpha_c = acc_tmp, alpha\n\ndt_clf_pruned = DecisionTreeClassifier(random_state=42, ccp_alpha=best_alpha_c)\ndt_clf_pruned.fit(Xtr, ytr)\n\n# 시각화 (분류)\nplt.figure(figsize=(12, 6))\nplot_tree(dt_clf_pruned, feature_names=X_clf.columns, class_names=[\"low\",\"high\"],\n          filled=True, max_depth=3, fontsize=8)\nplt.title(\"Decision Tree Classifier (pruned, depth≤3 view)\")\nplt.show()\n\nprint(\"\\n[분류 트리 규칙 일부]\")\nprint(export_text(dt_clf_pruned, feature_names=list(X_clf.columns), max_depth=3))\n[회귀 트리 변수 중요도 Top 7]  RM 0.600326  LSTAT 0.193328  DIS 0.070688  CRIM 0.051296  NOX 0.027148  AGE 0.013617  TAX 0.012464\n\n\n\n\n\n=== 분류 트리 ===\nACC=0.765 AUC=0.765  [분류 트리 변수 중요도 Top 7]  LSTAT 0.550320  RM 0.130962  CRIM 0.096375  AGE 0.073649  B 0.052958  DIS 0.035318  PTRATIO 0.022226\n\n\n\n\n\n트리 기반 주요 방법: 2. 랜덤 포레스트(Random Forest)\n랜덤 포레스트는 여러 개의 결정나무(weak learners)를 무작위성(randomness)과 배깅(bagging)으로 결합해 강한 예측기(strong learner) 를 만드는 앙상블 기법이다. 각 트리는 서로 다른 부트스트랩 표본(행 무작위 추출)과 분할 시 무작위로 선택된 일부 특성(열 무작위 추출)만을 사용하여 학습한다. 이렇게 다양성을 확보한 다수의 트리를 생성하고, 회귀는 평균, 분류는 다수결로 최종 예측을 만든다. 개별 트리는 편향이 크지만(underfit), 포레스트의 평균화가 분산을 크게 줄여 과적합을 억제한다.\n여러 개의 트리를 만들고, 그 결과를 평균(회귀) 또는 다수결(분류) 방식으로 결합한다. 부트스트랩 표본(bagging)과 무작위 변수 선택(random feature selection)을 결합하여 과적합을 크게 줄인다. 각 변수의 상대적 중요도(feature importance)를 계산할 수 있어 해석에도 도움을 준다.\n【변수선택】 여러 개의 트리에서 평균적으로 분할에 가장 많이 기여한 변수 → 중요 변수로 간주한다. 이를 통해 변수 중요도(feature importance) 값을 계산할 수 있다.\n# ==============================================\n# Boston Housing: Random Forest 중요도 비교\n# ==============================================\nimport numpy as np\nimport pandas as pd\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.inspection import permutation_importance\n\n# 1) 데이터 로드 & 전처리\nboston = fetch_openml(name=\"boston\", version=1, as_frame=True)\ndf = boston.frame.copy()\nfor c in df.columns:\n    if df[c].dtype.name in [\"object\", \"category\"]:\n        df[c] = pd.to_numeric(df[c].astype(str).str.strip(), errors=\"coerce\")\ndf = df.dropna().reset_index(drop=True)\n\nX = df.drop(columns=[\"MEDV\"]).astype(float)\ny = df[\"MEDV\"].astype(float)\nXtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# 2) 랜덤포레스트 학습\nrf = RandomForestRegressor(\n    n_estimators=500, random_state=42, n_jobs=-1,\n    max_features=\"sqrt\", min_samples_leaf=2,\n    oob_score=True, bootstrap=True\n)\nrf.fit(Xtr, ytr)\n\n# 3) 성능 평가\npred = rf.predict(Xte)\nrmse = np.sqrt(mean_squared_error(yte, pred))\nr2   = r2_score(yte, pred)\nprint(f\"RMSE={rmse:.3f}  R^2={r2:.3f}  OOB R^2={rf.oob_score_:.3f}\")\n\n# 4) 기본 중요도\nimp = pd.Series(rf.feature_importances_, index=X.columns).sort_values(ascending=False)\nprint(\"\\n[Feature Importance Top 7]\")\nprint(imp.head(7))\n\n# 5) Permutation Importance\npi = permutation_importance(rf, Xte, yte, n_repeats=10, random_state=42, n_jobs=-1)\nperm_imp = pd.Series(pi.importances_mean, index=X.columns).sort_values(ascending=False)\nprint(\"\\n[Permutation Importance Top 7]\")\nprint(perm_imp.head(7))\n랜덤 포레스트 중요도 개념\n기본 중요도(feature_importances_) → 트리 분할에서 자주 쓰이고 불순도를 많이 줄인 변수에 점수가 집중된다.\nPermutation Importance → 변수를 섞었을 때 예측 성능이 얼마나 떨어지느냐를 직접 측정하므로, 실제 예측 기여도를 더 잘 반영한다.\n따라서 두 방법을 나란히 보면 상위 주요 변수(LSTAT, RM 등)는 공통적으로 높게 나오고, 일부 변수는 기본 중요도에서는 높지만 Permutation에서는 낮을 수 있다. (편향 차이 때문)\nRMSE=3.255 R^2=0.856 OOB R^2=0.852\n[Feature Importance Top ]  RM 0.296902  LSTAT 0.240556  NOX 0.075812  PTRATIO 0.069155  INDUS 0.068880  CRIM 0.068249  DIS 0.054796\n[Permutation Importance Top 7]\nLSTAT 0.313767  RM 0.246843  NOX 0.064840  PTRATIO 0.046634  DIS 0.045469  CRIM 0.031338  TAX 0.020284\n트리 기반 주요 방법: 3. 부스팅(Boosting, Gradient Boosting, XGBoost 등)\n약한 모형(얕은 트리)을 순차적으로 연결하면서 이전 단계의 오차를 보완해 나가는 방법.\n높은 예측력을 가지지만, 매개변수 조정이 복잡하고 계산량이 많다는 단점이 있다.\n【변수선택】 순차적 학습 과정에서 반복적으로 선택되는 변수일수록 중요한 변수이다.\n# ==============================================\n# Boston Housing: Gradient Boosting 기반 변수선택 (회귀)\n#  - 방식 A: SelectFromModel (importance threshold)\n#  - 방식 B: Top-k 중요변수 교차검증 선택\n# ==============================================\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.model_selection import train_test_split, KFold, cross_val_score\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.feature_selection import SelectFromModel\n\n# ---------------------------\n# 0) 데이터 로드 & 전처리\n# ---------------------------\nboston = fetch_openml(name=\"boston\", version=1, as_frame=True)\ndf = boston.frame.copy()\n\n# 전 컬럼 숫자화 (object/category 방지)\nfor c in df.columns:\n    if df[c].dtype.name in [\"object\", \"category\"]:\n        df[c] = pd.to_numeric(df[c].astype(str).str.strip(), errors=\"coerce\")\n\n# 결측 제거\ndf = df.dropna().reset_index(drop=True)\n\ny = df[\"MEDV\"].astype(float)\nX = df.drop(columns=[\"MEDV\"]).astype(float)\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\n# ---------------------------\n# 1) 기준 성능 (모든 변수)\n# ---------------------------\nbase_gbr = GradientBoostingRegressor(\n    random_state=42,\n    n_estimators=500,      # 기본 강화 단계 수\n    learning_rate=0.05,   # 학습률(작게 → 일반화 안정)\n    max_depth=3,          # 개별 트리 깊이\n    subsample=1.0         # &lt; 1.0 이면 확률적 부스팅(과적합 완화)\n)\nbase_gbr.fit(X_train, y_train)\n\npred_base = base_gbr.predict(X_test)\nrmse_base = np.sqrt(mean_squared_error(y_test, pred_base))\nr2_base   = r2_score(y_test, pred_base)\n\nprint(\"=== 기준(모든 변수, GradientBoosting) ===\")\nprint(f\"RMSE: {rmse_base:.3f}   R^2: {r2_base:.3f}\")\n\n# ---------------------------\n# 2) 변수 중요도 표 보기\n# ---------------------------\nimp = pd.Series(base_gbr.feature_importances_, index=X.columns).sort_values(ascending=False)\nprint(\"\\n[변수 중요도 Top 7]\")\nprint(imp.head(7))\n\n# ==============================================\n# 방식 A) SelectFromModel: 중요도 임계값으로 자동 선택\n# ==============================================\nsfm = SelectFromModel(\n    estimator=GradientBoostingRegressor(\n        random_state=42, n_estimators=500, learning_rate=0.05, max_depth=3\n    ),\n    threshold=\"median\"  # 또는 \"mean\", 혹은 0.01 같은 절대값\n)\nsfm.fit(X_train, y_train)\n\nmask = sfm.get_support()\nselected_A = X.columns[mask].tolist()\n\nXtr_A = sfm.transform(X_train)\nXte_A = sfm.transform(X_test)\n\ngbr_A = GradientBoostingRegressor(\n    random_state=42, n_estimators=500, learning_rate=0.05, max_depth=3\n)\ngbr_A.fit(Xtr_A, y_train)\npred_A = gbr_A.predict(Xte_A)\nrmse_A = np.sqrt(mean_squared_error(y_test, pred_A))\nr2_A   = r2_score(y_test, pred_A)\n\nprint(\"\\n=== 방식 A: SelectFromModel (GBR) ===\")\nprint(f\"선택 변수({len(selected_A)}개): {selected_A}\")\nprint(f\"RMSE: {rmse_A:.3f}   R^2: {r2_A:.3f}\")\n\n# ==============================================\n# 방식 B) Top-k: 중요도 상위 k개를 CV로 최적 k 선택\n# ==============================================\nranked_features = imp.index.tolist()\n\ndef cv_rmse_for_k(k):\n    feats = ranked_features[:k]\n    gbr = GradientBoostingRegressor(\n        random_state=42, n_estimators=400, learning_rate=0.06, max_depth=3\n    )\n    cv = KFold(n_splits=5, shuffle=True, random_state=42)\n    neg_mse = cross_val_score(\n        gbr, X_train[feats], y_train,\n        scoring=\"neg_mean_squared_error\",\n        cv=cv, n_jobs=-1\n    )\n    rmse = np.sqrt(-neg_mse.mean())\n    return rmse, feats\n\nresults = []\nfor k in range(1, X_train.shape[1] + 1):\n    rmse_k, feats_k = cv_rmse_for_k(k)\n    results.append((k, rmse_k, feats_k))\n\nbest_k, best_cv_rmse, best_feats = min(results, key=lambda x: x[1])\n\ngbr_B = GradientBoostingRegressor(\n    random_state=42, n_estimators=500, learning_rate=0.05, max_depth=3\n)\ngbr_B.fit(X_train[best_feats], y_train)\npred_B = gbr_B.predict(X_test[best_feats])\nrmse_B = np.sqrt(mean_squared_error(y_test, pred_B))\nr2_B   = r2_score(y_test, pred_B)\n\nprint(\"\\n=== 방식 B: Top-k (CV로 k 선택, GBR) ===\")\nprint(f\"최적 k: {best_k}  | CV RMSE: {best_cv_rmse:.3f}\")\nprint(f\"선택 변수({len(best_feats)}개): {best_feats}\")\nprint(f\"Test RMSE: {rmse_B:.3f}   R^2: {r2_B:.3f}\")\n=== 기준(모든 변수, GradientBoosting) ===\nRMSE: 2.515 R^2: 0.914\n[변수 중요도 Top 7]  RM 0.406142  LSTAT 0.374462  DIS 0.072480  NOX 0.034583  PTRATIO 0.031870  CRIM 0.027173  AGE 0.021013\n=== 방식 A: SelectFromModel (GBR) ===\n선택 변수(7개): ['CRIM', 'NOX', 'RM', 'AGE', 'DIS', 'PTRATIO', 'LSTAT']\nRMSE: 2.474 R^2: 0.917\n=== 방식 B: Top-k (CV로 k 선택, GBR) ===\n최적 k: 11 | CV RMSE: 3.476\n선택 변수(11개): ['RM', 'LSTAT', 'DIS', 'NOX', 'PTRATIO', 'CRIM', 'AGE', 'B', 'TAX', 'RAD', 'INDUS']\nTest RMSE: 2.648 R^2: 0.904\n\n\n(5) 최종 임시 모형\n다중 공선성 진단 및 최종 회귀진단 전에 잠정적으로 유의한 예측변수들을 선택할 필요가 있다. 유의한 혹은 주요한 변수를 선택하는 방법은 기준은 다음과 같다.\n\n예측변수의 목표변수의 변동을 가장 잘 설명하는가? &lt;=&gt; 결정계수 (SSR Best Model)\n예측변수는 유의한가? 개별 유의성 (상관계수, Univariate Selection), 공동 유의성(후진제거), REF 방법\n영향력 높은 변수 선택 LASSO)\n\nBest subset(수정된 \\(R^{2}\\)) : ’CHAS', 'NOX', 'RM', 'DIS', 'PTRATIO', 'B', ’LSTAT'\nREF: CRIM CHAS NOX RM DIS PTRATIO. LSTAT\nLASSO: NOX RM DIS RAD TAX PTRATIO LSTAT\n회귀트리: RM LSTAT DIS CRIM NOX AGE TAX\n분류트리: LSTAT RM CRIM AGE B DIS PTRATIO\nFeature Importance Top ] RM LSTAT NOX PTRATIO. INDUS CRIM DIS\n[Permutation Importance Top 7] LSTAT RM NOX PTRATIO DIS CRIM TAX\nBoosting: RM LSTAT DIS NOX PTRATIO CRIM AGE"
  },
  {
    "objectID": "notes/math/vector.html",
    "href": "notes/math/vector.html",
    "title": "수학의 기초 1. 함수",
    "section": "",
    "text": "chapter 1. 선형대수 개념\n\n1. 선형대수 정의\n선형대수는 벡터 공간과 그 안에 존재하는 벡터 간의 관계를 다루는 수학의 한 분야로, 벡터와 행렬을 이용한 수학적 표현과 계산을 중심으로 구성된다. 이 분야의 기본 구성 요소로는 벡터, 행렬, 스칼라가 있으며, 주요 개념에는 선형 변환, 고유값과 고유벡터, 내적과 외적, 행렬의 분해 등이 포함된다.\n통계학에서 선형대수는 데이터를 벡터와 행렬의 형태로 구조화함으로써 복잡한 수치 연산을 간결하게 수행할 수 있도록 돕는다. 특히 고차원 데이터의 계산과 변환을 수학적으로 명확하게 정의할 수 있기 때문에, 데이터 구조를 이해하고 차원을 축소하는 데 핵심적인 도구로 사용된다.\n이러한 선형대수의 기법은 통계 모델링과 머신러닝의 기초가 되며, 회귀 분석이나 주성분 분석(PCA), 군집 분석 등 다양한 통계적 방법론에서 필수적인 역할을 수행한다. 결과적으로, 선형대수는 현대 통계학에서 이론적 기반뿐 아니라 실용적 계산의 핵심 수단으로 작용한다.\n\n\n2. 선형대수와 선형변환\n선형대수는 선형적인 관계를 다루는 수학의 한 분야로, 이 이론 체계 내에서 이루어지는 연산과 변환은 모두 선형성을 만족해야 한다. 이러한 특성 때문에 일반적인 함수는 선형대수의 중심 개념으로 다루어지지 않지만, 함수의 특수한 형태인 선형 변환은 예외적으로 핵심 개념으로 간주된다. 선형 변환은 벡터 공간의 구조를 보존하면서 벡터를 다른 벡터로 사상하는 과정을 의미하며, 행렬을 이용해 구체적으로 표현될 수 있다. 따라서 선형 변환은 선형대수의 이론과 응용 모두에서 중심적인 역할을 수행한다.\n함수 \\(y = f(x)\\)\n\n함수는 두 집합 사이의 관계로, 각 입력값(정의역, domain)에 대해 정확히 하나의 출력값(공역, range)을 대응시키는 규칙이다.\n함수는 일반적으로 \\(f:D \\rightarrow R\\)와 같이 표기되며, \\(D\\)는 정의역, \\(R\\)는 공역입니다.\n특정함수에 대하여 함수값이 0인 \\(f(x) = 0\\)를 방정식이라 하고 이를 만족하는 \\(x\\)를 방정식의 해(root, solution)라고 한다.\n\n\n\n\n\n\n선형함수\n선형함수는 입력 변수와 출력 변수 사이의 관계를 직선으로 나타내는 함수로, 일반적으로 다음과 같은 형태로 표현된다: \\(f(x) = a + bx\\), \\(a:\\) 절편, \\(b:\\) 기울기\n\n가법성 additivity: \\(f(x + y) = f(x) + f(y)\\)\n동차성 homogeniety: \\(f(cx) = cf(x)\\), \\(c\\)는 상수\n\n선형변환\n선형 변환(linear transformation)은 벡터 공간에서 정의된 함수 중 하나로, 한 벡터를 동일하거나 다른 벡터 공간의 또 다른 벡터로 변환하는 함수의 특수한 형태이다. 이 변환은 선형성(linearity)이라 불리는 다음의 두 가지 성질을 만족해야 한다. \\(\\underset{¯}{u},\\underset{¯}{v}\\) 동일 차원의 벡터에 대하여 함수 \\(T\\)가 다음 조건을 만족하면 선형변환이다.\n\n덧셈에 대한 선형성: \\(T(\\underset{¯}{u} + \\underset{¯}{v}) = T(\\underset{¯}{u}) + T(\\underset{¯}{v})\\)\n스칼라 곱에 대한 선형성: \\(T(c\\underset{¯}{u}) = cT(\\underset{¯}{u})\\)\n\n\n\n\n\n\n\n\n\nchapter 2. 벡터 vector 기초\n\n1. 벡터정의\n벡터는 정렬된 유한한 수들의 목록으로, 일반적으로 정사각형 괄호 또는 곡선 괄호로 둘러싸인 수직 형태의 배열로 표현된다. 이러한 형태는 수평 배열인 행벡터(row vector)와 구별하여 열벡터(column vector)라고 부른다.\n\\(\\left( \\begin{array}{r}\n1 \\\\\n- 2 \\\\\n0\n\\end{array} \\right)\\), \\(\\left\\lbrack \\begin{array}{r}\n1 \\\\\n- 2 \\\\\n0\n\\end{array} \\right\\rbrack\\)벡터를 행으로 사용할 때는 쉼표로 구분되고 괄호로 둘러싸인 숫자로 쓴다. \\(\\left( \\begin{array}{r}\n1, - 2,0\n\\end{array} \\right)\\)\n배열의 값을 벡터의 원소 element 라 하고 원소의 개수를 벡터의 크기(차원 demension)라고 한다. 위 벡터는 크기가 3 이고 세 번째 원소는 0 이다. n 크기의 벡터는 n-벡터라고 불리고 1벡터는 숫자와 같은 것으로 간주한다. 즉, 우리는 1-벡터 [ 13 ]와 숫자 13을 구별하지 않으며 숫자는 스칼라 scalar 라 한다. 벡터의 각 원소는 스칼라이고 원소가 실수인 \\(a_{i} \\in R^{n}\\) 벡터를 실수 벡터라 한다.\n\n\n2. 벡터 기호\nn-벡터를 나타내기 위해 \\({\\underset{¯}{a}}_{n}\\)(구별이 가능한 경우 알파벳 \\(a\\)를 벡터로 표현) 기호를 사용한다. \\(a_{n}\\)벡터 의 i-번째 요소는 \\(a_{i}\\)로 표시되며, 여기서 첨자 i는 벡터의 크기인 1에서 \\(n\\)까지 정의되는 정수 인덱스이다.\n두 벡터 \\(a_{n},b_{n}\\)가 동일하다는 것은 (1)크기(차수)도 \\(n\\) 동일하고 (2) 각 대응 원소가 동일 \\(a_{i} = b_{i}\\)함을 의미한다.\n\n\n3. 특수한 벡터\n\n(1) 영벡터 zero vector\n모든 원소가 0인 벡터이며 \\(0_{n}\\)으로 표현된다. 일반적으로 모든 0 벡터는 0으로 표시되며, 숫자 0을 나타내는데 사용되는 것과 동일한 기호이다. 다른 크기의 제로 벡터를 나타내기 위해 모두 같은 기호 0을 사용하므로 기호 0은 문맥에 따라 다른 것을 의미할 수 있기 때문에 컴퓨터에서는 이를 과부하라 한다.\n\n\n(2) 단위벡터 unit vector\n(표준) 단위 벡터는 1인 하나의 원소를 제외한 모든 요소가 0과 같은 벡터이다. i-번째 단위 벡터(n 크기)는 i-번째 원소만 1을 가진 단위 벡터이며, \\(e_{i}\\)로 표현한다. 이렇게 되면 크기를 나타내는 첨자와 1인 원소 위치를 나타내는 첨자가 구별이 되지 않는 모호성을 갖는다.\n\n\n(3) 일벡터 ones vector\n모든 원소가 1인 n-벡터이며 \\(1_{n}\\)로 표현한다. 우리는 또한 벡터의 크기가 문맥에서 결정될 수 있다면 1로 쓴다.\n\n\n\n4. 벡터 개념\n\n(1) 위치 location\n2차원 공간, 즉 평면의 위치를 나타내는 데 사용될 수 있다. 3-벡터는 3차원(3-D) 공간에서 어떤 지점의 위치나 위치를 나타내는 데 사용된다. 벡터의 원소는 위치의 좌표를 제공한다.\n\n\n\n\n\n벡터는 주어진 시간에 평면이나 3차원 공간에서 움직이는 지점의 속도나 가속도를 나타내는 데 사용될 수 있다.\n\n\n\n\n\n\n\n(2) 희소성\n많은 원소가 0이면 희소하다고 한다. 그것의 희소성 패턴은 0이 아닌 항목의 인덱스 집합이다. \\(n\\)-벡터 \\(a_{n}\\)의 0이 아닌 항목의 수는 \\(nnz(a_{n})\\)로 표시한다다. 단위벡터는 0이 아닌 항목이 하나만 있기 있고 0 벡터는 0이 아닌 항목이 없기 때문에 희소한 벡터이다.\n\n\n(3) 이미지\n3차원 벡터는 빨간색, 녹색 및 파란색(R-G-B) 강도 값(0에서 1 사이)을 제공하는 항목을 통해 색상을 나타낸다. 벡터(0,0,0)는 검은색을 나타내고, 벡터(0, 1, 0)는 밝은 순수한 녹색을 나타내며, 벡터(1, 0.5, 0.5)는 분홍색을 나타낸다.\n\n\n\n\nchapter 3. 벡터 연산과 크기\n\n1. 벡터 연산\n\n(1) 벡터 합\n두 벡터를 합을 구한다는 것은 (1) 차수가 동일한 두 벡터의 (2) 동일 위치의 원소를 합하여 하나의 벡터를 계산한다는 것을 의미한다. 차도 동일하다.\n\\(\\left\\lbrack \\begin{array}{r}\n1 \\\\\n- 2 \\\\\n0\n\\end{array} \\right\\rbrack + \\left\\lbrack \\begin{array}{r}\n1 \\\\\n2 \\\\\n3\n\\end{array} \\right\\rbrack = \\left\\lbrack \\begin{array}{r}\n2 \\\\\n0 \\\\\n3\n\\end{array} \\right\\rbrack\\), \\(\\left\\lbrack \\begin{array}{r}\n1 \\\\\n- 2 \\\\\n0\n\\end{array} \\right\\rbrack - \\left\\lbrack \\begin{array}{r}\n1 \\\\\n2 \\\\\n3\n\\end{array} \\right\\rbrack = \\left\\lbrack \\begin{array}{r}\n0 \\\\\n- 4 \\\\\n- 3\n\\end{array} \\right\\rbrack\\)\n성질\n차수가 동일한 벡터 \\(a,b,c\\)에 대하여 다음이 성립한다.\n\n교환법칙 : \\(a + b = b + a\\)\n교환법칙 : \\((a + b) + c = a + (b + c)\\)\n영벡터를 더하거나 빼도 영향을 받지 않는다. \\(a \\pm 0 = a\\)\n벡터에서 자체 벡터를 빼면 영벡터가 된다. \\(a - a = 0\\)\n\n\n\n(2) 스칼라-벡터 곱\n벡터에 스칼라(즉, 숫자)를 곱하는 스칼라-벡터 곱셈은 벡터의 모든 요소에 스칼라를 곱하여 수행한다. 일반적으로 스칼라를 왼쪽, 벡터를 오른쪽에 적지만 순서를 바꾸어 사용해도 되고 계산 결과는 동일하다.\n\\(a = \\left\\lbrack \\begin{array}{r}\n1 \\\\\n- 2 \\\\\n0\n\\end{array} \\right\\rbrack\\)이면 \\(3a = a3 = \\left\\lbrack \\begin{array}{r}\n3 \\\\\n- 6 \\\\\n0\n\\end{array} \\right\\rbrack\\)\n성질\n벡터 \\(a\\), 스칼라 \\(c,k\\)에 대하여 다음이 성립한다.\n\n교환법칙 : \\(ka = ak\\)\n배분법칙 : \\((c + k)a = ca + ka\\)\n\n\n\n(3) 선형 결합 linear combination\n차수 \\(n\\)-벡터 \\(a_{1},a_{2},...,a_{m}\\), 스칼라 \\(k_{1},k_{2},...,k_{m}\\)에 대하여 다음 \\(n\\)-벡터를 벡터 \\(a_{1},a_{2},...,a_{m}\\)의 선형결합이라 하고 스칼라 \\(k_{1},k_{2},...,k_{m}\\)는 선형결합의 계수라 한다.\n\\[k_{1}a_{1} + k_{2}a_{2} + ... + k_{m}a_{m}\\]\n\n\\(k_{1} = k_{2} = ... = k_{m} = 1\\)이면, 선형결합은 벡터 합이다.\n\\(k_{1} = k_{2} = ... = k_{m} = \\frac{1}{m}\\)이면, 선형결합은 벡터 평균이다.\n\\(k_{1} + k_{2} + ... + k_{m} = 1\\)이면, 선형결합은 affine 결합이라 하고 모든 계수가 양수인 경우 선형결합을 가중평균이라 한다.\n\n\n\n(4) 내적 inner product\n두 벡터 간의 관계를 정의하고 벡터의 길이와 각도 등의 개념을 도입하는 중요한 연산이다. 차수(\\(m\\))가 동일한 두 벡터 (\\(u,v\\))의 내적 곱은 다음과 같이 정의하고 결과는 스칼라이다.\n\\[u^{T}v = \\lbrack u_{1},u_{2},...,u_{m}\\rbrack\\left\\lbrack \\begin{array}{r}\nv_{1} \\\\\nv_{2} \\\\\n... \\\\\nv_{m}\n\\end{array} \\right\\rbrack = u_{1}v_{1} + u_{2}v_{2} + ... + u_{m}v_{m} = \\overset{m}{\\sum_{i = 1}}u_{i}v_{i}\\]\n단, \\(u^{T}\\)는 \\(u\\)의 전치 transpose라 하고 열벡터를 행벡터로 변환한 것이다.\n【예제】\n\\[\\lbrack 1,3,5\\rbrack^{T}\\left\\lbrack \\begin{array}{r}\n  0 \\\\\n   - 1 \\\\\n  1\n  \\end{array} \\right\\rbrack = (1)(0) + (3)( - 1) + (5)(1) = 2\\]\n내적 성질\n\nunit 벡터 : \\(e_{i}v = v_{i}\\)\n벡터 합 : \\(1_{m}^{T}v = \\overset{m}{\\sum_{i = 1}}v_{i}\\)\n벡터 평균 : \\(avg(v) = (1/n)1_{m}^{T}v = (1/n)\\overset{m}{\\sum_{i = 1}}v_{i}\\)\n벡터 제곱합 : \\(v^{T}v = v_{1}^{2} + v_{2}^{2} + ... + v_{m}^{2} = \\overset{m}{\\sum_{i = 1}}v_{i}^{2}\\)\n\nCauchy–Schwarz inequality\n차수 동일한 두 벡터의 내적 inner product에 대하여 다음이 성립한다.\n\\[\\parallel a^{T}b \\parallel \\leq \\parallel a \\parallel \\parallel b \\parallel\\]\n\\[|\\overset{n}{\\sum_{i}}a_{i}b_{i}| \\leq (\\sum a_{i}^{2})^{\\frac{1}{2}}(\\sum b_{i}^{2})^{\\frac{1}{2}}\\]\n\n\n(5) 외적 cross product\n주로 3차원 공간에서 두 벡터로부터 새로운 벡터를 생성하는 연산입니다. 이 연산의 결과는 두 벡터에 모두 수직인 벡터이며, 크기는 두 벡터가 이루는 평행사변형의 면적에 해당합니다.\n외적 정의\n벡터 \\(\\underset{¯}{a} = (a_{1},a_{2},a_{3})\\)와 벡터 \\(\\underset{¯}{b} = (b_{1},b_{2},b_{3})\\) 의 외적 \\(\\underset{¯}{a} \\times \\underset{¯}{b}\\) 는 다음과 같이 계산한다.\n\n\\(x\\) 성분: \\(a_{2}b_{3} - a_{3}b_{2}\\)\n\\(y\\) 성분: \\(a_{3}b_{1} - a_{1}b_{3}\\)\n\\(z\\) 성분: \\(a_{1}b_{2} - a_{2}b_{1}\\)\n\n\n\n\n\n\n【예제】 벡터 \\(\\underset{¯}{a} = (2,3,4)\\)와 벡터 \\(\\underset{¯}{b} = (5,6,7)\\)의 외적은 \\(\\underset{¯}{c} = \\underset{¯}{a} \\times \\underset{¯}{b} = ( - 3,6, - 3)\\) 이다.\n외적은 벡터 \\(\\underset{¯}{a},\\underset{¯}{b}\\)와 수직(\\({\\underset{¯}{c}}^{T}\\underset{¯}{a} = 0\\), \\({\\underset{¯}{c}}^{T}\\underset{¯}{b} = 0\\))이며 외적의 크기(놈 norm)는 두 벡터가 이루는 평행사면형 면적이다.\n\n\n\n2. 선형함수\n선형함수 정의\n\\(f:R^{n} \\rightarrow R\\)는 크기 n-벡터를 실수(스칼라)로 매핑하는 함수이다. 함수 \\(f(x)\\)의 \\(x_{1},x_{2},...,x_{n}\\)은 함수 \\(f\\)의 인수 argument라 하고 결과 값 스칼라는 함수 값이다. \\(f(x) = f(x_{1},x_{2},...,x_{n})\\)\n【예제】\n\\[f:R^{4} \\rightarrow R$ : $f(x) = x_{1} - x_{2} + x_{4}^{2}\\]\n차수 n-벡터 \\(a,x\\)에 대하여 내적 함수 \\(f(x) = a^{T}x = scalar\\)는 선형함수일 때 다음이 성립한다. 단, \\(\\alpha,\\beta\\)는 스칼라, \\((x,y)\\)는 n-벡터이다. \\(f(\\alpha x + \\beta y) = \\alpha f(x) + \\beta f(y)\\)\n선형함수 조건\n다음 조건을 만족하는 \\(f:R^{n} \\rightarrow R\\) 는 선형함수이다. 단, \\(\\alpha\\)는 스칼라, \\((x,y)\\)는 n-벡터이다.\n\nHomogeniety : \\(f(\\alpha x) = \\alpha f(x)\\)\nAdditivity : \\(f(x + y) = f(x) + f(y)\\)\n\n\n(1) 절편 Affine 함수\n선형 함수에 상수 항을 추가한 형태의 함수이다. 이는 선형 변환과 평행 이동을 결합한 함수로, 다음과 같은 수식으로 표현된다.\nn-벡터, \\(x\\)에 대하여 다음 \\(f\\)는 절편 함수이다. 단, \\(a\\)는 n-벡터, \\(k\\)는 스칼라이다. \\(f(a^{T}x + k) = a^{T}f(x) + k\\)\n【예제】 \\(f(x) = 7 - 2x_{1} + 3x_{2} - x_{3}\\), \\(k = 7,a = \\left\\lbrack \\begin{array}{r}\n   - 2 \\\\\n  3 \\\\\n   - 1\n  \\end{array} \\right\\rbrack\\)\n\n\n(2) 선형함수의 내적 표현\n\\(e_{i}\\) 단위벡터, \\(x_{n}\\) 차수 n-벡터, \\(f\\) 선형함수라 하면, \\[\\begin{matrix}\nf(x) & = f(x_{1}e_{1} + x_{2}e_{2} + ... + x_{n}e_{n}) \\\\\n& = x_{1}f(e_{1}) + x_{2}f(e_{2}) + ... + x_{n}f(e_{n}) \\\\\n& = a^{T}x,wherea^{T} = \\lbrack f(e_{1}),f(e_{2}),...,f(e_{n})\\rbrack\n\\end{matrix}\\]\n\n\n(3) 사례 : sag 처짐 (단위: mm)\n하중벡터 \\(w = \\left( \\begin{array}{r}\nw_{1} \\\\\nw_{2} \\\\\nw_{3}\n\\end{array} \\right)\\)(단위:톤), 변형 compliance 민감도 벡터 \\(c = \\left( \\begin{array}{r}\nc_{1} \\\\\nc_{2} \\\\\nc_{3}\n\\end{array} \\right)\\)(단위:mm/톤)이라면 교량 처짐 sag은 \\(s = c^{T}w\\) (하중 가중합)이다.\n\n\n\n\n\n\n\n(4) 테일러 근사 Taylor proximation\n함수 \\(f:R^{n} \\rightarrow R\\)이 1차 미분이 가능하다고 하면 \\(n\\)-벡터 함수 \\(f(x)\\)의 근사값은 다음과 같이 구한다. 이를 1차 테일러 근사라 한다. 단, n-벡터 \\(z\\)는 n-벡터 \\(x\\)와 가까운 값이다.\n\\[\\widehat{f}(x) = f(z) + \\frac{\\partial f}{\\partial x_{1}}(z)(x_{1} - z_{1}) + ... + \\frac{\\partial f}{\\partial x_{n}}(z)(x_{n} - z_{n})\\]\n【예제】\n함수 \\(f:R^{2} \\rightarrow R\\)을 \\(f(x) = x_{1} + \\exp(x_{2} - x_{1})\\)라 하자. 이 함수는 선형함수는 아니다. 이를 선형함수로 근사하는 것을 테일러 근사라 한다. \\(z = (1,2)\\)라 하면,\n\\[\\triangledown f(z) = \\left\\lbrack \\begin{array}{r}\n1 - \\exp(z_{2} - z_{1}) \\\\  \n\\exp(z_{2} - z_{1})  \n\\end{array} \\right\\rbrack|_{z_{1} = 1,z_{2} = 2} = ( - 1.72,2.72)\\]\n그러므로 \\(z = (1,2)\\)에서 \\(f(x)\\)의 테일러 근사값은 다음과 같다:\n\\[\\widehat{f}(x) = 3.718 + \\left\\lbrack \\begin{array}{r}  - 1.72 \\\\  2.72 \\end{array} \\right\\rbrack^{T}(\\left\\lbrack \\begin{array}{r} x_{1} \\\\  x_{2} \\end{array} \\right\\rbrack - \\left\\lbrack \\begin{array}{r}  1 \\\\ 2  \\end{array} \\right\\rbrack)\\]\n\n\n(5) 회귀모형\n차원 2-예측(설명, 독립) 벡터 \\(x = \\left\\lbrack \\begin{array}{r}\nx_{1} \\\\\nx_{2}\n\\end{array} \\right\\rbrack\\), 회귀계수 벡터 \\(b = \\left\\lbrack \\begin{array}{r}\nb_{1} \\\\\nb_{2}\n\\end{array} \\right\\rbrack\\), 그리고 \\(a\\)을 절편 스칼라라 하면 회귀모형은 다음과 같다.\n\\(\\widehat{y} = \\left\\lbrack \\begin{array}{r}\n1 \\\\\nx\n\\end{array} \\right\\rbrack^{T}\\left\\lbrack \\begin{array}{r}\na \\\\\nb\n\\end{array} \\right\\rbrack = {\\overset{˜}{x}}^{T}\\overset{˜}{b}\\) OLS 추정치 : \\(\\widehat{\\overset{˜}{b}} = ({\\overset{˜}{x}}^{T}\\overset{˜}{x})^{- 1}{\\overset{˜}{x}}^{T}y\\)\n\n\n\n3. 벡터놈 norm\n\n(1) 정의\n벡터의 유클리디안 놈, \\(\\parallel x \\parallel\\)은 벡터의 크기에 대한 척도로 다음과 같이 구한다. 놈은 벡터의 원점에서의 거리이다.\n\\[\\parallel x \\parallel = \\sqrt{x_{1}^{2} + x_{2}^{2} + ... + x_{n}^{2}} = \\sqrt{x^{T}x}\\]\n【예제】\n\\[\\parallel \\left\\lbrack \\begin{array}{r}\n  0 \\\\\n   - 1 \\\\\n  1\n  \\end{array} \\right\\rbrack \\parallel = \\sqrt{2}$,\n  $\\parallel \\left\\lbrack \\begin{array}{r}\n   - 1 \\\\\n  2\n  \\end{array} \\right\\rbrack \\parallel = \\sqrt{5}\\]\n성질\n\n비음수 동차성: \\(\\parallel \\beta x \\parallel = |\\beta| \\parallel x \\parallel\\), where \\(\\beta\\)는 스칼라\n삼각 부등식: \\(\\parallel x + y \\parallel \\leq \\parallel x \\parallel + \\parallel y \\parallel\\)\n비음수: \\(\\parallel x \\parallel \\geq 0\\)\n\n\n\n(2) 놈의 종류\n\nL1 norm : \\(L_{1} = \\overset{n}{\\sum_{i}}|x_{i}|\\) 절대값의 합으로 맨하튼 Manhattan 놈이라고도 한다. 지도의 거리 측정에 사용된다.\nL2 norm : \\(L_{2} = (\\overset{n}{\\sum_{i}}x_{i}^{2})^{\\frac{1}{2}}\\) 제곱합의 제곱근으로 유클리디안 놈이라 한다. 통계학에서 가장 많이 사용된다. 회귀계수 추정치를 구하는 최소제곱추정치 구할 때 사용된다.\n\n\n\n\n\n\n#행렬 정의\nimport numpy as np\nA=np.array([[1,2,3], [4,5,7],[8,9,10]])\n#L1 norm Mahattan\nla.norm(A,axis=1,ord=1)\n【결과】 array([ 6., 16., 27.])\n#L2 norm Euclidean\nla.norm(A,axis=1,ord=2)\n【결과】 array([ 3.74165739, 9.48683298, 15.65247584])\n\n\n(3) 평균 제곱근 RMS root mean square value\n데이터 크기를 정량화하는데 사용되며 데이터의 평균적인 크기를 나타낸다. \\(rms(x) = \\frac{\\parallel x \\parallel}{\\sqrt{n}} = \\sqrt{\\frac{1}{n}\\sum x_{i}^{2}}\\)\n\n\n(4) 두 벡터의 합의 놈\n\\[\\parallel x + y \\parallel = \\sqrt{\\parallel x \\parallel^{2} + 2x^{T}y + \\parallel y \\parallel^{2}}\\]\n\n\n(5) Chebyshev inequality\n차수 n-벡터 \\(x\\), \\(x_{i}^{2} \\geq a^{2}\\)을 만족하는 원소 개수를 \\(k\\)라 하면, \\(\\parallel x \\parallel^{2} = x_{1}^{2} + ... + x_{2}^{2} \\geq ka^{2}\\)이다. \\(k \\leq n\\)이므로 \\(n \\leq \\frac{\\parallel x \\parallel}{a^{2}}\\)이다. 즉, 벡터의 어떠한 원소도 그 벡터의 놈보다 크지 않다.\n\\(\\frac{k}{n} \\leq (\\frac{rms(x)}{a})^{2}\\). 왼쪽 항은 벡터의 성분 중 절대값이 최소한 \\(a\\)이상인 성분의 비율을 나타낸다. 오른쪽 항은 \\(a\\)와 \\(rms(x)\\)의 비율의 제곱에 대한 역수이다. 예를 들어, 벡터의 성분 중 1/25 = 4% 이상은 RMS 값의 5배를 초과할 수 없다는 것을 의미한다.\n\n\n\n\nchapter 4. 벡터간 거리\n\n1. 유클리디안 거리\n\n(1) 정의\n차수가 동일한 두 벡터(\\(a,b\\))의 놈을 유클리디안 거리로 정의한다.\n\\[dist(a,b) = \\parallel a - b \\parallel = \\parallel b - a \\parallel\\]\n\\[||a - b|| = \\sqrt{(a_{1} - b_{1})^{2} + (a_{2} - b_{2})^{2} + ... + (a_{n} - b_{n})^{2}}\\]\n두 벡터의 Root Mean Square 편차 = \\(\\frac{\\parallel x - y \\parallel}{\\sqrt{n}}\\)\n【예제】\n\\[a = \\left\\lbrack \\begin{array}{r} 0 \\\\ - 1 \\\\ 1 \\end{array} \\right\\rbrack,b = \\left\\lbrack \\begin{array}{r} 1 \\\\ - 2 \\\\ 1  \\end{array} \\right\\rbrack,c = \\left\\lbrack \\begin{array}{r} 1 \\\\ 0 \\\\3 \\end{array} \\right\\rbrack\\] \\[dist(a,b) = \\sqrt{2},dist(b,c) = 2.8284\\]\n#행렬 정의\nimport numpy as np\na=np.array([[0],[-1],[1]])\nb=np.array([[1],[-2],[1]])\nc=np.array([[1],[0],[3]])\n#거리 계산\nnp.linalg.norm(a-b),np.linalg.norm(b-c)\n【결과】 (np.float64(1.4142135623730951), np.float64(2.8284271247461903))\n\n\n(2) 활용\n\nfeature distance: \\(\\parallel x - y \\parallel\\) 차수가 동일한 두 벡터의 거리를 개체의 유사성 척도로 사용한다.\nNearest neighbor: \\(\\parallel x - z_{i} \\parallel\\) 두 개체 간의 거리를 이용하여 유사한 개체를 군집으로 묶는다. k-means 알고리즘\nRMS prediction error: \\(rms(y - \\widehat{y})\\) 관측치와 예측치의 거리를 예측의 정확도 척도로 사용한다.\n\n#감성 분석\nimport numpy as np\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n# 데이터 준비\ntexts = [\"I love this product\", \"This is terrible\", \"Absolutely fantastic\", \"Not good at all\"]\nlabels = [1, 0, 1, 0]  # 1: 긍정, 0: 부정\n# TF-IDF 벡터화\nvectorizer = TfidfVectorizer()\nX = vectorizer.fit_transform(texts)\n# KNN 모델\nknn = KNeighborsClassifier(n_neighbors=1, metric='euclidean')\nknn.fit(X, labels)\n# 새로운 리뷰 분류\nnew_text = [\"I hate this product\"]\nnew_vector = vectorizer.transform(new_text)\nprediction = knn.predict(new_vector)\nprint(f\"Prediction: {'Positive' if prediction[0] == 1 else 'Negative'}\")\n【결과】 Prediction: Positive\n\n\n(3) 삼각 부등식\n차수가 동일한 n벡터 \\(a,b,c\\)에 대하여 다음이 발생한다.\n\\[\\parallel a - c \\parallel \\leq \\parallel a - b \\parallel + \\parallel b - c \\parallel\\]\n\n\n(4) triangle 부등식\n\\[\\parallel a + b \\parallel^{2} \\leq ( \\parallel a \\parallel + \\parallel b \\parallel )^{2}\\]\n\n\n(5) 맨해튼 거리\n\\[d(\\mathbf{a},\\mathbf{b}) = \\overset{n}{\\sum_{i = 1}}|a_{i} - b_{i}|\\]\n맨해튼 거리는 벡터 간의 축을 따라 이동한 거리의 합으로 이는 그리드 기반 공간에서 이동하는 경우에 적합하다. 맨해튼 거리라는 이름은 도로망이 격자 형태로 이루어진 맨해튼 도시 구조에서 유래되었다. 자동차나 사람이 이동할 때 대각선으로 이동하지 못하고 도로를 따라 움직이는 경우에 적합하다. 예: 두 위치 간 최단 이동 거리 계산.\n\n\n\n\n\n\n\n\n2. 유클리디안 거리와 통계\n\n(1) de-meanded 벡터\n【reall】 치수 n-벡터 \\(x_{n}\\), 평균은 \\(avg(x) = (1_{n}^{T}x)/n = (instat) = \\overline{x}\\)\n【정의】 \\(\\overset{˜}{x} = x - avg(x)1_{n}\\) : 벡터의 각 원소를 평균을 뺀 벡터\n【성질】 \\(avg(\\overset{˜}{x}) = 0\\)\n\n통계 분석: 데이터의 평균을 제거함으로써 분산이나 공분산과 같은 통계적 특성을 더 명확하게 분석할 수 있다.\n주성분 분석(PCA): 데이터의 분산을 분석하기 전에 데이터를 중심에 맞추기 위해 사용된다.\n회귀 분석: 회귀 분석에서 독립 변수와 종속 변수의 평균을 제거하여 상수항 없이 회귀 모델을 구축할 수 있다.\n\n\n\n(2) 표준편차 standard deviation\n\\[std(x) = \\sqrt{\\frac{(x_{1} - avg(x))^{2} + (x_{2} - avg(x))^{2} + ... + (x_{n} - avg(x))^{2})}{n}}\\]\n\\[std(x) = \\frac{\\parallel x - (1^{T}x/n)1 \\parallel}{\\sqrt{n}}\\]\n【응용】 투자에서 평균은 일정기간 평균 수익율, 표준편차는 위험 척도이다.\n표준편차 성질\n상수를 더해도 표준편차는 동일하다. \\(std(x + a1) = std(x)\\)\n스칼라(상수) 곱 : \\(std(kx) = |k|std(x)\\)\n평균, RMS, STD 관계\n\\(std(x)^{2} = rms(x)^{2} - avg(x)^{2}\\)\n(in stat) \\(std(x)^{2} = var(x)\\) 분산\n표준편차와 Chebychev 부등식\n만약 차원 \\(n\\)-벡터에서 \\(|x_{i} - avg(x)| \\geq a\\)을 만족하는 원소 개수를 \\(k\\)라 하면 \\(\\frac{k}{n} \\leq (\\frac{std(x)}{a})^{2}\\)이다. 벡터 \\(x\\) 평균으로부터 \\(k\\) 표준편차 이내에 있는 성분 비율은 최소 \\(1 - 1/k^{2}\\)이다.\n\\[P(|X - \\mu| &gt; k\\sigma) \\leq 1 - \\frac{1}{k^{2}}\\]\n예를 들어, 일정 기간 투자 평균 수익률은 8%이고, 리스크(표준편차)는 3%입니다. 체비셰프의 부등식에 따르면, 손실을 기록한 기간의 비율(즉, 0% 이하인 기간, 16% 이상인 기간)은 최대 (3/8)^2 = 14.1%이다.\n\n\n(3) 실증적 규칙\n\\[P(|X - \\mu| \\leq k\\sigma)\\]\n\n\\(k = 1\\), 데이터의 68.3%가 \\((\\mu - \\sigma,\\mu + \\sigma)\\) 내에 있음\n\\(k = 2\\), 데이터의 95.4%, \\(k = 3\\), 데이터의 99.9%\n\n\n\n\n\n\n\n\n특징\n실증적 규칙\n체비세프 규칙\n\n\n\n\n분포가정\n정규분포에만 적용 가능\n모든 분포에 적용 가능\n\n\n그래프 모양\n종형 곡선(정규분포)\n다양한 분포(정규분포, 비대칭, 멀티모달 등)\n\n\n데이터 범위\n평균과 표준편차로 대칭적인 확률 분포\n최소한의 비율을 보장하며 보수적(더 큰 범위를 포함)\n\n\n데이터 비율\n±1σ: 68%, ±2σ: 95%, ±3σ: 99.7%\n±2σ: ≥75%, ±3σ: ≥88.9%\n\n\n\n\n\n\n\n3. 거리와 개체 군집화\n\n(1) 개념\n\\(N\\)개의 차수 \\(n\\)-벡터 \\((x_{1},x_{2},...,x_{N})\\)에 대하여 각 벡터(개체) 쌍 사이의 거리로 측정하여 서로 가까운 클러스터 또는 클러스터로 묶는 작업을 다룬다. 클러스터링의 목표는 가능한 경우 벡터들을 \\(k\\)개의 클러스터 또는 클러스터로 묶거나 나누어, 각 클러스터 내의 벡터들이 서로 가깝도록 하는 것이다. 클러스터링은 벡터들이 객체의 특징을 나타낼 때 널리 사용된다. 다음은 \\(n = 2\\)(군집변수 2개), \\(k = 3\\)으로 클러스터링 한 사례이다.\n\n\n\n\n\n\n\n(2) 클러스터 할당\n\\(N\\)개 개체, \\(x_{i}\\)를 개체(\\(i = 1,2,...,N\\)), \\(c_{i}\\)는 \\(i\\)-개체가 할당된 클러스터이고 (\\(j = 1,2,...k\\)), \\(G_{j}\\)을 \\(j\\)-클러스터에 속한 개체의 집합이라 하자.\n\\[G_{j} = \\{ i|c_{i} = j\\}\\]\n클러스터을 대표하는 차원 \\(n\\)-벡터를 \\(z_{1},z_{2},...,z_{k}\\)라 하자. \\(i\\)-개체가 \\(j = c_{i}\\)에 있다면 \\(\\parallel x_{i} - z_{c_{i}} \\parallel\\)은 모든 클러스터 중 가장 가까워야 한다.\n\n\n(3) 클러스터 목적\n\\(J^{clust} = ( \\parallel x_{1} - z_{c_{1}} \\parallel + \\parallel x_{2} - z_{c_{2}} \\parallel + ... + \\parallel x_{N} - z_{c_{N}} \\parallel )/N\\) 함수를 최소화 하는 \\(z_{c_{1}},z_{c_{2}},...,z_{c_{N}}\\)을 구한다.\n\n\n(4) 최적 클러스링\n목적함수 \\(J^{clust}\\)을 최소화 하는 \\(z_{c_{1}},z_{c_{2}},...,z_{c_{N}}\\)을 찾는 것은 개체 수가 많고 차원 개수가 커지면 계산 회수가 기하 급수적으로 늘어나 불가능하다. 그러므로 최적 대신 차선 sub-optimal 방법으로 대표 벡터를 고정화 하는 k-평균 방법을 사용한다.\n\n\n\n\n\n\n\n\n4. k-means 알고리즘\n\n(1) 개념\n클러스터 할당과 클러스터 대표자를 선택하여 \\(J^{clust}\\)를 최소화하는 문제를 해결할 수 있을 것처럼 보이나 두 가지 선택은 순환적입니다. 즉, 각각의 선택이 다른 하나에 의존한다. 클러스터 대표자를 선택하고 클러스터 할당을 선택하는 것을 반복하는 것이 벡터 집합을 클러스터링하는 데 있어서 유명한 k-means 알고리즘이다. k-means 알고리즘은 1957년에 Stuart Lloyd와 독립적으로 Hugo Steinhaus에 의해 처음 제안되어 때때로 Lloyd 알고리즘이라고도 불린다. k-means라는 이름은 1960년대부터 사용되었다.\n\n\n(2) k-평균 알고리즘\n\\(N\\)개 개체를 \\(k\\)개 클러스터으로 분류한다고 가정하자. \\(z_{1},z_{2},...,z_{k}\\)을 각 클러스터의 대표 벡터라 하자. k-평균 알고리즘은 다음 작업을 반복 실행한다.\n\n대표 벡터를 결정하고 각 개체를 가장 가까운 대표 벡터의 클러스터으로 분류한다.\n클러스터에 할당된 개체의 중심점(평균 벡터)을 대표 벡터로 설정한다.\n수렴 조건 만족 때까지 위의 작업을 반복한다.\n\n\n\n(3) 이슈사항\n타이 브레이커: 두 개 이상의 클러스터과 최소 거리인 개체는 클러스터 할당을 하지 않는다. 그러므로 이 개체는 다음 단계에서 대표 벡터 결정에는 활용되지 않는다.\n수렴 조건: 개체의 클러스터 이동이 더 이상 발생하지 않으면 대표 벡터는 움직이지 않음을 의미하므로 클러스터링 결과는 동일해진다.\nk-평균 알고리즘은 직관적이다.: 목표함수 \\(J^{clust}\\)을 최적화 하지 못하지만 반복을 통하여 줄여 나가게 된다.\n대표벡터 해석: 각 \\(N\\) 개의 회사마다 총 자본화, 분기별 수익 및 위험, 거래량, 손익, 배당금 등과 같은 금융 및 사업 속성을 구성 요소로 하는 n-벡터을 이용하여 k-평균 클러스터링 결과 얻은 대표벡터를 이용하여 클러스터(군집)에 이름을 부여한다. 기업연수, 기업종류, 매출액 등 군집변수로 사용하지 않은 특성 벡터를 이용하여 개체 군의 이름을 부여하고 해석한다.\n클러스터 \\(k\\) 결정: \\(k\\) 의 결정은 다소 주관적이고 시행착오 방법을 사용한다. \\((k,J^{clust})\\)을 이용하여 Elbow Method 팔꿈치 기법을 사용한다. 군집 개수가 증가할수록 \\(J^{clust}\\)는 감소하게 되지만, 이 감소율이 꺾이는 지점을 찾아내는 방법이다.\n고정 대표 벡터 분할하기: 만약 \\(j\\) 클러스터을 대표하는 벡터 \\(z_{1},z_{2},...,z_{j}\\)르 고정하면 모든 개체 \\(x_{1},x_{2},...,x_{N}\\)을 최적 클러스터으로 분류 문제는 다음과 같다.\n\\[\\parallel x_{i} - z_{c_{i}} \\parallel = min_{j = 1,2,...,k} \\parallel x_{i} - z_{j} \\parallel\\]\n고정 대표 벡터를 활용하면 최적 클러스터링 문제는 다음과 같이 sub 최적 문제로 변환된다. 각 \\(N\\)개 개체에 최적 \\(j\\)-클러스터(거리가 가장 가까운 클러스터)을 결정하는 개별적 문제와 동일하다.\n\\[J^{clust} = min_{j = 1,2,...,k} \\parallel x_{1} - z_{j} \\parallel + ... + min_{j = 1,2,...,k} \\parallel x_{N} - z_{j} \\parallel )/N\\]\n고정 벡터를 group(or cluster) centroid라 한다.\n\n\n(4) 사례\n# 60000(train 훈련)/10000(test 테스트), 28x28\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.datasets import mnist\n# MNIST 데이터셋 로드 및 훈련데이터, 테스트데이터 분할 \n(x_train, y_train), (x_test, y_test) = mnist.load_data()\n# 데이터 형태 출력\nprint(f\"x_train shape: {x_train.shape}\")\nprint(f\"y_train shape: {y_train.shape}\")\nprint(f\"x_test shape: {x_test.shape}\")\nprint(f\"y_test shape: {y_test.shape}\")\n# 첫 10개 샘플 이미지와 레이블 시각화\nnum_samples = 10\nplt.figure(figsize=(10, 1))\nfor i in range(num_samples):\n    plt.subplot(1, num_samples, i+1)\n    plt.imshow(x_train[i], cmap='gray')\n    plt.title(y_train[i])\n    plt.axis('off')\nplt.show()\n\n\n\n\n\n# 훈련 데이터 클러스트링, 첫 20개 군집결과\n# 이미지 데이터를 2차원 배열로 변환\nx_train2 = x_train.reshape((x_train.shape[0], -1))\nx_test2 = x_test.reshape((x_test.shape[0], -1))\n# 데이터 정규화\nx_train2 = x_train2 / 255.0\nx_test2 = x_test2 / 255.0\n# k-means 모델 생성 및 학습\nkmeans = KMeans(n_clusters=10, random_state=42)\nkmeans.fit(x_train2)\n# 클러스터 할당 결과\ny_kmeans = kmeans.predict(x_train2)\n# 첫 20개 분류결과 이미지와 레이블 시각화\nnum_samples = 20\nplt.figure(figsize=(10, 1))\nfor i in range(num_samples):\n    plt.subplot(1, num_samples, i+1)\n    plt.imshow(x_train[i], cmap='gray')\n    plt.title(y_kmeans[i])\n    plt.axis('off')\nplt.show()\n 10개 클러스터명은 임의로 정해져 숫자와 매칭이 되지 않는다. 클러스터에 속한 이미지를 이용하여 결정한다. 클러스터9, 클러스터1에는 이미지 6/2이 두개이므로 숫자6,숫자2 클러스터으로 하면 된다. 클러스터3에는 2개 이지지 중 숫자5, 3, 8이 각각 1개이므로 나머지 클러스터3으로 분류된 이미지 번호 확인하여 숫자번호를 결정한다. 클러스터5에는 이미지9 2개, 이미지7, 이미지4 각각 1개이므로 클러스터5는 이미지9 군집으로 한다.\n# 클러스터 대표 이미지\n# 클러스터 3 평균벡터 출력\nplt.figure(figsize=(10, 1))\nplt.imshow((x_train[0]+x_train[7]+x_train[17])/3, cmap='gray')\nplt.title('cluster 3')\nplt.axis('off')\nplt.show()\n\n\n\n\n\n\n\n\n5. 벡터의 각도\n\n(1) 코사인 유사도\n벡터의 코사인 유사도(Cosine Similarity)는 두 벡터 간의 방향적 유사성을 측정하는 지표로, 벡터 간의 각도 \\(\\theta\\)의 코사인 값을 이용하여 계산된다.\n\\[\\text{Cosine Similarity} = cos(\\theta) = \\frac{\\mathbf{A} \\cdot \\mathbf{B}}{\\parallel \\mathbf{A} \\parallel \\parallel \\mathbf{B} \\parallel}\\]\n코사인 유사도와 유클리드 거리의 차이는 다음과 같다.\n\n코사인 유사도는 두 벡터의 방향에 집중하며, 벡터 크기의 차이를 무시한다.\n유클리드 거리는 두 벡터 사이의 실제 거리(크기 차이 포함)를 측정한다.\n예를 들어, 텍스트 데이터에서 코사인 유사도는 문서 간의 내용적 유사성을 비교하는 데 유리하며, 추천 시스템, 정보 검색, 클러스터링 등에서 널리 사용된다.\n\n【예제】\n\n\n\n\n\n\n\n(2) 코사인 유사도의 특징\n코사인 유사도 값의 범위는 [-1, 1]이고 다음의 특징을 갖는다.\n\\(cos(\\theta) = 1\\): 두 벡터가 완전히 같은 방향\n\\(cos(\\theta) = 0\\): 두 벡터가 직교(Orthogonal, 90도)\n\\(cos(\\theta) = - 1\\): 두 벡터가 완전히 반대 방향.\n코사인 유사도는 벡터의 크기가 아닌 방향만 고려되므로 벡터를 정규화하지 않고도 비교할 수 있다. 고차원 벡터에도 적용 가능하여 텍스트 데이터, 사용자 선호도 등 고차원 데이터에서 벡터 간 유사성 측정에 많이 사용된다.\n각도 종류\n\n각도가 \\(\\theta = 90^{o} = \\pi/2\\)이면 두 벡터는 직교 orthogonal 한다.\n각도가 \\(\\theta = 0^{o}\\)이면 두 벡터는 정렬 aligned 되어 있다.\n각도가 \\(\\theta = 180^{o} = \\pi\\)이면 두 벡터는 역정렬 anti-aligned 되어 있다.\n각도가 \\(\\theta &gt; 90^{o} = \\pi/2\\)이면 두 벡터의 각은 둔각 obtuse, \\(\\theta &lt; 90^{o} = \\pi/2\\)이면 두 벡터의 각은 예각 acute 이다.\n\n\n\n\n\n\n두 벡터 합의 놈과 각도\n\\[\\parallel x + y \\parallel^{2} = \\parallel x \\parallel^{2} + 2 \\parallel x \\parallel \\parallel y \\parallel \\cos(\\theta) + \\parallel y \\parallel^{2}\\]\n만약 \\(\\theta = 90^{o} = \\pi/2\\)이면 \\(\\parallel x + y \\parallel^{2} = \\parallel x \\parallel^{2} + \\parallel y \\parallel^{2}\\) (피타고라스 정리)\n\n\n\n6. 상관계수\n\n(1) 상관계수 정의\n만약 \\(\\overset{˜}{a} = a - avg(a)1,\\overset{˜}{b} = b - avg(b)1\\)이면, 상관계수(correlation coefficient) \\(\\rho\\)는 다음과 같이 정의된다.\n\\(\\rho = \\frac{{\\overset{˜}{a}}^{T}\\overset{˜}{b}}{\\parallel \\overset{˜}{a} \\parallel \\parallel \\overset{˜}{b} \\parallel}\\) ⇔ \\(\\rho = (\\frac{\\overset{˜}{a}}{std(a)})^{T}(\\frac{\\overset{˜}{b}}{std(b)})/n\\)\n\n\\(cov(a,b) = {\\overset{˜}{a}}^{T}\\overset{˜}{b}/n\\): 두 벡터의 공분산\n\\(var(a) = std(a)^{2}\\): 벡터의 분산\n상관계수와 공분산 관계: \\(cov(a,b) = \\rho std(a)std(b)\\)\n\\(\\rho = \\pm 1\\) (완전 상관) : 두 벡터가 (역)정렬되어 있음\n\\(\\rho = 0\\) (독립) : 두 벡터가 직교되어 있음. \\(cov(a,b) = 0\\)\n\n\n\n(2) 두 벡터 합의 분산\n\\[var(a + b) = var(a) + 2cov(a,b) + var(b)\\]\n\\[var(a + b) = var(a) + 2\\rho std(a)std(b) + var(b)\\]\n\n만약 \\(\\rho = 0\\)이면, \\(var(a + b) = var(a) + var(b)\\)\n만약 \\(\\rho = 1\\)이면, \\(var(a + b) = (std(a) + std(b))^{2}\\)\n만약 \\(\\rho = - 1\\)이면, \\(var(a + b) = (std(a) - std(b))^{2}\\)\n\n\n\n(3) 헤징 hedging 투자\n두 개 회사 주가 벡터 \\((a,b)\\)의 평균은\\(\\mu\\), 표준편차(위험) \\(\\sigma\\)이고 상관계수는 \\(\\rho\\)이다. 각각 50% 투자, \\(c = \\frac{(a + b)}{2}\\)의 평균 수익율과 표준편차은 다음과 같다.\n\n평균 : \\(avg(\\frac{a + b}{2}) = \\mu\\)\n표준편차 : \\(std(c) = \\sigma\\sqrt{(1 + \\rho)/2}\\)\n상관계수 \\(\\rho = 0\\)이면 (독립) 표준편차는 \\(\\frac{1}{\\sqrt{2}}\\)만큼 줄어든다.\n완벽한 상관관계가 있는 경우에만 표준편차는 동일하다.\n\n\n\n\n\nchapter 5. 선형독립\n\n1. 선형독립 정의\n\n(1) 선형 종속 linear dependence\n\\(k \\geq 2\\)개의 크기 n-벡터 \\(x_{1},x_{2},...,x_{k}\\)가 다음을 만족하면 선형종속이라 한다. 만약 \\(a_{1}x_{1} + a_{2}x_{2} + ... + a_{k}x_{k} = 0\\)을 만족하는 \\(a_{i}\\)가 적어도 하나는 0이 아니다.\n선형독립이면 적어도 하나의 \\(a_{i}\\)는 0이 아니므로 벡터 \\(x_{i}\\) 다음과 같이 다른 벡터의 선형함수로 표현될 수 있다.\n\\[x_{k} = \\frac{- a_{1}}{a_{i}}x_{1} + ... + \\frac{- a_{i - 1}}{a_{i}}x_{i - 1} + \\frac{- a_{i + 1}}{a_{i}}x_{i + 1} + ... + \\frac{- a_{k}}{a_{i}}x_{k}\\]\n【예제】\n\n\\(x_{1} = \\left\\lbrack \\begin{array}{r} &gt; 0 \\\\ &gt;  - 1 \\\\  &gt; 1 &gt; \\end{array} \\right\\rbrack,x_{2} = \\left\\lbrack \\begin{array}{r} &gt; 1 \\\\ &gt;  - 2 \\\\ &gt; 1  &gt; \\end{array} \\right\\rbrack,x_{3} = \\left\\lbrack \\begin{array}{r} &gt; 1 \\\\ &gt; 0 \\\\ &gt;  - 1  &gt; \\end{array} \\right\\rbrack\\) ⬄\\(- 2x_{1} + x_{2} - x_{3} = 0\\)\n\n\n\n(2) 선형 독립 linear independence\n만약 \\(a_{1}x_{1} + a_{2}x_{2} + ... + a_{k}x_{k} = 0\\)이 모든 \\(a_{k} = 0\\)일 때만 만족한다면, n-벡터 \\(x_{1},x_{2},...,x_{k}\\)을 선형독립이라 한다.\n【예제】\n\n\n\n\n\n\n\n\\[x_{1} = \\left\\lbrack \\begin{array}{r}\n1 \\\\\n0 \\\\\n0\n\\end{array} \\right\\rbrack,x_{2} = \\left\\lbrack \\begin{array}{r}\n0 \\\\\n- 1 \\\\\n1\n\\end{array} \\right\\rbrack,x_{3} = \\left\\lbrack \\begin{array}{r}\n- 1 \\\\\n1 \\\\\n1\n\\end{array} \\right\\rbrack\\]\n\n\n\n\n\n\n\n\n(3) 선형독립 벡터의 선형결합\n선형독립인 \\(x_{1},x_{2},...,x_{k}\\)의 선형결합의 모든 계수(\\(a_{k}\\))는 유일하다. 선형결합 \\(x = a_{1}x_{1} + a_{2}x_{2} + ... + a_{k}x_{k}\\)\n【증명】 다른 계수를 \\(b_{k}\\)라 하자. \\(x = b_{1}x_{1} + b_{2}x_{2} + ... + b_{k}x_{k}\\) \\(0 = (a_{1} - b_{1})x_{1} + (a_{2} - b_{2})x_{2} + ... + (a_{k} - b_{k})x_{k}\\)이다. \\(x_{1},x_{2},...,x_{k}\\)가 선형독립이므로 모든 \\((a_{i} - b_{i}) = 0\\) 만족한다.\n\n\n\n2. 기저\n\n(1) 기저 개념\n벡터 공간은 다양한 차원의 벡터로 이루어진 공간이며, 그 공간 안의 벡터들을 다른 벡터들의 선형 조합으로 표현할 수 있다. 이때, 특정 벡터 공간의 기저 basis 는 그 공간 안의 모든 벡터들을 생성할 수 있는 최소한의 독립적인 벡터들의 집합이다.\n예를 들어, 2차원 공간에서의 기저는 일반적으로 (1,0)과 (0,1)이다. 이 두 벡터는 선형 독립이며, 이들의 모든 선형 조합으로 2차원 평면 상의 어떤 점이든 표현할 수 있다. 따라서 (1,0)과 (0,1)은 2차원 공간의 기저입니다. 단, 벡터 공간의 기저는 유일하지 않다.\n\n\n\n\n\n크기 2인 벡터의 기저 벡터는 \\(k = 2\\)개이다. 위의 그림에서 \\(a_{3}\\)벡터는 \\((a_{1},a_{2})\\)(기저 벡터)의 선형결합으로 만들 수 있다.\n\n\n(2) 기저 정의\nn개의 선형독립인 크기 n-벡터를 기저 basis 라 한다. 즉, n-벡터 \\((x_{1},x_{2},...,x_{n})\\)가 기저이면, 모든 크기 n-벡터는 \\((x_{1},x_{2},...,x_{n})\\)의 선형 결합으로 표현할 수 있다.\n【증명】 (n+1)개 차원 n-벡터 \\((x_{1},x_{2},...,x_{n},y)\\)개가 있다고 가정하자. 단,\\((x_{1},x_{2},...,x_{n})\\) 선형독립이며 기저이다. 이들 벡터는 선형독립(차원개수 n보다 벡터 개수가 (n+1)로 크다)이므로 다음을 만족하는 모든 \\(a_{i}\\)가 0은 아니다. \\(a_{1}x_{1} + a_{2}x_{2} + ... + a_{n}x_{n} + a_{n + 1}y = 0\\)\n만약 \\(a_{n + 1} = 0\\)이면, \\(a_{1}x_{1} + a_{2}x_{2} + ... + a_{n}x_{n} = 0\\)을 만족하는 모든 \\(a_{i} = 0\\)이다. 왜냐하면 \\((x_{1},x_{2},...,x_{n})\\) 선형독립이기 때문이다.(모순)\n\n\n\n3. 직교정규\n\n(1) 정의\n만약 \\(\\parallel x_{i} \\parallel = 1\\)이고 \\(x_{i}^{T}x_{j} = 0fori \\neq j\\) (두 벡터 \\((x_{i},x_{j})\\)는 직교)이면, \\((x_{1},x_{2},...,x_{k})\\) 벡터 집합은 직교 정규 orthonormal 벡터라고 한다.\n직교정규성은 선형종속, 선형독립처럼 집합의 속성이지 개별 벡터의 속성은 아니다.\n\n\n(2) 예제\n\nn개의 단위벡터는 직교정규 벡터이다.\n직교정규벡터 \\(\\left\\lbrack \\begin{array}{r}\n   - 1 \\\\0 \\\\ 0\n  \\end{array} \\right\\rbrack,\\frac{1}{\\sqrt{2}}\\left\\lbrack \\begin{array}{r}\n  0 \\\\ 1 \\\\ 1\n  \\end{array} \\right\\rbrack,\\frac{1}{\\sqrt{2}}\\left\\lbrack \\begin{array}{r}\n  0 \\\\ - 1 \\\\ 1\n  \\end{array} \\right\\rbrack\\)\n직교정규 벡터는 선형독립이다.\n\n\n\n(3) 직교정규 성질\n\n벡터 \\(x\\)가 직교정규벡터 선형결합이면 \\(x = a_{1}x_{1} + a_{2}x_{2} + ... + a_{k}x_{k}\\) 내적을 이용하여 다음을 얻으므로 내적을 이용하여 계수를 얻을 수 있다.\n\\[x_{i}^{T}x = x_{i}^{T}(a_{1}x_{1} + a_{2}x_{2} + ... + a_{k}x_{k}) = a_{i}\\]\n벡터 \\((x_{1},x_{2},...,x_{k})\\)가 직교정규 (선형독립이고 기저임) 벡터이면 \\(x = (x_{1}^{T}x)x_{1} + (x_{2}^{T}x)x_{2} + ... + (x_{k}^{T}x)x_{k}\\)이 성립한다.\n\n벡터 (1, 2, 3)을 직교정규 벡터의 선형결합으로 표현하자.\n\\[\\left\\lbrack \\begin{array}{r}\n1 \\\\\n2 \\\\\n3\n\\end{array} \\right\\rbrack = 1\\left\\lbrack \\begin{array}{r}\n1 \\\\\n0 \\\\\n0\n\\end{array} \\right\\rbrack + 2\\left\\lbrack \\begin{array}{r}\n0 \\\\\n1 \\\\\n0\n\\end{array} \\right\\rbrack + 3\\left\\lbrack \\begin{array}{r}\n0 \\\\\n0 \\\\\n1\n\\end{array} \\right\\rbrack\\]\n\\(\\lbrack - 1 0 0\\rbrack\\left\\lbrack \\begin{array}{r}\n  1 \\\\\n  2 \\\\\n  3\n  \\end{array} \\right\\rbrack = - 1\\), \\(\\frac{1}{\\sqrt{2}}\\lbrack 011\\rbrack\\left\\lbrack \\begin{array}{r}\n  1 \\\\\n  2 \\\\\n  3\n  \\end{array} \\right\\rbrack = \\frac{5}{\\sqrt{2}}\\), \\(\\frac{1}{\\sqrt{2}}\\lbrack 0 - 11\\rbrack\\left\\lbrack \\begin{array}{r}\n  1 \\\\\n  2 \\\\\n  3\n  \\end{array} \\right\\rbrack = \\frac{1}{\\sqrt{2}}\\)\n\\[\\left\\lbrack \\begin{array}{r}\n1 \\\\\n2 \\\\\n3\n\\end{array} \\right\\rbrack = - 1\\left\\lbrack \\begin{array}{r}\n- 1 \\\\\n0 \\\\\n0\n\\end{array} \\right\\rbrack + \\frac{5}{2}\\left\\lbrack \\begin{array}{r}\n0 \\\\\n1 \\\\\n1\n\\end{array} \\right\\rbrack + \\frac{1}{2}\\left\\lbrack \\begin{array}{r}\n0 \\\\\n- 1 \\\\\n1\n\\end{array} \\right\\rbrack\\]\n\n\n\n4. Gram-Schmidt 알고리즘\n\n(1) 개념\nn-벡터 \\(x_{1},x_{2},...,x_{k}\\)가 선형 독립인지 여부를 결정할 수 있는 알고리즘으로 수학자 Jørgen Pedersen Gram과 Erhard Schmidt의 이름을 따서 명명되었다.\n만약 벡터들이 선형 독립이라면, Gram–Schmidt 알고리즘은 다음과 같은 속성을 가진 직교정규 벡터 \\(q_{1},q_{2},...,q_{k}\\) 을 생성한다.\n\n각 \\(i = 1,2,...,k\\)에서 \\(x_{i}\\)는 \\(q_{1},q_{2},...,q_{i}\\)의 선형결합이다.\n각 \\(i = 1,2,...,k\\)에서 \\(q_{i}\\)는 \\(x_{1},x_{2},...,x_{i}\\)의 선형결합이다.\n만약 \\(x_{1},x_{2},...,x_{i - 1}\\) 선형독립이나 \\(x_{1},x_{2},...,x_{i}\\)는 선형종속이면 멈춘다.\n\n\n\n(2) 알고리즘\n주어진 n-벡터 \\(x_{1},x_{2},...,x_{k}\\), \\(i = 1,2,...,k\\)일 때\n\n직교화 : \\({\\overset{˜}{q}}_{i} = x_{i} - (q_{1}^{T}x_{i})q_{1} - ... - (q_{i - 1}^{T}x_{i})q_{i - 1}\\)\n선형종속 검증 : 만약 \\({\\overset{˜}{q}}_{i} = 0\\)이면, 멈춘다.\n정규화 : \\(q_{i} = \\frac{{\\overset{˜}{q}}_{i}}{\\parallel q_{i} \\parallel}\\).\n\n이렇게 얻은 \\(q_{1},q_{2},...,q_{i}\\)는 직교정규 벡터이다. 알고리즘 적용 중 중간에 중단되면 기저젝터가 아니다.\n\n\n(3) Gram-Schmidt 알고리즘 예제\n\\(x_{1} = ( - 1,1, - 1,1),x_{2} = ( - 1,3, - 1,3),x_{3} = (1,3,5,7)\\) 에 대하여 Gram–Schmidt 알고리즘을 적용하자.\ni=1\n\\(\\parallel {\\overset{˜}{q}}_{1} \\parallel = 2\\)이므로 \\(q_{1} = \\frac{{\\overset{˜}{q}}_{1}}{\\parallel {\\overset{˜}{q}}_{1} \\parallel} = \\left\\lbrack \\begin{array}{r}\n- 1/2 \\\\\n1/2 \\\\\n- 1/2 \\\\\n1/2\n\\end{array} \\right\\rbrack\\)이다.\ni=2\n\\(q_{1}^{T}x_{2} = 4\\)이므로 \\({\\overset{˜}{q}}_{2} = x_{2} - (q_{1}^{T}x_{2})q_{1} = \\left\\lbrack \\begin{array}{r}\n1 \\\\\n1 \\\\\n1 \\\\\n1\n\\end{array} \\right\\rbrack\\)이고 \\(\\parallel {\\overset{˜}{q}}_{2} \\parallel = 2\\)이다. 그러므로 \\(q_{2} = \\frac{{\\overset{˜}{q}}_{2}}{\\parallel {\\overset{˜}{q}}_{2} \\parallel} = \\left\\lbrack \\begin{array}{r}\n1/2 \\\\\n1/2 \\\\\n1/2 \\\\\n1/2\n\\end{array} \\right\\rbrack\\).\ni=3\n\\(q_{1}^{T}x_{3} = 2,q_{2}^{T}x_{3} = 8\\)이므로 \\({\\overset{˜}{q}}_{3} = x_{3} - (q_{1}^{T}x_{3})q_{1} - (q_{2}^{T}x_{3})q_{2} = \\left\\lbrack \\begin{array}{r}\n- 2 \\\\\n- 2 \\\\\n2 \\\\\n2\n\\end{array} \\right\\rbrack\\)이고 \\(\\parallel {\\overset{˜}{q}}_{3} \\parallel = 4\\)이다. 그러므로 \\(q_{3} = \\frac{{\\overset{˜}{q}}_{3}}{\\parallel {\\overset{˜}{q}}_{3} \\parallel} = \\left\\lbrack \\begin{array}{r}\n- 1/2 \\\\\n- 1/2 \\\\\n1/2 \\\\\n1/2\n\\end{array} \\right\\rbrack\\).\n# Gram-Schmidt 알고리즘\nimport numpy as np\n\ndef gram_schmidt(A):\n    # Get the number of rows (n) and columns (k) in A\n    n, k = A.shape\n    # Initialize matrix Q with zeros, same shape as A\n    Q = np.zeros((n, k))\n    \n    for j in range(k):\n        # Start with the current column vector of A\n        v = A[:, j]\n        for i in range(j):\n            # Subtract the projection of v onto the ith orthonormal vector\n            v -= np.dot(Q[:, i], A[:, j]) * Q[:, i]\n        \n        # Normalize the vector\n        Q[:, j] = v / np.linalg.norm(v)\n    return Q\n# Example usage\nA = np.array([[-1,-1,1],\n              [1,3,3],\n              [-1,-1,5],\n              [1,3,7]], dtype=float)\n\ngram_schmidt(A)\n【결과】 array([[-0.5, 0.5, -0.5], [ 0.5, 0.5, -0.5], [-0.5, 0.5, 0.5], [ 0.5, 0.5, 0.5]])"
  },
  {
    "objectID": "notes/math/matrix.html",
    "href": "notes/math/matrix.html",
    "title": "수학의 기초 4. 행렬",
    "section": "",
    "text": "chapter 1. 행렬 기초\n\n1. 개념\n\n(1) 통계학과 행렬\n행렬은 통계학에서 데이터를 표현하고 분석하는 데 핵심적인 도구로 사용된다. 행렬은 대규모 데이터의 구조를 간단히 표현하고, 계산을 효율적으로 수행하여 통계학에서 중요한 역할을 한다.\n데이터 표현: 데이터를 행렬로 저장하여 표 형식으로 표현한다. 다음은 관측값(행)과 변수(열)로 구성된 데이터 행렬이다.\n\\[X = \\begin{bmatrix}\nx_{11} & x_{12} & \\cdots & x_{1p} \\\\\nx_{21} & x_{22} & \\cdots & x_{2p} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nx_{n1} & x_{n2} & \\cdots & x_{np}\n\\end{bmatrix}\\]\n연산의 간결화: 여러 변수와 관측값 간의 관계를 분석할 때 행렬식으로 간단히 표현하고 행렬 연산을 이용하여 추정값을 계산한다.\n\\(Y = X\\beta + \\epsilon\\), OLS 추정=\\(\\widehat{\\beta} = (X'X)^{- 1}X'Y\\)\n\n\n(2) 정의\n행과 열로 배열된 숫자, 기호 또는 표현식의 직사각형 배열을 행렬이라 한다. 행의 차수는 \\(m\\), 열의 차수는 \\(n\\)이다.\n\\(A_{m \\times n} = \\begin{bmatrix}\na_{11} & a_{12} & \\cdots & a_{1n} \\\\\na_{21} & a_{22} & \\cdots & a_{2n} \\\\\n\\cdots & \\cdots & \\cdots & \\cdots \\\\\na_{m1} & a_{m2} & \\cdots & a_{mn}\n\\end{bmatrix}\\) (간편식) \\(A = \\{ a_{ij}\\}\\)\n\n행렬의 각 셀을 원소 element라 한다.\n행의 차수 \\(m = 1\\)인 행렬을 열 column 벡터이다.\n열의 차수 \\(n = 1\\)인 행렬을 행 row 벡터이다.\n행의 차수, 열의 차수 모두 1인 행렬을 스칼라 scalar이다.\n행렬을 \\(n\\)-열벡터로 표현 : \\(A_{m \\times n} = \\begin{bmatrix}\n  a_{1} & a_{2} & \\cdots a_{n}\n  \\end{bmatrix}\\)\n행렬을 \\(m\\)-헹벡터로 표현 : \\(A_{m \\times n} = \\left\\lbrack \\begin{array}{r}\n  a_{1} \\\\\n  a_{2} \\\\\n  \\cdots \\\\\n  a_{m}\n  \\end{array} \\right\\rbrack\\)\n\n\n\n(3) 동일 행렬이란\n\n행의 차수와 열의 차수가 같다. \\(A_{m \\times n} = B_{m \\times n}\\)\n대응하는 모든 원소 값은 동일하다. \\(\\{ a_{ij} = b_{ij}\\} foralli,j\\)\n\n\n\n\n2. 특수한 행렬\n영행렬 zero matrix: 행렬의 모든 원소가 0인 행렬입니다. 기호 : \\(0_{m \\times n}or0\\) 숫자 0에 해당된다.\n정방행렬 square matrix: 행렬의 행차수와 열차수가 동일한 행렬이다. 기호 : \\(A_{m \\times m} = A_{m}\\)\n대각행렬 diagonal matrix: 대각원소를 제외한 모든 원소가 0인 정방행렬이다. 기호 : \\(A_{ij} = 0fori \\neq j\\), \\(diag(a_{11},a_{22},...,a_{mm})\\)\n\\[D = \\begin{pmatrix}\n- 1 & 0 \\\\\n0 & 7\n\\end{pmatrix}\\]\n대각합 trace: 대각행렬의 대각원소의 합을 대각합이라 한다. \\(tr(D) = 6\\)\n단위행렬 identity matrix: 정방행렬의 대각 원소가 모두 1이고 그외 원소는 0인 행렬로 숫자 1과 같은 역할을 한다. 기호 : \\(I_{ij} = \\{\\begin{array}{r}\n1i = j \\\\\n0i \\neq j\n\\end{array}\\) , \\(I_{m \\times m}orI_{m}\\)\n\\(A = \\begin{bmatrix}\n1 & 2 & 3 \\\\\n3 & 4 & 5\n\\end{bmatrix}\\)⇨ \\(A = \\begin{bmatrix}\n1 & 0 & 1 & 2 & 3 \\\\\n0 & 1 & 3 & 4 & 5 \\\\\n0 & 0 & 1 & 0 & 0 \\\\\n0 & 0 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 0 & 1\n\\end{bmatrix} = \\begin{bmatrix}\nI & A \\\\\n0 & I\n\\end{bmatrix}\\)\n삼각행렬 triangular matrix\n\n【상삼각행렬】 대각원소 아래 원소가 모두 0인 정방행렬이다. 기호 : \\(A_{ij} = 0fori &gt; j\\)\n【하삼각행렬】 대각원소 윗 원소가 모두 0인 정방행렬이다. 기호 : \\(A_{ij} = 0fori &lt; j\\)\n\n희소행렬 Sparse matrices: 행렬 원소의 대부분이 0인 행렬을 의미하며 \\(nnz(A)\\)은 행렬 \\(A_{m \\times n}\\)에서 0인 아닌 원소의 개수를 나타내며 \\(nnz(A)/(m \\times n)\\) 을 행렬의 밀도라 정의한다.\n수학자 제임스 H. 윌킨슨(James H. Wilkinson)이 정의 : ”행렬이 충분히 많은 0 원소를 포함하고 있어 이를 활용하는 것이 유리한 경우, 그 행렬을 희소 행렬이라 한다.” 희소행렬은 컴퓨터에서 효율적으로 저장하고 조작할 수 있다.\n영행렬 &gt; 단위행렬 &gt; 대각행렬 &gt; 삼각행렬 : 대표적인 희소행렬\n\n\n3. 행렬 놈\n모든 원소의 제곱합의 양의 제곱근: \\(\\parallel A \\parallel = \\sqrt{\\overset{m}{\\sum_{i}}\\overset{n}{\\sum_{j}}a_{ij}}\\)\n행렬의 놈은 스칼라이며 행렬의 크기나 거리를 측정하며 행렬의 평균제곱근(Root Means Square)는 \\(RMS(A) = \\frac{\\parallel A \\parallel}{\\sqrt{mn}}\\)이다.\n\n\\(\\parallel A \\parallel \\geq 0\\) 행렬 놈은 0보다 크거나 같다.\n\\(\\parallel cA \\parallel = |c| \\parallel A \\parallel\\)\n\\(\\parallel A + B \\parallel \\leq \\parallel A \\parallel + \\parallel B \\parallel\\)\n\\(\\parallel A - B \\parallel\\) : 두 행렬의 유사성(거리)을 나타낸다.\n\\(\\parallel A \\parallel = \\parallel A^{T} \\parallel\\) : 원행렬 놈과 전치행렬 놈은 동일하다.\n\n\n\n4. 전치\n전치 transpose는 행과 열을 서로 바꾸는 연산: \\((A^{T})_{ij} = A_{ji}\\)\n\n\\((A^{T})^{T} = A\\) : 전치 행렬을 다시 전치하면 원래 행렬이 된다.\n\\((A + B)^{T} = A^{T} + B^{T}\\) : 행렬 합의 전치는 각 행렬의 전치 합과 같다.\n\\((cA)^{T} = cA^{T}\\) : 스칼라 곱의 전치는 스칼라 곱과 같다.\n\\((AB)^{T} = B^{T}A^{T}\\) : 행렬 곱의 전치는 각 행렬의 전치의 순서를 바꾼 곱과 같다.\n\n원행렬과 전치행렬과 동일한 행렬은 대칭행렬이다. \\(A = A^{T}\\)\n\n\n\nchapter 2. 행렬 연산\n\n1. 행렬 합 연산\n행렬의 합을 구하는 경우 두 행렬의 차수는 동일해야 하며(conformable for addition/substraction: 합 연산 적합) 각 행렬에서 대응하는 원소들의 합을 그 위치에 적으면 된다.\n\\[(A + B)_{m \\times n} = \\{ a_{ij} + b_{ij}\\}\\]\n\\[(A + B)_{m \\times n} = \\begin{bmatrix}\na_{11} + b_{11} & a_{12} + b_{12} & \\cdots & a_{1n} + b_{1n} \\\\\na_{21} + b_{21} & a_{22} + b_{22} & \\cdots & a_{2n} + b_{2n} \\\\\n\\cdots & \\cdots & \\cdots & \\cdots \\\\\na_{m1} + b_{m1} & a_{m2} + b_{m2} & \\cdots & a_{mn} + b_{mn}\n\\end{bmatrix}\\]\n\\[A = \\begin{bmatrix}\n1 & 3 & 5 \\\\\n7 & 3 & 1\n\\end{bmatrix}$, $B = \\begin{bmatrix}\n1 & 0 & 1 \\\\\n- 1 & 1 & 0\n\\end{bmatrix}$ ⇢ $A + B = \\begin{bmatrix}\n2 & 3 & 6 \\\\\n6 & 4 & 1\n\\end{bmatrix}\\]\n성질\n\n교환법칙 Commutativity : \\(A + B = B + A\\)\n결합법칙 Associativity : \\(A + (B + C) = (A + B) + C = A + B + C\\)\n영행렬과 합 : \\(A + 0 = 0 + A = A\\)\n합의 전치 : \\((A + B)^{T} = A^{T} + B^{T}\\)\n\n\n\n2. 스칼라-행렬 곱하기\n행렬 모든 원소에 스칼라 곱을 하여 결과는 원행렬과 동일한 차수의 행렬이다. (기호) \\(cA = \\{ ca_{ij}\\} = Ac\\) 다음의 성질을 갖는다.\n\n\\((cA)^{T} = cA^{T}\\)\n\\((c + d)A = cA + dA\\)\n\n\n\n3. 행렬x벡터 곱하기\n행렬 \\(A_{m \\times n}\\)와 행벡터 \\(x_{n}\\) 곱 연산은 다음과 같이 정의되며 결과는 행벡터 \\(y_{m \\times 1} = A_{m \\times n}x_{n \\times 1}\\)이며 차수는 \\(m\\)이다.\n\n\n\n\n\n연산 가능: 앞의 행렬(\\(A_{m \\times n}\\))의 열차수와 뒤의 행벡터(\\(x_{n}\\)) 행차수가 동일해야 한다.\n행 측면: 행렬 \\(A\\)의 \\(i\\)-번째 행벡터을 \\(a_{i}^{T}\\)라 하면 \\(y_{i} = a_{i}^{T}x\\)(내적)이다.\n열 측면: \\(A\\)의 \\(k\\)-번째 열벡터을 \\(a_{k}\\)라 하면 \\(y = x_{1}a_{1} + x_{2}a_{2} + + ... + x_{n}a_{n}\\).\n\n\n\n\n\n행렬 \\(A\\)의 열벡터 선형독립이다\n만약 \\(x = 0\\)인 경우에만 \\(Ax = 0\\)이 성립하면, 열벡터는 선형독립이다.\n활용\n\n행렬 \\(A\\)가 영행렬이면 \\(Ax = 0\\)는 영벡터이다.\n행렬 \\(A\\)가 단위행렬이면 \\(Ax = x\\)이다.\n행렬 \\(A\\)의 \\(j\\)-번째 열벡터는 \\(Ae_{j} = a_{j}\\)이다.\n행렬 \\(A\\)의 \\(i\\)-번째 행벡터는 \\((A^{T}e_{i})^{T}\\)이다.\n\n예제\n(예측데이터 행렬) Feature matrix \\(X_{N \\times n}\\)는 \\(N\\)개의 객체에 대한 특성 \\(n\\)-벡터, 객체들에 대한 가중치 \\(w\\)-벡터(차수 \\(N\\))라 하자. \\(X^{T}w\\)는 객체들에 대한 가중 점수 벡터이다.\n(포트폴리오 자산 수익율) 포트폴리오 자산 수익율 행렬 \\(R_{T \\times n}\\)(\\(T\\) 기간 동안 \\(n\\)개의 자산의 수익률)이라 하고 \\(w\\)을 포트폴리오 \\(n\\)-벡터라 하면 \\(Rw\\)는 \\(T\\)기간 포트폴리오 수익률이다.\n(오디오 믹싱) \\(A\\)의 \\(k\\)개 열이 길이 \\(T\\)의 오디오 신호나 트랙을 나타내는 벡터들이고, \\(w\\)가 \\(k\\)-벡터인 경우를 가정하면 \\(Aw\\)는 오디오 신호들을 믹싱한 결과를 나타내는 \\(T\\)-벡터이다.\n(문서 점수화) 검색 엔진은 검색 쿼리를 기반으로 w를 선택하여 문서의 점수를 예측한다. \\(A\\)는 \\(N \\times n\\)크기의 문서-단어 행렬로, \\(N\\)개의 문서가 \\(n\\)개의 단어 사전을 사용하여 단어의 출현 빈도, \\(w\\)는 \\(n\\)-벡터로, 단어 사전 내 단어들에 대한 가중치로 \\(Aw\\)는 \\(N\\)-벡터로, 각 문서의 점수를 나타낸다.\n\n\n4. 행렬x행렬 곱하기\n\n(1) 정의\n행렬을 곱하기 위해서는 앞 행렬의 열 차수와 뒤 행렬의 행의 차수와 일치해야 곱이 가능하다. conformable for product 결과의 차수는 앞 행렬의 행 차수, 뒤 행렬의 열 차수를 갖는다.\n\\(A_{m \\times n}B_{n \\times p} = (AB)_{m \\times p}\\)\n\\(A = \\{ a_{ij}\\}\\), \\(B = \\{ b_{ij}\\}\\) ⇢ \\(AB = \\{\\overset{n}{\\sum_{k = 1}}a_{ik}b_{kj}\\}\\)\n\n\n\n\n\n\n\n(2) 곱의 성질\n\n결합 associate 법칙: \\((AB)C = A(BC)\\)\n배분 distribution 법칙: \\(A(B + C) = AB + AC\\)\n전치 : \\((AB)^{T} = B^{T}A^{T}\\)\n\\((A + B)(C + D) = AC + AD + BC + BD\\)\n\\(y^{T}(Ax) = (y^{T}A)x = (A^{T}y)^{T}x\\)\n\n\n\n(3) 행렬의 거듭제곱\n\\[A^{2} = AA$, $A^{3} = AAA$, $A^{4} = AAAA \\cdots \\]\ndirected graph: 인접 adjacency 행렬을 다음과 같이 정의하자.\n\\[A_{ij} = \\{\\begin{array}{r}\n\\text{1 there is a edge from vertex j to vertex i} \\\\\n\\text{0 otherwise}\n\\end{array}\\]\n\n\n\n\n\n\n\n멱등행렬 idempotent\n자신의 행렬 곱이 자신이 되는 행렬을 멱등행렬이라 한다. \\(M^{2} = M^{3} = ... = M\\) 자신의 곱이 연산 가능해야 하므로 멱등행렬이려면 정방행렬이어야 한다.\n\n\n\n5. QR 분해, Q는 직교행렬, R은 상삼각행렬\n\n(1) 직교행렬 orthonormal matrix\n열벡터 \\(A_{m \\times n}\\)의 n-벡터 \\(a_{1},a_{2},...,a_{m}\\)들이 orthonomal 하면, 즉 \\(A^{T}A = I\\)을 만족하는 행렬을 직교정규행렬이라 한다.만약 \\(A_{m \\times n}\\)는 직교정규행렬, \\(x,y\\)는 n-벡터라 하고 \\(f:R^{n} \\rightarrow R^{m}\\) 함수가 \\(z\\)를 \\(Az\\)로 매핑한다고 가정하자.\n\n\\(\\parallel Ax \\parallel = \\parallel x \\parallel\\) : 함수 \\(f\\)는 놈을 보존한다.\n\\((Ax)^{T}(Ay) = x^{T}y\\) : 함수 \\(f\\)는 두 벡터의 내적을 보존한다.\n\\(\\angle(Ax,Ay) = \\angle(x,y)\\) : 함수 \\(f\\)는 두 벡터의 각도을 보존한다.\n\n【recall】 Gram-Schmidt 알고리즘\n만약 벡터들이 선형 독립이라면, Gram–Schmidt 알고리즘은 다음과 같은 속성을 가진 직교정규 벡터 \\(q_{1},q_{2},...,q_{k}\\) 을 생성한다.\n\n\n(2) QR분해 \\(A = QR\\)\n행렬 \\(A_{n \\times k}\\)의 n-벡터 \\(a_{1},a_{2},...,a_{k}\\)가 선형 독립인 행렬이다. 여기에 Gram-Schmidt 알고리즘을 적용하여 얻은 직교정규 벡터 \\(q_{1},q_{2},...,q_{k}\\)으로 직교정규 행렬 \\(Q\\)을 생성하자. \\(Q^{T}Q = I\\)이다.\n\\(a_{i}\\)와 \\(q_{i}\\)의 관계식 : \\(a_{i} = (q_{1}^{T}a_{i})q_{1} + \\cdots + (q_{i - 1}^{T}a_{i})q_{i - 1} + \\parallel {\\overset{˜}{q}}_{i} \\parallel q_{i}\\)\n이를 다시 쓰면 \\(a_{i} = R_{1i} + \\cdots + R_{ii}q_{1}\\)이다. \\(R_{ij} = q_{i}^{T}a_{j}fori &lt; j\\), \\(R_{ij} = 0fori &gt; j\\), 그리고\\(R_{ii} = \\parallel {\\overset{˜}{q}}_{i} \\parallel\\)\n그러므로 \\(A_{n \\times k}\\) (열이 독립인 행렬)은 직교정규 행렬 \\(Q_{n \\times k}\\)과 \\(R_{k \\times k}\\) 상삼각행렬로 분해된다.\n\n\n(3) QR 분해 활용\n선형 시스템의 해 구하기, 최소자승 문제, 정규방정식 문제\n선형 방정식 \\(Ax = b\\)를 푸는 데 사용될 수 있다. \\(A = QR\\)로 분해하면 \\(QRx = b\\)가 되고 \\(R_{x} = Q^{T}b\\)이므로 \\(R\\)이 상삼각 행렬이므로 후진 대입을 사용하여 해, \\(x\\)를 효율적으로 구할 수 있다.\n고유값 계산\n\\(QR\\) 알고리즘을 이용하여 특정 행렬의 고유값을 계산할 수 있다. \\(QR\\) 분해를 사용한 고유값 계산 알고리즘은 변환 행렬을 상삼각 행렬로 변환하고, 이로부터 고유값을 추출한다.\n행렬의 특성 분석\n\\(QR\\) 분해는 행렬의 특성을 분석하는 데 도움을 준다. 예를 들어, 행렬의 계수(rank)를 결정하거나, 행렬이 정칙인지 (역행렬이 존재하는지) 파악하는데 사용될 수 있다.\nimport numpy as np\n# 행렬 A 정의\nA = np.array([[1, 1], [1, -1], [1, 1]])\n# QR 분해\nQ, R = np.linalg.qr(A)\n# 결과 출력\nprint(\"Q:\")\nprint(Q)\nprint(\"\\nR:\")\nprint(R)\n【결과】 Q: [[-0.57735027 0.40824829] [-0.57735027 -0.81649658] [-0.57735027 0.40824829]]\nR: [[-1.73205081 -0.57735027] [ 0. 1.63299316]]\n\n\n\n6. 역행렬\n\n(1) 왼쪽 오른쪽 역행렬\n만약 \\(XA = I\\) 만족하는 \\(X\\)가 존재하면 A는 left-invertible 이라 한다. 동일하게 \\(AX = I\\) 만족하는 \\(X\\)가 존재하면 A는 right-invertible 이라 한다.\nleft-invertible과 열 벡터는 선형독립: 만약 행렬 \\(A\\)가 left-inverse 행렬 \\(C\\) 갖는다면 행렬 \\(A\\)의 열벡터는 선형 독립이다.\n【증명】 \\(Ax = 0\\)을 만족하는 \\(x = 0\\)이므로 \\(A\\)의 열벡터는 선형 독립이다. \\(0 = CAx = Ix = x\\)\nleft-invertible 행렬(\\(C\\)) 갖는 \\(A\\) 선형방정식 \\(Ax = b\\) 해 구하기\n\\[C_{m \\times m}A_{m \\times n}x_{n} = C_{n \\times n}b_{n} \\rightarrow x_{n} = C_{n \\times n}b_{n}\\]\nright-invertible과 행 벡터는 선형독립: 만약 행렬 \\(A\\)가 right-inverse 행렬 \\(B\\) 갖는다면 행렬 \\(A\\)의 행벡터는 선형 독립이다.\nleft, right invertible 관계: 행렬 \\(A\\)의 right inverse \\(B\\)을 가지면 \\(B^{T}\\)는 \\(A^{T}\\)의 left inverse 행렬이다.\n【증명】 \\(AB = I \\rightarrow (AB)^{T} = I^{T} \\rightarrow B^{T}A^{T} = I\\)\n\n\nright-invertible 행렬(\\(B\\)) 갖는 \\(A\\) 선형방정식 \\(Ax = b\\) 해 구하기\n해는 \\(x = Bb\\)이다. 【증명】 \\(Ax = A(Bb) = (AB)b = b\\)\n\n\n(2) 역행렬 구하기\n행렬의 역수 개념이다. 3에 어떤 수를 곱하면 1이 될까? 답은 \\(\\frac{1}{3}\\)(역수)이다. 마찬가지로 행렬 \\(A\\)에 무엇을 곱하면 항등행렬 \\(I\\)가 될까? 이를 역행렬이라 한다. \\(AA^{- 1} = A^{- 1}A = I\\)\n행렬식 determinant: 행렬식은 정방행렬에서만 계산되며 결과는 스칼라이다. 기호는 \\(det(A)\\)혹은 \\(|A|\\)으로 표현한다. 다음은 행렬식 계산 방법이다.\n\\(A_{2 \\times 2} = \\begin{bmatrix}\na & b \\\\\nc & d\n\\end{bmatrix}\\) ⇢ \\(det(A) = ad - bc\\) \\(A = \\begin{bmatrix}\n1 & 3 \\\\\n2 & 4\n\\end{bmatrix}\\), \\(|A| = - 2\\)\n\n\n\n\n\n행렬식 성질\n\n\\(|A^{T}| = |A|\\)\n\\(|AB| = |BA|\\)\n\\(|AB| = |A||B|\\)\n한 열에 \\(k\\)배 한 후 다른 열에 더하여도 행렬식은 변하지 않는다.\n한 열이 다른 열의 선형결합으로 표현된다면 행렬식은 0이다.\n\n소행렬 minor: \\(i\\)행, \\(j\\)열은 제외한 행렬을 소행렬(\\(M_{ij}\\))이라 하고 소행렬의 행렬식을 소행렬식(\\(|M_{ij}|\\))이라 한다. 일반적으로 소행렬은 소행렬식을 의미한다.\n\n\n\n\n\n여인수 cofactor\n\n\\(C_{ij} = ( - 1)^{i + j}|M_{ij}|\\)을 여인수라 한다. 여인수를 이용하여 다음과 같이 행렬식을 구할 수 있다.\n\\(|A_{n \\times n}| = \\overset{n}{\\sum_{i = 1}}a_{ij}( - 1)^{i + j}|M_{ij}|\\),\\(|A_{n \\times n}| = \\overset{n}{\\sum_{j = 1}}a_{ij}( - 1)^{i + j}|M_{ij}|\\)\n\n여인수 행렬 / 수반행렬 adjoint\n\\(C_{ij} = \\begin{bmatrix}\nC_{11} & C_{12} & C_{13} \\\\\nC_{21} & C_{22} & C_{23} \\\\\nC_{31} & C_{32} & C_{33}\n\\end{bmatrix}\\)⇢ \\(adj(A) = \\begin{bmatrix}\nC_{11} & C_{21} & C_{31} \\\\\nC_{12} & C_{22} & C_{32} \\\\\nC_{13} & C_{23} & C_{33}\n\\end{bmatrix}\\)\n역행렬 구하기: 정방행렬 \\(A\\)에 대하여 \\(AB = BA = I\\)을 만족하는 행렬 \\(B\\)를 \\(A\\)의 역행렬이라 하며 \\(A^{- 1}\\)로 표현한다.\n\\[A^{- 1} = \\frac{1}{|A|}adj(A)\\]\n역행렬 성질\n\n역행렬은 유일하고 \\((A^{- 1})^{- 1} = A\\)이 성립한다.\n\\((AB)^{- 1} = B^{- 1}A^{- 1}\\)\n\\((A^{T})^{- 1} = (A^{- 1})^{T}\\)\n\\(|A^{- 1}| = \\frac{1}{|A|}\\)\n\n계수 rank: 차수가 \\(n\\)인 정방행렬 \\(A_{n \\times n}\\)의 열벡터에 대하여 \\(k_{1}\\underset{¯}{a_{1}} + k_{2}\\underset{¯}{a_{2}} + ... + k_{n}\\underset{¯}{a_{n}} = \\underset{¯}{0}\\) 방정식이 모든 상수 \\(k_{j}\\)가 0일 때만 만족하는 경우 열벡터(\\(\\underset{¯}{a_{j}}\\))는 선형독립 linearly independent이라 한다. 만약 적어도 0이 아닌 상수가 하나라도 존재하면 종속이라 한다.\n정방행렬 \\(A_{n \\times n}\\)에 대하여 선형 독립인 행의 개수와 열의 개수 중 작은 것을 행렬의 계수라 한다. 행렬의 차수와 계수가 동일하면 이를 full-rank라 한다.\n행렬 \\(A_{n \\times n}\\)에 대하여 각 열은 동일하다.\n\n\n\n\n\n\n\n역행렬 \\(A^{- 1}\\)은 존재한다.\n역행렬 \\(A^{- 1}\\)은 존재하지 않는다.\n\n\n\n\n행렬식은 0이 아니다. \\(det(A) \\neq 0\\)\n행렬식은 0이다. \\(det(A) = 0\\)\n\n\nfull rank이다. \\(rank(A) = n\\)\nfull rank 아니다. \\(rank(A) &lt; n\\)\n\n\n행렬 A는 non-singular이다.\n행렬 A는 singular이다.\n\n\n\\(AX = \\underset{¯}{b}\\) 해가 존재한다.\n\\(AX = \\underset{¯}{b}\\) 해가 존재하지 않는다.\n\n\n\n\n\n\n\nchapter 3. 행렬 활용\n\n1. 연립방정식 해 구하기 \\(Ax = b\\)\n\n(1) \\(QR\\) 분해 이용\n\n행렬 \\(A\\)을 \\(QR\\)분해 한다. \\(A = QR\\)\n\\(Q^{T}b\\)을 구한다.\n후진 제거 방법으로 \\(Rx = Q^{T}b\\)을 구한다.\n\n\n\n(2) 역행렬 계산 \\(A^{- 1}\\)\n행렬 \\(A\\)의 역행렬 \\(A^{- 1}\\)을 이용하여 \\(\\widehat{x} = A^{- 1}b\\) 해를 구한다.\n\n\n\n2. 최소자승법 \\(Ax = b\\)\n\n(1) 최소자승 문제\n\\(A_{m \\times n}x_{n} = b_{m}\\)(단 \\(m &gt; n\\)) 선형방정식에서는 \\(m\\)개의 방정식이 \\(n\\)개 변수보다 많으므로 \\(b\\)가 행렬 \\(A\\)의 열의 선형결합일 때만 해를 갖는다. \\(b\\)을 어떻게 구할 것인가? 잔차 \\(r = Ax - b\\)최소화 하는 \\(x\\)을 찾는 것을 최소자승법이라 한다. \\(minmize \\parallel Ax - b \\parallel\\) \\(2x_{1} = 1, - x_{1} + x_{2} = 0,2x_{2} = - 1\\) : 방정식 3개, 미지수 2개\n\\(Ax = b\\): \\(\\begin{bmatrix}\n2 & 0 \\\\\n- 1 & 1 \\\\\n0 & 2\n\\end{bmatrix}\\left\\lbrack \\begin{array}{r}\nx_{1} \\\\\nx_{2}\n\\end{array} \\right\\rbrack = \\begin{bmatrix}\n1 & 0 & 1\n\\end{bmatrix}\\)\n\n\n(2) 최소자승 해 구하기\n\\(minmizef(x) = \\parallel Ax - b \\parallel^{2}\\) 해 \\(\\widehat{x}\\)는 \\(\\frac{\\partial f}{\\partial x_{i}}(\\widehat{x}) = 0,i = 1,2,...,n\\)을 만족하므로 \\(\\nabla f(x) = 2A^{T}(Ax - b)\\) 방정식에서 \\(\\nabla f(\\widehat{x}) = 0\\)이다. 그러므로 최소자승 해는 \\(\\widehat{x} = (A^{T}A)^{- 1}A^{T}b\\)이다.\n\n\n\n\n\n\\(A = QR\\) 분해 이용\n\\(Ax = b\\)의 최소자승 해는 \\(\\widehat{x} = R^{- 1}Q^{T}b\\)이다.\n\\[RMS = \\sqrt{\\parallel b - A\\widehat{x} \\parallel^{2}}\\]\n매출 광고\n행은 사회인구학적 특성 10개이고 열은 3개 광고 채널이고 \\(R_{ij}\\)는 \\(i\\)-사회인구학적특성의 \\(j\\)-광고채널의 1달러당 노출회수(단위: 1000)이다. 만약 각 사회인구학적 특성 집단별로 노출회수를 \\(10^{3}\\)으로 할 경우 광고비는 얼마?\n\n\n\n\n\n\\(R_{10 \\times 3}x_{3} = 10^{3}1_{3}\\)에 대한 최소자승해는 \\(\\widehat{x} = (62,100,1443)\\)으로 각 채널당 광고비이다. \\(RMS = 13.2\\%\\)이다.\n\n\n(3) 최소자승 데이터 적합\n\\(n\\)-벡터 \\(x\\)(feature 벡터, 독립변수), 스칼라 \\(y\\)는 다음 근사 함수 관계가 있다고 하자. \\(f:R^{n} \\rightarrow R,y \\approx f(x)\\)\n데이터\n\\[x^{(1)},x^{(2)},...,x^{(N)},y^{(1)},y^{(2)},...,y^{(N)}\\]\n모델 관측치 개수 \\(N\\), 예측변수 개수 \\(p\\)\nfeature 벡터와 스칼라 벡터 사이 함수 관계는 \\(f\\)(예측함수)은\\(y \\approx \\widehat{f}(x),where\\widehat{f}:R^{n} \\rightarrow R\\)\n\\(\\widehat{f}(x)\\)는 파라미터 \\(p\\)-벡터 \\(\\theta\\)의 선형 함수이다.\n\\(\\widehat{f}(x) = \\theta_{1}f_{1}(x) + \\theta_{2}f_{2}(x) + \\cdots + \\theta_{p}f_{p}(x)\\), where \\(f_{i}:R^{n} \\rightarrow R\\)\n예측값과 예측오차\n\\(y^{(i)} \\approx \\widehat{f}(x^{(i)})\\)이고 예측오차(잔차)는 \\(r^{(i)} = y^{(i)} - {\\widehat{y}}^{(i)}\\)이다.\n최소자승 모델 적합\n\\(i = 1,2,\\cdots,N,j = 1,2,\\cdots,p\\)\n\\(y^{d} = (y^{(1)},y^{(2)},...,y^{(N)})\\), \\({\\widehat{y}}^{d} = ({\\widehat{y}}^{(1)},{\\widehat{y}}^{(2)},...,{\\widehat{y}}^{(N)})\\)\n예측오차합 \\(\\parallel r^{d} = y^{d} - {\\widehat{y}}^{d} \\parallel^{2}\\)을 최소화 하는 모수 \\(\\theta\\)을 찾는다.\n\\[{\\widehat{y}}^{(i)} = A_{i1}\\theta_{1} + A_{i1}\\theta_{2} + \\cdots + A_{i1}\\theta_{p},whereA_{ij} = {\\widehat{f}}_{j}(x^{(i)})\\]\n\\({\\widehat{y}}^{d} = A\\theta\\)이므로 \\(\\parallel r^{d} \\parallel^{2} = \\parallel y^{d} - A\\theta \\parallel^{2}\\)이다.\n최소자승 추정 : \\(\\widehat{\\theta} = (A^{T}A)^{- 1}A^{T}y^{d}\\)\n상수항(절편) 있는 선형함수 최소자승 추정\n모든 \\(x\\)에 대하여 \\(f_{1}(x) = 1\\)을 갖는 상수함수를 고려하자. \\(\\widehat{f}(x) = \\theta_{1}\\)이고 \\(A_{(N \\times 1)} = 1_{N}\\)이다.\n\\[\\widehat{\\theta} = (A^{T}A)^{- 1}A^{T}y^{d} = N^{- 1}1^{T}y^{d} = avg(y^{d})\\]\n\n\n(4) 다항식 적합\n모형 \\(\\widehat{f}(x) = \\theta_{1} + \\theta_{2}x + \\cdots + \\theta_{p}x^{p - 1}\\)\n\\[A = \\begin{bmatrix}\n1 & x^{(1)} & \\cdots & (x^{(1)})^{p - 1} \\\\\n1 & x^{(2)} & \\cdots & (x^{(2)})^{p - 1} \\\\\n\\cdots & & & \\\\\n1 & x^{(N)} & \\cdots & (x^{(N)})^{p - 1}\n\\end{bmatrix}\\]\nPiecewise-Linear Fit 분절선형 적합\n\n절단점 식별: 선의 기울기가 변하는 지점을 결정한다.\n선형 구간 적합: 절단점으로 분리된 각 데이터 구간에 선형 모델을 적합한다.\n구간 결합: 절단점에서 구간함수를 연결하여 연속적인 분절선형 함수를 형성한다.\n\n\n\n\n\n\n# Piecewise-Linear Fit\nimport numpy as np\n# 합성 데이터 생성\nnp.random.seed(0)\nx = np.linspace(0, 10, 100)\ny = np.piecewise(x, [x &lt; 4, (x &gt;= 4) & (x &lt; 7), x &gt;= 7],[lambda x: 2 * x + 1 + np.random.normal(size=len(x)),lambda x: -x + 5 + np.random.normal(size=len(x)),lambda x: 0.5 * x - 1 + np.random.normal(size=len(x))])\nimport matplotlib.pyplot as plt\nfrom scipy.optimize import curve_fit\n# 분절선형 함수 정의\ndef piecewise_linear(x, x0, x1, y0, y1, y2, k1, k2, k3):\n    conds = [x &lt; x0, (x &gt;= x0) & (x &lt; x1), x &gt;= x1]\n    funcs = [lambda x: k1 * x + y0, lambda x: k2 * x + y1, lambda x: k3 * x + y2]\n    return np.piecewise(x, conds, funcs)\n# 초기 파라미터 추정값\np0 = [4, 7, 1, 5, -1, 2, -1, 0.5]\n# 데이터를 분절선형 함수에 적합시킴\nparams, _ = curve_fit(piecewise_linear, x, y, p0=p0)\n# 데이터를 적합한 결과와 함께 플로팅\nx_fit = np.linspace(0, 10, 100)\ny_fit = piecewise_linear(x_fit, *params)\n\nplt.scatter(x, y, label='Data')\nplt.plot(x_fit, y_fit, color='red', label='Piecewise Linear Fit')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.legend()\nplt.show()\n\n\n\n3. 간선행렬 \\(Ax = b\\)\n간선 행렬 Incidence matrix은 그래프 이론에서 사용되는 개념으로, 정점과 vertices 간선 edges, nodes 사이의 관계를 나타내는 행렬이다.\n간선 행렬 \\(G_{n \\times m}\\)은 정점이 \\(n\\)개, 간선이 \\(m\\)개이다.\n\n\\(A_{ij} = 1\\) : 정점 \\(i\\)와 간선 \\(j\\)와 연결되어 있고 정점 \\(i\\)는 끝 정점이 아니다.\n\\(A_{ij} = 1\\) : 정점 \\(i\\)와 간선 \\(j\\)와 연결되어 있고 정점 \\(i\\)는 끝 정점이다.\n\\(A_{ij} = 0\\) : 정점 \\(i\\)와 간선 \\(j\\)와 연결되어 않음\n\n\n\n\n\n\n\n\n4. 네트워크\n만약 \\(x\\)가 네트워크에서의 흐름을 나타내는 \\(m\\)-벡터라면, \\(x_{j}\\)는 간선 \\(j\\)를 통한 흐름으로 해석된다. 여기서 양의 값은 흐름이 간선 \\(j\\)의 방향으로 이동하고, 음의 값은 흐름이 간선 \\(j\\)의 반대 방향으로 이동함을 의미한다. 네트워크에서 간선이나 링크의 방향은 흐름의 방향을 지정하지 않고 그저 흐름 flow의 방향을 고려하는 것을 나타내는 것이다.\n네트워크에서의 흐름 보존은 흐름이 노드와 간선을 통해 어떻게 이동하는지를 설명하며, 각 노드로 들어오는 총 흐름이 노드에서 나가는 총 흐름과 같음을 보장한다.\n네트워크 구조를 나타내는 \\(G_{n \\times m}\\)를 사용하여\n\\(y = Gx\\)는 각 노드로 들어오는 순흐름을 나타내는 \\(n\\)-벡터이다.\n\\(y_{i}\\)는 \\(i\\)-노드로 들어오는 총 흐름에서 \\(i\\)-노드에서 나가는 총 흐름을 뺀 값이다 즉, \\(i\\)-노드에서의 흐름 잉여 surplus이다.\n요약하면, \\(y = Gx\\)는 네트워크 이론에서의 흐름 보존 원칙을 요약한 것으로, 각 요소 \\(y_{i}\\)는 노드 \\(i\\)에서의 순 흐름 균형을 나타내며 모든 들어오는 흐름과 나가는 흐름을 고려한다.\n만약 \\(Gx = 0\\)인 상태를 각 노드에서 총 들어오는 흐름과 총 나가는 흐름이 일치하기 때문에 흐름 보존이 일어난다고 말한다.\n\n\n\n\n\n위의 그래프에 의해 나타낸 네트워크에서 \\(x = (1, - 1,1,0,1)\\)이다. 소스는 source 노드에서 네트워크로 들어오거나 나가지만, 간선을 따라 흐르지는 않습니다. 위 그림에서 보여지는 것처럼 이러한 흐름들은 5-벡터 4소스로 나타낸다. \\(s_{i}\\)를 노드 \\(i\\)에서 외부에서 네트워크로 들어오는 흐름으로 생각할 수 있다. 즉, 어떤 간선을 통해서도 들어오지 않는 것이다. \\(s_{i} &gt; 0\\)일 때 외부흐름은 소스라고 부르며 \\(s_{i} &lt; 0\\)일 때 외부흐름은 싱크라고 부른다.\n소스 포함된 흐름 보전 : \\(Ax + s = 0\\)\n\n\n5. 선형함수 모델 \\(Ax = b\\)\n필드에서 발생하는 많은 함수나 변수 간의 관계는 선형 또는 아핀 함수로 근사될 수 있는데, 두 변수 집합 간의 선형 함수를 모형(model) 또는 근사(approximation) 값으로 정의한다.\n\n(1) 수요의 가격 탄력성(Price elasticity of demand)\n가격이 n개의 상품(서비스)에 의해 결정되는 n-벡터 p로 주어지고, 상품에 대한 수요가 n-벡터 d로 주어진다. n-벡터 \\(\\delta^{price}\\)를 가격변화 벡터라 하면 \\(\\delta^{price} = \\frac{(p_{i}^{new} - p_{i})}{p_{i}}\\)라 하자(\\(p^{new}\\)는 새로운 가격 n-벡터). n-벡터 \\(\\delta^{dem}\\)를 수요변화 벡터라 하면 \\(\\delta^{dem} = \\frac{(d_{i}^{new} - d_{i})}{d_{i}}\\)라 하자. \\(\\delta^{dem} = E^{d}\\delta^{price}\\), \\(E^{d}\\)는 (\\(n \\times n\\)) 수요 탄력성 행렬이다.\n\\(E_{11}^{d} = - 0.4\\), \\(E_{21}^{d} = 0.2\\) 가정해 보자. 이는 첫 번째 상품의 가격이 1% 증가할 때, 다른 가격은 동일한 상태에서 첫 번째 상품의 수요가 0.4% 감소하고, 두 번째 상품의 수요가 0.2% 증가할 것임을 의미한다. 두 번째 상품은 첫 번째 상품의 부분 대체품으로 작용하고 있다.\n\n\n(2) 탄성 변형 Elastic deformation\nf 를 구조물에 작용하는 특정 위치(및 방향)에 대한 힘(하중)을 나타내는 n-벡터라고 합시다. 구조물은 하중으로 인해 약간 변형될 것입니다. d는 하중으로 인해 구조물의 m개 지점에서 발생하는 변위(특정 방향으로)를 나타내는 m-벡터입니다. 변위와 하중 사이의 관계는 선형으로 잘 근사된다. d= Cf 여기서 C 는 m × n 컴플라이언스(compliance) 행렬이고 C 의 항목의 단위는 m/N입니다.\n\n\n(3) 테일러 근사\n함수 \\(f:R^{n} \\rightarrow R^{n}\\)이 1차 미분이 가능하다고 하면 테일러 근사는 \\(\\widehat{f}(x)_{i} = f_{i}(z) + \\triangledown f_{i}(z)^{T}(x - z)\\), 단 n-벡터 \\(z\\)는 n-벡터 \\(x\\)와 가까운 값이다.\n\\(\\widehat{f}(x) = f(z) + Df(z)(x - z)\\), 단.\\(Df(z)_{ij} = \\frac{\\partial f_{i}}{\\partial x_{i}}(z),i = 1,...,m,j = 1,...,n\\)\n\n\n(4) 회귀모형\n표본 크기 \\(N\\), 예측변수 벡터 \\(x^{(1)},x^{(2)},...,x^{(N)}\\)이다. \\(i\\)-개체의 예측치는 \\({\\widehat{y}}^{(i)} = (x^{(i)})^{T}\\beta + v,i = 1,2,...,N\\)이다. 그리고 \\(X\\)는 예측변수 행렬, \\(y\\)는 목표변수 벡터이다.\n\n잔차는 \\(r^{(i)} = y^{(i)} - {\\widehat{y}}^{(i)}\\).\n절편 없는 회귀모형 : \\({\\widehat{y}}^{d} = X^{T}\\beta + v1\\)\n절편 회귀모형 : \\({\\widehat{y}}^{d} = \\left\\lbrack \\begin{array}{r}\n1^{T} \\\\\nX\n\\end{array} \\right\\rbrack^{T}\\left\\lbrack \\begin{array}{r}\nv \\\\\n\\beta\n\\end{array} \\right\\rbrack\\)\n\n\n\n\n6. 선형 동적 시스템\n시간에 따라 변하는 상태 벡터의 선형 관계를 설명하는 모델로 시스템의 현재 상태가 다음 상태를 예측할 수 있는 간단한 수학적 구조이다. \\(x_{t}\\)가 현재 상태인 \\(x_{1},x_{2},\\cdots\\) n-벡터 시계열이라 하자. 예를 들면, \\((x_{5})_{3}\\) 3번째 포트폴리오의 5일째 주가가 된다.\n\n(1) 입력이 포함된 선형 동적 시스템\n\\[x_{t + 1} = A_{t}x_{t} + B_{t}u_{t},t = 1,2,...\\]\n\\(u_{t}\\) 는 시간 t 에서의 입력벡터이고 .B 는 입력행렬로, 입력 \\(u_{t}\\)(외생 변수라고도 함)가 상태 벡터 \\(x_{t}\\)에 미치는 영향을 설명한다.\n\n\n(2) \\(K\\)-Markov 모형\n\\[x_{t + 1} = A_{1}x_{t} + \\cdots + A_{K}x_{t - K + 1},t = K,K + 1,...\\]\n\n상태 State : 시스템이 존재할 수 있는 모든 가능한 상태들의 집합. 예를 들어, 날씨 예측 모델에서 상태는 ”맑음”, ”흐림”, ”비” 등이 될 수 있다. 시스템이 가질 수 있는 모든 상태들의 집합을 상태 공간 \\(S\\)라 한다.\n상태 전이 State Transition : 한 상태에서 다른 상태로의 전이. 상태 전이는 확률적으로 이루어지며 \\(P_{i}\\)는 초기상태 확률분포이다.\n전이 확률 Transition Probability : 현재 상태에서 다음 상태로 전이될 확률을 나타낸다. 이는 \\(P(x_{t + 1} = s_{j}|x_{t} = s_{i})\\)로 표현되며, 현재 상태 \\(i\\)에서 다음 시점에 상태 \\(j\\)로 전이될 확률이다.\n\n# Markov model\nimport numpy as np\n# 전이 행렬 정의\nP = np.array([[0.8, 0.2],[0.4, 0.6]])\n# 초기 상태 분포 정의\npi_0 = np.array([0.6, 0.4])\n# 상태 이름 정의\nstates = [\"Sunny\", \"Rainy\"]\n# 시뮬레이션을 위한 시간 단계 수\nnum_steps = 10\n# 초기 상태 선택\ncurrent_state = np.random.choice(states, p=pi_0)\nprint(f\"Day 0: {current_state}\")\n# 시뮬레이션 시작\nfor t in range(1, num_steps + 1):\n    if current_state == \"Sunny\":\n        next_state = np.random.choice(states, p=P[0])\n    else:\n        next_state = np.random.choice(states, p=P[1])\n    print(f\"Day {t}: {next_state}\")\n    current_state = next_state\n\n\n\n\n\n\n\n\n7. 인구 동태\n100-벡터 \\((x_{t})_{i}\\)는 \\(t\\) 시점의 \\((i - 1)\\)세 인구이다. 100- 벡터 \\(b\\)의 \\(b_{i}\\)는 \\((i - 1)\\)의 평균 출생율이다. 가임 연령을 고려하면 벡터 b의 원소는\\(b_{I} = 0fori &lt; 13ori &gt; 50\\)이다. 만약 사망, 이민 없다고 가정하면 내년 0세 인구는 \\((x_{t + 1})_{1} = b^{T}x_{t}\\)이다.\n나이 \\(i\\)세 \\((t + 1)\\) 시점의 인구수는 다음과 같다. \\(d_{i}\\)는 \\(i\\)세 사망자수이다.\\((x_{t + 1})_{i + 1} = (1 - d_{i})(x_{t})_{i},i = 1,2,\\cdots,99\\). 최종적으로 인구 동태 모형은 \\(x_{t + 1} = Ax_{t},t = 1,2,\\cdots\\)이다.\n전이행렬 \\(A\\)\n\\[A = \\begin{bmatrix}\nb_{1} & b_{2} & b_{3} & \\cdots & b_{98} & b_{99} & b_{100} & \\\\\n1 - d_{1} & 0 & 0 & \\cdots & 0 & 0 & 0 & \\\\\n0 & 1 - d_{2} & 0 & \\cdots & 0 & 0 & 0 & \\\\\n\\cdots & \\cdots & \\cdots & \\cdots & \\cdots & \\cdots & \\cdots & \\\\\n0 & 0 & 0 & \\cdots & 1 - d_{98} & 0 & 0 & \\\\\n0 & 0 & 0 & \\cdots & & 0 & 1 - d_{99} & 0\n\\end{bmatrix}\\]\n이민을 고려한 인구 동태 모형\n\\(x_{t + 1} = Ax_{t} + u_{t},t = 1,2,\\cdots\\), 벡터 \\((u_{t})_{i}\\)는 t-시점에 나이 \\((i - 1)\\)세의 순이민자수이다.\n간단한 인구동태 방정식\n\\[P_{t + 1} = P_{t} + (B_{t} - D_{t}) + M_{t}\\]\n\\(P_{t}\\) : \\(t\\) 시점의 인구수, \\(B_{t}\\) : \\(t\\) 시점의 출생자수, \\(D_{t}\\) : \\(t\\) 시점의 사망자수, \\(M_{t}\\) : \\(t\\) 시점의 순 이민자수\n# 인구동태모형\nimport numpy as np\nimport matplotlib.pyplot as plt\n# 초기 인구와 파라미터 설정 미국 23년 기준\ninitial_population = 330_000_000\nbirth_rate = 12.4 / 1000\ndeath_rate = 8.9 / 1000\nannual_net_migration = 1_000_000\nyears = 10\n# 인구 예측을 위한 배열 초기화\npopulation = np.zeros(years + 1)\npopulation[0] = initial_population\n# 연도별 인구 예측\nfor t in range(1, years + 1):\n    births = population[t - 1] * birth_rate\n    deaths = population[t - 1] * death_rate\n    population[t] = population[t - 1] + births - deaths + annual_net_migration\n# 결과 출력\nfor t in range(years + 1):\n    print(f\"Year {2023 + t}: {population[t]:,.0f}\")\n【결과】 Year 2024: 332,155,000 Year 2025: 334,317,542 Year 2026: 336,487,654 Year 2027: 338,665,361 Year 2028: 340,850,689 Year 2029: 343,043,667 Year 2030: 345,244,320 Year 2031: 347,452,675 Year 2032: 349,668,759Year 2033: 351,892,600\n\n\n8. 전염병 동태\n전염 역할 모델른 전염병의 전파와 확산을 연구하는 분야로, 이는 질병의 전염 방식과 전파 속도를 이해하고 예측하는 데 중점을 둔다.\n\\(SIRD\\) 모델 상태\n\\(x_{t} = (S,I,R,D),whereS + R + I + D = 1\\)\n\n감염 가능성 Susceptible (S): 현재는 비감염이지만 내일에는 질병에 감염될 수 있는 사람들\n감염 Infected (I): 현재 질병에 감염된 사람들.\n회복 Recovered (R): 질병을 회복하고 면역을 획득한 사람들.\n사망 Deceased (D): 질병으로 사망한 사람들.\n\n약학 모델 동력학\n\\(\\beta\\) : 감염 가능성에서 감염으로 전환될 감염율, \\(\\gamma\\) : 감염에서 회복으로 전화되는 회복율 \\(\\mu\\) : 감염에서 사망으로 전환되는 사망율이라면\n\\[\\begin{matrix}\n& \\frac{dS}{dt} = - \\beta SI,\\frac{dI}{dt} = - \\beta SI - \\gamma I\\mu I \\\\\n& \\frac{dR}{dt} = - \\gamma I,\\frac{dD}{dt} = \\mu I\n\\end{matrix}\\]\n사례연구\n만약 t기의 SIRD 벡터가 \\(x_{t} = (0.99,0.01,0,0)\\)라 하자. 그리고 감염 가능성 있는 인구 중 30%(\\(\\beta = 0.3\\))는 전염되고 전염자의 2%(\\(\\mu = 0.02\\))는 사망하고 회복율은 10%(\\(\\gamma = 0.1)\\)이라 하자. 그러므로 전염 상태로 남아 있는 전염자는 88%이다.\n\\(x_{t + 1} = Ax_{t}\\) 모형에서 \\(A = \\begin{bmatrix}\n0.99 & 0.1 & 0 & 0 \\\\\n0.01 & 0.88 & 0 & 0 \\\\\n0 & 0.1 & 1 & 0 \\\\\n0 & 0.02 & 0 & 1\n\\end{bmatrix}\\)\n# 전염병 동태모델 사례\nimport numpy as np\nfrom scipy.integrate import odeint\nimport matplotlib.pyplot as plt\n# 초기 조건\nS0 = 0.99   # 초기 감수성 인구 비율\nI0 = 0.01   # 초기 감염 인구 비율\nR0 = 0.0    # 초기 회복 인구 비율\nD0 = 0.0    # 초기 사망 인구 비율\ninitial_conditions = [S0, I0, R0, D0]\n# 파라미터\nbeta = 0.3   # 전염율\ngamma = 0.1  # 회복율\nmu = 0.02    # 사망율\n# SIRD 모델 미분 방정식\ndef sird_model(y, t, beta, gamma, mu):\n    S, I, R, D = y\n    dS_dt = -beta * S * I\n    dI_dt = beta * S * I - gamma * I - mu * I\n    dR_dt = gamma * I\n    dD_dt = mu * I\n    return [dS_dt, dI_dt, dR_dt, dD_dt]\n# 시간 벡터 (일 단위)\nt = np.linspace(0, 160, 160)\n# ODE 풀기\nsolution = odeint(sird_model, initial_conditions, t, args=(beta, gamma, mu))\nS, I, R, D = solution.T\n# 결과 그래프 출력\nplt.figure(figsize=(10, 6))\nplt.plot(t, S, label='Susceptible')\nplt.plot(t, I, label='Infected')\nplt.plot(t, R, label='Recovered')\nplt.plot(t, D, label='Deceased')\nplt.xlabel('Time (days)')\nplt.ylabel('Proportion of Population')\nplt.legend()\nplt.title('SIRD Model')\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nchapter 4. 고유치와 고유벡터\n\n1. 기초\n\n(1) 개념\n고유치는 행렬의 선형변환에서 중요한 특성을 나타내는 값이다. 특정 벡터(고유벡터)가 행렬 \\(A\\)에 의해 변환될 때, 방향은 변하지 않고 크기만 일정 비율로 변한다면, 이 비율을 고유치라고 한다.\n\n\n\n\n\n위 그래프는 행렬 \\(A = \\begin{bmatrix}\n3 & 1 \\\\\n0 & 2\n\\end{bmatrix}\\)의 고유치(\\(\\lambda = 3,2\\))와 고유벡터의 변환을 시각적으로 보여준다.\n\n빨간색 화살표: 첫 번째 고유벡터 \\(\\mathbf{v}_{1}\\)\n투명 빨간색 화살표: 첫 번째 고유벡터가 행렬 \\(A\\)에 의해 변환된 결과로, 고유치 \\(\\lambda_{1} = 3\\)에 의해 크기만 3배로 늘어난다.\n파란색 화살표: 두 번째 고유벡터 \\(\\mathbf{v}_{2}\\).\n투명 파란색 화살표: 두 번째 고유벡터가 행렬 A 에 의해 변환된 결과로, 고유치 \\(\\lambda_{2} = 2\\)에 의해 크기만 2배로 늘어난다.\n\n고유벡터의 방향은 행렬 변환 후에도 유지되며, 크기만 고유치 값에 따라 변한다. 이를 통해 고유치와 고유벡터의 개념을 시각적으로 이해할 수 있다.\n\n\n(2) 통계학 활용\n고유치 분석을 통해 얻을 수 있는 통계적 통찰은 다음과 같다.\n\n데이터의 분산 설명: 공분산 행렬의 고유치는 각 축의 분산 크기를 나타내며, 데이터가 어떤 축에서 더 많은 정보를 가지고 있는지 보여준다.\n중요한 변수 식별: PCA나 LDA에서 고유치를 사용해 데이터를 가장 잘 설명하는 주성분이나 판별 방향을 찾는다.\n데이터의 차원 축소: 가장 큰 고유치를 가진 축만 선택함으로써 데이터의 복잡성을 줄이고, 분석의 효율성을 높는다.\n시각화: MDS, PCA를 활용해 고차원 데이터를 저차원으로 투영하여 시각화할 수 있는다.\n\n주성분 분석(PCA, Principal Component Analysis)\nPCA는 데이터의 고차원 공간을 낮은 차원으로 축소하면서 데이터의 주요 정보를 보존하는 방법이다.\n\n데이터의 공분산 행렬에서 고유치를 계산하여 주성분의 중요도를 평가한다.\n가장 큰 고유치는 데이터의 분산을 가장 많이 설명하는 방향(주성분)을 나타낸다.\n예: 변수 100개로 구성된 데이터를 분석할 때, 고유치를 계산하여 주요한 2~3개의 주성분만 선택해 데이터 차원을 축소할 수 있다.\n\n선형 판별 분석(LDA, Linear Discriminant Analysis)\nLDA는 여러 클래스 간의 분산을 극대화하면서 각 클래스 내의 분산을 최소화하는 투영 방향을 찾는 방법이다.\n클래스 간 분산 행렬과 클래스 내 분산 행렬의 비율로 구성된 행렬의 고유치를 계산하여 최적의 분리 축을 결정한다.\n다차원 척도법(MDS, Multidimensional Scaling)\nMDS는 데이터 간의 거리 행렬을 기반으로 저차원 공간에 데이터를 시각화하는 방법이다.\n\n거리 행렬을 고유치 분해하여 데이터를 저차원 공간에 배치한다.\n가장 큰 고유치를 가진 방향이 데이터 구조의 주요 변화를 설명한다.\n\n공분산 행렬 및 상관 행렬 분석\n공분산 행렬이나 상관 행렬의 고유치는 데이터의 선형 독립성과 분산 구조를 분석하는 데 사용된다.\n\n고유치가 큰 방향은 데이터의 분산이 큰 축(정보가 많이 분포된 축)을 나타낸다.\n고유치가 0에 가까운 경우 변수들 간의 선형 종속성을 암시한다.\n\n행렬 분해 및 차원 축소\n고유치와 고유벡터는 행렬 분해 방법(예: 특이값 분해(SVD), 고유분해(Eigendecomposition))의 핵심이다.\n\n차원 축소, 데이터 압축, 노이즈 제거 등에 사용된다.\n예: 특이값 분해(SVD)는 추천 시스템이나 텍스트 분석(Latent Semantic Analysis, LSA)에서 널리 사용된다.\n\n시계열 데이터 분석 Autoregressive 모델(AR)\n시계열 모델에서 안정성을 분석할 때, 고유치를 통해 시스템의 특성을 평가한다. 예: 고유치가 1보다 크면 시스템이 불안정함을 나타낸다.\n\n\n\n2. 고유치, 고유벡터 구하기\n대칭행렬 \\(A_{n \\times n}\\)에 대하여 고유치 \\(\\lambda\\), 고유벡터 \\(\\underset{¯}{v}\\)는 다음 방정식이 성립한다. \\(A\\underset{¯}{v} = \\lambda\\underset{¯}{v}\\)\n\n(1) 고유치 eigenvalue 구하기\n\\(det(A - \\lambda I) = 0\\)을 만족하는 \\(\\lambda\\)를 고유치라 한다.\n고유치는 행렬 \\(A\\)의 차수만큼 존재한다. \\(\\lambda_{1},\\lambda_{2},...,\\lambda_{n}\\)\n\n\n(2) 고유벡터 eigenvector 구하기\n\\(A\\underset{¯}{v_{i}} = \\lambda_{i}\\underset{¯}{v_{i}}\\) 을 만족하는 벡터(\\(\\underset{¯}{v}\\))를 고유벡터라 한다.\n\\(det(A - \\lambda I) = 0\\)(singlular)가 성립하므로 고유벡터는 무수히 많이 존재한다.\n고유벡터 중 Norm(\\(\\underset{¯}{v}'\\underset{¯}{v} = 1\\))이 1인 고유 벡터를 주성분분석에서 사용한다.\n\n\n\n3. 고유치 활용\n\n(1) 고유치 분해 eigenvalue decomposition\n정방행렬 \\(A_{n \\times n}A\\)의 고유치(\\(\\lambda_{i}\\))를 대각원소로 하는 대각행렬 \\(\\Lambda\\), 고유벡터(\\(\\underset{¯}{v_{i}}\\))로 이루어진 직교 orthogonal 행렬 \\(Q\\)라 하면 행렬 \\(A\\)는 다음과 같이 고유치 분해 된다. \\(A = Q\\Lambda Q^{- 1}\\)\n\n\n(2) 주성분분석\n데이터 행렬 : \\(X_{n \\times p} = \\begin{bmatrix}\nx_{11} & x_{12} & \\cdots & x_{1p} \\\\\nx_{21} & x_{22} & \\cdots & x_{2p} \\\\\n\\cdots & \\cdots & \\cdots & \\cdots \\\\\nx_{n1} & x_{n2} & \\cdots & x_{np}\n\\end{bmatrix}\\) (변수 개수 \\(p\\))\n\n\\(\\underset{¯}{y} = P\\underset{¯}{x}\\) : 원 변수의 선형결합(선형계수 행렬은 고유벡터)으로 주성분변수를 만든다.\n\\(X'X\\) 고유치분해 : \\(X'X = (Q\\Lambda Q^{- 1})'(Q\\Lambda Q^{- 1}) = Q\\Lambda Q^{- 1}\\)\n\\(X\\)의 공분산행렬(측정 단위가 다른 경우 상관계수 행렬)로부터 고유치와 고유벡터(Norm=1인 정규고유벡터)를 구하여 서로 독립인 차원으로 변환한다.\n공분산행렬에 대한 고유치, 고유벡터 : \\(COV_{p \\times p}\\underset{¯}{v} = \\lambda\\underset{¯}{v}\\)\n공분산 행렬은 양의 정부호 행렬이므로 변수의 차수만큼의 고유치, 그에 대응하는 고유벡터가 존재한다.\n고유벡터는 원변수를 직교 축을 갖는 주성분 변수로 변환한다. 그러므로 차수는 줄어들지 않으나 모든 차원에서 관측값은 직교(독립)이다.\n주요 2~3개 차원만으로 \\(p\\)차원의 원변수 변동(정보)를 축약한다. 이를 주성분분석이라 한다.\n\n\n\n\n\n\n\n\n(3) 특이값 분해 Singular Value Decomposition\n\n\n\n\n\n\n직교행렬 \\(U\\)(\\(UU' = I\\)) : \\(AA'\\)의 고유벡터\n직교행렬 \\(V'\\)(\\(V'V = I\\)) : \\(A'A\\)의 고유벡터\n대각행렬 \\(\\Sigma\\)의 대각원소 : \\(AA'\\), \\(A'A\\)의 고유치분해 대각원소의 제곱근 값을 대각원소로 한다.\n\n\n\n(4) Cholesky factorization\n대칭행렬 \\(A\\)가 양의 정부호 행렬일 경우 사용되는 분해방법이다.\n\\(A = LL^{T}\\), \\(L\\) : 대각원소가 양이 하단 삼각행렬\n【활용】 최소제곱추정과 같은 최적해를 구할 때 사용하면 빠른 연산이 가능하다. \\(A\\underset{¯}{x} = \\underset{¯}{b}\\) (연립방정식) \\(\\underset{¯}{x} = A^{- 1}\\underset{¯}{b}\\) ➠ \\(LL^{T}\\underset{¯}{x} = \\underset{¯}{b}\\) 이것을 풀면 연산이 더 간편하다. \\(\\underset{¯}{x} = (LL^{T})^{- 1}\\underset{¯}{b} = (L^{- 1})'L^{- 1}\\underset{¯}{b}\\)\n#고유치, 고유벡터\nimport numpy as np\nA=np.array([[1,2,3], [4,5,7],[8,9,10]])\nimport numpy.linalg as la\nval,vec=la.eig(A)\nval,vec\n【결과】 (array([17.71571559, -1.44163052, -0.27408507]), array([[-0.21078452, -0.49872133, 0.47929184], [-0.52147269, -0.47685414, -0.81047488], [-0.82682291, 0.7238005 , 0.33676373]]))\n#고유벡터 분해\nimport numpy as np\nA=np.array([[1,2,3], \n  [4,5,7],\n  [8,9,10]])\nimport numpy.linalg as la\nval,vec=la.eig(A)\nS=np.diag(val); P=vec\nP@S@la.inv(P)\n【결과】 array([[ 1., 2., 3.], [ 4., 5., 7.], [ 8., 9., 10.]])\n#SVD decomposition\nu, s, vh = np.linalg.svd(A, full_matrices=True)\nu,s,vh\n【결과】 (array([[-0.19462586, -0.6193003 , -0.76064966], [-0.5071685 , -0.6002356 , 0.61846369], [-0.83958376, 0.50614657, -0.19726824]]), array([18.62202941, 1.46779937, 0.25609691]), array([[-0.48007495, -0.56284671, -0.67285334], [ 0.70100172, 0.21497525, -0.67998694], [ 0.52737523, -0.79811604, 0.29135228]]))\n#Cholesky decomposition\nimport numpy as np\nA=np.array([[25,15,-5], \n  [15,18,0],\n  [-5,0,11]])\nimport numpy.linalg as la\nnp.linalg.cholesky(A)\n【결과】 array([[ 5., 0., 0.], [ 3., 3., 0.], [-1., 1., 3.]])\n#확인 LL'\nnp.linalg.cholesky(A)@np.linalg.cholesky(A).T\n【결과】 array([[25., 15., -5.], [15., 18., 0.], [-5., 0., 11.]])\n\n\n\n\nchapter 5. 행렬미분\n\n1. 미분 공식\n\n(1) 벡터미분\n상수벡터 : \\({\\underset{¯}{a}}_{n} = \\left\\lbrack \\begin{array}{r}\na_{1} \\\\\na_{2} \\\\\n... \\\\\na_{n}\n\\end{array} \\right\\rbrack\\) 확률변수 벡터 : \\({\\underset{¯}{x}}_{n} = \\left\\lbrack \\begin{array}{r}\nx_{1} \\\\\nx_{2} \\\\\n... \\\\\nx_{n}\n\\end{array} \\right\\rbrack\\)\n확률변수 \\(x_{i} \\sim (iid)f(x)\\)는 확률표본이다.\n\\(\\frac{\\partial(\\underset{¯}{a}'\\underset{¯}{x})}{\\partial\\underset{¯}{x}} = \\underset{¯}{a}\\), \\(\\frac{\\partial(\\underset{¯}{x}'\\underset{¯}{a})}{\\partial\\underset{¯}{x}} = \\underset{¯}{a}\\)\n\n\n(2) 이차형식 미분\n\\(\\frac{\\partial(\\underset{¯}{x}'A\\underset{¯}{x})}{\\partial\\underset{¯}{x}} = (A + A')\\underset{¯}{x}\\) 만약 A가 대칭행렬이면) \\(2A\\underset{¯}{x}\\)\n\n\n\n2. 이차형식\n\n(1) 이차형식 정의\n정방행렬 : \\(A_{n \\times n} = \\begin{bmatrix}\na_{11} & a_{12} & \\cdots & a_{1n} \\\\\na_{21} & a_{22} & \\cdots & a_{2n} \\\\\n\\cdots & \\cdots & \\cdots & \\cdots \\\\\na_{n1} & a_{n2} & \\cdots & a_{nn}\n\\end{bmatrix}\\)\n이차형식 : \\(Q(x_{1},x_{2},...,x_{n}) = \\underset{¯}{x}'A\\underset{¯}{x}\\)\n\n2차형식의 경우 대칭행렬인 \\(A\\)는 적어도 한 개는 존재한다.\n\n\n\n(2) 이차형식 종류\n대칭행렬 \\(A\\), 이차형식 \\(Q(x_{1},x_{2},...,x_{n}) = \\underset{¯}{x}'A\\underset{¯}{x}\\)에 대하여\n모든 \\(x \\neq 0\\)에 대하여 \\(Q &gt; 0\\)이면 양의 정부호 positive definite\n모든 \\(x \\neq 0\\)에 대하여 \\(Q \\geq 0\\)이면 양의 반부호 positive semidefinite\n\n\n(3) 주축정리 The Principal Axes Theorem\n이차형식 \\(\\underset{¯}{x}'A\\underset{¯}{x}\\)을 교차항이 없는 이차형식 \\(\\underset{¯}{y}'D\\underset{¯}{y}\\)으로 변환하는 직교변환 \\(\\underset{¯}{x} = P\\underset{¯}{y}\\) 존재한다. \\(P\\)를 주축행렬이라 하고 대칭행렬 \\(A\\)의 고유벡터로 이루어져 있다.\n\n교차항이 없는 이차형식은 주축 변량에 대칭이다.\n\n\n\n\n\n\n\n\n(4) 이차형식과 고유치 관계\n\n이차형식 \\(Q = \\underset{¯}{x}'A\\underset{¯}{x}\\)이 양의 정부호이면 모든 고유치는 0보다 크다.\n양의 정부호 행렬의 역행렬도 양의 정부호 행렬이다.\n공분산 행렬은 양의 정부호 행렬이다.\n\n\n\n\n3. 이차형식 만들기\n\\[Q(x) = x_{1}^{2} + 2x_{2}^{2} - 7x_{3}^{2} - 4x_{1}x_{2} + 8x_{1}x_{3}\\]\n\n이차형식으로 만들면 다음과 같다. 제곱항은 그대로 대각원소로 하고 교차항은 1/2로 하여 각 셀에 배분한다.\n\n\\[Q(x) = \\begin{bmatrix}\nx_{1} & x_{2} & x_{3}\n\\end{bmatrix}\\begin{bmatrix}\n1 & - 2 & 4 \\\\\n- 2 & 2 & 0 \\\\\n4 & 0 & - 7\n\\end{bmatrix}\\left\\lbrack \\begin{array}{r}\nx_{1} \\\\\nx_{2} \\\\\nx_{3}\n\\end{array} \\right\\rbrack = \\underset{¯}{x}'A\\underset{¯}{x}\\]\n\n\\(\\underset{¯}{x} = P\\underset{¯}{y}\\), 주축행렬 \\(P\\)는 대칭행렬 \\(A\\)의 고유벡터이다.\n\\(A\\)의 교유치를 대각원소로 하는 행렬 \\(D = diag(\\lambda_{1},\\lambda_{2},\\lambda_{3})\\)를 이용하여 교차항이 없는 이차형식으로 변형한다.\n이렇게 되면 주축 변환된 이차형식의 변수 간에는 교차항이 없으므로 두 변수간에는 서로 독립이 된다.\n\\(Q(x) = \\underset{¯}{x}'A\\underset{¯}{x}\\) ⇢ \\(Q(y) = \\underset{¯}{y}'D\\underset{¯}{y}\\) (\\(\\underset{¯}{x} = P\\underset{¯}{y}\\))\n\n\n\n4. 선형 회귀모형\n\n(1) 데이터 구조\n목표변수 1개, \\(p\\)개 예측변수, 표본크기 n인 데이터를 가정하면 선형 회귀모형은 다음과 같다. \\(\\underset{¯}{y} = X\\underset{¯}{\\beta} + \\underset{¯}{e}\\)\n\\(\\left\\lbrack \\begin{array}{r}\ny_{1} \\\\\ny_{2} \\\\\n\\cdots \\\\\ny_{n}\n\\end{array} \\right\\rbrack\\)=\\(\\begin{bmatrix}\n1 & x_{11} & x_{12} & \\cdots & x_{1p} \\\\\n1 & x_{21} & x_{22} & \\cdots & x_{2p} \\\\\n\\cdots & \\cdots & \\cdots & \\cdots & \\\\\n1 & x_{n1} & x_{n2} & \\cdots & x_{np}\n\\end{bmatrix}\\left\\lbrack \\begin{array}{r}\na \\\\\nb_{1} \\\\\n\\cdots \\\\\nb_{p}\n\\end{array} \\right\\rbrack\\)+\\(\\left\\lbrack \\begin{array}{r}\ne_{1} \\\\\ne_{2} \\\\\n\\cdots \\\\\ne_{n}\n\\end{array} \\right\\rbrack\\)\n\n\n(2) 예측변수 데이터 행렬/벡터\n\\(X_{n \\times p} = \\begin{bmatrix}\nx_{11} & x_{12} & \\cdots & x_{1p} \\\\\nx_{21} & x_{22} & \\cdots & x_{2p} \\\\\n\\cdots & \\cdots & \\cdots & \\cdots \\\\\nx_{n1} & x_{n2} & \\cdots & x_{np}\n\\end{bmatrix}\\), \\(X_{n \\times p} = \\begin{bmatrix}\n{\\underset{¯}{x}}_{1} & {\\underset{¯}{x}}_{2} & \\cdots & {\\underset{¯}{x}}_{p} &\n\\end{bmatrix}\\)\n(데이터 벡터) \\({\\underset{¯}{x}}_{k} = \\left\\lbrack \\begin{array}{r}\nx_{1k} \\\\\nx_{2k} \\\\\n\\cdots \\\\\nx_{nk}\n\\end{array} \\right\\rbrack\\)\n\n\n(3) 확률변수 벡터, 평균벡터, 공분산행렬\n\\(\\underset{¯}{x} = \\left\\lbrack \\begin{array}{r}\nx_{1} \\\\\nx_{2} \\\\\n\\cdots \\\\\nx_{p}\n\\end{array} \\right\\rbrack\\), \\(x_{i}\\)는 확률변수이고 \\(E(x_{i}) = \\mu_{i},V(x_{i}) = \\sigma_{ii}\\),\n(두 변수의 공분산) \\(COV(x_{i},x_{j}) = \\sigma_{ij}\\)\n(평균벡터) \\(E(\\underset{¯}{x}) = \\underset{¯}{\\mu} = \\left\\lbrack \\begin{array}{r}\n\\mu_{1} \\\\\n\\mu_{2} \\\\\n\\cdots \\\\\n\\mu_{p}\n\\end{array} \\right\\rbrack\\)\n(공분산행렬) \\(COV(\\underset{¯}{x}) = \\Sigma = \\begin{bmatrix}\n\\sigma_{11} & \\sigma_{12} & \\cdots & \\sigma_{1p} \\\\\n\\sigma_{21} & \\sigma_{22} & \\cdots & \\sigma_{2p} \\\\\n\\cdots & \\cdots & \\cdots & \\cdots \\\\\n\\sigma_{p1} & \\sigma_{p2} & \\cdots & \\sigma_{pp}\n\\end{bmatrix}\\)\n상수벡터 : \\(\\underset{¯}{a} = \\left\\lbrack \\begin{array}{r}\na_{1},a_{2},\\cdots a_{p}\n\\end{array} \\right\\rbrack\\)\n\\(\\underset{¯}{a}'\\underset{¯}{x}\\)의 평균 : \\(E(\\underset{¯}{a}'\\underset{¯}{x}) = \\underset{¯}{a}'\\underset{¯}{\\mu}\\), 분산 \\(V(\\underset{¯}{a}'\\underset{¯}{x}) = \\underset{¯}{a}'\\underset{¯}{\\Sigma}\\underset{¯}{a}\\)\n\n\n(4) 선형 회귀모형\n\\(\\underset{¯}{y} = X\\underset{¯}{b} + \\underset{¯}{e}\\), \\(\\underset{¯}{e} \\sim N(\\underset{¯}{0},\\sigma^{2}I)\\)\n최소제곱법 추정\n\\[min_{a,b_{1},b_{2},...,b_{p}}\\sum e_{i}^{2} = min_{\\underset{¯}{b}}\\underset{¯}{e}'\\underset{¯}{e}\\]\n\\[Q(\\underset{¯}{b}) = \\underset{¯}{e}'\\underset{¯}{e} = (\\underset{¯}{y} - X\\underset{¯}{b})'(\\underset{¯}{y} - X\\underset{¯}{b}) = \\underset{¯}{y}'\\underset{¯}{y} + \\underset{¯}{b}'X'X\\underset{¯}{b} - 2\\underset{¯}{y}'X\\underset{¯}{b}\\]\n\\(\\frac{\\partial Q}{\\partial\\underset{¯}{b}} = 2X'X\\underset{¯}{b} - 2X'\\underset{¯}{y} = 0\\) ⇢ \\(\\widehat{\\underset{¯}{b}} = (X'X)^{- 1}X'\\underset{¯}{y}\\)\n적합치 fitted values 와 잔차 residuals\n적합치 : \\(\\widehat{\\underset{¯}{y}} = X\\widehat{\\underset{¯}{b}} = X(X'X)^{- 1}X'\\underset{¯}{y} = H\\underset{¯}{y}\\),\n\\(H = X(X'X)^{- 1}X'\\) hat 행렬이라 하고 대칭행렬이고 멱등행렬이다. \\(HH = H,H' = H\\)\n잔차 : \\(\\widehat{\\underset{¯}{e}} = \\underset{¯}{y} - \\widehat{\\underset{¯}{y}} = (I - H)\\underset{¯}{y}\\) \\(H\\)가 멱등행렬이면 \\((I - H)\\)도 멱등행렬이다.\n잔차의 분포 \\(\\widehat{\\underset{¯}{e}} \\sim N(\\underset{¯}{0},\\sigma^{2}I)\\)\n오차의 가정 : \\(\\underset{¯}{e} \\sim N(\\underset{¯}{0},\\sigma^{2}I)\\) ⇢ \\(\\underset{¯}{y} \\sim N(X\\underset{¯}{b},\\sigma^{2}I)\\)\n그러므로 \\(E(\\widehat{\\underset{¯}{e}}) = (I - H)E(\\underset{¯}{y}) = (I - H)(X\\underset{¯}{b}) = (X\\underset{¯}{b} - HX\\underset{¯}{b}) = \\underset{¯}{0}V(\\widehat{\\underset{¯}{e}}) = V((I - H)\\underset{¯}{y}) = (I - H)\\sigma^{2}I(I - H)' = \\sigma^{2}I\\)\n목표변수 분해\n\\(\\underset{¯}{y} = H\\underset{¯}{y} + (I - H)\\underset{¯}{y}\\)=(설명하는 변동) + (설명하지 못하는 변동)\n\n\n\n\n\n높이를 최소화 하는 \\(\\underset{¯}{b}\\)를 구하는 것이 최소제곱추정법이다.\n추정치 분포\n\\(\\widehat{\\underset{¯}{b}} = (X'X)^{- 1}X'\\underset{¯}{y}\\)이고 \\(\\underset{¯}{y} \\sim N(X\\underset{¯}{b},\\sigma^{2}I)\\)이므로\n\\[E(\\widehat{\\underset{¯}{b}}) = (X'X)^{- 1}X'E(\\underset{¯}{y}) = (X'X)^{- 1}X'X\\underset{¯}{b} = \\underset{¯}{b}\\]\n\\[V(\\widehat{\\underset{¯}{b}}) = \\sigma^{2}(X'X)^{- 1}\\]\n\\(\\widehat{\\underset{¯}{b}} \\sim N(\\underset{¯}{b},\\sigma^{2}(X'X)^{- 1})\\), \\({\\widehat{\\sigma}}^{2} = SSE\\)\n변동 분해 ANOVA\n총변동 Total Sum of Squares : \\(SST = \\sum(y_{i} - \\overline{y})^{2}\\)\n\\(SST = \\sum y_{i}^{2} - \\frac{(\\sum y_{i})^{2}}{n} = \\underset{¯}{y}'\\underset{¯}{y} - (\\frac{1}{n})\\underset{¯}{y}'J_{n \\times n}\\underset{¯}{y}\\), \\(J\\)는 1행렬\n\\[SST = \\underset{¯}{y}'(I - (\\frac{1}{n})J)\\underset{¯}{y}\\]\n오차변동 Error Sum of Squares\n\\[SSE = \\sum(y_{i} - \\widehat{y_{i}})^{2}\\]\n\\[SSE = (\\underset{¯}{y} - X\\underset{¯}{b})'(\\underset{¯}{y} - X\\underset{¯}{b}) = \\underset{¯}{y}'\\underset{¯}{y} - \\underset{¯}{b}'X'\\underset{¯}{y} = \\underset{¯}{y}'(I - H)\\underset{¯}{y}\\]\n회귀변동 Regression Sum of Squares\n\\(SSR = \\sum(\\widehat{y_{i}} - \\overline{y})^{2}\\), \\(SSR = \\underset{¯}{y}'(H - (\\frac{1}{n})J)\\underset{¯}{y}\\)\n\\[SSR = SST - SSE = \\underset{¯}{b}X'\\underset{¯}{y} - (\\frac{1}{n})\\underset{¯}{y}'J\\underset{¯}{y}\\]\n결정계수\n\\(R^{2} = \\frac{SSR}{SST} = 1 - \\frac{SSE}{SST}\\) : 모형의 총변동 설명 비중\nSSE, SSR 분포 및 \\(\\sigma^{2}\\) 추정량\n\\(\\underset{¯}{x} \\sim N(\\underset{¯}{\\mu},\\Sigma)\\) 이면 이차형식 \\(\\underset{¯}{x}'A\\underset{¯}{x}\\)의 평균은\n\\(E(\\underset{¯}{x}'A\\underset{¯}{x}) = tr(A\\Sigma) + \\mu'A\\mu\\)이다.\n\\(\\underset{¯}{x} \\sim N(\\underset{¯}{\\mu},\\sigma^{2}I)\\) 이면 이차형식 \\(\\underset{¯}{x}'A\\underset{¯}{x}\\)(\\(A\\) 대칭행렬이고 멱등행렬이면)에 대하여 \\(\\frac{\\underset{¯}{x}'A\\underset{¯}{x}}{\\sigma^{2}} \\sim \\chi^{2}(df = rank(A))\\)이다.\n\\(SSE = \\underset{¯}{y}'(I - H)\\underset{¯}{y}\\), 이차형식이고 \\((I - H)\\)는 멱등행렬\n\\(rank(I - H) = n - p - 1\\)이므로 \\(\\frac{SSE}{\\sigma^{2}} \\sim \\chi^{2}(n - p - 1)\\)이다.\n오차 분산의 추정량: \\(\\widehat{\\sigma^{2}} = MSE\\).\n\\(\\frac{SSR}{\\sigma^{2}} \\sim \\chi^{2}(p)\\), \\(F = \\frac{SSR/p}{SSE/(n - p - 1)} \\sim F(p,n - p - 1)\\)\n분산분석 표\n\n\n\n\n\n\n\n\n\n\n변동\n제곱변동\n자유도\n평균제곱\nF\n\n\n\n\n회귀\n\\[SSR\\]\n\\[p\\]\n\\[MSR = \\frac{SSR}{p}\\]\n\\[\\frac{MSR}{MSE}\\]\n\n\n오차\n\\[SSE\\]\n\\[n - p - 1\\]\n\\[MSE = \\frac{SSE}{n - p - 1}\\]\n\n\n총변동\n\\[SST\\]\n\\[n - 1\\]\n\\[{E(MSE) = \\sigma^{2}\n}{E(MSR) = \\sigma^{2} + b_{1}^{2}\\sum(x_{i} - \\overline{x})^{2}}\\]"
  },
  {
    "objectID": "notes/mldl/mldl_concepts02.html",
    "href": "notes/mldl/mldl_concepts02.html",
    "title": "MLDL AI 통계학 part02",
    "section": "",
    "text": "Chapter 7. 전통적 통계 vs 머신러닝: 방법론 비교\n\n\n\n\n\n\n\n\n구분\n전통적 통계방법론\n머신러닝\n\n\n핵심 목적\n추론, 설명\n예측, 성능\n\n\n출발점\n확률모형과 가정\n데이터와 손실함수\n\n\n해석 가능성\n높음\n제한적\n\n\n데이터 규모\n소표본에 강함\n대용량에 강함\n\n\n불확실성 표현\n신뢰구간, 검정\n성능 기반 간접 평가\n\n\n\n\n1. 추론과 예측의 목적 차이\n전통적 통계방법론의 1차적 목적은 추론이다. 제한된 표본 자료로부터 모집단의 특성, 즉 모수의 값이나 변수 간의 관계를 추론하고, 그 추론 결과가 어느 정도의 불확실성을 가지는지를 함께 제시하는 것이 핵심이다. 이에 따라 통계학적 분석에서 던지는 질문은 모집단 평균은 얼마인가, 특정 요인의 효과는 존재하는가, 그 효과는 어느 범위 내에서 신뢰할 수 있는가와 같은 형태를 띤다.\n반면 머신러닝의 핵심 목적은 예측 성능에 있다. 모델이 왜 그러한 결과를 산출하는지보다는, 새로운 데이터에 대해 얼마나 정확한 예측을 수행하는지가 우선적인 관심사가 된다. 분석의 질문 역시 다음 관측값을 얼마나 정확히 맞출 수 있는가, 예측 오차를 최소화하는 함수는 무엇인가로 전환된다.\n이러한 목적의 차이는 방법론의 철학을 규정한다. 전통적 통계학은 설명 가능하고 해석 가능한 결론을 중시하는 반면, 머신러닝은 실용적 정확도와 성능을 중심에 둔다.\n\n\n2. 가정 기반 모델 vs 데이터 기반 모델\n전통적 통계학은 확률모형과 가정을 분석의 출발점으로 삼는다. 정규성, 독립성, 동일분포와 같은 가정은 현실을 단순화한 것이지만, 분석 결과를 해석하고 모집단으로 일반화할 수 있는 이론적 기반을 제공한다. 통계학에서 가정은 숨겨진 약점이 아니라, 결과 해석의 전제 조건으로 명시된다.\n이에 비해 머신러닝은 가정을 최소화하고, 데이터로부터 직접 규칙을 학습하는 접근을 취한다. 특정 분포를 가정하기보다는 손실함수와 검증 성능을 기준으로 모델의 적절성을 판단한다. 이 차이는 다음과 같이 요약할 수 있다. 통계학은 가정이 명시적이며 그 가정 위에서 결과를 해석하고, 머신러닝은 가정이 암묵적으로 모델 구조와 학습 과정에 내재되어 있으며 성능을 통해 결과를 평가한다.\n즉, 머신러닝은 가정이 없는 방법론이 아니라, 가정을 다른 방식으로 표현하는 방법론이라 할 수 있다.\n\n\n3. 해석 가능성과 성능 중심 접근\n전통적 통계모형은 해석 가능성을 핵심 가치로 둔다. 회귀계수의 부호와 크기, 신뢰구간, 가설검정 결과는 모두 변수의 역할과 효과를 설명하기 위한 장치이다. 이러한 해석 구조는 과학적 설명이나 정책적 의사결정에서 중요한 역할을 한다.\n머신러닝은 성능 중심 접근을 취한다. 비선형 모델이나 앙상블 기법, 딥러닝 모델은 해석이 어렵지만, 복잡한 패턴을 포착하여 예측 정확도를 크게 향상시킬 수 있다. 이로 인해 해석 가능성과 성능 사이의 긴장이 발생한다. 통계학은 해석은 명확하지만 복잡한 구조를 포착하는 데 한계가 있으며, 머신러닝은 성능은 뛰어나지만 결과 해석이 제한적이다.\n최근의 설명가능한 AI 연구는 이러한 간극을 줄이기 위한 시도로 이해할 수 있다.\n\n\n4. 소표본 환경과 대용량 데이터 환경\n전통적 통계방법론은 소표본 환경에서도 안정적인 추론이 가능하도록 발전해 왔다. 제한된 데이터에서 효율적인 추정량과 검정 이론을 통해 의미 있는 결론을 도출하는 데 강점을 가진다.\n반면 머신러닝은 대용량 데이터 환경에서 진가를 발휘한다. 데이터의 양이 증가할수록 복잡한 모델의 일반화 성능이 개선되며, 개별 가정의 중요성은 상대적으로 감소한다. 이 차이는 실제 적용에서 중요한 선택 기준이 된다. 데이터가 적고 해석이 중요한 경우에는 통계학적 접근이 적합하며, 데이터가 많고 예측 정확도가 중요한 경우에는 머신러닝 접근이 효과적이다.\n\n\n5. 재현성, 검정, 불확실성 표현의 차이\n전통적 통계학은 분석 결과의 재현성과 검증 가능성을 중시한다. 동일한 가정과 절차를 적용하면 유사한 결론에 도달해야 하며, 유의확률과 신뢰구간은 불확실성을 공식적으로 표현하는 도구로 사용된다.\n머신러닝에서는 재현성이 데이터 분할 방식, 초기값 설정, 학습 알고리즘의 확률성에 의해 영향을 받는 경우가 많다. 또한 불확실성은 명시적으로 제공되지 않는 경우가 일반적이며, 주로 성능 지표의 평균이나 분산을 통해 간접적으로 평가된다. 최근에는 예측 불확실성을 정량화하려는 다양한 시도가 이루어지고 있으나, 이는 여전히 전통적 통계학이 강점을 가지는 영역으로 남아 있다.\n\n\n\nChapter 8. 통계학 관점에서 본 AI 모델 평가\n\n\n\n\n\n\n\n평가 관점\n핵심 질문\n\n\n일반화\n새로운 데이터에서도 성능이 유지되는가\n\n\n변동성\n성능의 분산은 어느 정도인가\n\n\n비교\n차이가 통계적으로 의미 있는가\n\n\n단순성\n더 단순한 모델로 대체 가능한가\n\n\n공정성\n특정 집단에 체계적 불이익이 있는가\n\n\n\n\n1. 학습 성능과 일반화 성능\n머신러닝과 딥러닝 모델의 성능은 일반적으로 학습 성능과 일반화 성능으로 구분된다. 학습 성능은 학습 데이터에서 모델이 얼마나 잘 적합되는지를 나타내며, 일반화 성능은 학습에 사용되지 않은 새로운 데이터에서의 예측 능력을 의미한다.\n통계학적 관점에서 중요한 것은 학습 성능이 아니라 일반화 성능이다. 학습 데이터에서의 높은 적합도는 단순히 표본을 잘 설명했다는 의미에 불과하며, 모집단에 대한 타당한 추론이나 예측을 보장하지 않는다. 학습 성능이 높음에도 불구하고 일반화 성능이 낮다면, 이는 표본의 우연적 특성을 과도하게 학습한 결과로 해석할 수 있다.\n이러한 의미에서 일반화 성능은 통계학에서 말하는 외삽 가능성, 즉 표본에서 얻은 결과를 모집단이나 미래 상황으로 확장할 수 있는지 여부와 직접적으로 연결된다. AI 모델 평가의 핵심은 결국 새로운 데이터에 대해 얼마나 안정적인 성능을 보이는가에 있다.\n\n\n\n\n\n\n\n2. 교차검증과 재표본추출 방법\n통계학은 오래전부터 표본 변동성을 이해하고 추정의 안정성을 평가하기 위해 재표본추출 기법을 발전시켜 왔다. 머신러닝에서 널리 사용되는 교차검증은 이러한 통계적 아이디어의 직접적인 확장이라 할 수 있다.\n대표적인 방법으로는 데이터를 여러 부분으로 나누어 반복적으로 학습과 평가를 수행하는 k-겹 교차검증이 있다. 또한 표본을 복원추출하여 통계량의 분포를 평가하는 부트스트랩 기법은 성능 지표의 변동성을 이해하는 데 유용하다. 반복 교차검증은 데이터 분할에 따른 우연성을 줄이고, 보다 안정적인 성능 평가를 가능하게 한다.\n통계학적 관점에서 교차검증의 중요성은 단일한 성능 값이 아니라 성능의 분포를 관찰할 수 있다는 데 있다. 이는 성능 평가를 하나의 숫자로 고정하는 것이 아니라, 불확실성을 포함한 추론 문제로 다루게 해 준다. 결국 교차검증은 AI 모델 평가에서 통계학의 핵심 원리인 변동성 인식을 구현하는 장치라 할 수 있다.\n\n\n3. 성능지표의 통계적 해석\nAI 모델의 성능은 정확도, 평균제곱근오차, AUC 등 다양한 지표로 요약된다. 그러나 통계학적으로 더 중요한 질문은 성능 지표의 크기 자체가 아니라, 관측된 성능 차이가 우연에 의한 것인지 아니면 체계적인 차이인지를 판단하는 것이다.\n예를 들어 두 모델의 정확도가 각각 0.92와 0.93이라 하더라도, 표본 크기가 작거나 데이터 변동성이 크다면 이 차이는 통계적으로 의미가 없을 수 있다. 따라서 성능지표는 점추정치로만 해석되어서는 안 되며, 반복 평가를 통한 분산, 신뢰구간, 그리고 모델 간 성능 비교 검정과 함께 해석되어야 한다.\n이는 전통적 통계학의 ”추정값과 불확실성은 함께 제시되어야 한다”는 원칙이 AI 모델 평가에도 그대로 적용됨을 의미한다. 성능 지표를 절대적인 기준으로 받아들이는 것은 통계적 사고에 반하는 접근이다.\n\n\n4. 과적합 진단과 모델 비교\n과적합은 통계학과 머신러닝을 관통하는 공통된 문제이다. 통계학에서는 이를 모형의 복잡도와 표본 크기 사이의 불균형 문제로 해석해 왔다. 머신러닝에서도 과적합은 동일한 구조를 가지며, 학습 성능과 일반화 성능의 괴리로 나타난다.\n과적합을 의심할 수 있는 대표적인 신호로는 학습 데이터에서는 높은 성능을 보이지만 검증 데이터에서는 성능이 크게 저하되는 경우, 그리고 모델의 복잡도가 증가할수록 성능의 변동성이 확대되는 경우가 있다. 이러한 상황에서는 모델이 데이터의 구조를 학습한 것이 아니라, 우연적 잡음을 학습했을 가능성이 크다.\n모델 비교 역시 단순히 가장 높은 성능을 보이는 모델을 선택하는 문제로 접근해서는 안 된다. 동일한 조건에서의 반복 평가, 성능 차이의 안정성, 그리고 더 단순한 기준 모델과의 비교를 통해 판단해야 한다. 통계학적 시각에서 볼 때, 조금 더 복잡한 모델은 항상 추가적인 검증이 요구되는 의심의 대상이다.\n\n\n5. 편향, 공정성, 데이터 누락 문제\nAI 모델 평가는 기술적 성능에 국한되지 않으며, 사회적·통계적 타당성까지 포함한다. 특히 편향, 공정성, 데이터 누락 문제는 통계학의 전통적인 핵심 관심사이자, 현대 AI 평가에서 더욱 중요한 이슈로 부각되고 있다.\n편향은 표본이 모집단을 제대로 대표하지 못할 때 발생하며, 이는 높은 예측 성능에도 불구하고 체계적으로 왜곡된 결과를 낳을 수 있다. 공정성 문제는 특정 집단에 대해 예측 오류가 불균등하게 발생하는 구조를 의미하며, 평균적인 성능 지표만으로는 포착되지 않는 경우가 많다. 또한 비무작위 결측에 따른 데이터 누락은 모델 학습 과정에서 심각한 왜곡을 초래할 수 있다.\n통계학은 이러한 문제를 표본설계, 가중치 조정, 결측 메커니즘 분석 등을 통해 체계적으로 다뤄 왔다. AI 모델 역시 이러한 점검 없이 적용될 경우, ”정확하지만 불공정한 모델”이 될 위험을 안고 있다. 이 점에서 통계학적 사고는 AI 모델 평가에서 선택이 아니라 필수적인 기준이라 할 수 있다.\n\n\n\nChapter 9. 통계방법론의 AI·ML 확장 사례\n\n1. 회귀분석에서 Lasso, Ridge, Elastic Net으로의 확장\n전통적 선형회귀는 다음과 같은 최소제곱 문제로 표현된다.\n\\[\\widehat{\\beta} = \\arg\\min_{\\beta}\\overset{n}{\\sum_{i = 1}}(y_{i} - x_{i}^{\\top}\\beta)^{2}\\]\n이 추정량은 설명과 추론에 강점을 가지지만, 설명변수의 수가 많거나 다중공선성이 존재하는 고차원 환경에서는 분산이 크게 증가하여 불안정해질 수 있다.\n이를 보완하기 위해 정규화가 도입되었다. Ridge 회귀는 계수의 제곱합에 패널티를 부과한다.\n\\[{\\widehat{\\beta}}_{\\text{Ridge}} = \\arg\\min_{\\beta}\\left\\{ \\overset{n}{\\sum_{i = 1}}(y_{i} - x_{i}^{\\top}\\beta)^{2} + \\lambda\\overset{p}{\\sum_{j = 1}}\\beta_{j}^{2} \\right\\}\\]\n이는 모든 계수를 전반적으로 축소하여 분산을 감소시키는 효과를 가진다.\nLasso 회귀는 절댓값 패널티를 사용한다.\n\\[{\\widehat{\\beta}}_{\\text{Lasso}} = \\arg\\min_{\\beta}\\left\\{ \\overset{n}{\\sum_{i = 1}}(y_{i} - x_{i}^{\\top}\\beta)^{2} + \\lambda\\overset{p}{\\sum_{j = 1}}|\\beta_{j}| \\right\\}\\]\n이 경우 일부 계수가 정확히 0이 되어 변수 선택 효과가 발생한다.\nElastic Net은 두 패널티를 결합한다.\n\\[{\\widehat{\\beta}}_{\\text{EN}} = \\arg\\min_{\\beta}\\left\\{ \\overset{n}{\\sum_{i = 1}}(y_{i} - x_{i}^{\\top}\\beta)^{2} + \\lambda_{1}\\sum_{j}|\\beta_{j}| + \\lambda_{2}\\sum_{j}\\beta_{j}^{2} \\right\\}\\]\n통계학적으로 이들 방법은 편향을 허용하는 대신 분산을 줄이는 벌점화 추정이며, 머신러닝에서는 고차원 예측 문제의 표준적 해결책으로 활용된다. 즉, 회귀분석은 AI 시대에도 여전히 핵심이며, 정규화가 그 확장 장치이다.\n\n\n2. 분류모형에서 로지스틱 회귀와 신경망으로의 확장\n로지스틱 회귀는 이진 반응변수 \\(Y \\in \\{ 0,1\\}\\)에 대해 다음과 같은 확률모형을 가정한다.\n\\[P(Y = 1 \\mid x) = \\sigma(x^{\\top}\\beta) = \\frac{1}{1 + \\exp( - x^{\\top}\\beta)}\\]\n이는 명확한 확률 해석을 제공하는 판별형 모델이다.\n최대우도추정은 교차엔트로피 손실 최소화 문제와 동치이다.\n\\[\\widehat{\\beta} = \\arg\\min_{\\beta}\\overset{n}{\\sum_{i = 1}}\\left\\lbrack y_{i}\\log p_{i} + (1 - y_{i})\\log(1 - p_{i}) \\right\\rbrack\\]\n신경망 분류기는 이 구조를 다층으로 일반화한다.\n\\[h^{(l)} = \\phi(W^{(l)}h^{(l - 1)} + b^{(l)}),p = \\text{softmax}(W^{(L)}h^{(L - 1)})\\]\n중요한 점은 출력층의 소프트맥스 구조와 교차엔트로피 손실이 로지스틱 회귀와 동일한 확률적 해석을 유지한다는 사실이다. 차이는 표현력에 있으며, 신경망은 복잡한 비선형 결정경계를 학습하는 대신 해석 가능성은 감소한다.\n\n\n3. 차원축소에서 PCA와 오토인코더로의 확장\n주성분분석은 다음 최적화 문제로 정의된다.\n\\[\\max_{W^{\\top}W = I}Var(XW) \\leftrightarrow \\min_{W} \\parallel X - XWW^{\\top} \\parallel_{F}^{2}\\]\n이는 분산을 최대한 보존하는 선형 투영이다.\n오토인코더는 이를 비선형으로 확장한다.\n\\[z = f_{\\theta}(x),\\widehat{x} = g_{\\phi}(z)\\min_{\\theta,\\phi}\\overset{n}{\\sum_{i = 1}} \\parallel x_{i} - {\\widehat{x}}_{i} \\parallel^{2}\\]\n통계학적으로 오토인코더는 비선형 PCA로 해석할 수 있으며, 선형 가정으로 설명하기 어려운 고차원·비선형 구조를 효과적으로 요약한다. 다만 잠재공간의 해석은 PCA에 비해 상대적으로 어렵다.\n\n\n\n\n\n\n\n4. 군집분석에서 k-means와 표현 기반 군집으로의 확장\nk-means는 다음 목적함수를 최소화한다.\n\\[\\min_{\\{ C_{k}\\}}\\overset{K}{\\sum_{k = 1}}\\sum_{i \\in C_{k}} \\parallel x_{i} - \\mu_{k} \\parallel^{2}\\]\n이는 거리 기반, 저차원 가정에 효과적인 방법이다.\nAI·ML 환경에서는 군집 이전에 표현학습을 결합한다.\n\\[z_{i} = f_{\\theta}(x_{i}),\\text{cluster}(z_{i})\\]\n또는 오토인코더의 잠재공간에서 군집을 수행한다.\n이는 군집이 단순히 거리의 문제가 아니라, 어떤 표현 공간에서 거리를 정의할 것인가의 문제로 확장되었음을 의미한다. 군집의 본질은 유지되되, 표현학습이 핵심 전처리 단계로 결합된다.\n\n\n5. 시계열 분석과 딥러닝 모델의 접점\n전통적 시계열 분석은 자기회귀 구조를 명시적으로 가정한다.\n\\[y_{t} = \\phi_{1}y_{t - 1} + \\cdots + \\phi_{p}y_{t - p} + \\varepsilon_{t}\\]\n이는 해석과 추론에 강점을 가진다.\n딥러닝 기반 시계열 모델은 이를 비선형·고차원으로 확장한다.\n\\[h_{t} = f(h_{t - 1},x_{t}),y_{t} = g(h_{t})\\]\n이 구조는 장기 의존성, 다변량 입력, 비정상성, 외생 변수 효과를 자연스럽게 통합한다.\n통계학적으로 이는 자기회귀 구조의 비선형 일반화로 해석할 수 있다. 예측 성능은 향상될 수 있으나, 모형 해석과 불확실성 표현은 별도의 장치가 필요하다.\n\n\n\nChapter 10. 책임 있는 AI와 통계적 검증\n책임 있는 AI는 기술적 성능의 문제가 아니라, 통계적 사고의 문제이다. AI 모델을 평가하고 활용하는 과정에서 통계학이 제공하는 추론, 불확실성, 검증, 편향 점검의 원리는 여전히 중심적인 역할을 수행한다. 통계학은 AI의 한계를 드러내는 비판자가 아니라, AI가 사회적으로 신뢰받을 수 있도록 만드는 이론적 기반이다.\n결국 AI 시대에 요구되는 핵심 역량은 새로운 알고리즘을 아는 것이 아니라, 그 알고리즘의 결과를 어디까지 믿을 수 있는지를 판단할 수 있는 통계적 검증 능력이라 할 수 있다.\n\n1. 책임 있는 AI의 개념과 필요성\nAI 시스템이 사회 전반에 광범위하게 적용되면서, 단순히 성능이 높은 모델을 넘어 책임 있는 AI의 필요성이 강조되고 있다. 책임 있는 AI란 예측 정확도나 효율성뿐 아니라, 공정성, 투명성, 재현성, 그리고 사회적 영향까지 함께 고려하는 AI 시스템을 의미한다.\n특히 정책 결정, 행정 서비스, 금융·의료 분야와 같이 개인과 집단의 삶에 직접적인 영향을 미치는 영역에서는, AI의 판단이 정당하고 검증 가능해야 한다. 이때 ”모델이 잘 맞는다”는 사실만으로는 충분하지 않으며, 그 결과가 얼마나 신뢰할 수 있는지, 어떤 조건에서 유효한지, 누구에게 불리하게 작동할 가능성은 없는지에 대한 점검이 필수적이다.\n이러한 요구는 본질적으로 통계학이 오랫동안 다뤄 온 문제들과 맞닿아 있다.\n\n\n2. 성능 중심 평가의 한계와 통계적 검증의 역할\n머신러닝과 딥러닝에서는 종종 단일 성능 지표의 개선이 모델 우수성의 근거로 제시된다. 그러나 통계학적 관점에서 볼 때, 단일 수치의 성능은 항상 표본 변동성과 우연성을 내포하고 있다. 성능 향상이 관측되었다 하더라도, 그것이 우연에 의한 결과인지 체계적인 개선인지는 별도의 검증 없이는 판단할 수 없다.\n통계적 검증은 이러한 문제에 대한 해답을 제공한다. 반복 실험, 교차검증, 재표본추출을 통해 성능의 분포를 평가하고, 성능 차이에 대한 불확실성을 함께 제시함으로써 모델 평가를 추론의 문제로 다룬다. 이는 AI 모델의 성능을 절대적인 진리로 받아들이는 대신, 제한된 데이터에서 관측된 결과로 해석하도록 만든다.\n책임 있는 AI란 결국 ”성능 수치에 겸손한 AI”라고도 볼 수 있다.\n\n\n3. 편향과 공정성 문제에 대한 통계학적 시각\nAI 모델의 편향과 공정성 문제는 기술적 오류라기보다 데이터와 추론 구조의 문제로 이해하는 것이 적절하다. 특정 집단에 대한 예측 오류가 체계적으로 다르게 나타난다면, 이는 표본 설계, 데이터 누락, 혹은 모델 구조에서 비롯된 통계적 문제일 가능성이 높다.\n통계학은 이러한 문제를 오래전부터 다뤄 왔다. 표본이 모집단을 대표하지 못할 때 발생하는 편향, 비무작위 결측에 따른 추정 왜곡, 하위 집단 분석에서의 불안정성 등은 모두 전통적인 통계적 주제이다. AI 모델 역시 이러한 점검 없이 사용될 경우, 평균적으로는 정확하지만 특정 집단에는 구조적으로 불리한 결과를 초래할 수 있다.\n따라서 책임 있는 AI는 공정성 지표의 계산을 넘어, 데이터 생성 과정과 표본 구조에 대한 통계적 검토를 포함해야 한다.\n\n\n4. 불확실성 표현과 의사결정 책임\n전통적 통계학의 중요한 특징 중 하나는 불확실성을 숨기지 않는다는 점이다. 신뢰구간, 분산, 유의확률은 분석 결과가 어디까지 신뢰 가능한지를 함께 제시한다. 이는 분석 결과를 의사결정의 보조 자료로 사용할 때 중요한 안전장치로 작동한다.\n반면 많은 AI 모델은 점 예측이나 단일 분류 결과만을 제공하며, 그 결과의 불확실성은 명시적으로 드러나지 않는 경우가 많다. 이러한 상황에서 AI의 판단이 절대적인 것으로 받아들여질 위험이 커진다.\n통계적 검증과 불확실성 표현은 AI 결과를 ”결정”이 아닌 ”근거 있는 제안”으로 위치시키는 역할을 한다. 이는 책임 있는 의사결정을 가능하게 하는 핵심 요소이다.\n\n\n5. 재현성과 검증 가능성\n책임 있는 AI에서 또 하나 중요한 요소는 재현성이다. 동일한 데이터와 절차를 사용했을 때 유사한 결과가 도출되어야 하며, 모델의 성능과 판단 과정은 외부에서 검증 가능해야 한다.\n통계학은 분석 절차의 명시, 가정의 공개, 검증 방법의 표준화를 통해 재현성을 확보해 왔다. AI 모델 역시 데이터 분할 방식, 학습 과정의 무작위성, 하이퍼파라미터 선택 기준 등을 투명하게 공개하지 않는다면, 그 결과를 신뢰하기 어렵다.\n이 점에서 통계적 검증은 책임 있는 AI의 사후 점검 도구가 아니라, 설계 단계부터 내재되어야 할 핵심 원리라 할 수 있다.\n\n\n\nChapter 11. 통합적 시각: 통계학과 AI의 공존\n\n1. 통계학은 AI의 기초인가, 대안인가\n통계학을 인공지능의 기초 학문으로만 이해하는 것은 충분하지 않다. 확률, 추정, 최적화, 일반화 이론이 AI 알고리즘의 수학적 토대를 이루고 있음은 분명하지만, 통계학의 역할은 그에 그치지 않는다. 통계학은 AI를 만들어내는 기반인 동시에, AI의 결과를 평가하고 통제하는 독립적인 방법론이다.\n기초로서의 통계학은 손실함수와 확률모형의 대응 관계, 학습과 추정의 동형성, 일반화 이론과 표본 이론의 연결을 통해 AI 알고리즘의 수학적 정합성을 제공한다. 반면 대안으로서의 통계학은 설명과 추론이 요구되는 문제, 소표본 환경에서의 고신뢰 분석, 정책·공공 의사결정과 같이 책임성과 검증이 중요한 영역에서 여전히 우선적인 선택지가 된다.\n따라서 통계학은 AI의 전 단계에 위치한 과거의 학문이 아니라, AI와 병렬적으로 공존하며 상호 보완적인 역할을 수행하는 분석 패러다임이다.\n\n\n2. AI 시대의 통계적 사고\nAI 시대에 통계적 사고의 중요성은 오히려 더욱 커졌다. 모델은 점점 복잡해지고, 데이터는 대규모화되며, 알고리즘은 자동화되고 있지만, 다음과 같은 질문은 여전히 인간의 판단을 요구한다.\n이 데이터는 무엇을 대표하는가.\n이 성능은 얼마나 안정적인가.\n이 결과는 우연에 의해 나타났을 가능성이 있는가.\n다른 집단이나 상황에서도 동일하게 작동하는가.\n이 질문들은 모두 전통적 통계학이 오랫동안 다뤄온 핵심 문제들이다. 이는 통계적 사고가 특정 알고리즘에 종속된 기술이 아니라, 알고리즘 이전에 존재하는 사고 체계임을 보여준다. AI는 이러한 사고를 계산적으로 구현하는 도구일 뿐이며, 통계적 판단을 대체하는 존재가 아니다.\n\n\n3. 방법론 통합을 위한 교육 방향\nAI 시대의 통계 교육은 단순히 머신러닝 과목을 추가하는 방식으로는 충분하지 않다. 필요한 것은 교육 구조 자체의 재설계이다. 통계 이론과 머신러닝 알고리즘을 분리하여 가르치기보다는, 두 방법론의 대응 관계와 공통 수학적 기반을 함께 제시해야 한다.\n이를 위해 추정과 검정을 학습과 검증의 관점에서 재해석하고, 수식 중심의 이론 교육과 계산 실험 기반의 교육을 병행할 필요가 있다. 특히 중요한 것은 학생들이 특정 방법을 기계적으로 적용하는 것이 아니라, 주어진 문제에 대해 왜 통계적 접근이 필요한지, 언제 머신러닝이나 딥러닝이 더 적절한지를 스스로 판단할 수 있도록 훈련하는 것이다.\n통계 교육의 목표는 알고리즘 숙련이 아니라 분석 판단 능력의 함양에 있어야 한다.\n\n\n4. 책임 있는 AI와 통계적 검증\nAI의 사회적 영향력이 커질수록, 그 책임은 기술 자체가 아니라 이를 설계하고 사용하는 인간에게 귀속된다. 이때 통계학은 책임 있는 AI를 구현하는 데 필요한 핵심 언어와 도구를 제공한다.\n통계학은 성능의 불확실성을 정량화하고, 편향과 공정성을 측정하며, 데이터 누락과 대표성 문제를 진단하고, 모델 비교의 정당성을 검증하는 체계를 이미 갖추고 있다. 이러한 검증 절차 없이는 AI 모델은 높은 정확도를 가지더라도 사회적으로 신뢰받기 어렵다.\n즉, 책임 있는 AI란 단순히 정확한 AI가 아니라, 검증 가능하고 설명 가능한 AI이며, 그 검증의 언어가 바로 통계학이다.\n\n\n5. 향후 데이터 과학자의 핵심 역량\n미래의 데이터 과학자에게 요구되는 핵심 역량은 특정 알고리즘에 대한 숙련도가 아니다. 중요한 것은 다음과 같은 판단 능력이다.\n첫째, 문제 정의 능력이다. 이는 분석 문제를 확률적·통계적 질문으로 재정의할 수 있는 능력을 의미한다.\n둘째, 방법 선택 판단력이다. 통계학, 머신러닝, 딥러닝 중 무엇이 목적에 부합하는지를 구분할 수 있어야 한다.\n셋째, 검증과 평가 설계 능력이다. 일반화, 비교, 불확실성을 함께 고려한 평가가 가능해야 한다.\n넷째, 윤리와 공정성에 대한 감수성이다. 편향과 책임 문제를 기술 외부의 문제가 아니라 분석의 일부로 인식해야 한다.\n다섯째, 통합적 사고력이다. 이론, 계산, 응용을 분절하지 않고 하나의 분석 흐름으로 연결할 수 있어야 한다."
  },
  {
    "objectID": "notes/mldl/mldl_uncertainty.html",
    "href": "notes/mldl/mldl_uncertainty.html",
    "title": "MLDL 불학실성",
    "section": "",
    "text": "Chapter 1. ML 예측에는 왜 신뢰구간이 없는가\n머신러닝 예측 결과에는 보통 신뢰구간이 함께 제시되지 않는다. 이는 머신러닝이 불완전해서가 아니라, 전통적 신뢰구간이 성립하는 통계적 전제와 머신러닝 예측의 목적이 서로 다르기 때문이다. 신뢰구간은 모수 추론을 위한 도구인 반면, 머신러닝은 모수보다 예측 함수와 그 성능을 목표로 한다.\n이 장에서는 전통적 신뢰구간이 어떤 전제 위에서 정의되는지 정리하고, 머신러닝 예측에서 다루는 불확실성의 성격이 왜 근본적으로 다른지를 명확히 구분한다. 이를 통해 머신러닝 예측에 신뢰구간이 붙지 않는 이유를 결핍이 아닌 설계 철학의 차이로 이해한다.\n\n전통적 신뢰구간은 모수 추론을 위한 장치이다.\n머신러닝 예측은 모수가 아니라 함수와 성능을 목표로 한다.\n따라서 머신러닝 예측에는 전통적 의미의 신뢰구간이 붙지 않는다.\n머신러닝의 불확실성은 데이터·모델·절차의 결합으로 나타난다.\n불확실성 표현은 신뢰구간이 아닌 다른 도구로 이루어진다.\n\n\n1. 전통적 신뢰구간의 전제\n\n신뢰구간은 무엇에 대한 구간인가\n전통적 신뢰구간은 모수에 대한 구간이다. 예를 들어 선형회귀에서 관심 대상은 회귀계수 \\beta이며, 신뢰구간은 다음 질문에 답한다.\n같은 표본추출과 추정 절차를 무한히 반복했을 때, 이 구간이 참값 \\(\\beta\\)를 포함할 장기적 빈도는 얼마인가.\n즉, 신뢰구간은 단일 데이터셋에 대한 불확실성 진술이 아니라, 반복 표본추출이라는 가상의 실험을 전제로 한 확률적 진술이다.\n\n\n신뢰구간이 성립하기 위한 핵심 전제\n신뢰구간이 통계적으로 정당하려면 다음 전제들이 필요하다.\n첫째, 명시적 확률모형이 존재해야 한다.\n둘째, 모수는 고정된 상수이며 확률적 대상은 추정량이다.\n셋째, 추정량의 분포 이론이 확보되어야 한다. 정규근사, 점근성, 분산 추정 가능성이 여기에 포함된다.\n넷째, 표본 설계가 독립성 가정을 만족해야 한다. 일반적으로 i.i.d. 혹은 그에 준하는 구조가 요구된다.\n이 전제들이 충족될 때에만 \\hat{\\beta}의 표본분포가 정의되고, 그 위에서 신뢰구간이 구성된다.\n\n\n예측구간과의 구분\n전통 통계에서도 신뢰구간과 예측구간은 구분된다. 신뢰구간은 모수의 불확실성을 다루는 반면, 예측구간은 새로운 관측값의 불확실성을 다룬다. 그러나 예측구간 역시 명시적 오차분포와 모형 적합의 정당성을 전제로 한다는 점에서, 전통적 확률모형의 틀 안에 있다.\n\n\n\n2. ML 예측과 불확실성의 차이\n\n머신러닝의 1차 목표는 모수가 아니다\n머신러닝의 학습 목표는 다음과 같이 표현된다.\n\\[\\min_{f}\\mathbb{E}\\lbrack L(Y,f(X))\\rbrack\\]\n여기서 관심 대상은 특정 모수가 아니라 예측 함수 f의 일반화 성능이다. 즉, 머신러닝은 추론보다 예측을 우선하며, 모수 해석은 부차적이거나 존재하지 않는다.\n\n\n머신러닝에서 확률적 대상이 달라지는 이유\n전통 통계에서는 모수를 고정하고 추정량을 확률변수로 다룬다. 반면 머신러닝에서는 예측 함수 자체가 데이터, 알고리즘, 초기값, 미니배치, 최적화 경로에 의존한다. 학습 절차 전체가 비선형이고 비볼록이기 때문에, 함수 f의 표본분포를 정의하는 것은 어렵거나 의미가 없다.\n따라서 전통적 의미의 추정량 분포라는 개념이 성립하지 않는다.\n\n\n머신러닝에서의 불확실성 구성 요소\n머신러닝에서의 불확실성은 다음 세 가지로 나누어 이해할 수 있다.\n첫째, 데이터 불확실성은 관측 잡음이나 내재적 변동성에 해당한다.\n둘째, 모델 불확실성은 데이터 부족이나 함수 선택의 불확실성에서 비롯된다.\n셋째, 절차 불확실성은 초기값, 미니배치 구성, 최적화 경로의 차이로 인해 발생한다.\n전통적 신뢰구간은 이 중 일부를 모수 분포로 흡수하지만, 머신러닝에서는 이 모든 요소가 하나의 함수 출력에 복합적으로 반영된다.\n\n\n왜 신뢰구간을 붙이지 않는가가 아니라 붙일 수 없는가\n머신러닝 예측에 신뢰구간이 없는 이유는 다음과 같다.\n모수 중심의 확률 진술이 아니며, 추정량 분포를 정의할 공통 전제가 없고, 알고리즘 의존성이 커서 반복 표본추출 개념이 모호하다.\n이는 머신러닝의 결함이 아니라, 문제 설정과 목표의 차이에서 비롯된 필연적 결과이다.\n\n\n\n3. ML에서 불확실성을 다루는 다른 방식들\n\n재표본 기반 접근\n부트스트랩을 통해 예측의 변동성을 경험적으로 근사할 수 있다. 이는 데이터 민감도를 평가하고 표본 변동성에 따른 예측 분산을 추정하는 방식이다.\n\n\n베이즈적 접근\n베이즈 신경망이나 사후 예측분포를 사용하면, 확률적 예측 구간을 구성할 수 있다. 이는 전통적 신뢰구간과 유사한 형태를 제공하지만, 강한 가정과 높은 계산 비용을 수반한다.\n\n\n실무적 대안\n실무에서는 예측 분위수, 보정된 확률 출력, 신뢰 점수, 구간 예측 등이 활용된다. 이들은 신뢰구간이 아니라 의사결정을 보조하기 위한 불확실성 표현 수단이다.\n\n\n\n4. 흔한 오해 정리\n머신러닝은 불확실성을 무시하지 않는다. 다만 전통적 신뢰구간이 전제하는 불확실성과는 다른 종류의 불확실성을 다룬다. 신뢰구간은 특정 통계적 전제하에서만 의미가 있으며, 그 전제를 벗어난 문제에 적용되지 않을 뿐이다.\n\n\n\nChapter 2. 불확실성 추정 방법\n전통적 신뢰구간이 성립하지 않는다고 해서, 머신러닝 예측에서 불확실성을 전혀 다룰 수 없는 것은 아니다. 다만 머신러닝에서는 불확실성을 모수의 분포가 아니라, 예측의 변동성·민감도·분포 형태로 접근한다. 이는 추론을 위한 확률 진술이 아니라, 예측 결과를 어떻게 사용할 것인가에 대한 정보 제공이라는 점에서 성격이 다르다.\n이 장에서는 실무와 연구에서 가장 널리 사용되는 세 가지 접근, 즉 Bootstrap, Ensemble, 예측 분포 해석을 통계적으로 정리한다. 각 방법이 어떤 종류의 불확실성을 반영하며, 어디까지가 가능하고 어디부터가 한계인지를 명확히 구분한다.\n\n머신러닝에서는 전통적 신뢰구간 대신 다양한 불확실성 추정 방법을 사용한다.\nBootstrap은 데이터 민감도를, Ensemble은 모델 불확실성을 반영한다.\n예측 분포는 점 예측이 숨기는 위험 구조를 드러낸다.\n불확실성 추정은 하나의 수치가 아니라 다층적 해석 문제이다.\n머신러닝의 불확실성은 통계적 진술이 아니라 의사결정 지원 수단이다.\n\n\n1. Bootstrap 기반 접근\n\nBootstrap의 기본 아이디어\nBootstrap은 다음 질문에서 출발한다. 우리가 가진 이 데이터가 모집단의 유일한 근사라면, 다시 표본을 뽑는다면 결과는 얼마나 달라질까. 절차는 단순하다. 원 데이터에서 복원추출로 여러 표본을 만들고, 각 표본마다 동일한 학습 절차를 적용한 뒤, 예측 결과의 분포를 관찰한다.\n\n\n머신러닝 예측에서의 Bootstrap\n관측치 \\(x_{0}\\)에 대한 예측값 \\(\\widehat{f}(x_{0})\\)를 생각하자. Bootstrap은 재표본별 예측 \\({\\widehat{f}}^{*(b)}(x_{0})\\)의 분포를 통해 예측의 분산, 분위수, 범위를 제공한다. 이는 표본 변동성에 대한 경험적 추정으로, 데이터가 달라질 때 예측이 얼마나 흔들리는지를 보여준다.\n\n\n통계적 의미와 한계\nBootstrap이 반영하는 것은 주로 데이터 불확실성과 표본 변동성이다. 반면 모델 구조의 불확실성이나 데이터 생성 과정의 오가정은 반영하지 못한다. 또한 계산 비용이 크고, 고차원이나 딥러닝 환경에서는 불안정해질 수 있다. 즉, Bootstrap은 데이터 민감도를 측정하는 도구이지, 모든 불확실성을 포괄하는 해법은 아니다.\n\n\n\n2. Ensemble 기반 불확실성\n\nEnsemble의 핵심 원리\nEnsemble은 여러 개의 합리적인 모델이 존재할 때, 그들의 예측 차이 자체를 불확실성으로 해석한다. 이는 데이터뿐 아니라 모델 선택과 학습 절차의 불확실성까지 포함한다는 점에서 Bootstrap과 구별된다.\n\n\n대표적 Ensemble 방식\n서로 다른 초기값을 가진 동일 모델, 서로 다른 하이퍼파라미터 설정, 혹은 서로 다른 서브모델을 결합하는 방식이 대표적이다. 관측치 \\(x_{0}\\)에 대해, \\({\\widehat{y}}^{(m)}(x_{0}),m = 1,\\ldots,M\\)의 분산을 계산한다.\n\n\n\n\n\n\n\n통계적 해석\nEnsemble 분산은 주로 모델 불확실성과 최적화·초기값 의존성을 반영한다. 데이터가 적을수록 분산은 커지고, 데이터가 충분해질수록 줄어든다. 이는 더 많은 데이터가 주어질수록 모델 선택의 불확실성이 감소한다는 직관과 일치한다. 다만 데이터 불확실성은 간접적으로만 반영된다.\n\n\n\n3. 예측 분포 해석\n\n점 예측에서 분포 예측으로\n머신러닝 예측을 하나의 값이 아니라 조건부 분포 전체로 확장할 수 있다. 즉, 목표는 \\(\\widehat{y}\\)가 아니라 \\(p(y \\mid x)\\) 혹은 그 근사이다. 이는 예측의 불확실성을 직접적으로 표현한다.\n\n\n분포 기반 접근의 예\n분위수 회귀는 특정 분위수의 예측을 제공하여 비대칭적 위험을 드러낸다. 분포 회귀는 평균과 분산을 동시에 예측한다. 베이즈 신경망은 사후 예측분포를 통해 예측의 변동성을 확률적으로 표현한다. 이들은 전통적 신뢰구간과 달리, 예측이 얼마나 흔들릴 수 있는지를 직접 보여준다.\n\n\n예측 분포 해석의 원칙\n예측 분포를 해석할 때는 다음을 구분해야 한다. 분포의 폭은 불확실성의 크기를 의미하지만, 참값 포함 확률을 보장하지 않는다. 분포의 형태는 위험의 비대칭성을 드러내지만, 모델 가정에 의존한다. 또한 분포 예측은 의사결정 비용 구조와 결합될 때 의미를 갖는다. 즉, 예측 분포는 해석의 종착점이 아니라 판단의 입력이다.\n\n\n\n\n\n\n\n구분\n의미\n\n\n분산이 큼\n예측 불확실성 높음\n\n\n꼬리가 김\n극단값 가능성\n\n\n비대칭\n위험의 방향성\n\n\n다봉\n이질적 패턴 공존\n\n\n\n\n\n\n4. 방법 간 비교와 통합적 관점\n\n불확실성 종류별 대응\nBootstrap은 데이터 민감도와 표본 변동성에 강하며, Ensemble은 모델 선택과 학습 절차의 불확실성을 반영한다. 예측 분포 접근은 위험의 형태와 크기를 직접 표현한다. 어느 하나가 모든 불확실성을 포괄하지는 않는다.\n\n\n\n\n\n\n\n불확실성\n대응 방법\n\n\n표본 변동성\nBootstrap\n\n\n모델 선택\nEnsemble\n\n\n예측 결과\n분포 예측\n\n\n\n\n\n통합적 사용의 원칙\n실무에서는 이들 방법을 배타적으로 선택하기보다 보완적으로 사용한다. 예를 들어, Ensemble로 모델 불확실성을 평가하고, 그 위에 예측 분포를 해석하여 의사결정 규칙을 설계할 수 있다. Bootstrap은 데이터 변경에 대한 민감도 점검에 활용된다.\n\n\n실무적 해석의 핵심\n머신러닝에서의 불확실성 추정은 참값을 포함할 확률을 말하는 것이 아니다. 이는 이 예측을 어느 정도 신뢰해도 되는지, 그리고 어떤 위험을 감수하는지를 판단하기 위한 정보이다. 불확실성은 추론의 결과가 아니라, 의사결정을 위한 입력이다.\n\n\n\n\nChapter 3. Explainable AI와 통계의 역할\n머신러닝 모델, 특히 딥러닝은 뛰어난 예측 성능을 보이지만, 그 결과가 왜 그렇게 나왔는지에 대해서는 종종 침묵한다. 이로 인해 최근 AI 분야에서는 설명가능 인공지능, 즉 Explainable AI(XAI)가 중요한 연구 및 실무 주제로 부상하였다. 그러나 XAI가 요구하는 설명은 통계학에서 전통적으로 다루어 온 설명과 동일하지 않다.\n이 장에서는 설명가능성이 왜 필요한지, 통계적 해석과 기계적 설명의 차이는 무엇인지, 그리고 머신러닝 시대에 통계학이 수행해야 할 역할을 정리한다. 이를 통해 XAI를 과대평가하거나 오해하지 않고, 통계적 사고와 어떻게 결합해야 하는지를 분명히 한다.\n\nExplainable AI는 성능 이후의 필수 요구이다.\nXAI의 설명은 기계적이고 국소적인 설명이다.\n통계적 해석은 불확실성과 구조를 전제로 한 설명이다.\n두 설명은 목적과 수준이 다르며, 대체 관계가 아니다.\n머신러닝 시대 통계학의 역할은 설명을 비판적으로 해석하고 검증하는 기준을 제공하는 것이다.\n\n\n1. 설명가능성의 필요성\n\n왜 예측만으로는 충분하지 않은가\n많은 머신러닝 시스템은 단순한 예측기를 넘어 의사결정 시스템으로 사용된다. 신용평가, 의료 진단, 정책 대상자 선정, 위험 탐지와 같은 영역에서는 예측 결과 자체보다 그 결과가 사용되는 방식이 더 중요해진다.\n이러한 상황에서는 다음 질문이 필수적으로 제기된다. 이 결과를 신뢰해도 되는가, 어떤 요인이 이 결정을 이끌었는가, 다른 상황에서도 같은 판단이 나올 것인가. 즉, 설명가능성은 성능의 문제가 아니라 책임성과 신뢰성의 문제이다.\n\n\n규제와 윤리적 요구\n설명가능성은 기술적 요구를 넘어 제도적 요구이기도 하다. 차별과 편향의 검증, 결정에 대한 이의 제기 가능성, 정책과 법적 정당성 확보는 모두 설명을 전제로 한다. 이때 설명은 단순한 시각화나 중요도 나열이 아니라, 논리적으로 납득 가능한 근거 제시를 의미한다.\n\n\n\n2. 통계적 해석과 기계적 설명의 차이\n\n통계적 해석이란 무엇인가\n통계학에서의 해석은 다음 질문에 답한다. 어떤 변수가 결과에 체계적으로 관련되어 있는가, 그 관계는 어떤 방향과 크기를 가지는가, 이 관계는 우연이 아닌가. 즉, 통계적 해석은 불확실성을 전제로 한 구조적 설명이다.\n회귀계수, 신뢰구간, 효과 크기, 가설검정은 모두 관계의 안정성과 재현성을 묻는 도구이다. 통계적 해석은 단일 사례가 아니라, 데이터 생성 구조 전반에 대한 진술을 목표로 한다.\n\n\nXAI에서의 기계적 설명\n많은 XAI 기법은 다른 질문에 답한다. 이 예측에서 어떤 입력이 얼마나 기여했는가, 특정 입력을 바꾸면 출력은 어떻게 변하는가. SHAP, LIME, feature importance와 같은 방법들은 개별 예측 또는 국소 영역에서의 기여도를 설명한다.\n이러한 설명은 모델 내부 작동을 이해하는 데 유용하지만, 그 자체로 구조적 해석이나 인과적 설명을 제공하지는 않는다.\n\n\n근본적 차이\n통계적 해석은 관계의 안정성과 우연성을 묻는 반면, XAI의 기계적 설명은 특정 모델과 특정 입력에 조건부로 정의된다. 따라서 XAI의 설명은 데이터 생성 구조에 대한 진술이 아니라, 모델 내부에서의 계산 결과에 대한 설명이다.\n\n\n\n\n\n\n\n\n구분\n통계적 해석\n기계적 설명\n\n\n대상\n구조·관계\n개별 예측\n\n\n관점\n모집단\n특정 관측\n\n\n불확실성\n명시적으로 고려\n대개 미고려\n\n\n질문\n왜 이런 관계가 존재하는가\n왜 이 값이 나왔는가\n\n\n\n\n\n흔한 오해\n이 차이를 혼동하면 다음과 같은 오해가 발생한다. 기여도가 크면 원인이라는 해석, 중요도가 높으면 인과적이라는 주장이다. 그러나 이는 모델 내부에서의 상대적 기여를 보여줄 뿐, 현실 세계의 인과 구조를 보장하지 않는다.\n\n\n\n3. ML 시대 통계학의 역할\n\n설명을 검증하는 학문\n머신러닝 시대에 통계학의 첫 번째 역할은 모델이 제시한 설명을 검증하는 것이다. 설명 결과가 표본 변화에 얼마나 민감한지, 재표본을 통해 얼마나 안정적인지, 데이터 분포가 바뀌어도 유지되는지를 평가한다. 이는 예측이 아니라 설명 자체에 대한 통계적 검증이다.\n\n\n구조적 사고의 제공\n통계학은 설명에 대해 끊임없이 구조적 질문을 던진다. 이 설명은 어떤 가정 위에 서 있는가, 데이터 생성 과정이 달라지면 유지되는가, 다른 설명도 가능한가. 이는 머신러닝이 종종 간과하는 모형 외부의 사고이며, 설명의 남용을 막는 안전장치이다.\n\n\nXAI와 통계의 바람직한 관계\nXAI와 통계는 경쟁 관계가 아니다. XAI는 모델 내부를 들여다보는 도구이고, 통계학은 그 결과를 해석하고 검증하며 일반화하는 틀이다. XAI는 설명을 만들어내고, 통계는 그 설명이 얼마나 믿을 만한지를 판단한다.\n\n\n통계학의 재정의\n머신러닝 시대의 통계학은 단순한 회귀 기법의 집합도 아니고, 과거의 추론 도구에 머물지도 않는다. 통계학의 핵심 역할은 불확실성의 언어를 제공하고, 구조와 우연을 구분하며, 설명의 정당성을 검증하고, 의사결정의 합리성을 확보하는 데 있다."
  },
  {
    "objectID": "notes/mldl/mldl_deeplearning.html",
    "href": "notes/mldl/mldl_deeplearning.html",
    "title": "MLDL 딥러닝",
    "section": "",
    "text": "Chapter 1. 신경망은 왜 overfit이 잘 되는가\n딥러닝 모델은 놀라울 정도로 높은 표현력을 가지며, 동시에 매우 쉽게 과적합된다. 이 현상은 단순히 데이터가 적어서이거나 모델이 복잡해서라는 설명만으로는 충분하지 않다. 실제로 신경망은 데이터가 충분해 보이는 상황에서도 과적합과 일반화 실패를 반복적으로 보여준다.\n이 장에서는 신경망이 구조적으로 과적합되기 쉬운 이유를 파라미터 수와 자유도, 고차원 통계 환경, 그리고 암묵적 정규화라는 관점에서 통계적으로 해석한다. 이를 통해 딥러닝에서의 과적합을 ”훈련오차가 작다”는 현상이 아니라, 어떤 함수를 선택했는가의 문제로 재정의한다.\n\n신경망은 구조적으로 과적합되기 쉬운 모델이다.\n이는 파라미터 수, 자유도, 고차원 통계 환경의 결합된 결과이다.\n딥러닝의 일반화는 명시적 정규화보다 암묵적 정규화에 크게 의존한다.\n과적합은 적합의 문제가 아니라 함수 선택의 문제이다.\n\n\n1. 파라미터 수와 자유도\n\n신경망의 파라미터 규모\n완전연결 신경망에서 파라미터 수는 층 수와 노드 수에 따라 급격히 증가한다. 입력 차원이 p, 은닉층 노드 수가 h, 출력 차원이 1인 단일 은닉층 신경망의 경우, 파라미터 수는 대략 \\((p + 1)h + (h + 1)\\)로 주어진다.\n실제 딥러닝 모델에서는 이 값이 표본 크기 n보다 훨씬 커지는 경우가 일반적이며, 이는 과잉 매개변수화(overparameterization)의 전형적인 특징이다.\n\n\n자유도의 통계적 의미\n통계학에서 자유도는 데이터에 얼마나 독립적으로 적응할 수 있는지를 나타낸다. 선형회귀에서는 자유도가 파라미터 수와 거의 일치한다. 그러나 신경망에서는 파라미터 수와 유효 자유도가 일치하지 않는다. 활성화 함수, 네트워크 구조, 가중치 공유, 학습 알고리즘 등이 자유도를 왜곡한다.\n그럼에도 불구하고 신경망은 극도로 높은 표현 자유도를 갖는다. 이는 신경망이 매우 다양한 함수들을 실현할 수 있음을 의미한다.\n\n\n왜 훈련오차는 쉽게 0이 되는가\n신경망은 임의의 레이블 할당에도 완벽히 적합할 수 있으며, 노이즈를 포함한 데이터까지 그대로 기억할 수 있다. 이는 신경망이 단순히 좋은 함수 근사기이기 때문이 아니라, 과잉 매개변수화된 구조를 갖기 때문이다. 즉, 훈련오차가 0이 되는 현상 자체는 딥러닝에서 특별한 일이 아니다.\n\n\n\n2. 고차원 통계 관점의 딥러닝\n\n고차원 영역의 통계적 특성\n딥러닝은 전형적으로 표본 수 n보다 파라미터 수 p가 훨씬 큰 영역에서 작동한다.\n\\[p \\gg n\\]\n이 영역에서는 고전적 통계 직관이 붕괴한다. 추정량의 일관성은 보장되지 않으며, 분산은 매우 커지고, 훈련 성능과 일반화 성능 사이의 괴리는 커진다. 즉, 딥러닝은 고차원 통계의 극단적인 영역에서 작동하는 학습 체계이다.\n\n\n보간(interpolation) 영역과 double descent\n최근 이론은 모델 복잡도가 증가함에 따라 테스트오차가 감소했다가 증가한 뒤, 다시 감소하는 현상을 설명한다. 이를 double descent 현상이라 한다. 딥러닝 모델은 종종 훈련오차가 거의 0이 되는 보간 영역에서 학습된다.\n이는 전통적 통계에서 기대했던 ”과적합 이후 성능 악화”라는 단순한 직관과 충돌한다. 즉, 훈련오차가 0이라는 사실만으로 일반화 실패를 단정할 수 없다.\n\n\n\n\n\n\n\n고차원에서의 직관적 설명\n고차원 공간에서는 동일한 훈련 데이터에 완벽히 적합하는 함수가 매우 많다. 그러나 이들 함수의 일반화 성능은 크게 다를 수 있다. 신경망 학습은 이 중 하나의 함수를 선택하는 과정이며, 과적합은 적합 실패가 아니라 잘못된 함수 선택의 결과로 이해해야 한다.\n\n\n\n3. 암묵적 정규화 (Implicit Regularization)\n\n명시적 정규화 없이도 일반화가 되는 이유\n실제 딥러닝에서는 L1이나 L2 정규화를 강하게 사용하지 않아도 테스트 성능이 상당히 우수한 경우가 많다. 이는 학습 과정 자체가 암묵적인 정규화 역할을 수행하기 때문이다.\n\n\n암묵적 정규화의 원천\n신경망 학습 과정에는 여러 암묵적 제약이 존재한다. 확률적 경사하강법(SGD)은 특정한 해, 특히 노름이 작은 해를 선호하는 경향이 있다. 초기화 방식은 함수 공간에서의 출발점을 제한하며, 조기 종료는 L2 정규화와 유사한 효과를 낸다. 또한 배치 크기는 학습 과정의 잡음을 조절함으로써 탐색 가능한 해의 영역을 제한한다.\n이러한 요소들은 모두 가능한 해 중에서 상대적으로 덜 복잡한 해를 선택하도록 유도한다.\n\n\n통계적 해석\n암묵적 정규화는 명시적인 prior를 두지 않더라도, 학습 알고리즘 자체가 prior의 역할을 수행한다는 의미로 해석할 수 있다. 즉, 딥러닝에서의 학습은 손실함수와 최적화 알고리즘의 결합으로 정의되며, 정규화는 이 중 최적화 과정에 숨어 있다.\n\n\n\n4. ”왜 overfit이 잘 되는가”의 정리\n신경망이 과적합되기 쉬운 이유는 다음 요인들이 동시에 작동하기 때문이다. 첫째, 과잉 매개변수화로 인해 매우 큰 자유도를 가진다. 둘째, 고차원 통계 영역에서 작동하여 고전적 일반화 직관이 성립하지 않는다. 셋째, 명시적 제약이 약한 상태에서는 데이터가 조금만 부족해도 즉시 과적합이 발생한다. 넷째, 암묵적 정규화는 설정과 학습 조건에 따라 일반화 성능이 크게 달라질 수 있다.\n이러한 관점에서 과적합은 ”훈련오차가 작다”는 현상이 아니라, 어떤 함수를 선택했는가에 대한 문제이다. 딥러닝의 일반화는 적합 자체보다, 방대한 함수 공간 중 어떤 해를 선택하도록 유도했는가에 의해 결정된다.\n\n\n\nChapter 2. Cross-Entropy와 MLE의 연결\n딥러닝 분류에서 가장 널리 사용되는 손실함수는 Cross-Entropy이다. 이 손실은 흔히 분류용 손실로 소개되지만, 통계적으로는 확률모형의 최대우도추정과 정확히 대응된다. 즉, 딥러닝 분류 학습은 본질적으로 조건부 확률모형의 추정 문제이다.\n이 장에서는 딥러닝 분류 손실의 구조를 확률모형 관점에서 해석하고, 신경망 출력이 의미하는 바를 통계적으로 정리한다. 이를 통해 손실함수, 확률 출력, 분류 결정, 성능지표의 역할을 명확히 분리한다.\n\n딥러닝 분류 손실은 Cross-Entropy이며, 이는 로그우도의 음수이다.\nCross-Entropy 최소화는 확률모형의 최대우도추정과 동치이다.\n신경망의 출력은 분류 결과가 아니라 확률 예측이다.\n분류 결정, 성능지표, 손실함수는 서로 다른 역할을 가진다.\n딥러닝 분류는 본질적으로 통계적 추정 문제이다.\n\n\n1. 딥러닝 분류 손실의 구조\n\n다중 분류의 설정\n입력 x에 대해 클래스 \\(y \\in \\{ 1,\\ldots,K\\}\\)를 예측하는 문제를 고려하자. 신경망은 마지막 층에서 점수 벡터 \\(z(x) = (z_{1},\\ldots,z_{K})\\)를 출력한다. 이 점수는 softmax 함수를 통해 확률로 변환된다.\n\\[p_{k}(x) = \\frac{\\exp(z_{k})}{\\sum_{j = 1}^{K}\\exp(z_{j})}\\]\n이때 \\(p_{k}(x)\\)는 \\(P(Y = k \\mid X = x)\\)로 해석된다.\n\n\nCross-Entropy 손실\n원-핫 레이블 y에 대해 단일 관측치의 Cross-Entropy 손실은 다음과 같다. \\(\\ell(y,p(x)) = - \\overset{K}{\\sum_{k = 1}}1(y = k)\\log p_{k}(x)\\). 표본 전체에 대해 평균을 취한 값이 학습의 목표 함수가 된다.\n\n\n\n2. Cross-Entropy = 음의 로그우도\n\n범주형 분포와 우도\n다중 분류는 다음 확률모형을 전제한다.\n\\[Y \\mid X = x \\sim \\text{Categorical}(p_{1}(x),\\ldots,p_{K}(x))\\]\n이때 단일 관측치의 로그우도는\\(\\log p(Y = y \\mid X = x) = \\overset{K}{\\sum_{k = 1}}y_{k}\\log p_{k}(x)\\)이다.\n\n\nMLE와 손실 최소화의 동치성\n표본 전체의 로그우도를 최대화하는 문제는 \\(\\max_{\\theta}\\overset{n}{\\sum_{i = 1}}\\log p(y_{i} \\mid x_{i};\\theta)\\)와 같으며, 이는 곧 Cross-Entropy 손실의 합을 최소화하는 문제와 동일하다. 즉, Cross-Entropy 최소화는 범주형 확률모형에 대한 최대우도추정이다.\n\n\n\n3. 확률 출력의 의미\n\n신경망 출력은 점수가 아니라 확률이다\nSoftmax 출력 \\(p_{k}(x)\\)는 단순한 점수가 아니라, 클래스에 대한 불확실성을 정량화한 확률적 산출물이다. 이는 경계 근처 관측치의 애매함을 표현하고, 비용 민감 의사결정의 기반을 제공한다.\n\n\n분류 결정과의 분리\n분류는 확률 예측 위에 추가 규칙을 얹어 이루어진다. 예를 들어\\(\\widehat{y} = \\arg\\max_{k}p_{k}(x)\\) 또는 비용을 고려한 임계값 규칙을 사용할 수 있다. 중요한 점은 Cross-Entropy가 확률을 학습하고, 분류 규칙은 그 확률을 사용하는 단계라는 사실이다. 이 둘을 혼동하면 성능지표 해석 오류가 발생한다.\n\n\nCalibration과의 연결\nCross-Entropy는 확률의 정합성, 즉 calibration에 민감하다. 잘못된 예측에 대해 과신할수록 큰 페널티를 부과하며, 과신을 강하게 억제한다. 이 점에서 Cross-Entropy는 순위만을 평가하는 AUC와 다른 정보를 제공한다.\n\n\n\n4. 신경망의 통계적 해석\n신경망은 유연한 확률모형이다 딥러닝 분류기는 다음과 같이 해석할 수 있다. \\(p(y \\mid x) = \\text{Softmax}(f_{\\theta}(x))\\) 즉, 신경망은 규칙 기반 분류기가 아니라, 매우 유연한 조건부 확률모형이다.\n\n로지스틱 회귀의 일반화\n이진 분류에서 Cross-Entropy는 로지스틱 회귀의 손실과 동일하다. 다중 분류에서 softmax와 Cross-Entropy는 다항 로지스틱 회귀에 해당한다. 신경망은 이 구조를 비선형 함수 공간으로 확장한 것이다.\n\n\n정규화와 베이즈 관점의 연결\n명시적·암묵적 정규화는 파라미터에 대한 사전적 제약으로 해석할 수 있으며, 이는 최대사후확률 추정의 흔적이다. 이 관점에서 딥러닝 학습은 손실함수와 최적화 과정의 결합으로 정의된다.\n\n\n\n5. 흔한 오해 정리\nCross-Entropy는 단순한 분류 손실이 아니다. 이는 로그우도의 음수이며, 확률모형의 추정 기준이다. Softmax 출력은 점수가 아니라 조건부 확률이다. 분류 성능은 이 확률을 어떻게 사용하는지에 따라 달라지며, 손실함수, 지표, 임계값은 서로 다른 역할을 수행한다.\n\n\n\nChapter 3. Dropout과 Batch Normalization 통계적 해석\n딥러닝에서 Dropout과 Batch Normalization(BatchNorm)은 거의 표준처럼 사용된다. 그러나 이 기법들은 흔히 학습을 잘 되게 하는 요령이나 성능을 올려주는 테크닉으로만 소개된다. 통계적 관점에서 보면, Dropout과 BatchNorm은 불확실성과 분포 안정성을 다루는 확률적 장치이며, 신경망의 일반화 성능을 설명하는 핵심 요소이다.\n이 장에서는 Dropout과 BatchNorm을 각각 불확실성 주입과 분포 안정화라는 관점에서 해석하고, 이들이 어떻게 암묵적 정규화로 작동하는지를 통계적으로 정리한다. 이를 통해 딥러닝의 성능이 네트워크 구조뿐 아니라 확률적 학습 장치의 결과임을 분명히 한다.\n\nDropout은 확률적 뉴런 제거를 통해 불확실성을 학습에 주입한다.\n이는 모델 평균화 및 암묵적 베이즈 추론으로 해석할 수 있다.\nBatchNorm은 활성값 분포를 안정화하여 학습 과정을 통계적으로 제어한다.\n두 기법 모두 일반화 성능을 높이는 암묵적 정규화 메커니즘이다.\n딥러닝의 성능은 구조뿐 아니라 확률적 학습 장치의 결과이다.\n\n\n1. Dropout의 불확실성 해석\nDropout은 학습 과정에서 각 뉴런을 확률적으로 제거한다. 은닉층의 출력 h에 대해, 학습 시에는 다음과 같은 연산이 적용된다.\n\\(\\overset{˜}{h} = m \\odot h,m_{j} \\sim \\text{Bernoulli}(1 - p)\\). 즉, 각 뉴런은 확률 p로 비활성화되며, 테스트 시에는 전체 뉴런을 사용하되 스케일 보정을 통해 기대값을 맞춘다. 이 과정은 의도적인 무작위성을 학습에 도입한다.\n\n단순한 규제가 아닌 이유\nDropout을 단순한 과적합 방지 기법으로 설명하면 핵심을 놓친다. Dropout의 본질은 파라미터와 표현에 대한 불확실성을 학습 과정에 주입하는 데 있다. 이는 파라미터를 하나의 값으로 고정하지 않고, 여러 가능한 모델을 평균내는 효과를 만든다.\n\n\n모델 평균화 관점\nDropout 학습은 서로 다른 서브네트워크를 반복적으로 학습하는 과정으로 해석할 수 있다. 테스트 시의 예측은 이들 서브네트워크의 평균 예측을 근사한다. 이 관점에서 Dropout은 앙상블의 확률적 근사이며, 베이즈 관점에서는 사후 예측 평균과 유사한 역할을 한다.\n\n\nDropout과 불확실성\nDropout은 표현 수준의 불확실성을 반영하고, 특정 뉴런이나 경로에 대한 과도한 의존을 억제하며, 함수 선택의 다양성을 확보한다. 따라서 Dropout은 암묵적 베이즈 추론의 흔적으로 해석할 수 있다.\n\n\n\n2. Batch Normalization과 분포 안정화\n\nBatchNorm의 기본 연산\nBatchNorm은 각 미니배치에서 활성값을 정규화한다.\n\\(\\widehat{h} = \\frac{h - \\mu_{B}}{\\sqrt{\\sigma_{B}^{2} + \\epsilon}},y = \\gamma\\widehat{h} + \\beta\\), 여기서 \\(\\mu_{B}\\)와 \\(\\sigma_{B}^{2}\\)는 미니배치 평균과 분산이며, \\(\\gamma\\)와 \\(\\beta\\)는 학습 가능한 재조정 파라미터이다.\n\n\n\n\n\n\n\n내부 공변량 변화의 통계적 의미\nBatchNorm의 동기는 흔히 내부 공변량 변화의 감소로 설명된다. 통계적으로 이는 각 층 입력 분포의 위치와 스케일 변동을 억제하고, 학습 과정에서의 분포 불안정성을 줄이는 것을 의미한다. 즉, BatchNorm은 조건부 분포를 강제로 안정화시키는 장치이다.\n\n\nBatchNorm은 왜 정규화인가\nBatchNorm은 L1이나 L2 정규화처럼 파라미터 크기를 직접 제한하지 않는다. 대신 활성값의 분포를 제어함으로써 기울기 폭주와 소실을 완화하고, 최적화 경로를 안정화하며, 학습률에 대한 민감도를 감소시킨다.\n\n\n확률적 효과\n미니배치 평균과 분산은 표본 추정량이므로, BatchNorm은 본질적으로 확률적 잡음을 학습에 주입한다. 이로 인해 BatchNorm은 Dropout과 유사한 정규화 효과를 갖게 된다.\n\n\n\n3. 일반화 성능과의 관계\n\nDropout과 일반화\nDropout은 특정 경로에 대한 의존을 줄이고, 표현의 분산을 증가시키며, 과도한 신뢰를 억제함으로써 일반화를 돕는다. 이는 앞서 논의한 암묵적 정규화의 전형적인 사례이다.\n\n\nBatchNorm과 일반화\nBatchNorm의 일반화 효과는 간접적이다. 최적화가 쉬워지면서 더 나은 해에 도달할 가능성이 커지고, 분포 안정화를 통해 학습 불안정성이 감소하며, 배치 기반 잡음으로 인해 규제 효과가 발생한다. 즉, BatchNorm은 일반화를 직접 강제하기보다 일반화 가능한 해를 찾기 쉽게 만든다.\n\n\nDropout과 BatchNorm의 대비\nDropout은 불확실성을 통해 함수 공간을 넓게 탐색하도록 유도하는 반면, BatchNorm은 학습 경로를 안정화하여 탐색을 효율적으로 만든다. 두 기법은 서로 다른 방식으로 일반화를 지원한다.\n\n\n\n\n\n\n\n\n관점\nDropout\nBatchNorm\n\n\n핵심 역할\n불확실성 주입\n분포 안정화\n\n\n통계적 성격\n모델 평균 근사\n조건부 분포 제어\n\n\n규제 방식\n표현 제거\n스케일·위치 조정\n\n\n일반화 효과\n직접적\n간접적\n\n\n\n\n\n\n4. 통합적 해석\nDropout과 BatchNorm은 고차원·과잉매개변수화된 신경망에서 어떤 함수가 선택되는가라는 동일한 문제를 서로 다른 방식으로 해결한다. Dropout은 확률적 제거를 통해 함수 공간을 넓게 탐색하고, BatchNorm은 분포 안정화를 통해 탐색 경로를 제어한다.\n이 둘은 모두 명시적 정규화 항이 없는 상태에서도 일반화 성능을 개선하는 통계적 장치이다. 이 관점에서 딥러닝의 일반화는 구조적 설계뿐 아니라, 학습 과정에 내재된 확률적 메커니즘의 결과로 이해할 수 있다."
  },
  {
    "objectID": "notes/mldl/mldl_evaluation.html",
    "href": "notes/mldl/mldl_evaluation.html",
    "title": "MLDL 모델평가",
    "section": "",
    "text": "Chapter 1. 훈련오차 vs 테스트오차\n머신러닝에서 가장 자주 등장하는 구분 중 하나가 훈련오차와 테스트오차이다. 그러나 이 구분은 단순한 데이터 분할 기법이 아니라, 통계적 추론과 일반화에 대한 관점 차이를 반영한다. 훈련오차와 테스트오차의 관계를 이해하는 것은 모델 성능을 해석하는 출발점이다.\n이 장에서는 학습 성능과 일반화 성능의 개념을 명확히 구분하고, 데이터 분할이 가지는 통계적 의미를 체계적으로 설명한다. 이를 통해 테스트 데이터가 단순히 남겨둔 데이터가 아니라, 일반화 성능을 추론하기 위한 통계적 도구임을 분명히 한다.\n\n훈련오차는 적합도를 측정한다.\n테스트오차는 일반화 성능을 측정한다.\n두 오차의 차이는 과적합과 분산의 결과이다.\n데이터 분할은 성능 평가의 편향을 제거하기 위한 통계적 장치이다.\n훈련–검증–테스트 구분은 통계적 정당성을 확보하기 위한 필수 구조이다.\n\n\n1. 학습 성능과 일반화 성능\n\n훈련오차란 무엇인가\n훈련오차는 학습에 사용된 데이터에서의 평균 손실을 의미한다.\n\\[{\\widehat{R}}_{\\text{train}}(f) = \\frac{1}{n_{\\text{train}}}\\sum_{i \\in \\text{train}}L(y_{i},f(x_{i}))\\]\n훈련오차는 모델이 이미 관측한 데이터에 얼마나 잘 맞는지를 나타내며, 모델 복잡도가 증가할수록 단조 감소하는 경향을 가진다. 최적화 관점에서 보면, 훈련오차는 학습 과정에서 직접 최소화되는 가장 직접적인 목표이다. 이 의미에서 훈련오차는 적합도의 지표이다.\n\n\n테스트오차란 무엇인가\n테스트오차는 학습에 사용되지 않은 데이터에서의 평균 손실이다.\n\\[{\\widehat{R}}_{\\text{test}}(f) = \\frac{1}{n_{\\text{test}}}\\sum_{i \\in \\text{test}}L(y_{i},f(x_{i}))\\]\n테스트오차는 새로운 데이터에 대한 예측 성능을 측정하며, 모형의 일반화 능력을 평가한다. 통계적으로는 모집단에서의 기대 위험을 표본을 통해 근사한 값으로 해석할 수 있다. 즉, 테스트오차는 모형의 외삽 능력을 평가하는 지표이다.\n\n\n왜 두 오차는 다를 수밖에 없는가\n훈련오차와 테스트오차의 차이는 우연이 아니라 구조적 필연이다. 훈련 데이터는 모델 선택과 파라미터 추정에 사용되며, 테스트 데이터는 모델 학습 과정에서 소비되지 않는다. 동일한 손실함수를 사용하더라도 두 오차는 서로 다른 조건부 분포 하에서 계산된다.\n이로 인해 일반적으로 다음 관계가 성립한다.\n\\[{\\widehat{R}}_{\\text{train}}(f) \\leq {\\widehat{R}}_{\\text{test}}(f)\\]\n\n\nBias–Variance 관점에서의 해석\n모델 복잡도가 증가하면 훈련오차는 지속적으로 감소하지만, 테스트오차는 처음에는 감소하다가 일정 지점 이후 증가한다. 이는 편향–분산 상충효과의 전형적인 결과이다. 훈련오차는 편향과 분산을 모두 반영하지만, 테스트오차는 분산 증가에 특히 민감하게 반응한다.\n\n\n\n\n\n\n\n\n2. 데이터 분할의 통계적 의미\n\n데이터 분할은 무엇을 추정하기 위한 것인가\n지도학습의 궁극적인 목표는 모집단에서의 기대 위험을 최소화하는 것이다.\n\\[R(f) = \\mathbb{E}\\lbrack L(Y,f(X))\\rbrack\\]\n데이터 분할은 이 값을 표본 기반으로 편향 없이 추정하기 위한 통계적 장치이다.\n\n\n훈련-테스트 분할의 핵심 원리\n데이터를 분할한다는 것은 모델 선택 과정과 성능 평가 과정을 확률적으로 분리하는 것을 의미한다. 훈련 데이터는 모델을 학습하고 조정하는 데 사용되며, 테스트 데이터는 고정된 모델을 평가하는 데 사용된다. 이는 통계학에서 독립 표본을 이용해 가설을 검증하는 사고방식과 동일하다.\n”데이터를 아낀다”는 오해\n데이터를 나누면 학습 데이터가 줄어들어 손해라는 오해가 흔하다. 그러나 통계적으로는 반대이다. 테스트 데이터가 없으면 성능 평가는 낙관적 편향을 갖게 되며, 훈련오차는 기대 위험의 일관된 추정량이 아니다. 오히려 과도한 자신감과 잘못된 결론을 유발한다.\n\n\n검증 데이터의 역할\n실제 학습에서는 훈련, 검증, 테스트 데이터의 세 집합을 구분한다. 검증 데이터는 모델 선택과 튜닝에 사용되며, 테스트 데이터는 최종 성능 평가를 위해 보존된다. 이 구조는 다중 비교로 인한 성능 평가의 오염을 막기 위한 통계적 완충 장치이다.\n\n\n\n\n\n\n\n데이터\n역할\n\n\nTraining\n파라미터 추정\n\n\nValidation\n모델·하이퍼파라미터 선택\n\n\nTest\n최종 성능 평가\n\n\n\n\n\n\n3. 반복 분할과 확률적 사고\n\n단일 분할의 불안정성\n하나의 훈련–테스트 분할 결과는 우연에 크게 의존한다. 어떤 관측치가 어느 집합에 포함되느냐에 따라 테스트오차는 크게 달라질 수 있다. 이는 표본 변동성에 따른 불가피한 현상이다.\n\n\n교차검증의 통계적 위치\n교차검증은 기대 위험 추정의 분산을 줄이고, 데이터 분할의 우연성을 평균화하기 위한 절차이다. 교차검증은 새로운 학습 기법이 아니라, 성능을 추정하기 위한 통계적 방법이다. 이 관점에서 교차검증은 위험 추정의 안정성을 높이기 위한 반복 표본 추출 과정으로 이해할 수 있다.\n\n\n\n4. 단일 테스트 성능을 얼마나 믿을 수 있는가\n테스트오차는 일반화 성능을 평가하는 핵심 지표이지만, 그 자체로 확정적인 값은 아니다. 테스트오차 역시 표본에 의해 계산된 추정량이며, 분할 방식과 표본 구성에 따라 변동성을 가진다.\n따라서 단일 테스트 성능 결과만으로 모델의 우열을 단정하는 것은 위험하다. 특히 테스트 성능이 근소한 차이를 보이는 경우, 이는 통계적으로 의미 있는 차이가 아닐 수 있다. 이 점에서 모델 성능 비교는 값의 크기뿐 아니라, 그 변동성과 추정 불확실성까지 함께 고려해야 한다.\n\n\n\nChapter 2. Cross-validation의 통계적 의미\n교차검증(cross-validation, CV)은 흔히 데이터가 적을 때 사용하는 기법이나 모델 성능을 안정적으로 재는 방법으로 소개된다. 그러나 통계적 관점에서 교차검증의 본질은 일반화 오차를 추정하기 위한 확률적 절차에 있다. 교차검증은 단일 표본에서 발생하는 성능 추정의 우연성을 평균화하려는 시도이다.\n이 장에서는 교차검증이 왜 필요한지, 표본 변동성이 성능 추정에 어떤 영향을 미치는지, 그리고 CV 결과를 어떻게 해석해야 하며 어디까지 신뢰할 수 없는지를 명확히 정리한다. 이를 통해 교차검증을 만능 도구가 아닌, 통계적 가정과 한계를 지닌 추정 절차로 이해한다.\n\n교차검증은 일반화 오차를 추정하기 위한 통계적 절차이다.\n단일 테스트 분할의 표본 변동성을 평균화하는 것이 목적이다.\nCV는 위험 추정의 분산을 줄이지만, 편향을 제거하지는 않는다.\nCV 결과는 불확실성을 동반하며, 과도한 해석은 위험하다.\n모델 선택과 성능 평가는 반드시 분리되어야 한다.\n\n\n1. 교차검증의 필요성\n\n우리가 알고 싶은 것은 무엇인가\n지도학습의 궁극적인 관심사는 모집단에서의 기대 위험이다.\n\\[R(f) = \\mathbb{E}\\lbrack L(Y,f(X))\\rbrack\\]\n그러나 모집단 분포는 알 수 없으며, 실제로는 하나의 표본만 주어진다. 교차검증은 이 단일 표본로부터 기대 위험을 추정하기 위한 방법이다.\n\n\n단일 테스트 분할의 한계\n단일 훈련–테스트 분할에는 구조적인 한계가 있다. 어떤 관측치가 테스트 집합에 포함되느냐에 따라 테스트오차 추정값은 크게 달라질 수 있다. 즉, 단일 분할에서 계산된 테스트오차는 분산이 큰 추정량이다.\n\n\n교차검증의 핵심 아이디어\n교차검증의 핵심 사고는 다음과 같다. 테스트 표본을 바꿔가며 여러 번 위험을 추정하고, 그 평균을 일반화 성능의 근사로 사용하자는 것이다. 이는 통계적으로 표본 분할에 따른 우연성을 평균화하는 절차이다.\n\n\n\n2. 표본 변동성과 성능 추정\n\nK-fold 교차검증의 구조\nK-fold 교차검증에서는 데이터를 K개의 부분집합으로 나눈다. 각 단계에서 하나의 fold를 테스트 데이터로 사용하고, 나머지 K-1개를 사용해 모델을 학습한다. 이를 K번 반복하여 성능을 평가한다.\n\\[{\\widehat{R}}_{\\text{CV}} = \\frac{1}{K}\\overset{K}{\\sum_{k = 1}}{\\widehat{R}}_{- k}(f^{( - k)})\\]\n여기서 \\(f^{( - k)}\\)는 k번째 fold를 제외한 데이터로 학습된 모델이다.\n\n\n\n\n\n\n\n교차검증은 무엇을 줄이는가\n교차검증의 중요한 역할은 기대 위험 추정의 분산을 줄이는 데 있다. 교차검증은 기대 위험의 불편추정량을 만들어 주지는 않는다. 대신 분산이 큰 위험 추정을 보다 안정화한다. 즉, 교차검증의 주된 효과는 분산 감소이다.\n\n\n훈련 크기와 편향-분산 절충\nK가 커질수록 각 모델은 더 많은 데이터를 사용해 학습되므로 학습 편향은 감소한다. 그러나 동시에 fold 간 추정값의 상관이 커지며, 전체 추정량의 분산이 증가할 수 있다. 이로 인해 교차검증은 편향–분산 절충의 문제로 이해해야 한다.\n\n\n\n3. CV 결과 해석의 한계\n\nCV 점수는 ’진짜 성능’이 아니다\n교차검증 결과는 모집단에서의 정확한 성능도 아니고, 모델 간 우열에 대한 절대적 증거도 아니다. CV 점수는 표본 기반 위험 추정값일 뿐이다.\n\n\n모델 비교에서의 함정\n실무에서는 다음과 같은 비교가 흔하다. ”모델 A의 CV 정확도는 0.812, 모델 B는 0.808이므로 A가 더 좋다.” 그러나 통계적으로 이는 문제가 있다. CV 점수의 차이는 표본 변동성 범위 내에 있을 수 있으며, 차이에 대한 불확실성이나 표준오차가 고려되지 않는다. 따라서 CV 평균만으로 모델의 우열을 단정할 수 없다.\n\n\n하이퍼파라미터 튜닝과 CV 오염\n교차검증을 사용해 하이퍼파라미터를 선택하면, 해당 CV 데이터는 이미 모델 선택 과정에 사용된 것이다. 이 상태에서 동일한 CV 결과로 성능을 보고하면 낙관적 편향이 발생한다. 이를 피하기 위해서는 중첩 교차검증(nested cross-validation)이나 독립된 테스트 데이터가 필요하다.\n\n\nCV는 규칙이 아니라 확률적 절차이다\n교차검증은 자동화된 정답 생성기가 아니다. 데이터의 크기, 관측치 간 의존성, 시간적·공간적 구조에 따라 교차검증의 적절성은 달라진다. 특히 시계열, 패널, 공간 데이터에서는 무작위 분할을 전제로 한 교차검증이 부적절할 수 있다.\n\n\n\n4. 교차검증과 모델 선택의 위험\n교차검증은 성능을 추정하기 위한 도구이지, 모델 선택을 정당화하는 장치는 아니다. 여러 후보 모델을 교차검증 점수로 비교하고 그중 가장 높은 값을 선택하는 과정은 다중 비교 문제를 내포한다. 이 경우 선택된 모델의 성능은 우연에 의해 과대평가될 가능성이 크다.\n이러한 위험을 줄이기 위해서는 모델 선택과 성능 평가를 명확히 분리해야 한다. 교차검증은 모델을 탐색하고 튜닝하는 단계에서 사용하고, 최종 성능 평가는 독립된 테스트 데이터나 중첩 교차검증을 통해 수행하는 것이 통계적으로 바람직하다.\n\n\n\nChapter 3. ROC, AUC, Precision–Recall의 오해\n분류 문제에서 ROC, AUC, Precision–Recall(PR)은 널리 사용되는 성능 지표이다. 그러나 이 지표들은 종종 확률모형의 적합도나 모델의 절대적 우열을 말해주는 것처럼 오해된다. 실제로 이 지표들은 확률 예측 그 자체가 아니라, 확률 예측 위에 얹힌 결정 결과를 평가한다.\n이 장에서는 분류 성능지표의 통계적 의미를 정리하고, 임계값의 역할을 확률 예측과 분리해 이해하며, 불균형 데이터에서 지표 선택이 왜 중요한지를 설명한다. 이를 통해 지표를 ”정답을 알려주는 숫자”가 아니라, 의사결정을 지원하는 도구로 재해석한다.\n\n분류 성능지표는 확률모형이 아니라 결정 결과를 평가한다.\nROC/AUC는 순위 분리 능력을 요약하며 임계값과 무관하다.\nPR 곡선은 불균형 데이터에서 더 직접적인 정보를 제공한다.\n임계값은 통계 문제가 아니라 의사결정 문제이다.\n지표 선택은 모델 선택이 아니라 문제 정의의 일부이다.\n\n\n1. 분류 성능지표의 통계적 의미\n\n분류 지표는 무엇을 평가하는가\n분류 성능지표는 공통적으로 확률 예측 \\(\\widehat{p}(x)\\)위에 적용된 결정 규칙의 결과를 평가한다. 즉, 지표는 모델이 산출한 확률의 품질 그 자체가 아니라, 그 확률을 어떻게 사용했는지의 성과를 측정한다.\n\n\n혼동행렬과 지표의 출발점\n이진 분류에서 모든 지표는 혼동행렬에서 출발한다.\n\n\n\n\n\n\n\n\n\n실제 1\n실제 0\n\n\n예측 1\nTP\nFP\n\n\n예측 0\nFN\nTN\n\n\n\n대표적인 지표는 다음과 같다.\n\\[\\text{TPR (Recall)} = \\frac{TP}{TP + FN},\\text{FPR} = \\frac{FP}{FP + TN},\\text{Precision} = \\frac{TP}{TP + FP}\\]\n이 지표들은 확률분포를 평가하지 않으며, 사건 발생의 비율을 측정한다.\n\n\n\n2. ROC와 AUC의 올바른 해석\n\nROC 곡선은 무엇을 그리는가\nROC 곡선은 임계값 \\(c\\)를 변화시키며 \\((\\text{FPR}(c),\\text{TPR}(c))\\)의 궤적을 그린 것이다. x축은 FPR, y축은 TPR이며, ROC는 임계값 전 범위에서의 분류 행태를 요약한다.\n\n\nAUC의 통계적 의미\nAUC는 다음 확률로 해석된다.\n\\[\\text{AUC} = \\mathbb{P}(\\widehat{s}(X^{+}) &gt; \\widehat{s}(X^{-}))\\]\n즉, 임의로 선택한 양성 표본이 음성 표본보다 더 높은 점수를 받을 확률이다. 중요한 점은 AUC가 특정 임계값을 반영하지 않으며, 확률의 보정(calibration)을 평가하지 않는다는 것이다.\n\n\nROC/AUC에 대한 대표적 오해\nAUC가 높다고 해서 특정 운영 임계값에서의 분류 성능이 반드시 좋은 것은 아니다. 또한 ROC는 불균형 데이터에 대해 시각적으로 안정적으로 보일 수 있으나, 실제 오류 비용을 반영하지는 않는다. AUC는 순위 분리 능력을 말할 뿐이다.\n\n\n\n\n\n\n\n\n3. Precision–Recall 곡선의 의미\n\nPR 곡선은 무엇에 민감한가\nPR 곡선은 임계값 변화에 따른 Precision과 Recall의 관계를 나타낸다. PR 곡선은 양성 클래스의 비율(prevalence)과 거짓 양성(FP)의 비용에 민감하다.\n이로 인해 PR 곡선은 불균형 데이터에서 실질적인 성능을 더 직접적으로 반영한다. 불균형이 심할수록 ROC는 낙관적 착시를 줄 수 있다.\n\n\n\n\n\n\n\n\n구분\nROC\nPR\n\n\n기준\nTPR vs FPR\nPrecision vs Recall\n\n\n음성 클래스 영향\n큼\n작음\n\n\n불균형 데이터\n둔감\n민감\n\n\n해석 초점\n순위\n양성 예측의 신뢰\n\n\n\n\n\nAverage Precision(AP)의 해석\nAP는 PR 곡선 아래 면적으로, 전 임계값 구간에서의 평균 정밀도를 요약한다. 이는 양성으로 예측된 사례들이 얼마나 신뢰 가능한지를 나타내는 지표로 해석할 수 있다.\n\n\n\n4. 임계값(threshold)의 역할\n\n임계값은 통계적 산출물이 아니다\n임계값 c는 모델이 자동으로 정해주지 않는다.\n\\[\\widehat{y} = \\mathbf{1}\\{\\widehat{p}(x) \\geq c\\}\\]\n임계값은 오류 비용, 운영 목적, 자원 제약에 의해 외생적으로 결정된다. 즉, 임계값은 통계적 추정량이 아니라 의사결정 변수이다.\n\n\n”최적 임계값”의 환상\nROC에서 가장 먼 점이나 F1-score를 최대화하는 기준은 모두 암묵적인 비용 가정을 포함한다. 비용 구조가 달라지면 ”최적” 임계값도 달라진다. 따라서 임계값의 최적성은 보편적이지 않다.\n\n\n\n5. 불균형 데이터에서의 지표 선택\n\n정확도의 붕괴\n불균형 데이터에서 정확도는 다수 클래스 예측의 편의를 가릴 수 있으며, 소수 클래스를 완전히 무시해도 높은 값이 나올 수 있다. 이 경우 정확도는 통계적으로 거의 의미를 잃는다.\n\n\n지표 선택의 원칙\n불균형 상황에서는 문제의 목적에 따라 지표를 선택해야 한다. 소수 클래스의 탐지가 중요하면 Recall이나 PR이, 거짓 양성 비용이 크면 Precision이, 순위 기반 선별이 목적이면 AUC가 적합하다. 지표는 문제 정의에 종속된다.\n\n\n\n6. 하나의 지표로 모델을 비교할 수 없는 이유\n모델 성능을 하나의 숫자로 요약하려는 시도는 본질적으로 위험하다. 각 지표는 서로 다른 측면의 성능을 측정하며, 암묵적으로 서로 다른 비용 구조와 의사결정 목적을 반영한다. 따라서 지표 간의 우열은 보편적으로 비교될 수 없다.\n좋은 모델이란 특정 지표에서 최고 점수를 얻는 모델이 아니라, 주어진 문제의 목적과 비용 구조에 가장 잘 부합하는 모델이다. 이 관점에서 성능지표는 결론이 아니라, 의사결정을 돕는 요약 정보로 이해되어야 한다."
  },
  {
    "objectID": "notes/mldl/mldl_unsupervised.html",
    "href": "notes/mldl/mldl_unsupervised.html",
    "title": "MLDL 비지도학습",
    "section": "",
    "text": "Chapter 1. PCA는 차원축소가 아니라 분산 모형이다\n비지도학습은 반응변수 \\(Y\\)가 주어지지 않은 상태에서 데이터 자체의 구조를 요약하는 방법론이다. 그중 주성분분석(PCA)은 흔히 차원축소 기법으로 소개되지만, 통계적 관점에서 PCA의 본질은 차원을 줄이는 데 있지 않다. PCA는 공분산 구조를 가장 효율적으로 설명하는 방향을 찾는 분산 기반 모형이다.\n이 장에서는 PCA에 대한 대표적인 오해를 바로잡고, 분산 최대화가 가지는 통계적 의미와 공분산 행렬과의 연결을 체계적으로 설명한다. 이를 통해 PCA를 단순한 전처리 기법이 아니라, 비지도학습의 대표적인 분산 모형으로 재해석한다.\nPCA의 목적은 차원 축소가 아니라 분산 구조의 요약이다.  분산 최대화는 신호를 가장 잘 보존하는 방향을 찾는 과정이다.  주성분은 공분산 행렬의 고유벡터이며, 고유값은 각 방향이 설명하는 분산이다.  PCA는 비지도학습의 대표적인 분산 기반 모형이다.\n\n1. PCA 목적에 대한 오해\n\n”차원을 줄이기 위한 기법”이라는 오해\nPCA는 흔히 고차원 데이터를 저차원으로 축소하는 기법으로 설명된다. 그러나 이는 PCA의 결과에 대한 설명일 뿐, 목적에 대한 설명은 아니다. PCA의 핵심 목적은 차원을 줄이는 것이 아니라, 데이터의 변동성을 가장 잘 설명하는 방향을 찾는 데 있다.\n차원 축소는 이 과정에서 자연스럽게 따라오는 결과일 뿐이며, PCA 자체가 해결하려는 문제는 데이터가 어떤 방향으로 가장 크게 변동하는가를 파악하는 것이다.\n\n\nPCA가 답하려는 질문\nPCA는 다음과 같은 질문에 답한다. 데이터는 어떤 방향으로 가장 많이 퍼져 있는가, 여러 변수 간의 상관 구조는 어떤 축으로 요약될 수 있는가, 정보 손실을 최소화하면서 변동성을 표현하려면 어떤 좌표계가 적절한가. 이 관점에서 PCA는 새로운 변수를 만드는 문제가 아니라, 좌표계를 회전시키는 문제로 이해하는 것이 정확하다.\n\n\n\n2. 분산 최대화의 통계적 의미\n\n첫 번째 주성분의 정의\n중심화된 데이터 행렬 \\(X\\)에 대해, 첫 번째 주성분 방향 \\(w_{1}\\)은 다음 최적화 문제의 해로 정의된다.\n\\(w_{1} = \\arg\\max_{\\parallel w \\parallel = 1}w^{\\top}Sw,S = \\frac{1}{n}X^{\\top}X\\), 여기서 \\(S\\)는 표본 공분산 행렬이다. 즉, 첫 번째 주성분은 해당 방향으로 투영했을 때 분산이 최대가 되도록 하는 방향이다.\n왜 분산을 최대화하는가\n분산은 통계적으로 데이터가 얼마나 넓게 퍼져 있는지를 나타내며, 관측값 간 차이의 크기와 구조적 신호의 크기를 반영한다. 노이즈가 평균적으로 균등하게 분포한다고 가정하면, 분산이 큰 방향일수록 신호 대비 노이즈 비율이 큰 방향으로 해석할 수 있다. 이러한 이유로 PCA는 신호를 가장 잘 보존하는 방향을 찾는 절차로 이해된다.\n\n\n최소제곱 관점에서의 동치성\nPCA는 다음과 같은 최소제곱 문제와도 동치이다.\n\\[\\min_{\\text{rank}(Z) = k} \\parallel X - Z \\parallel_{F}^{2}\\]\n즉, PCA는 최소제곱 의미에서 원래 데이터를 가장 잘 근사하는 저차원 표현을 제공한다. 이 역시 차원을 줄이기 위한 기법이라기보다, 분산 설명력을 최적화하는 문제로 해석하는 것이 적절하다.\n\n\n\n\n\n\n\n\n3. 공분산 구조와 주성분\n\n공분산 행렬의 고유분해\n공분산 행렬 S는 다음과 같이 고유분해된다.\n\\(S = V\\Lambda V^{\\top}\\), 여기서 V의 열벡터는 주성분 방향에 해당하는 고유벡터이며, \\(\\Lambda\\)의 대각 원소는 각 방향이 설명하는 분산 크기인 고유값이다. 고유값이 클수록 해당 주성분이 설명하는 변동성의 비중이 크다.\n\n\n주성분 점수의 의미\n\\(j\\)번째 주성분 점수는 다음과 같이 정의된다.\n\\(Z_{j} = Xv_{j}\\).\n이는 원래 변수들의 선형결합으로 이루어진 새로운 변수이며, 서로 비상관(상관계수=0)이고 분산이 고유값 \\(\\lambda_{j}\\)인 특징을 가진다. 이 관점에서 PCA는 상관된 변수들을 비상관 변수로 재표현하는 방법이다.\n\n\n설명분산비율의 통계적 해석\n\\(\\text{Explained Variance Ratio}_{j} = \\frac{\\lambda_{j}}{\\sum_{k = 1}^{p}\\lambda_{k}}\\)\n설명분산비율은 전체 변동성 중에서 특정 주성분이 차지하는 비중을 나타낸다. 이는 단순한 비율이 아니라, 데이터 구조 중 특정 방향이 가지는 상대적 중요도를 나타내는 통계적 지표로 해석된다.\n\n\n\n4 PCA를 분산 모형으로 다시 보기\n\n암묵적 확률모형\nPCA는 다음과 같은 확률적 해석을 갖는다.\n\\[X = ZW^{\\top} + \\varepsilon,\\varepsilon \\sim \\mathcal{N}(0,\\sigma^{2}I)\\]\n여기서 Z는 저차원 잠재변수, W는 적재행렬(loadings), \\(\\varepsilon\\)는 등분산 잡음이다. 이는 확률적 요인모형의 특수한 형태로 볼 수 있다.\n\n\n비지도학습에서 PCA의 위치\n이러한 관점에서 PCA는 단순한 전처리 기법이 아니라, 분산 구조를 통해 데이터의 잠재적 구조를 요약하는 비지도 확률모형에 해당한다. PCA는 라벨 없이도 데이터의 주요 변동 패턴을 포착하는 대표적인 분산 기반 학습 방법이다.\n\n\n\n\n\n\n\n관점\nPCA의 역할\n\n\n알고리즘\n차원축소\n\n\n선형대수\n직교기저 변환\n\n\n통계\n분산 구조 요약 모형\n\n\n확률\n잠재변수 모형의 근사\n\n\n\n\n\n\n5. PCA의 한계와 오해: 언제 쓰면 안 되는가\nPCA는 강력한 도구이지만, 모든 데이터 분석 상황에 적합한 방법은 아니다. PCA에 대한 대표적인 오해는 분산이 크면 곧 유용한 정보가 많다고 믿는 것이다. 그러나 분산이 크다는 사실 자체가 분석 목적에 직접적으로 연결되는 것은 아니다.\n첫째, PCA는 반응변수를 전혀 고려하지 않는다. 예측이나 분류를 목적으로 하는 문제에서, 반응변수와 무관한 방향의 분산이 크게 나타날 수 있다. 이 경우 PCA는 예측에 도움이 되지 않는 방향을 우선적으로 선택할 수 있다.\n둘째, PCA는 스케일에 민감하다. 변수 간 단위나 분산 차이가 크면, 분산이 큰 변수의 영향이 과도하게 반영된다. 이는 표준화 여부가 PCA 결과에 결정적인 영향을 미친다는 것을 의미한다.\n셋째, PCA는 이상치에 취약하다. 분산 기반 방법이므로, 소수의 극단값이 주성분 방향을 왜곡할 수 있다.\n넷째, PCA는 선형 구조만을 포착한다. 비선형 구조가 지배적인 데이터에서는 주요 패턴을 충분히 설명하지 못할 수 있다.\n이러한 이유로 PCA는 정보를 자동으로 걸러주는 도구가 아니라, 분산 구조를 요약하는 도구로 이해되어야 한다. PCA를 사용할 때에는 분석 목적, 변수의 의미, 분산 구조가 문제 설정과 부합하는지를 먼저 점검하는 것이 필수적이다.\n\n\n\nChapter 2. K-means는 왜 평균을 쓰는가\nK-means는 가장 널리 사용되는 군집화 알고리즘 중 하나이다. 그러나 K-means는 단순히 비슷한 점들을 묶는 방법이 아니라, 군집 내 분산을 최소화하는 통계적 모형으로 이해하는 것이 정확하다. K-means에서 군집은 거리의 유사성으로 정의되지 않고, 평균을 중심으로 한 분산 구조로 정의된다.\n이 장에서는 K-means의 목적함수 구조를 통해 왜 평균이 군집 중심이 되는지를 설명하고, 거리 기반 분할이 어떤 통계적 가정을 전제로 하는지를 살펴본다. 이를 통해 K-means가 어떤 데이터 구조에서 잘 작동하며, 어떤 상황에서는 실패할 수밖에 없는지를 명확히 한다.\nK-means는 거리 기반 군집 알고리즘이 아니라 분산 최소화 모형이다.  평균은 제곱거리 손실을 최소화하는 최적의 대표값이다.  K-means는 구형 군집, 동일 분산, 변수 동등성이라는 가정을 전제한다.  따라서 K-means의 성능은 데이터의 분산 구조와 거리 가정에 강하게 의존한다.\n\n1. K-means 목적함수의 구조\n\n기본 설정\n관측치 \\(x_{1},\\ldots,x_{n} \\in \\mathbb{R}^{p}\\)가 주어졌을 때, K-means는 데이터를 K개의 군집 \\(C_{1},\\ldots,C_{K}\\)로 분할하고 각 군집의 중심 \\(\\mu_{k}\\)를 추정한다. K-means의 목적함수는 다음과 같다.\n\\[\\min_{\\{ C_{k}\\},\\{\\mu_{k}\\}}\\overset{K}{\\sum_{k = 1}}\\sum_{i \\in C_{k}} \\parallel x_{i} - \\mu_{k} \\parallel^{2}\\]\n이 식은 군집 내 제곱거리의 합(WCSS, within-cluster sum of squares)을 최소화하는 문제이다.\n\n\n목적함수의 통계적 해석\n이 목적함수는 본질적으로 각 군집 내부의 변동성을 최소화하는 것을 목표로 한다. 군집 간 분리는 직접적으로 고려되지 않으며, ”같은 군집”이란 평균 주변에 밀집해 있는 관측치들의 집합으로 정의된다. 즉, K-means에서 군집은 기하학적 경계가 아니라 분산 구조에 의해 규정된다.\n2. 평균 중심의 통계적 의미\n\n\n왜 중심은 ’평균’인가\n군집 C_k가 고정되어 있다고 가정하고 다음 문제를 고려하자.\n\\[\\min_{\\mu}\\sum_{i \\in C_{k}} \\parallel x_{i} - \\mu \\parallel^{2}\\]\n이 문제의 해는 잘 알려져 있듯이 표본 평균 \\({\\widehat{\\mu}}_{k} = \\frac{1}{|C_{k}|}\\sum_{i \\in C_{k}}x_{i}\\)이다. 즉, 제곱거리 손실을 최소화하는 대표값은 평균이며, 이는 평균제곱오차(MSE)를 최소화하는 추정량의 성질과 동일한 논리이다.\n\n\n평균 = 최우도 추정량\n확률모형 관점에서 보면, K-means는 다음 가정을 암묵적으로 포함한다.\n\\(x_{i} \\mid z_{i} = k \\sim \\mathcal{N}(\\mu_{k},\\sigma^{2}I)\\).\n확률모형 관점에서 보면 K-means는 다음과 같은 가정을 암묵적으로 포함한다. 각 군집은 평균이 \\(\\mu_{k}\\)인 정규분포에서 생성되며, 모든 군집은 동일한 분산을 갖고, 변수 간 공분산은 0이다. 즉, 공분산 행렬은 구형 구조를 갖는다.\n이 가정하에서 K-means의 목적함수는 분산을 고정한 정규 혼합모형의 최대우도추정 문제와 대응된다. 이때 평균은 각 군집의 최우도 추정량이 된다.\n\n\n평균 중심의 한계\n평균은 이상치에 민감하며, 비대칭 분포에 취약하고, 구형 구조의 군집을 선호한다. 따라서 평균을 중심으로 하는 K-means는 특정한 데이터 구조에 최적화된 방법이다.\n\n\n\n\n\n\n\n\n3. 거리 기반 분할의 가정\n\n거리란 무엇을 의미하는가\nK-means에서 사용하는 거리는 일반적으로 유클리드 거리이다.\n\\[\\parallel x_{i} - \\mu_{k} \\parallel^{2} = (x_{i} - \\mu_{k})^{\\top}(x_{i} - \\mu_{k})\\]\n이 선택은 변수의 스케일이 동일하며, 모든 변수가 동일한 중요도를 갖고, 방향성보다는 크기 차이가 중요하다는 가정을 전제한다. 즉, 거리의 선택 자체가 이미 강한 통계적 가정을 내포한다.\n\n\n거리 기반 분할의 결과적 성질\n유클리드 거리와 평균 중심의 결합은 군집 경계를 선형적인 Voronoi 분할로 만들며, 군집의 형태를 구형으로 제한한다. 또한 분산이 큰 방향이 군집 형성에 우선적으로 반영된다. 이 점에서 K-means의 사고방식은 PCA의 분산 중심 사고와 직접적으로 연결된다.\n\n\n거리 기반 분할의 통계적 해석\nK-means의 군집 할당은 가장 가까운 중심을 선택하는 규칙에 의해 이루어진다.\n\\[\\text{assign}x_{i} \\rightarrow \\arg\\min_{k} \\parallel x_{i} - \\mu_{k} \\parallel^{2}\\]\n이는 확률적 할당이 아닌 결정적 분할이다. 즉, 관측치의 소속에 대한 불확실성은 모델에 포함되지 않는다.\n\n\n\n4. K-means를 분산 모형으로 다시 보기\n\n전체 분산의 분해\nK-means는 다음과 같은 분산 분해를 암묵적으로 수행한다.\n\\[\\text{Total Variance} = \\text{Between-cluster Variance} + \\text{Within-cluster Variance}\\]\nK-means는 이 중 군집 내 분산을 최소화하는 방식으로 군집을 정의한다. 군집 간 분산을 직접적으로 최대화하지는 않지만, 전체 분산이 고정되어 있을 때 군집 내 분산을 줄이는 것은 결과적으로 군집 간 분산을 증가시키는 효과를 갖는다.\n\n\n비지도학습에서의 K-means의 위치\n이러한 관점에서 K-means는 거리 기반 군집 알고리즘이라기보다, 평균과 분산 구조를 이용해 데이터를 요약하는 비지도 분산 모형으로 이해하는 것이 적절하다.\n\n\n\n\n\n\n\n관점\nK-means\n\n\n알고리즘\n반복적 군집화\n\n\n손실함수\n제곱거리\n\n\n통계\n평균 기반 분산 모형\n\n\n확률\n단순화된 혼합모형\n\n\n\n\n\n\n5. K-means가 실패하는 데이터 구조\nK-means는 강력하고 계산 효율적인 방법이지만, 그 성능은 전제된 가정이 데이터 구조와 얼마나 잘 부합하는지에 따라 결정된다. 이러한 가정이 깨지는 경우 K-means는 구조적으로 실패할 수밖에 없다.\n첫째, 군집이 구형이 아닌 경우이다. 길게 늘어진 형태나 비선형 구조를 가진 군집에서는 평균 중심과 유클리드 거리가 실제 구조를 반영하지 못한다.\n둘째, 군집별 분산이 크게 다른 경우이다. 분산이 큰 군집은 여러 개로 쪼개지고, 분산이 작은 군집은 다른 군집에 흡수될 수 있다.\n셋째, 이상치가 존재하는 경우이다. 소수의 극단값이 평균을 크게 이동시켜 군집 구조 전체를 왜곡할 수 있다.\n넷째, 변수 간 중요도가 서로 다른 경우이다. 모든 변수가 동일한 중요도를 가진다는 가정이 성립하지 않으면, 거리 기반 분할은 의미를 잃는다.\n이러한 이유로 K-means는 보편적인 군집 알고리즘이 아니라, 평균과 분산이 군집을 대표할 수 있는 상황에서만 적절한 방법이다. K-means를 사용할 때에는 데이터의 분산 구조와 거리 가정이 문제 설정과 부합하는지를 먼저 점검해야 한다.\n\n\n\nChapter 3. 거리·유사도의 통계적 함정\n비지도학습에서 거리와 유사도는 가장 기본적인 개념처럼 보인다. 그러나 통계적 관점에서 거리는 단순한 계산 규칙이 아니라, 데이터 구조에 대한 강한 가정의 집합이다. 어떤 거리 척도를 선택하느냐에 따라 데이터의 구조는 전혀 다르게 해석될 수 있다.\n이 장에서는 거리 척도가 내포하는 통계적 가정, 스케일과 분포가 거리 계산에 미치는 영향, 그리고 고차원 공간에서 발생하는 거리 붕괴 현상을 체계적으로 살펴본다. 이를 통해 거리와 유사도가 중립적인 계산 도구가 아니라, 명확한 통계적 선택임을 강조한다.\n\n거리 척도는 데이터 구조에 대한 강한 가정을 내포한다.\n스케일과 분포 문제는 거리 기반 분석을 쉽게 왜곡한다.\n고차원에서는 거리 개념 자체가 붕괴한다.\n따라서 거리·유사도는 계산 도구가 아니라 통계적 선택이다.\n\n\n1. 거리 척도가 내포하는 가정\n\n거리는 무엇을 ”같다”고 정의하는가\n두 관측치 \\(x,x' \\in \\mathbb{R}^{p}\\) 사이의 거리를 정의한다는 것은, 어떤 차이를 중요하게 보고 어떤 차이를 무시할 것인지를 결정하는 행위이다. 즉, 거리 척도는 관측치 간 유사성에 대한 명시적인 정의를 포함한다.\n예를 들어 유클리드 거리 \\(d(x,x') = \\parallel x - x' \\parallel_{2}\\)는 다음과 같은 가정을 전제한다. 각 변수는 동일한 단위와 중요도를 가지며, 변수 간 차이는 제곱되어 누적되고, 변수 간 상관 구조는 고려되지 않는다. 이처럼 거리 척도는 결코 중립적인 도구가 아니라, 데이터 생성 구조에 대한 강한 가정이다.\n\n\n자주 쓰이는 거리들의 암묵적 가정\n유클리드 거리는 정규분포와 동일 분산 가정과 밀접하게 연결되어 있으며, 평균과 분산이 데이터를 대표할 수 있을 때 적합하다. 반면 마할라노비스 거리는 변수 간 상관 구조를 고려하지만, 공분산 추정의 안정성을 전제로 한다. 코사인 유사도는 크기보다 방향을 중시하며, 문서나 고차원 희소 데이터에 적합하다.\n이처럼 거리 선택은 단순한 계산 방식의 문제가 아니라, 데이터 구조에 대한 해석의 선택이다.\n\n\n\n\n\n\n\n거리\n암묵적 가정\n\n\n유클리드\n등분산, 독립, 구형 구조\n\n\n맨해튼\n절대편차 중심, 이상치 완화\n\n\n코사인\n크기보다 방향 중요\n\n\n마할라노비스\n공분산 구조 반영\n\n\n\n\n\n거리 선택은 모델 선택이다\n거리 척도를 바꾼다는 것은 데이터 생성 과정에 대한 해석을 바꾸는 것과 같다. 따라서 거리 선택은 항상 통계적 정당화가 필요하며, 문제의 목적과 데이터 특성에 근거해 이루어져야 한다.\n\n\n\n2. 스케일과 분포 문제\n\n스케일 문제: 단위가 거리를 지배한다\n변수들의 스케일이 다를 경우, \\(x = (\\text{나이},\\text{소득},\\text{거리})\\) 유클리드 거리에서 분산이 큰 변수 하나가 전체 거리를 지배하게 된다. 이로 인해 의미 없는 변수가 과도하게 반영되고, 군집 구조가 왜곡되며, 결과 해석이 어려워진다.\n이러한 이유로 거리 기반 방법에서는 변수 표준화가 선택 사항이 아니라 전제 조건에 가깝다.\n\n\n분포 문제: 비대칭성과 이상치\n많은 거리 척도는 대칭 분포와 얇은 꼬리를 암묵적으로 가정한다. 그러나 실제 데이터는 비대칭 분포를 가지거나 극단값을 포함하는 경우가 많다. 이 경우 평균과 분산에 기반한 거리, 특히 유클리드 거리는 구조적 패턴보다 이상치에 반응하게 된다.\n\n\n표준화의 한계\n표준화는 스케일 문제를 완화하지만, 분포의 형태, 변수 간 상관성, 그리고 이상치의 영향까지 해결하지는 못한다. 즉, 표준화는 필요조건이지 충분조건이 아니다.\n\n\n\n3. 고차원 공간에서의 거리 붕괴\n\n거리 붕괴 현상이란 무엇인가\n차원이 증가할수록 관측치 간 거리 분포는 다음과 같은 성질을 갖는다.\n\\[\\frac{\\max_{i}d(x,x_{i}) - \\min_{i}d(x,x_{i})}{\\min_{i}d(x,x_{i})} \\longrightarrow 0\\]\n즉, 가장 가까운 점과 가장 먼 점 사이의 상대적 거리 차이가 거의 사라진다. 이를 거리 붕괴(distance concentration) 현상이라고 한다.\n\n\n\n\n\n\n\n통계적 직관\n차원이 커질수록 각 차원의 작은 오차가 누적되며, 거리의 분포는 점점 좁아진다. 이로 인해 ”가깝다”는 개념 자체가 통계적으로 무의미해진다. 이는 계산상의 문제가 아니라 확률적 현상이다.\n\n\n거리 붕괴가 초래하는 결과\n이러한 현상은 최근접 이웃의 의미 상실, K-means 군집 품질 저하, 거리 기반 이상치 탐지의 실패로 이어진다. 즉, 고차원 공간에서는 거리 자체가 정보를 충분히 담지 못한다.\n\n\n\n4. 거리·유사도 사용의 통계적 교훈\n거리 기반 방법이 의미 있으려면 차원이 충분히 낮고, 변수의 스케일과 분포가 안정적이며, 선택한 거리 정의가 문제 구조와 일치해야 한다. 이 중 하나라도 어긋나면 결과는 쉽게 왜곡된다.\n이러한 한계를 인식하면, 거리 자체를 개선하기보다는 다음과 같은 대안적 사고로 이어지게 된다. 차원 축소나 표현 학습을 통해 의미 있는 저차원 공간을 먼저 구성하거나, 확률 기반 유사도를 사용하는 방법, 또는 순위나 이웃 관계를 중심으로 한 분석으로 관점을 전환하는 것이다.\n\n\n5. 거리·유사도 사용을 위한 점검 질문\n거리 기반 방법을 적용하기 전에 다음 질문에 답할 수 있어야 한다. 변수의 단위와 스케일은 비교 가능한가, 거리로 표현되는 차이가 실제 의미 있는 차이인가, 변수 간 상관 구조는 무시해도 되는가, 데이터의 차원과 분포에서 거리 개념이 유지되는가.\n이 질문들에 명확히 답하지 못한다면, 거리 기반 분석의 결과는 해석보다 오해를 낳을 가능성이 크다. 이러한 점에서 거리와 유사도는 계산의 문제가 아니라, 통계적 사고의 문제이다.\n\n\n\nChapter 4. 확률적 군집: GMM은 무엇이 다른가\n앞선 장들에서 살펴본 K-means와 거리 기반 군집 방법은 군집을 평균과 거리, 그리고 분산 최소화라는 관점에서 정의한다. 이러한 접근은 계산적으로 효율적이지만, 군집의 불확실성이나 겹침을 표현하지 못한다는 한계를 갖는다. Gaussian Mixture Model(GMM)은 이러한 한계를 극복하기 위해 군집을 확률분포의 혼합으로 정의하는 확률적 군집 모형이다.\n이 장에서는 거리 기반 군집과 확률 기반 군집의 근본적인 차이를 정리하고, GMM의 생성 모형과 EM 알고리즘의 통계적 의미를 설명한다. 또한 K-means가 GMM의 특수한 경우임을 수식적으로 연결함으로써, 두 방법이 단절된 알고리즘이 아니라 동일한 틀 위에 놓여 있음을 보인다.\n\n1. 거리 기반 군집과 확률 기반 군집의 근본적 차이\n\n거리 최소화와 우도 최대화\n거리 기반 군집에서의 핵심 질문은 ”어느 중심에 가장 가까운가”이다. K-means는 관측치와 군집 중심 사이의 제곱거리를 최소화함으로써 군집을 정의한다. 이때 군집은 평균을 중심으로 한 분산 구조로 규정되며, 확률적 해석은 명시적으로 등장하지 않는다.\n반면 확률 기반 군집에서의 핵심 질문은 ”이 관측치는 어느 분포에서 생성되었을 가능성이 가장 큰가”이다. GMM은 각 군집을 하나의 확률분포로 모델링하고, 전체 데이터는 이 분포들의 혼합으로 생성된다고 가정한다. 군집화는 거리의 문제가 아니라 우도(likelihood)의 문제로 바뀐다.\n\n\n결정적 분할과 확률적 할당\nK-means에서는 각 관측치가 하나의 군집에만 속한다. 군집 할당은 결정적이며, 불확실성은 모델에 포함되지 않는다. 반면 GMM에서는 각 관측치가 모든 군집에 속할 확률을 동시에 갖는다. 군집 할당은 확률적이며, 이 확률은 관측치가 각 군집에 속할 가능성에 대한 정량적 표현이다.\n이 차이는 단순한 출력 형식의 차이가 아니라, 군집을 해석하는 방식 자체의 차이를 의미한다.\n\n\n\n2. Gaussian Mixture Model의 생성 모형\n\n잠재변수와 혼합분포\nGMM은 다음과 같은 생성 모형을 가정한다. 각 관측치 \\(x_{i}\\)에 대해 잠재변수 \\(Z_{i}\\)가 먼저 생성되며, \\(Z_{i} = k\\)일 확률은 \\(\\pi_{k}\\)이다. 이후 \\(Z_{i}\\)가 주어졌을 때 관측치는 다음 분포에서 생성된다.\n\\[X_{i} \\mid Z_{i} = k \\sim \\mathcal{N}(\\mu_{k},\\Sigma_{k})\\]\n따라서 주변 분포는 다음과 같은 혼합분포 형태를 갖는다.\n\\[p(x_{i}) = \\overset{K}{\\sum_{k = 1}}\\pi_{k}\\mathcal{N}(x_{i} \\mid \\mu_{k},\\Sigma_{k})\\]\n이 관점에서 군집은 중심점이 아니라, 평균과 공분산을 가진 확률분포로 정의된다.\n\n\n확률적 군집의 의미\nGMM에서의 군집은 서로 겹칠 수 있으며, 하나의 관측치는 여러 군집에 부분적으로 속할 수 있다. 이는 군집 경계가 명확하지 않은 현실 데이터의 구조를 보다 자연스럽게 반영한다.\n\n\n\n3. EM 알고리즘의 통계적 의미\n\nE-step: 책임도의 기대값\nGMM의 학습은 직접적인 최대우도추정이 어렵기 때문에 EM 알고리즘을 사용한다. E-step에서는 현재 파라미터 값 하에서 각 관측치가 군집 k에서 생성되었을 확률을 계산한다.\n\\[\\gamma_{ik} = \\mathbb{P}(Z_{i} = k \\mid x_{i}) = \\frac{\\pi_{k}\\mathcal{N}(x_{i} \\mid \\mu_{k},\\Sigma_{k})}{\\sum_{j = 1}^{K}\\pi_{j}\\mathcal{N}(x_{i} \\mid \\mu_{j},\\Sigma_{j})}\\]\n이 값은 책임도(responsibility)로 불리며, 관측치 x_i에 대해 군집 k가 설명하는 비중을 의미한다.\n\n\nM-step: 가중 최대우도추정\nM-step에서는 책임도를 가중치로 사용하여 파라미터를 갱신한다. 즉, 각 군집의 평균과 공분산은 해당 군집에 대한 책임도가 높은 관측치들에 의해 더 크게 영향을 받는다.\n이 과정은 확률적 할당 하에서의 가중 MLE로 해석할 수 있다. EM 알고리즘은 우도를 단조 증가시키며, 확률적 군집 구조를 점진적으로 정교화한다.\n\n\n\n4. K-means는 GMM의 특수한 경우이다\nK-means와 GMM은 전혀 다른 알고리즘처럼 보이지만, 특정 가정 하에서는 동일한 문제로 귀결된다. 모든 군집이 동일한 공분산 \\(\\Sigma_{k} = \\sigma^{2}I\\)를 갖고, 혼합 비율이 동일하며, 책임도가 0 또는 1로 제한되는 경우를 생각해 보자.\n이 경우 GMM의 우도 최대화 문제는 각 관측치를 가장 가까운 평균에 할당하는 문제로 단순화된다. 이는 곧 K-means의 목적함수와 동일해진다.\n즉, K-means는 구형 공분산과 결정적 할당을 전제한 GMM의 극단적인 특수 사례로 이해할 수 있다. 이 연결은 거리 기반 군집과 확률 기반 군집이 서로 단절된 방법이 아님을 보여준다.\n\n\n5. 언제 GMM이 더 적합한가\nGMM은 다음과 같은 상황에서 특히 유리하다. 첫째, 군집마다 분산 구조가 다른 경우이다. K-means는 동일 분산을 가정하므로 이러한 차이를 반영하지 못한다.\n둘째, 군집이 서로 겹치는 경우이다. 거리 기반 방법은 경계를 강제로 나누지만, GMM은 겹침을 확률로 표현할 수 있다.\n셋째, 군집 할당의 불확실성이 중요한 문제이다. 예를 들어 위험 평가나 의사결정 문제에서는 단일 군집 할당보다 확률 정보가 더 유용하다.\n이러한 관점에서 GMM은 단순한 군집 알고리즘이 아니라, 군집 구조에 대한 확률적 해석을 제공하는 통계적 모형으로 이해할 수 있다."
  },
  {
    "objectID": "notes/mda/mda_cancorr_manova.html",
    "href": "notes/mda/mda_cancorr_manova.html",
    "title": "다변량분석 7. 정준상관분석 | 다변량분산분석",
    "section": "",
    "text": "Chapter 1. 정준상관분석\n\n1. 정준상관분석이란?\n\n필요 이유\n다변량분석에서는 하나의 변수만으로 현상을 설명하기 어려운 경우가 많다. 사회과학, 경영학, 심리학, 보건학 등 다양한 분야에서 연구자는 여러 개의 설명변수와 여러 개의 반응변수를 동시에 관측하게 된다. 이러한 상황에서 분석의 핵심 질문은 개별 변수 간의 단순한 관계를 넘어, 변수 집합과 변수 집합 사이에 어떤 구조적 연관성이 존재하는가로 확장된다.\n전통적인 회귀분석은 하나의 종속변수를 기준으로 설명변수의 영향을 추정하는 데 초점을 둔다. 이 접근법은 예측이나 인과 해석에는 유용하지만, 종속변수가 여러 개인 경우 각각의 회귀모형을 따로 적합해야 하며, 그 과정에서 종속변수들 간의 상관 구조는 충분히 반영되지 않는다. 반면 주성분분석이나 요인분석은 여러 변수의 정보를 요약하는 데 효과적이지만, 하나의 변수 집합 내부의 구조만을 다룰 뿐 서로 다른 두 변수 집합 간의 관계를 직접적으로 설명하지는 못한다.\n정준상관분석(canonical correlation analysis)은 이러한 분석상의 공백을 메우기 위해 제안된 다변량 기법이다. 이 방법은 두 개의 변수 집합이 주어졌을 때, 각각의 집합에서 선형 결합을 구성하여 그 결합들 사이의 상관을 최대화하는 방향을 찾는다. 즉, 개별 변수 간의 관계가 아니라 두 변수 집합 전체를 대표하는 요약 변수들 간의 관계를 분석의 대상으로 삼는다. 이를 통해 연구자는 한 변수 집합이 다른 변수 집합과 어떤 방식으로 연결되어 있는지를 보다 구조적인 관점에서 이해할 수 있다.\n\n\n정준상관분석 특성\n정준상관분석의 중요한 특징은 분석의 초점이 예측이나 검정에만 있지 않다는 점이다. 이 방법은 두 변수 집합 간의 대응 구조를 탐색하는 데 목적이 있으며, 여러 개의 정준변수를 순차적으로 도출함으로써 복수의 관계 축을 제시한다. 각 정준변수 쌍은 이전 쌍과 직교하도록 구성되므로, 서로 중복되지 않는 독립적인 관계 구조를 단계적으로 해석할 수 있다. 이러한 특성은 복잡한 다차원 관계를 체계적으로 분해하여 이해하는 데 큰 장점을 제공한다.\n정준상관분석은 또한 다변량 분산분석이나 판별분석과 같은 다른 다변량 기법을 이론적으로 연결해 주는 역할을 한다. 예를 들어, 집단 정보를 더미 변수로 구성하여 설명변수 집합으로 두는 경우, 다변량 분산분석은 정준상관분석의 특수한 형태로 해석될 수 있다. 이러한 관점은 다변량분석을 개별 기법의 나열이 아니라, 공통된 수리적 틀 위에서 이해되는 하나의 체계로 인식하게 한다는 점에서 교육적으로 중요한 의미를 갖는다.\n그럼에도 불구하고 정준상관분석은 실무 연구에서 자주 사용되는 기법은 아니다. 해석이 직관적이지 않고, 표본 크기와 가정에 대한 요구가 상대적으로 크기 때문이다.\n\n\n\n2. 개념 및 기본설정\n정준상관분석은 여러 변수로 이루어진 두 변수 군(집합) 간의 선형 상관 관계를 분석하는 다변량 통계 방법이다. 이 방법의 목적은 개별 변수 간의 관계를 각각 살펴보는 것이 아니라, 두 변수 집합을 대표하는 선형 결합을 구성하여 그 결합들 사이의 상관을 최대화하는 데 있다.\n예를 들어, 신체적 조건을 나타내는 변수군(키, 몸무게, 가슴둘레 등)과 운동 능력을 나타내는 변수군(달리기 기록, 윗몸일으키기 횟수, 턱걸이 횟수 등)이 주어졌다고 하자. 이 경우 연구의 관심은 특정 신체 조건 하나가 특정 운동 능력 하나와 얼마나 관련되는가가 아니라, 신체적 조건 전반과 운동 능력 전반 사이에 체계적인 선형 관계가 존재하는가에 있다. 정준상관분석은 이러한 질문에 답하기 위한 분석 방법이다.\n\n변수 군 간 선형 결합과 정준변수\n정준상관분석에서는 다음과 같이 두 개의 변수 집합을 고려한다.\n\\[(X_{1},X_{2},\\ldots,X_{m}),(Y_{1},Y_{2},\\ldots,Y_{n})\\]\n분석의 핵심은 각 변수 집합에 대해 선형 결합을 구성하고, 이 두 선형 결합 간의 상관계수를 최대화하는 것이다. 즉,\n\\(U = a_{1}X_{1} + a_{2}X_{2} + \\cdots + a_{m}X_{m}\\),\n\\(V = b_{1}Y_{1} + b_{2}Y_{2} + \\cdots + b_{n}Y_{n}\\)와 같은 두 개의 선형 결합을 정의하고, \\(\\text{Corr}(U,V)\\)가 최대가 되도록 계수 \\(a_{1},\\ldots,a_{m},b_{1},\\ldots,b_{n}\\)을 결정한다. 여기서 U와 V를 각각 정준변수(canonical variate) 라고 한다.\n이 과정은 단일한 상관계수를 계산하는 데서 끝나지 않으며, 첫 번째 정준변수 쌍과 직교하면서 그다음으로 큰 상관을 갖는 두 번째 정준변수 쌍을 순차적으로 구할 수 있다. 이를 통해 두 변수 집합 간의 다차원적 관계 구조를 단계적으로 파악할 수 있다.\n\n\n확률적 설정과 공분산 구조\n전체 변수를 하나의 벡터로 묶어 \\(\\mathbf{x} = (x_{1},x_{2},\\ldots,x_{p})^{\\top}\\)라 하고, 이를 두 개의 부분 벡터로 나눈다고 하자. \\(\\mathbf{x} = \\left( \\begin{array}{r}\n\\mathbf{x}_{1} \\\\\n\\mathbf{x}_{2}\n\\end{array} \\right)\\). 정준상관분석에서는 일반적으로 이 벡터가 다변량 정규분포를 따른다고 가정한다.\n\\(\\mathbf{x} \\sim \\mathcal{N}\\left( \\left( \\begin{array}{r}\n\\mathbf{\\mu}_{1} \\\\\n\\mathbf{\\mu}_{2}\n\\end{array} \\right),\\begin{pmatrix}\n\\Sigma_{11} & \\Sigma_{12} \\\\\n\\Sigma_{21} & \\Sigma_{22}\n\\end{pmatrix} \\right)\\) 여기서 \\(\\Sigma_{12}\\)와 \\(\\Sigma_{21}\\)은 두 변수 집합 간의 공분산 구조를 나타내며, 정준상관분석은 이 공분산 구조를 바탕으로 두 집합의 선형적 연관성을 최대화하는 방향을 찾는다.\n\n\n정준상관분석의 특수한 경우\n정준상관분석은 여러 익숙한 통계 분석 방법을 포괄하는 일반적인 틀로 이해할 수 있다.\n첫째, 각 변수 군이 모두 하나의 변수로만 구성되어 있다면, 정준상관분석은 두 변수 간의 단순 상관계수로 귀결된다.\n둘째, 한쪽 변수 군이 하나의 변수로만 구성되어 있고, 다른 한쪽이 여러 변수로 이루어진 경우에는 다중회귀분석과 밀접한 관련을 갖는다. 이때 정준상관계수는 다중회귀모형에서의 결정계수 \\(R^{2} = \\frac{\\text{SSR}}{\\text{SST}}\\)의 제곱근 \\(\\sqrt{R^{2}}\\)에 해당한다. 즉, 종속변수와 설명변수들의 선형 결합 사이의 최대 상관을 의미한다.\n이와 같이 정준상관분석은 단순 상관분석과 다중회귀분석을 포함하는 보다 일반적인 분석 방법이며, 다변량 자료에서 변수 집합 간 관계를 체계적으로 이해하기 위한 이론적 기반을 제공한다.\n\n\n\n3. 정준변수 구하기\n정준상관분석의 핵심은 두 변수 군을 각각 하나의 대표 변수로 요약하고, 이 두 요약 변수 간의 상관을 최대화하는 데 있다. 이를 위해 각 변수 군에 대한 선형 결합을 정의하고, 그 선형 결합 간 상관계수가 최대가 되도록 하는 계수를 구한다. 이때 얻어지는 선형 결합을 정준변수(canonical variate) 라 한다.\n\n제1 정준변수\n두 변수 군의 선형 결합 간 상관계수를 가장 크게 하는 경우를 먼저 고려하자. 두 변수 군을 각각 \\(\\mathbf{x}_{1},\\mathbf{x}_{2}\\)라 하면, 다음과 같은 선형 결합을 정의할 수 있다. \\(V_{1} = \\mathbf{a}_{1}^{\\top}\\mathbf{x}_{1},W_{1} = \\mathbf{b}_{1}^{\\top}\\mathbf{x}_{2}\\)\n정준상관분석에서는 다음의 최적화 문제를 고려한다.\n\\(\\rho_{1} = \\max_{\\mathbf{a}_{1} \\neq 0,\\mathbf{b}_{1} \\neq 0}\\text{corr}(V_{1},W_{1})\\) 이 조건을 만족하는 \\(\\mathbf{a}_{1},\\mathbf{b}_{1}\\)에 의해 정의되는 \\(V_{1},W_{1}\\)을 제1정준변수라 하고, 이때의 상관계수 \\(\\rho_{1}\\)을 제1정준상관계수라 한다. 정준변수는 크기의 임의성이 존재하므로, 해의 유일성을 확보하기 위해 다음과 같은 정규화 조건을 부여한다.\n\\[\\text{Var}(V_{1}) = \\text{Var}(W_{1}) = 1\\]\n이는 공분산 행렬을 이용하여 다음과 같이 표현된다.\n\\[\\mathbf{a}_{1}^{\\top}\\Sigma_{11}\\mathbf{a}_{1} = 1,\\mathbf{b}_{1}^{\\top}\\Sigma_{22}\\mathbf{b}_{1} = 1\\]\n\n\n제2 정준변수\n제1정준변수가 두 변수 군 간의 가장 강한 선형 관계를 설명한다면, 그다음 단계에서는 이미 설명된 관계와 독립적인 추가 관계를 탐색한다. 이를 위해 다음과 같은 선형 결합을 정의한다.\n\\(V_{2} = \\mathbf{a}_{2}^{\\top}\\mathbf{x}_{1},W_{2} = \\mathbf{b}_{2}^{\\top}\\mathbf{x}_{2}\\). 제2정준변수는 다음의 조건을 만족하도록 정의된다.\n\\(V_{2}\\)와 \\(W_{2}\\)는 각각 \\(V_{1}\\)과 \\(W_{1}\\)에 대해 직교(독립)한다.\n\\[\\text{Var}(V_{2}) = \\text{Var}(W_{2}) = 1\\]\n이 조건하에서 \\(\\rho_{2} = \\text{corr}(V_{2},W_{2})\\)를 제2정준상관계수라 한다.\n이와 같은 방식으로 이후의 정준변수들도 순차적으로 정의할 수 있으나, 실제 분석에서는 해석의 어려움으로 인해 보통 2–3개의 정준변수까지만 해석하는 경우가 많다.\n\n\n정준상관계수의 개수\n정준상관계수의 개수는 두 변수 군의 차원에 의해 결정된다. 한 변수 군의 변수 수가 p, 다른 변수 군의 변수 수가 q일 때, 정의될 수 있는 정준상관계수의 최대 개수는 \\(\\min(p,q)\\)이다. 이는 두 변수 군 중 차원이 더 작은 변수 군의 정보만큼만 독립적인 관계 축을 구성할 수 있기 때문이다. 일반적으로 각각 2개 정준변수를 사용한다.\n\n\n정준상관계수의 유의성 검정\n정준상관계수의 통계적 유의성은 두 변수 군 간에 선형 관계가 존재하는지를 검정하는 문제로 귀결된다. 먼저 제1정준상관계수에 대해 다음의 가설을 고려한다.\n\\(H_{01}:\\rho_{1} = 0\\text{vs.}H_{01}:\\rho_{1} \\neq 0\\) 이는 공분산 행렬의 관점에서 다음의 가설과 동치이다.\n\\[H_{01}:\\Sigma_{12} = 0\\text{vs.}H_{01}:\\Sigma_{12} \\neq 0\\]\n이때 사용되는 검정 통계량은 Wilks의 람다(Wilks’ Lambda)로,\n\\(T = \\frac{|\\widehat{\\Sigma}|}{|{\\widehat{\\Sigma}}_{11}||{\\widehat{\\Sigma}}_{22}|} = \\overset{k}{\\prod_{i = 1}}(1 - {\\widehat{\\rho}}_{i}^{2}),k = \\min(p,q)\\)로 정의된다.\n보다 일반적으로 r번째 이후의 정준상관계수에 대한 검정은\n\\(H_{0r}:\\rho_{r} = 0\\text{vs.}H_{0r}:\\rho_{r} \\neq 0\\) 의 형태로 설정되며, 이에 대한 검정 통계량은 \\(T_{r} = \\overset{k}{\\prod_{i = r}}(1 - {\\widehat{\\rho}}_{i}^{2})\\)이다. 이 통계량은 적절한 변환을 거쳐 카이제곱 분포를 따르며, 이를 이용해 정준상관계수의 유의성을 판단한다.\n\n\n\n4. 정준변수 특성\n정준상관분석에서 정준변수는 두 변수군 각각에 대해 구성된 선형결합 변수로서, 서로 짝을 이루어 해석된다. 각 정준변수 쌍은 두 변수군 간의 관계를 가장 잘 요약하는 축을 의미하며, 이 축은 상관계수가 최대가 되도록 정의된다. 따라서 정준변수 간의 관계는 개별 관측변수 간의 단순한 상관관계가 아니라, 두 변수군 전체의 구조적 연관성을 요약한 결과로 이해할 수 있다.\n정준변수의 관계는 순차적이고 계층적인 구조를 가진다. 제1정준변수 쌍은 두 변수군 사이의 가장 강한 선형적 연관성을 설명하며, 이후의 제2, 제3 정준변수 쌍은 앞선 정준변수 쌍과 서로 직교하도록 구성된다. 이 직교성은 각 정준변수 쌍이 이전 정준관계로 설명되지 않은 새로운 관계 구조를 독립적으로 포착함을 의미한다. 따라서 정준상관분석은 단일한 관계 요약에 그치지 않고, 두 변수군 간에 존재하는 다차원적 연관 구조를 단계적으로 분해하는 분석 방법이라 할 수 있다.\n정준변수 간의 상관관계는 각 정준쌍 내부에서만 의미를 가지며, 서로 다른 차수의 정준변수 사이에는 상관이 존재하지 않는다. 이는 각 정준변수 쌍이 서로 중복되지 않는 정보만을 담고 있음을 보장한다. 이러한 성질로 인해 연구자는 정준상관계수의 크기와 통계적 유의성을 기준으로, 해석에 포함할 정준변수 쌍의 수를 합리적으로 결정할 수 있다.\n해석 측면에서 정준변수 간의 관계는 두 변수군을 대표하는 잠재적 요약 축 간의 관계로 이해된다. 즉, 정준변수는 개별 변수의 효과를 직접적으로 설명하기보다는, 여러 변수가 결합된 종합적인 경향이나 패턴을 나타낸다. 따라서 정준변수 간의 높은 상관관계는 두 변수군이 공통적으로 공유하는 구조적 특성이 강하다는 것을 의미하며, 반대로 낮은 상관관계는 두 변수군 간의 연관성이 제한적이거나 특정 차원에만 국한되어 있음을 시사한다.\n요약하면, 정준변수 간의 관계는 두 변수군 사이의 연관성을 가장 효과적으로 요약한 잠재 축 간의 관계이며, 이러한 관계는 순차적·직교적 구조를 통해 체계적으로 분해된다. 정준상관분석은 이와 같은 정준변수 관계를 바탕으로, 복수의 변수군이 어떻게 연결되어 있는지를 전체 구조 차원에서 이해하도록 돕는 분석 방법이다.\n\n정준변수 적재량\n정준상관분석에서 정준적재량(canonical loadings)은 각 관측변수가 동일한 변수군에서 도출된 정준변수와 어느 정도의 선형적 연관성을 가지는지를 나타내는 지표이다. 이는 관측변수와 정준변수 간의 상관계수로 정의되며, 정준변수가 어떤 변수들의 공통된 변동을 주로 반영하고 있는지를 파악하는 데 사용된다. 따라서 정준적재량은 정준변수의 해석을 가능하게 하는 핵심적인 도구로, 정준변수를 단순한 수학적 결합이 아닌 의미 있는 요약 축으로 이해하도록 돕는다.\n정준적재량은 정준계수와 구별되어 해석되어야 한다. 정준계수가 정준변수를 구성하는 선형결합의 가중치를 의미한다면, 정준적재량은 그 결과로 형성된 정준변수와 원변수 간의 실제 연관 정도를 보여준다. 이로 인해 정준적재량은 변수의 단위나 다중공선성의 영향을 상대적으로 덜 받으며, 해석의 안정성이 높다. 실제 분석에서는 정준변수의 성격을 규정할 때 정준계수보다 정준적재량을 중심으로 해석하는 것이 일반적이다.\n교차적재량(cross-loadings)은 한 변수군의 관측변수가 다른 변수군에서 도출된 정준변수와 가지는 상관관계를 의미한다. 이는 두 변수군 간의 관계가 정준변수 수준에서만 존재하는 것이 아니라, 개별 변수 차원에서 어떻게 연결되는지를 구체적으로 보여주는 지표이다. 교차적재량을 통해 연구자는 두 변수군 간의 연관 구조가 어떤 변수들을 매개로 형성되는지를 보다 직접적으로 파악할 수 있다.\n교차적재량은 두 변수군 사이의 실질적인 연결 강도를 평가하는 데 활용된다. 정준상관계수가 두 정준변수 간의 전반적인 연관성을 요약한다면, 교차적재량은 그 연관성이 각 관측변수 수준에서 어떻게 분해되는지를 설명한다. 따라서 교차적재량은 정준상관분석 결과를 해석 가능한 형태로 구체화하며, 분석 결과를 실천적·정책적 논의로 확장하는 데 중요한 근거를 제공한다.\n종합하면, 정준적재량은 각 정준변수가 무엇을 대표하는지를 설명하는 내부적 해석 지표이며, 교차적재량은 두 변수군이 어떻게 연결되는지를 변수 수준에서 드러내는 외부적 해석 지표라 할 수 있다. 정준상관분석의 해석에서는 정준상관계수만으로 결론을 도출하기보다는, 정준적재량과 교차적재량을 함께 고려함으로써 두 변수군 간 관계의 구조적 의미를 보다 명확하고 체계적으로 이해할 수 있다.\n\n\n\n5. 정준변수 활용\n정준변수는 두 변수군에 포함된 다수의 변수를 각각 하나의 요약된 축으로 축약함으로써, 변수 차원 축소의 역할을 수행한다. 원래 정준상관분석에서는 p개의 설명변수와 q개의 반응변수를 동시에 고려해야 하지만, 정준변수 쌍을 이용하면 이들 변수군 사이의 관계를 소수의 정준축으로 요약할 수 있다. 이러한 축은 단순한 분산 설명이 아니라, 두 변수군 간 상관구조가 가장 강하게 드러나는 방향으로 구성된다는 점에서 의미를 가진다. 즉, 정준변수는 ”두 변수군 사이의 핵심적인 연관 구조”를 저차원 공간에서 표현하는 도구로 이해할 수 있다. 이 점에서 단일 변수군 내부의 분산 구조를 요약하는 주성분분석(PCA)과 대비되며, 정준상관분석은 두 변수군 간의 공통된 구조를 요약하는 데 목적이 있다.\n정준변수는 또한 두 변수군 간 관계를 해석하기 위한 핵심적인 분석 도구로 활용된다. 이를 통해 연구자는 두 변수군 사이에 통계적으로 의미 있는 연관성이 존재하는지, 그 연관성이 어떤 변수들의 결합을 통해 주로 형성되는지, 그리고 제1정준관계 이후에도 서로 독립적인 추가적 관계 구조가 존재하는지를 체계적으로 검토할 수 있다. 이러한 해석을 위해서는 정준상관계수의 크기를 통해 각 정준변수 쌍의 중요성을 평가하고, 정준적재량을 통해 각 관측변수가 해당 정준변수에 기여하는 정도를 파악하며, 교차적재량을 통해 한 변수군의 정준변수가 다른 변수군의 개별 변수들과 어떻게 연관되는지를 함께 살펴보는 것이 일반적이다.\n더 나아가 정준변수는 해석 가능한 잠재지표로서의 역할도 수행한다. 정준변수는 개별 관측변수들의 단순한 집합이 아니라, 이들 변수를 선형 결합하여 구성한 잠재적인 종합 지표로 이해될 수 있다. 예를 들어, 학습투입을 나타내는 공부시간, 출석률, 과제수행도로 구성된 변수군과 학습성과를 나타내는 시험점수와 프로젝트 평가 점수로 구성된 변수군을 고려할 경우, 정준변수는 각각 ”학습투입의 종합적 수준”과 ”학습성과의 종합적 수준”을 대표하는 지표로 해석될 수 있다. 이와 같이 정준변수는 복수의 관측변수를 하나의 해석 가능한 축으로 통합함으로써, 다변량 자료의 구조적 관계를 직관적으로 이해할 수 있도록 돕는다.\n\n\n6. 정준상관분석 사례\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cross_decomposition import CCA\n\nnp.random.seed(123)\n\nn = 40  # 표본 수\n\ndata = pd.DataFrame({\n    # X 변수군: 학습투입\n    \"study_time\": np.random.normal(10, 3, n),\n    \"attendance\": np.random.normal(90, 5, n),\n    \"assignment\": np.random.normal(80, 8, n),\n    \"participation\": np.random.normal(4, 0.6, n),\n\n    # Y 변수군: 학습성과\n    \"exam\": np.random.normal(82, 7, n),\n    \"project\": np.random.normal(85, 6, n),\n    \"achievement\": np.random.normal(84, 6, n)\n})\nprint(data.head())\nstudy_time attendance assignment participation exam project achievement  0 6.743108 85.973167 90.041899 3.834528 82.142212 89.219861 85.745246  1 12.992036 81.361653 74.489048 3.668735 80.642253 81.411368 85.677976  (이하 생략)\n#정준상관분석은 척도에 민감하므로 표준화가 필수이다.\nX = data[[\"study_time\", \"attendance\", \"assignment\", \"participation\"]]\nY = data[[\"exam\", \"project\", \"achievement\"]]\n\nscaler_X = StandardScaler()\nscaler_Y = StandardScaler()\n\nX_std = scaler_X.fit_transform(X)\nY_std = scaler_Y.fit_transform(Y)\n\n#정준상관분석 적합\ncca = CCA(n_components=2)\ncca.fit(X_std, Y_std)\n\nX_c, Y_c = cca.transform(X_std, Y_std)\n\n#정준상관계수 계산\ncorrs = [np.corrcoef(X_c[:, i], Y_c[:, i])[0, 1]\n         for i in range(X_c.shape[1])]\n\nfor i, c in enumerate(corrs, start=1):\n    print(f\"제{i}정준상관계수: {c:.3f}\")\n제1정준상관계수는 0.376으로, 두 변수군(X: 학습투입, Y: 학습성과) 사이에 중간 이하 수준의 양(+)의 선형 연관성이 존재함을 의미한다. 즉, 학습투입 변수들의 특정 선형결합과 학습성과 변수들의 특정 선형결합 사이에 일정한 방향성 있는 관계는 관찰되지만, 그 크기는 강하다고 보기는 어렵다. 순차적 Wilks’ Lambda 검정 결과, 제1정준상관에 대한 p-value가 0.8662로 매우 크게 나타나, 모집단 수준에서 해당 정준상관이 통계적으로 유의하다고 판단할 근거는 없다.\n제2정준상관계수는 0.208로, 제1정준상관보다 더 약한 수준의 연관성을 보인다. 이에 대한 Wilks’ Lambda 검정에서도 p-value가 0.9562로 나타나, 제2정준상관 역시 통계적으로 전혀 유의하지 않다. 이는 첫 번째 정준관계를 제거한 이후에는 추가적인 의미 있는 공통 구조가 거의 남아 있지 않음을 시사한다.\n종합하면, 본 결과는 표본에서는 약한 정준상관이 관찰되지만, 두 변수군 간의 관계를 통계적으로 유의한 구조적 연관성으로 일반화하기는 어렵다는 결론에 해당한다. 즉, 학습투입과 학습성과 사이에 명확하고 안정적인 다변량적 연결 구조가 존재한다고 말하기에는 근거가 부족한 상황이다.\n#정준 적재량\nloadings_X = np.corrcoef(X_std.T, X_c.T)[:X.shape[1], X.shape[1]:]\nloadings_Y = np.corrcoef(Y_std.T, Y_c.T)[:Y.shape[1], Y.shape[1]:]\n\nload_X = pd.DataFrame(loadings_X,\n                      index=X.columns,\n                      columns=[f\"Can{i+1}\" for i in range(cca.n_components)])\n\nload_Y = pd.DataFrame(loadings_Y,\n                      index=Y.columns,\n                      columns=[f\"Can{i+1}\" for i in range(cca.n_components)])\n\nload_X, load_Y\n정준상관분석 결과, 두 개의 정준축이 도출되었으며, 해석의 중심은 제1정준축에 두는 것이 타당하다. 제1정준축에서 X 변수군의 정준적재량을 보면, assignment가 0.96으로 매우 높은 값을 보여 다른 변수들에 비해 압도적인 기여를 하고 있다. 반면 study_time은 거의 0에 가까운 적재량을 보이며, attendance와 participation 역시 상대적으로 작은 값을 나타내어 제1정준축 형성에 있어 핵심적인 역할을 하지 않는 것으로 판단된다. 이는 제1정준변수가 학습투입 중에서도 단순한 시간 투자나 출석보다는 과제 수행 정도를 중심으로 구성된 축임을 의미한다.\nY 변수군의 제1정준축 적재량을 살펴보면, achievement가 −0.78로 가장 큰 절댓값을 보이며, project 또한 0.45로 중간 수준의 기여를 하고 있다. 반면 exam은 거의 0에 가까운 값을 보여 제1정준관계에서 실질적인 역할을 하지 않는다. 이를 종합하면, 제1정준변수는 학습성과 측면에서 종합적인 성취도를 중심으로 형성된 축으로 해석할 수 있다. 정준적재량의 부호는 상대적인 방향을 의미할 뿐이므로, 해석에서는 부호보다는 절댓값의 크기와 변수 조합에 주목하는 것이 중요하다.\n따라서 제1정준관계는 과제 수행 중심의 학습투입과 종합적 학습 성취도 사이의 연관 구조를 나타내는 것으로 해석된다. 이는 학습성과를 설명하는 데 있어 단순한 시험 점수나 출석보다 과제 기반 학습 활동이 더 핵심적인 역할을 할 수 있음을 시사한다.\n한편 제2정준축에서는 X 변수군 중 participation이 0.69로 가장 높은 적재량을 보여, 이 축이 수업 참여도를 중심으로 형성되어 있음을 알 수 있다. Y 변수군에서는 project와 achievement가 각각 −0.79, −0.60으로 비교적 큰 절댓값을 보여 프로젝트 및 성취도와 관련된 성과 축으로 해석된다. 다만 제2정준상관계수의 크기가 작고, 유의성 검정에서도 통계적으로 유의하지 않은 것으로 나타났기 때문에, 이 축은 독립적인 의미 있는 관계 구조라기보다는 보조적·탐색적 수준에서만 해석하는 것이 적절하다.\n종합하면, 본 분석에서 실질적인 해석 가치는 제1정준축에 집중되며, 학습투입 변수 중 과제 수행이 학습성과 중 종합 성취도와 가장 밀접하게 연결되는 구조가 확인된다. 이는 정준상관분석이 두 변수군 간의 관계를 개별 변수 간 상관이 아니라, 변수들의 선형 결합을 통한 구조적 관계로 이해하게 해주는 분석 방법임을 잘 보여주는 사례라 할 수 있다.\n( Can1 Can2  study_time 0.026286 0.215788  attendance -0.293917 -0.333222  assignment 0.960446 -0.265262  participation 0.157126 0.686076,\nCan1 Can2  exam -0.005259 0.163121  project 0.451057 -0.793028  achievement -0.775036 -0.602302)\n# 교차 적재량 (Cross-loadings)\n\n# X 변수 ↔ Y 정준변수\ncross_XY = np.corrcoef(X_std.T, Y_c.T)[:X.shape[1], X.shape[1]:]\ncross_XY = pd.DataFrame(\n    cross_XY,\n    index=X.columns,\n    columns=[f\"CanY{i+1}\" for i in range(cca.n_components)]\n)\n\n# Y 변수 ↔ X 정준변수\ncross_YX = np.corrcoef(Y_std.T, X_c.T)[:Y.shape[1], Y.shape[1]:]\ncross_YX = pd.DataFrame(\n    cross_YX,\n    index=Y.columns,\n    columns=[f\"CanX{i+1}\" for i in range(cca.n_components)]\n)\n\ncross_XY, cross_YX\n교차적재량은 한 변수군의 개별 변수가 상대 변수군의 정준변수와 얼마나 직접적으로 연결되는지를 보여주는 지표이다. 본 분석에서 X 변수군(학습투입)의 교차적재량을 살펴보면, 제1정준축(CanY1)에서 assignment가 0.36으로 가장 큰 값을 나타내어, 학습투입 변수 중 과제 수행이 학습성과 측 정준변수와 가장 밀접하게 관련되어 있음을 알 수 있다. 반면 study_time과 participation은 매우 작은 값을 보이며, attendance 역시 −0.11 수준에 그쳐 학습성과 정준축과의 직접적인 연관성은 크지 않은 것으로 나타난다. 이는 학습투입 중에서도 단순한 시간 투자나 출석보다는 과제 수행이 성과 측 정준구조와 실질적으로 연결되는 핵심 요소임을 시사한다.\nY 변수군(학습성과)의 교차적재량을 보면, 제1정준축(CanX1)에서 achievement가 −0.29로 가장 큰 절댓값을 보이며, project는 0.17로 제한적인 기여를 하고 있다. 반면 exam은 거의 0에 가까운 값을 보여 학습투입 정준변수와의 직접적인 관련성이 거의 없음을 확인할 수 있다. 이는 학습성과 중에서도 종합적인 성취도가 학습투입 구조와 가장 밀접하게 연결되어 있으며, 시험 점수는 이 정준관계에서 중심적인 역할을 하지 않는다는 점을 의미한다.\n제2정준축(CanY2, CanX2)에서는 모든 교차적재량 값이 전반적으로 작게 나타난다. X 변수군에서는 participation이 0.14로 상대적으로 큰 값을 보이지만 절댓값 기준으로 해석 임계치에는 미치지 못하며, Y 변수군에서도 project와 achievement가 각각 −0.16, −0.13 수준에 그친다. 이는 제2정준관계가 두 변수군 사이의 추가적인 독립적 연관 구조를 충분히 설명하지 못함을 시사하며, 앞서 정준상관계수의 유의성 검정 결과와도 일관된 해석이다.\n종합하면, 교차적재량 분석 결과는 과제 수행 중심의 학습투입이 종합적 학습 성취도와 가장 직접적으로 연결되어 있음을 보여주며, 단순한 시험 점수나 출석, 학습 시간은 정준관계의 핵심 연결 고리로 작용하지 않는 것으로 나타난다. 이러한 결과는 정준적재량 해석에서 도출된 구조를 교차적 관점에서 다시 확인해 주는 보완적 증거로 이해할 수 있다.\n( CanY1 CanY2  study_time 0.009878 0.044975  attendance -0.110385 -0.069362  assignment 0.360666 -0.055188  participation 0.059028 0.142663,\nCanX1 CanX2  exam -0.001975 0.033933  project 0.169384 -0.164955  achievement -0.291048 -0.125320)\n#제1 정준변수 산점도\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(6, 5))\nplt.scatter(X_c[:, 0], Y_c[:, 0])\nplt.axhline(0, linestyle='--')\nplt.axvline(0, linestyle='--')\nplt.xlabel(\"Canonical Variable U1 (X)\")\nplt.ylabel(\"Canonical Variable V1 (Y)\")\nplt.title(\"First Canonical Variable Scatter\")\nplt.show()\n제1정준변수 산점도는 X 변수군의 제1정준변수(U1)와 Y 변수군의 제1정준변수(V1) 사이에 완만한 양(+)의 선형 경향이 존재함을 시각적으로 보여준다. 이는 제1정준상관계수(ρ = 0.376)가 양의 값을 갖는다는 점과 일관된다. 다만 점들의 분포가 대각선 방향으로 조밀하게 집중되어 있지 않고, 사분면 전반에 비교적 넓게 흩어져 있어 관계의 강도는 제한적임을 알 수 있다.\n정준적재량을 함께 고려하면, 제1정준변수(U1)는 X 변수군 중에서 과제 수행 정도(assignment)의 기여가 압도적으로 크며, 나머지 변수들(study_time, attendance, participation)은 상대적으로 작은 적재량을 보인다. 따라서 U1 축의 양(+) 방향은 주로 과제 수행 수준이 높은 학습자를 의미하는 축으로 해석할 수 있다.\n한편, Y 변수군의 제1정준변수(V1)는 achievement(성취도)가 큰 음(-)의 적재량을 가지며, project는 중간 정도의 양(+) 적재량을 보인다. 이는 V1 축이 단일 성과 지표라기보다는 성과 구성 간의 대비 구조를 반영하고 있음을 시사한다.\n이러한 적재량 구조를 바탕으로 산점도를 해석하면, 우상단 사분면에 위치한 관측치는 과제 수행 수준이 높고(project 중심의 성과가 상대적으로 높은) 집단으로, 좌하단 사분면의 관측치는 과제 수행 수준이 낮고 성취도 중심 성과가 상대적으로 높은 집단으로 해석할 수 있다. 그러나 사분면 간 경계가 명확하지 않고 중심부에 많은 점들이 분포해 있다는 점에서, 이러한 구분은 뚜렷한 군집이라기보다는 경향 수준의 해석에 그친다.\n종합하면, 제1정준변수 산점도는 정준적재량에서 확인된 핵심 변수(assignment, achievement)의 구조를 반영하여 두 변수군 간의 약한 공통 경향을 시각적으로 보여주지만, 점 분포의 산포가 커 강한 구조적 연관성이나 명확한 집단 구분을 주장하기에는 한계가 있다.\n\n\n\n\n\n\n\n\nChapter 2. 다변량분산분석\n\n1. 다변량분산분석이란?\n\n개념과 필요성\n분산분석은 집단 간 평균 차이가 존재하는지를 검정하는 통계적 방법이다. 예를 들어, 한 지역에 세 개의 호수가 존재할 때 호수별 산소량에 차이가 있는지를 알고자 한다면, 호수를 요인으로 하는 일원 분산분석을 통해 각 호수의 평균 산소량 차이를 검정할 수 있다. 또한 호수의 오염 정도를 파악하기 위해 수은 함유량을 측정하였다면, 이 역시 호수별 평균 수은 함유량의 차이를 일원 분산분석으로 분석할 수 있다. 이처럼 하나의 종속변수에 대해 집단 간 평균 차이를 검정하는 경우에는 단변량 분산분석으로 충분하다.\n그러나 실제 문제에서는 관심 있는 종속변수가 하나에 그치지 않는 경우가 많다. 예를 들어 호수의 수질 상태를 평가할 때 산소량과 수은 함유량은 서로 독립적인 특성이 아니라 일정한 상관관계를 가질 가능성이 크다. 이러한 상황에서 산소량과 수은 함유량을 각각 따로 분산분석으로 검정한다면, 두 변수 간의 상관 구조를 무시한 채 부분적인 결론만을 얻게 된다. 이 경우 각 변수에 대해 반복적으로 검정을 수행함으로써 제1종 오류가 증가하는 문제도 발생할 수 있다.\n이처럼 분산분석에서 동시에 고려해야 할 종속변수가 두 개 이상인 경우, 이들 변수 간의 상관관계를 반영하여 집단 간 차이를 검정하는 방법이 다변량 분산분석이다. 다변량 분산분석은 여러 종속변수를 하나의 평균 벡터로 묶어 집단 간 차이를 검정함으로써, 단변량 분석으로는 포착하기 어려운 종합적인 차이를 평가할 수 있도록 한다. 즉, 다변량 분산분석은 개별 변수의 평균 차이가 아니라, 종속변수들의 결합된 구조적 차이를 분석하는 방법이다.\n다변량 분산분석은 심리학, 사회과학, 교육학 등 다양한 분야에서 특히 유용하다. 예를 들어 근심 지수, 불면 지수, 불쾌 지수와 같이 서로 상관된 여러 심리적 지표를 동시에 고려할 때, 성별이나 재산 수준에 따른 차이를 분석하기 위해 다변량 분산분석을 적용하는 것이 바람직하다. 이러한 경우 각각의 지표를 따로 분석하는 것보다, 여러 지표를 하나의 다변량 반응으로 다루는 것이 현상을 보다 정확하고 일관되게 설명할 수 있다. 요인이 하나인 경우에는 이를 일원 다변량 분산분석이라 하며, 요인이 둘 이상인 경우에는 이원 또는 다원 다변량 분산분석으로 확장된다.\n일원 다변량 분산분석은 본질적으로 집단 간 다변량 평균 벡터의 차이를 검정하는 방법이다. 다시 말해, 여러 종속변수로 구성된 평균 벡터가 집단에 따라 동일한지 여부를 검정하는 것이다. 본 교재에서는 개념 이해와 방법론적 설명의 명확성을 위해 일원 다변량 분산분석을 중심으로 다루며, 이원 다변량 분산분석은 단변량 분산분석의 확장과 유사한 방식으로 이해할 수 있음을 함께 제시한다.\n\n\n정준상관분석과 관계\n다변량 분산분석(MANOVA)은 집단 간에 여러 종속변수로 이루어진 평균 벡터의 차이가 존재하는지를 검정하는 방법이다. MANOVA의 1차적인 목적은 집단 효과의 존재 여부를 판단하는 데 있으며, 이 단계에서는 Wilks’ Lambda, Pillai의 추적, Hotelling–Lawley 추적과 같은 다변량 검정 통계량을 통해 전체적인 차이가 유의한지를 평가한다. 그러나 이러한 검정 결과만으로는 집단 간 차이가 구체적으로 어떤 방향과 구조를 통해 나타나는지를 파악하기 어렵다.\n이때 MANOVA의 해석 단계에서 핵심적인 역할을 하는 것이 정준변수이다. MANOVA는 본질적으로 집단 효과를 가장 잘 분리하는 종속변수들의 선형결합을 찾는 문제로 해석할 수 있으며, 이 선형결합이 바로 정준변수에 해당한다. 즉, 정준변수는 집단 간 차이를 최대화하도록 구성된 종속변수의 요약 축으로, MANOVA에서 관측된 집단 효과가 어떤 다변량 방향에서 가장 뚜렷하게 나타나는지를 설명해 준다.\nMANOVA에서의 정준변수는 집단 요인과 종속변수 집합 사이의 관계를 정준상관의 관점에서 이해할 수 있게 한다. 집단을 나타내는 범주형 요인은 적절한 지시변수 형태로 표현될 수 있으며, 이 집단 정보와 종속변수 집합 사이의 관계를 정준상관분석으로 해석하면, 집단 차이를 가장 잘 설명하는 정준축이 도출된다. 따라서 MANOVA의 정준변수는 집단 구분을 최대로 드러내는 방향으로 종속변수들이 결합된 결과라고 볼 수 있다.\n정준변수 해석을 통해 연구자는 단순히 ”집단 간 차이가 있다”는 결론을 넘어서, 어떤 종속변수들이 결합되어 집단 차이를 주도하는지를 파악할 수 있다. 정준적재량을 살펴보면 각 종속변수가 해당 정준변수에 기여하는 정도를 확인할 수 있으며, 이를 통해 집단 차이의 실질적 내용을 해석할 수 있다. 다시 말해, 정준변수는 MANOVA 결과를 해석 가능한 잠재 차원으로 전환하는 역할을 수행한다.\n요약하면, MANOVA는 집단 간 다변량 평균 차이의 존재 여부를 검정하는 단계이며, 정준변수 해석은 그 차이가 어떤 다변량 구조를 통해 형성되는지를 설명하는 단계이다. 이 두 단계는 서로 분리된 분석이 아니라, MANOVA의 검정 결과를 구조적으로 해석하기 위해 정준변수 분석이 필연적으로 뒤따르는 하나의 연속적인 과정으로 이해하는 것이 바람직하다. MANOVA에서 정준변수는 집단 효과의 방향과 성격을 밝혀주는 핵심적인 해석 도구라 할 수 있다.\n\n\n\n2. 다변량분산분석 모형과 추정\n\n다변량 선형모형의 기본 설정\n다변량분석에서는 각 개체 i에 대해 하나의 반응값이 아닌, p차원의 반응벡터가 관측된다.\n\\(\\mathbf{y}_{i} = \\left( \\begin{array}{r}\ny_{i1} \\\\\ny_{i2} \\\\\n\\vdots \\\\\ny_{ip}\n\\end{array} \\right),i = 1,\\ldots,n\\) 이를 행렬 형태로 모으면 다음과 같은 다변량 선형모형이 된다.\n\\(\\mathbf{Y} = \\mathbf{XB} + \\mathbf{E}\\), 여기서 \\(\\mathbf{Y}(n \\times p)\\)는 반응변수 행렬, \\(\\mathbf{X}(n \\times k)\\)는 설계행렬 (요인 또는 설명변수), \\(\\mathbf{B}(k \\times p)\\)는 모수 행렬, 그리고 \\(\\mathbf{E}(n \\times p)\\)는 오차 행렬이다.\n\n\n확률적 가정 (오차 구조)\n각 행의 오차벡터 \\(\\mathbf{\\varepsilon}_{i}\\)는 다음을 따른다고 가정한다. \\(\\mathbf{\\varepsilon}_{i} \\sim N_{p}(\\mathbf{0},\\mathbf{\\Sigma})\\). 즉, 평균은 0 벡터, 공분산 행렬은 \\(\\mathbf{\\Sigma}\\)이며, 서로 독립이다. 행렬 형태로는 \\(\\mathbf{E} \\sim MN_{n \\times p}(\\mathbf{0},\\mathbf{I}_{n},\\mathbf{\\Sigma})\\)로 표현할 수 있다.\n\n\n모수 추정 (최소제곱 및 최대우도)\n모수 행렬 \\(\\mathbf{B}\\)의 추정\n다변량 최소제곱 추정량은 \\(\\widehat{\\mathbf{B}} = (\\mathbf{X}^{\\top}\\mathbf{X})^{- 1}\\mathbf{X}^{\\top}\\mathbf{Y}\\)이며, 이는 각 종속변수에 대해 동시에 최소제곱을 적용한 결과이다.\n공분산 행렬 \\(\\mathbf{\\Sigma}\\)의 추정\n잔차행렬 \\(\\widehat{\\mathbf{E}} = \\mathbf{Y} - \\mathbf{X}\\widehat{\\mathbf{B}}\\)를 이용하여 \\(\\widehat{\\mathbf{\\Sigma}} = \\frac{1}{n - k}{\\widehat{\\mathbf{E}}}^{\\top}\\widehat{\\mathbf{E}}\\)로 추정한다.\n\n\nMANOVA에서의 변동 행렬 분해\nMANOVA에서는 전체 변동을 행렬 단위로 분해한다. \\(\\mathbf{T} = \\mathbf{H} + \\mathbf{E}\\), 여기서 \\(\\mathbf{T}\\)는 전체 변동 행렬(Total SSCP), \\(\\mathbf{H}\\)는 집단 간 변동 행렬 (Hypothesis SSCP), 그리고 \\(\\mathbf{E}\\)는 집단 내 변동 행렬 (Error SSCP)이다. 이는 단변량 ANOVA의 \\(SS_{T} = SS_{B} + SS_{W}\\)의 다변량 확장이다.\n\n\nMANOVA 검정 통계량\nWilks’ Lambda: \\(\\Lambda = \\frac{|\\mathbf{E}|}{|\\mathbf{H} + \\mathbf{E}|}\\) → 집단 간 변동이 클수록 \\Lambda는 작아진다.\nPillai의 추적: \\(V = tr\\left( \\mathbf{H}(\\mathbf{H} + \\mathbf{E})^{- 1} \\right)\\)\nHotelling–Lawley 추적: \\(U = tr(\\mathbf{E}^{- 1}\\mathbf{H})\\)\n\n\n정준변수와의 수식적 연결\nMANOVA에서 정준변수는 다음의 일반화된 고유값 문제의 해로 정의된다. \\(|\\mathbf{H} - \\lambda\\mathbf{E}| = 0\\), 여기서 \\(\\lambda_{1} \\geq \\lambda_{2} \\geq \\cdots\\)는 정준근(canonical roots)이고 각 고유벡터가 정준변수의 계수에 해당된다.\n정준변수는 \\(\\mathbf{u} = \\mathbf{a}^{\\top}\\mathbf{Y}\\) 형태의 선형결합으로 나타나며, 집단 간 분리를 최대화하는 방향이다.\n\n\n정준상관분석(CCA) 모형과의 통합 표현\n두 변수군 \\(\\mathbf{X},\\mathbf{Y}\\)에 대해 \\(\\mathbf{u} = \\mathbf{a}^{\\top}\\mathbf{X},\\mathbf{v} = \\mathbf{b}^{\\top}\\mathbf{Y}\\)가 되도록 \\(\\max_{\\mathbf{a},\\mathbf{b}}corr(\\mathbf{u},\\mathbf{v})\\) 를 만족하는 \\(\\mathbf{a},\\mathbf{b}\\)를 찾는다. 이 역시 공분산 행렬을 이용한 고유값 문제로 귀결되며, MANOVA의 정준변수와 동일한 수학적 구조를 가진다.\n\n\n모형 관점에서의 통합 정리\n다변량 선형모형: \\(\\mathbf{Y} = \\mathbf{XB} + \\mathbf{E}\\), 집단 효과에 대한 \\(\\mathbf{H}\\)와 \\(\\mathbf{E}\\) 비교\n정준분석: \\(\\mathbf{H}\\)와 \\(\\mathbf{E}\\)의 고유값 분해\n즉, MANOVA와 정준분석은 동일한 모형의 서로 다른 해석 단계이다. 다변량분석은 반응변수를 벡터로 취급하는 선형모형을 기반으로 하며, 모수 추정은 최소제곱 및 최대우도 원리에 의해 이루어진다. MANOVA에서는 변동을 행렬로 분해하고, 정준분석에서는 이 행렬 구조를 고유값 문제로 변환하여 집단 차이를 최대화하는 잠재 축을 도출한다.\n\n\n\n3. 사례연구\n\n데이터 만들기\n이 데이터는 호수 유형(lake: Lake_A, Lake_B, Lake_C)이라는 하나의 범주형 요인이, 산소량(oxygen)과 수은 함유량(mercury)이라는 두 개의 연속형 종속변수의 결합된 평균 벡터에 차이를 유발하는지를 검정하기 위해 설정되었다. 즉, 관심의 초점은 개별 변수의 평균 차이가 아니라, 두 환경 지표를 동시에 고려했을 때 호수 간에 통계적으로 유의한 차이가 존재하는가에 있다.\nimport numpy as np\nimport pandas as pd\n\nnp.random.seed(42)\n\n# 설정: 3개 집단, 각 40명\ngroups = [\"Lake_A\", \"Lake_B\", \"Lake_C\"]\nn_per = 40\n\n# 종속변수 2개: oxygen, mercury (서로 상관되도록 공분산 설정)\nSigma = np.array([[1.0, -0.6],\n                  [-0.6, 1.0]])   # 산소량-수은함유량 음의 상관\n\n# 집단별 평균(다변량 평균 벡터): 집단 차이를 일부러 줌\nmu = {\n    \"Lake_A\": np.array([ 0.8, -0.6]),\n    \"Lake_B\": np.array([ 0.0,  0.0]),\n    \"Lake_C\": np.array([-0.7,  0.7]),\n}\n\nrows = []\nfor g in groups:\n    Y = np.random.multivariate_normal(mean=mu[g], cov=Sigma, size=n_per)\n    for i in range(n_per):\n        rows.append([g, Y[i, 0], Y[i, 1]])\n\ndf = pd.DataFrame(rows, columns=[\"lake\", \"oxygen\", \"mercury\"])\nprint(df.head())\nlake oxygen mercury  0 Lake_A 0.293892 -0.217559  1 Lake_A 0.901809 0.660430  (이하 생략)\n\n\n요인별 기술통계량\n#호수별 산소량-수은 평균, 표준편차 출력\nsummary = df.groupby(\"lake\").agg(\n    oxygen_mean=(\"oxygen\", \"mean\"),\n    oxygen_sd=(\"oxygen\", \"std\"),\n    mercury_mean=(\"mercury\", \"mean\"),\n    mercury_sd=(\"mercury\", \"std\")\n)\nprint(summary)\n호수별 기술통계를 살펴보면, Lake_A는 평균 산소량이 가장 높고 수은 함유량은 가장 낮은 반면, Lake_C는 산소량 평균이 가장 낮고 수은 평균은 가장 높게 나타난다. Lake_B는 두 변수 모두에서 중간적인 수준을 보이며, 세 호수 간에 산소량과 수은 함유량의 조합에서 뚜렷한 차이가 존재함을 시사한다. 이러한 결과는 두 종속변수를 동시에 고려하는 다변량분산분석의 필요성을 뒷받침한다.\noxygen_mean oxygen_sd mercury_mean mercury_sd  Lake_A 0.921449 0.892285 -0.788171 0.916423  Lake_B 0.106840 0.923339 -0.049165 0.947157  Lake_C -0.603968 0.950955 0.820634 0.941350\n\n\n다변량분산분석\nfrom statsmodels.multivariate.manova import MANOVA\n\n# MANOVA 적합\nmaov = MANOVA.from_formula('oxygen + mercury ~ lake', data=df)\n\n# mv_test 결과에서 lake 부분만 선택 출력\nres = maov.mv_test()\nprint(res.results['lake'])\n본 다변량분산분석은 호수 유형에 따라 산소량과 수은 함유량으로 구성된 다변량 평균 벡터가 동일한지를 검정한 것이다. 즉, 관심의 대상은 개별 변수의 평균 차이가 아니라, 두 환경 지표를 동시에 고려했을 때 호수 간 환경 구조에 차이가 존재하는가이다.\n호수 요인에 대해 보고된 네 가지 다변량 검정 통계량이 모두 매우 작은 p-value(p &lt; 0.001)를 보이며 일관되게 유의하게 나타났다. 이는 어떤 검정 기준을 적용하더라도 호수 유형에 따라 산소량과 수은 함유량의 결합된 평균 구조가 통계적으로 유의하게 다르다는 것을 의미한다. 특히 Wilks’ Lambda = 0.6039라는 값은, 집단 내 변동에 비해 집단 간 변동이 상당한 비중을 차지하고 있음을 시사한다. 다시 말해, 두 종속변수를 동시에 고려할 때 호수 간 차이는 우연적 변동으로 설명되기 어려운 수준이며, 환경 특성이 다변량 공간에서 뚜렷이 구분된다고 해석할 수 있다.\n이 결과는 산소량과 수은 함유량이 서로 상관된 변수라는 점에서 더욱 의미를 가진다. MANOVA는 이러한 공분산 구조를 고려한 상태에서 집단 효과를 검정하므로, 단변량 ANOVA를 각각 수행하는 것보다 환경 상태의 종합적 차이를 보다 적절하게 반영한다. 따라서 본 단계의 결론은 ”어떤 변수가 더 중요한가”가 아니라, ”호수 유형이 환경 프로파일 전체에 영향을 미치는가”에 대한 판단이다.\n종합하면, 본 MANOVA 결과는 호수 유형이 산소량과 수은 함유량으로 구성된 다변량 환경 특성에 유의한 영향을 미친다는 것을 명확히 보여준다. 이 결론을 바탕으로 이후 단계에서 정준판별축, LDA 계수, 또는 사후 단변량 분석을 통해 어떤 방향에서, 어떤 변수 조합이 호수 간 차이를 주도하는지를 보조적으로 해석하는 것이 논리적으로 타당하다.\n{'stat': Value Num DF Den DF F Value Pr &gt; F  Wilks' lambda 0.603905 4 232.0 16.635179 0.0  Pillai's trace 0.397509 4.0 234.0 14.51131 0.0  Hotelling-Lawley trace 0.653548 4 138.166667 18.899687 0.0  Roy's greatest root 0.649946 2 117 38.021819 0.0,\n#사후 해석용: 단변량 ANOVA(참고)\nimport statsmodels.formula.api as smf\nfrom statsmodels.stats.anova import anova_lm\n\nm1 = smf.ols(\"oxygen ~ C(lake)\", data=df).fit()\nm2 = smf.ols(\"mercury ~ C(lake)\", data=df).fit()\n\nprint(\"ANOVA: oxygen\")\nprint(anova_lm(m1, typ=2), \"\\n\")\n\nprint(\"ANOVA: mercury\")\nprint(anova_lm(m2, typ=2))\n산소량(oxygen)에 대한 해석\n호수 유형(lake)에 따른 산소량의 평균 차이를 검정한 결과, 집단 효과가 매우 유의하게 나타났다 (F = 27.38, p &lt; 0.001). 이는 세 호수 간에 산소량 평균이 동일하다는 귀무가설을 기각할 수 있음을 의미한다.\n즉, 호수 유형에 따라 산소 공급 상태 또는 수질 환경이 통계적으로 뚜렷하게 구분된다고 해석할 수 있다.\n수은 함유량(mercury)에 대한 해석\n수은 함유량에 대해서도 호수 유형의 효과가 매우 유의하게 나타났다\n(F = 29.67, p &lt; 0.001). 이는 세 호수 간에 수은 오염 수준의 평균이 유의하게 다르다는 것을 의미하며, 호수 유형에 따라 오염 정도가 체계적으로 구분된다는 해석이 가능하다.\nMANOVA 결과와의 관계에서의 위치 정리  이 두 단변량 ANOVA 결과는 앞선 MANOVA에서 확인된 다변량 집단 차이를 각 종속변수 수준에서 구체화해 주는 보조적 증거에 해당한다.\nMANOVA → 산소량과 수은 함유량을 동시에 고려했을 때 호수 유형 간 환경 프로파일이 다르다.\n단변량 ANOVA → 그 차이가 산소량과 수은 함유량 각각에서도 모두 유의하게 나타난다.\n따라서 본 결과는 호수 유형에 따른 환경 차이는 특정 단일 지표에 국한되지 않고, 산소량과 수은 함유량 모두에서 일관되게 관찰된다는 점을 뒷받침한다.\nANOVA: oxygen  sum_sq df F PR(&gt;F)  C(lake) 46.609756 2.0 27.384833 1.755918e-10  Residual 99.568645 117.0 NaN NaN\nANOVA: mercury  sum_sq df F PR(&gt;F)  C(lake) 51.879101 2.0 29.666932 3.786457e-11  Residual 102.300010 117.0 NaN NaN\n호수별 산소량, 수은량 평균 비교 측면에서 해석\nLake_A는 산소량 평균이 0.92로 가장 높고, 수은 함유량 평균은 –0.79로 가장 낮아, 산소가 풍부하고 수은 오염 수준이 낮은 환경을 나타낸다. 반면 Lake_C는 산소량 평균이 –0.60으로 가장 낮고, 수은 함유량 평균이 0.82로 가장 높아, 산소 부족과 높은 수은 오염이 동시에 나타나는 환경으로 해석된다. Lake_B는 두 변수 모두 평균이 0 부근에 위치하여, 중간적 성격의 환경 상태를 보인다.\n한편, 산소량과 수은 함유량의 표준편차는 세 호수 모두에서 유사한 수준(약 0.89~0.95)으로 나타나, 집단 간 차이는 변동성의 크기보다는 평균 위치의 이동, 즉 다변량 평균 구조의 차이에 의해 주도되고 있음을 시사한다. 이는 MANOVA에서 관측된 집단 효과가 특정 집단의 분산 증가 때문이 아니라, 환경 특성의 체계적인 수준 차이에 기인함을 뒷받침한다.\n종합하면, 본 결과는 호수 유형에 따라 “Lake_A → 고산소·저수은, Lake_B → 중간적 환경, Lake_C → 저산소·고수은” 이라는 뚜렷한 다변량 환경 프로파일 차이가 존재함을 보여준다. 이러한 평균 구조의 대비가 MANOVA에서 확인된 유의한 집단 효과의 실질적 해석 근거에 해당한다.\n\n\n선형판별분석(LDA, Linear Discriminant Analysis\n선형판별분석(여러 변수의 선형결합 중에서 집단 간 분리를 가장 크게 만드는 축을 찾는 분석)을 이용해 호수를 가장 잘 구분하는 1차 판별축 점수를 계산한 것이다.\n연속형 설명변수 산소량과 수은 함유량을 어떤 비율로 묶으면 세 호수가 가장 잘 분리되는가? 그 결과가 1차 판별축 점수이다.\n\\(z_{i} = a \\times Oxygen_{i} + b \\times Mercury_{i}\\), a,b는 집단 간 분산 / 집단 내 분산 비율을 최대화하도록 추정된 계수이다. 즉, 집단 평균 벡터 사이 거리를 가장 크게 만드는 방향이다. 이 LDA 계수는 ”누구를 어디로 분류할 것인가”를 위한 가중치이며, 해석의 핵심은 산소–수은의 대비 구조가 집단 분리를 만든다는 점이다.\n#집단 분리 축(정준변수 관점) 간단 시각화\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nimport matplotlib.pyplot as plt\n\nX = df[[\"oxygen\", \"mercury\"]].values\ny = df[\"lake\"].values\n\nlda = LinearDiscriminantAnalysis(n_components=1)\nz = lda.fit_transform(X, y).ravel()\n\nplt.figure(figsize=(7,4))\nfor g in groups:\n    plt.hist(z[y==g], bins=12, alpha=0.6, label=g)\nplt.title(\"Group separation along 1st discriminant axis (interpretation aid)\")\nplt.xlabel(\"Axis score\")\nplt.ylabel(\"Frequency\")\nplt.legend()\nplt.show()\n\nprint(\"LDA coefficients for axis:\", lda.coef_)\nLDA coefficients for axis: [[ 0.61766397 -0.59103148]  [-0.08770726 -0.09303387]  [-0.52995671 0.68406535]]\n판별축은 산소량(+)과 수은 함유량(–)의 선형결합으로 구성된 축으로,앞선 결과를 종합하면 오른쪽으로 갈수록 저산소·고수은, 왼쪽으로 갈수록 고산소·저수은을 의미하는 환경 대비 축이다.\nLake_A: 분포가 주로 음(-)의 영역에 위치, 판별축 값이 낮음 → 산소량이 높고 수은 함유량이 낮은 환경이 지배적 → Lake_A의 대표적 환경 특성을 잘 반영\nLake_B: 분포가 0 근처를 중심으로 비교적 넓게 퍼짐, 음·양 영역 모두에 일부 중첩 → 중간적·전이적 성격의 환경 → Lake_A와 Lake_C 사이의 완충 집단 역할\nLake_C: 분포가 양(+)의 영역에 집중, 판별축 값이 가장 큼 → 산소량이 낮고 수은 함유량이 높은 환경 → Lake_C의 환경적 불리함(오염·저산소)을 명확히 반영\n중첩(overlap)에 대한 해석: 히스토그램을 보면 Lake_A ↔︎ Lake_B, Lake_B ↔︎ Lake_C 사이에 일부 중첩이 존재한다. 이는 다음을 의미한다.\n집단 간 차이는 확률적·경향적 차이\n완벽한 분리는 아니다. 환경 프로파일의 평균적 차이가 존재함을 보여줄 뿐, 개별 관측치는 일부 겹칠 수 있다. 이는 MANOVA와 LDA의 전형적 결과로, 현실적인 데이터 특성에 부합합니다.\nMANOVA 결과와의 연결 해석\nMANOVA → 호수 유형에 따라 산소–수은의 결합된 평균 구조가 다르다 (통계적 검정)\nLDA 히스토그램 → 그 차이가 어떤 축에서, 어떤 방향으로 나타나는가 (기하학적·시각적 해석)\n따라서 이 그림은 MANOVA 결과를 직관적으로 설명하는 보조 도구이다. 제1 판별축 상에서 Lake_A는 음의 영역, Lake_C는 양의 영역에 주로 분포하고 Lake_B는 그 사이에 위치하여, 산소량–수은 함유량의 대비로 형성된 환경 축을 따라 호수 유형이 체계적으로 구분됨을 확인할 수 있다."
  },
  {
    "objectID": "notes/mda/index.html",
    "href": "notes/mda/index.html",
    "title": "다변량분석",
    "section": "",
    "text": "다변량분석은 여러 변수의 상호관계를 동시에 분석하는 통계 방법론이다.\n차원축소, 분류, 군집, 관계 구조 분석 등 다양한 기법이 포함되며,\n사회과학·자연과학·머신러닝 분석의 핵심 도구로 활용된다.\n왼쪽 메뉴에서 주제를 선택하시오."
  },
  {
    "objectID": "notes/mda/mda_mds_ca.html",
    "href": "notes/mda/mda_mds_ca.html",
    "title": "다변량분석 6. 다차원척도법과 대웅분석",
    "section": "",
    "text": "Chapter 1. 다차원 척도법\n\n다차원 척도법이란?\n\n(1) 개념\n다차원척도법(MDS)이란 주어진 n개의 개체 간 유사성 혹은 거리 정보를 이용하여 이들을 저차원의 가시적 공간, 일반적으로 2차원 또는 3차원 공간에 배치하는 기법이다. 핵심 목적은 고차원적 유사성 구조를 시각적으로 해석 가능한 형태로 축소하되, 원래 자료가 가진 상대적 거리 구조를 최대한 보존하는 것이다. 즉, 개체들이 서로 유사할수록 저차원 공간에서도 서로 가깝게, 유사하지 않을수록 멀리 배치되도록 한다.\nMDS에서는 유사성을 거리 척도로 변환하거나, 혹은 직접 거리 행렬이 주어져 있는 경우 이를 그대로 사용한다. 이후 알고리즘은 해당 거리 구조를 가장 잘 재현하는 좌표들을 찾기 위해 스트레스(stress) 함수 또는 손실 함수를 최소화하는 방향으로 작동한다. 그 결과로 얻어진 저차원 좌표는 복잡한 다변량 구조를 직관적으로 파악할 수 있게 하며, 제품 비교, 소비자 인식 지도(perceptual map) 작성 등 다양한 분야에서 활용된다.\n다차원척도법(MDS)이란 n개의 개체를 저차원의 가시적 공간, 일반적으로 2차원 공간에 배치하여 개체 간 유사성 구조를 시각적으로 표현하는 방법이다. 이를 위해서는 먼저 각 개체 간 거리 또는 유사성을 적절한 방식으로 측정해야 한다. MDS는 개체들 사이의 상대적 거리 관계를 유지하면서 이들을 평면 위에 배치하는 데 목적이 있으며, 이러한 점에서 군집분석과 유사해 보일 수 있다. 그러나 두 방법의 목적과 결과는 명확히 구별된다.\n군집분석은 거리 또는 유사성이 가까운 개체들끼리 묶어 집단을 형성하는 데 초점을 둔 분석 방법이다. 즉, 개체들을 몇 개의 군집으로 분류하는 것이 주된 목적이다. 반면 다차원척도법은 개체 간 유사성을 2차원 또는 3차원 공간에 좌표 형태로 나타내어, 전체 구조를 시각적으로 파악할 수 있게 하는 데 목적이 있다. 따라서 군집분석이 집단화의 결과를 제공하는 분류 기법이라면, MDS는 개체들 사이의 인지적·심리적 또는 수량적 거리 구조를 시각적으로 해석할 수 있는 좌표를 제공하는 차원 축소 기법이다.\n\n\n(2) 개체간 유사성 측정\n다차원척도법(MDS)에서 가장 중요한 입력 자료는 개체 간 거리 또는 유사성 행렬이다. 이 거리 행렬을 어떻게 구성하느냐에 따라 MDS의 형태가 달라지며, 크게 metric 방법과 non-metric 방법으로 구분된다. 아래는 세 가지 대표적인 유사성 측정 방식에 대한 상세 설명이다.\n\n\n측정형 변수 기반 거리: Euclidean distance\n가장 전통적인 방식은 측정된 변수값을 이용하여 개체 간 거리를 직접 계산하는 방법이다. 예를 들어 제품에 대한 향기, 냄새 제거 성능, 사용 편리성 등의 속성이 모두 수치로 측정되어 있다면, 개체 \\(i\\)와 \\(j\\)간의 거리는 다음과 같은 유클리드 거리로 계산된다.\n\\(d_{ij} = \\sqrt{\\overset{p}{\\sum_{k = 1}}(x_{ik} - x_{jk})^{2}}\\), 여기에서 \\(x_{ik}\\)는 개체 \\(i\\)의 \\(k\\)번째 속성값, \\(p\\)는 속성(변수)의 수를 의미한다.\n이 방법의 특징은 거리 값 자체가 수치적으로 의미 있는 척도(metric scale)를 가진다는 점이다. 변수들 간의 차이가 직접적으로 거리 계산에 반영되므로, 계산된 거리를 그대로 이용한 metric MDS의 적용이 가능하다. 다만 각 속성이 동일한 중요도를 가진다고 가정하는 것이 타당한지에 대한 검토가 필요하며, 변수의 단위 차이를 제거하기 위한 표준화 여부, 그리고 변수 간 상관 구조 등이 분석 결과의 품질에 중요한 영향을 미친다.\n이와 같은 방식은 소비자 평가 점수(예: 1~10점), 화학적 성분이나 물리적 특성값과 같이 실측된 수치 자료, 또는 설문조사에서 사용되는 리커트(Likert) 척도와 같은 등간척도 자료에 널리 사용된다. 이러한 자료들은 이미 수량화가 이루어져 있으므로, 개체 간 거리를 계산하는 데 있어 가장 신뢰할 수 있는 정보원을 제공한다.\n\n\n사람들이 유사성(거리)을 직접 평가하여 얻는 자료\n두 번째 방법은 응답자들이 개체 쌍에 대해 유사성 또는 거리감을 직접 평가하도록 하여 자료를 수집하는 방식이다. 예를 들어 제품 A와 B의 유사성을 1에서 7 또는 1에서 10 사이의 점수로 표시하게 하거나, 두 제품이 얼마나 다르게 느껴지는지를 0에서 100 사이의 수치로 표현하게 하는 방식이 이에 해당한다. 이 방법은 개체에 대한 소비자의 인지적 거리를 직접 자료화한다는 점에서 중요한 장점을 가진다.\n이러한 자료는 점수의 성격에 따라 metric MDS 또는 non-metric MDS로 구분하여 사용된다. 만약 점수 간의 간격이 실제 수량적 의미를 가진다고 판단되는 경우, 예컨대 유사성 1~10점에서 8점과 4점의 차이가 응답자들이 지각한 거리 차이를 그대로 반영한다고 볼 수 있을 때에는 metric 방법을 적용할 수 있다.\n반대로 응답자의 평가는 순서 정보만을 담고 있으며, 각 점수 간 간격이 응답자마다 다르게 해석될 가능성이 큰 경우에는 non-metric MDS를 적용하는 것이 타당하다. 예를 들어 ”A-B가 A-C보다 더 유사하다”라는 순서적 정보만 신뢰되는 상황에서는 점수의 등간성을 가정하기 어렵기 때문에 순서(ordinal) 관계만을 이용하여 비모수적 변환 후 분석을 수행한다.\n이 방식의 가장 큰 강점은 소비자의 실제 인식 구조를 직접 반영할 수 있다는 점이다. 그러나 단점도 분명하다. 개체가 n개일 때 응답자가 평가해야 하는 쌍의 수가 n(n-1)/2개로 매우 많아지므로, 응답 부담이 커지고 시간과 비용이 크게 증가한다. 그럼에도 불구하고 직접적 유사성 평가 자료는 실증 연구에서 가장 정밀한 정보를 제공한다는 점에서 여전히 널리 활용되는 방법이다.\n\n\n평가자들이 개체를 자유롭게 분류 후 교차빈도표\n세 번째 방법은 응답자들에게 개체들을 자유롭게 분류하도록 한 뒤, 동일한 분류군에 포함된 빈도를 이용하여 개체 간 유사성을 추정하는 방식이다. 즉, 응답자가 개체들을 자신의 기준에 따라 마음대로 묶도록 한 후, 특정 두 개체가 같은 그룹에 포함된 빈도가 높을수록 두 개체는 유사한 것으로 판단한다. 예를 들어 20명의 소비자가 제품들을 자유롭게 분류한 결과, A와 B를 같은 그룹에 넣은 사람이 18명이고 A와 C를 같은 그룹에 넣은 사람이 4명이라면, A–B는 높은 유사성을, A–C는 매우 낮은 유사성을 가진 것으로 해석한다.\n이러한 자료는 일반적으로 공동 출현 행렬로 정리되며, 이후 이를 거리 행렬로 변환하여 MDS의 입력 자료로 사용한다. 개체 \\(i\\)와 \\(j\\)의 거리 \\(d_{ij}\\)는 흔히 다음과 같이 정의된다.\n\\[d_{ij} = 1 - \\frac{\\text{두 개체를 같은 집단에 분류한 응답자 수}}{\\text{전체 응답자 수}}\\]\n이 방법이 non-metric MDS에 적합한 이유는, 공동 분류 빈도라는 자료가 본질적으로 순서 정보 또는 범주적 의미만을 갖는다는 점에 있다. 빈도 값이 단순히 ”더 많이 함께 분류되었다”는 서열적 정보를 나타낼 뿐, 수량 자체의 절대적인 간격이나 비율적 의미를 지니는 것은 아니기 때문이다. 따라서 이러한 자료는 거리의 등간성이나 비율적 해석을 전제로 하는 metric MDS보다는, 순서 구조를 유지하면서 거리 변환을 수행하는 non-metric MDS에서 다루는 것이 타당하다.\n이 방식의 장점은 응답자에게 개체 간 유사성을 직접 수치로 평가하게 하는 방식보다 부담이 적으며, 소비자가 자연스럽게 지각하는 분류 기준이 그대로 반영된다는 점이다. 반면 거리 계산이 직접적이지 않고 빈도를 기반으로 간접적으로 구성되며, 응답자마다 분류 기준이 다를 수 있으므로 결과 해석이 어려워질 수 있다는 한계도 존재한다.\n\n\n\n2. 기본 알고리즘\nMDS 알고리즘은 거리 또는 유사성 행렬을 기반으로 개체의 내적 행렬을 복원하고, 이를 고유값분해하여 저차원 좌표를 얻는 방식으로 작동한다. metric MDS는 고전적 행렬 대수 접근법을 사용하여 직접 좌표를 계산하지만, non-metric MDS는 순서 정보를 유지하는 비모수적 거리 변환과 스트레스 최소화 반복 알고리즘을 통해 최적의 좌표를 찾는다.\n유사성 행렬 계산\n분석의 출발점은 n개의 개체 간 거리 또는 유사성 행렬이다. 측정형 변수로부터 계산한 거리, 응답자의 직접적 유사성 평가, 자유 분류에서 얻어진 공동 출현 빈도 기반 거리 등이 모두 사용될 수 있다. metric MDS는 거리의 등간성을 가정하며, non-metric MDS는 순서 정보만을 사용하여 거리의 형태를 비모수적으로 변환하여 사용한다.\n유사성 행렬의 제곱 계산\n고전적 MDS는 거리 행렬을 제곱한 행렬 \\(D^{(2)}\\)를 구성하는 것으로 시작한다. \\(d_{ij}^{2}\\), 제곱 거리는 이후 내적행렬 복원을 위한 변환에 사용된다.\n이중 중심화를 통한 내적 행렬 계산\n거리 행렬을 직접 좌표로 변환할 수 없기 때문에, ”이중 중심화”를 통해 내적행렬 B를 복원한다. 중심화 행렬을 \\(H = I - \\frac{1}{n}\\mathbf{11}'\\)라고 하면, 행렬 \\(B = - \\frac{1}{2}HD^{(2)}H\\)는 저차원 공간에서의 점들의 내적을 의미하며, 고전적 MDS에서 핵심 역할을 한다.\n고유값분해(Eigendecomposition)\n복원된 내적 행렬 B에 대하여 고유값분해를 수행한다. \\(B = Q\\Lambda Q'\\), 여기에서 \\(\\Lambda\\)는 고유값 행렬, Q는 고유벡터 행렬이다. 고유값은 저차원 공간에서의 분산(정보량)에 대응하며, 양의 고유값만이 유효한 좌표를 구성한다.\n좌표 계산: 차원 축소\n원하는 차원(보통 2차원)을 \\(r\\)이라고 하면, 상위 \\(r\\)개의 고유값과 그에 대응하는 고유벡터를 사용하여 좌표 행렬 \\(X = Q_{r}\\Lambda_{r}^{\\frac{1}{2}}\\)를 계산한다. 이때 X의 각 행은 개체 하나가 저차원 공간에서 가지는 좌표이다.\n적합도 평가 (Stress 또는 RSQ)\nmetric/non-metric MDS에서는 주어진 거리와 저차원에서의 거리 간 차이를 평가하기 위해 적합도 지표를 계산한다. 대표적으로 다음과 같은 크루스칼(Kruskal) 스트레스 함수가 사용된다.\n\\(\\text{Stress} = \\sqrt{\\frac{\\sum_{i &lt; j}(d_{ij} - {\\widehat{d}}_{ij})^{2}}{\\sum_{i &lt; j}^{}d_{ij}^{2}}}\\). 값이 작을수록 적합도가 좋다. Stress 값은 원래 거리와 저차원에서 재현된 거리 간의 차이를 상대적으로 나타내는 척도로서, 값이 작을수록 모델이 자료의 거리 구조를 더 잘 재현한다는 의미를 가진다. 일반적으로 Stress 해석은 크루스칼(Kruskal, 1964)이 제시한 기준을 따르며 다음과 같이 이해된다.\n\n\n\n\n\n\n\nStress 값\n해석\n\n\n0.05 이하\n매우 좋음 (Excellent fit)\n\n\n0.05 ~ 0.10\n좋음 (Good fit)\n\n\n0.10 ~ 0.20\n보통 (Fair fit)\n\n\n0.20 이상\n나쁨 (Poor fit)\n\n\n\n반복 알고리즘 (Non-Metric MDS)\nnon-metric MDS는 직접 좌표가 계산되지 않기 때문에 순위 조건을 충족하도록 거리 변환을 반복하며 Stress를 최소화한다. Stress 최소화 반복 절차가 사용된다. 가장 대표적인 방법이 SMACOF 알고리즘 이다.\nmetric MDS는 반복 없이 한 번의 고유값분해로 해결되는 반면, non-metric MDS는 순서 제약을 고려해야 하므로 수렴까지 다수의 반복이 필요하다.\n\n\n3. 사례실습\n\n(1) 미국 10개 도시 거리\n다차원척도법의 원리를 설명할 때 가장 널리 사용되는 예제는 미국 주요 대도시들 간의 실제 물리적 거리(도로거리 또는 직선거리)를 이용하여 이 도시들을 2차원 공간에 배치하는 사례이다. 도시 간 거리는 이미 수치적 의미를 갖는 metric 자료이므로, 고전적 MDS를 적용할 수 있다.\n이 도시들 사이의 실제 거리(마일 단위)를 행렬로 구성하고, 이를 MDS에 적용하면 저차원의 좌표를 얻을 수 있다. 알고리즘은 도시 간 거리 구조를 가능한 한 보존하는 방향으로 2차원 공간에서의 위치를 결정하며, 그 결과 도시들이 실제 미국 지도와 매우 유사한 형태로 배치된다.\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.manifold import MDS\n\n# -----------------------------\n# 1. 도시 이름 정의\n# -----------------------------\ncities = [\n    \"New York\",\n    \"Chicago\",\n    \"Atlanta\",\n    \"Houston\",\n    \"Denver\",\n    \"Los Angeles\",\n    \"San Francisco\",\n    \"Seattle\"\n]\n\n# -----------------------------\n# 2. 도시 간 거리 행렬 (예시, 단위: mile)\n#    - 대략적인 미국 지도 기반 예시값\n#    - 대칭 행렬, 대각선은 0\n# -----------------------------\nD = np.array([\n    [   0,  713,  748, 1420, 1627, 2451, 2565, 2402],  # New York\n    [ 713,    0,  587, 1081,  920, 1745, 1858, 1721],  # Chicago\n    [ 748,  587,    0,  702, 1400, 1935, 2135, 2180],  # Atlanta\n    [1420, 1081,  702,    0, 1020, 1374, 1631, 1895],  # Houston\n    [1627,  920, 1400, 1020,    0,  830,  957, 1020],  # Denver\n    [2451, 1745, 1935, 1374,  830,    0,  381,  960],  # Los Angeles\n    [2565, 1858, 2135, 1631,  957,  381,    0,  679],  # San Francisco\n    [2402, 1721, 2180, 1895, 1020,  960,  679,    0],  # Seattle\n])\n\n# (선택) Pandas DataFrame으로 보기 좋게 출력\ndf_dist = pd.DataFrame(D, index=cities, columns=cities)\nprint(\"도시 간 거리 행렬 (마일 단위):\")\nprint(df_dist)\n\n# -----------------------------\n# 3. Metric MDS 실행\n# -----------------------------\nmds = MDS(\n    n_components=2,           # 2차원으로 축소\n    dissimilarity='precomputed',  # 이미 거리행렬이므로 'precomputed'\n    random_state=42,\n    normalized_stress=False   # sklearn 1.6 이상이면 True/False 지정 가능\n)\n\ncoords = mds.fit_transform(D)   # shape: (8, 2)\nstress = mds.stress_\n\nprint(\"\\nMDS 좌표 (2차원):\")\nfor city, (x, y) in zip(cities, coords):\n    print(f\"{city:12s} : ({x:8.3f}, {y:8.3f})\")\n\nprint(f\"\\nStress 값: {stress:,.3f}\")\n\n# -----------------------------\n# 4. 결과 시각화 (Perceptual Map)\n# -----------------------------\nplt.figure(figsize=(8, 6))\nplt.scatter(coords[:, 0], coords[:, 1])\n\nfor i, city in enumerate(cities):\n    plt.text(coords[i, 0] + 0.02, coords[i, 1] + 0.02, city, fontsize=10)\n\nplt.title(\"MDS of Major U.S. Cities (2D Configuration)\")\nplt.xlabel(\"Dimension 1\")\nplt.ylabel(\"Dimension 2\")\nplt.axhline(0, linewidth=0.5)\nplt.axvline(0, linewidth=0.5)\nplt.gca().set_aspect(\"equal\", adjustable=\"datalim\")  # 비율 유지\nplt.tight_layout()\nplt.show()\n도시 간 거리 행렬 (마일 단위):  New York Chicago Atlanta Houston Denver Los Angeles San Francisco Seattle  New York 0 713 748 1420 1627 2451 2565 2402  Chicago 713 0 587 1081 920 1745 1858 1721  Atlanta 748 587 0 702 1400 1935 2135 2180  Houston 1420 1081 702 0 1020 1374 1631 1895  Denver 1627 920 1400 1020 0 830 957 1020  Los Angeles 2451 1745 1935 1374 830 0 381 960  San Francisco 2565 1858 2135 1631 957 381 0 679  Seattle 2402 1721 2180 1895 1020 960 679 0\nMDS 좌표 (2차원):  New York : (1284.692, 592.056)  Chicago : ( 608.177, 404.009)  Atlanta : (1015.790, -95.546)  Houston : ( 488.391, -613.292)  Denver : (-262.049, 57.935)  Los Angeles : (-877.823, -515.130)  San Francisco : (-1131.511, -255.654)  Seattle : (-1125.668, 425.620)\nStress 값: 33,543.450\n\n\n\n\n\n\n\n(2) 측정형 지표 metric\n이 자료는 1888년경 스위스의 프랑스어 사용 47개 주(province)에 대한 표준화된 출생지표와 사회경제적 지표를 포함하고 있다.\n\n\n\n\n\n\n\n변수\n설명\n\n\nFertility\nIg, 일반적으로 사용되는 표준화된 출생력 지표\n\n\nAgriculture\n남성 중 농업을 직업으로 하는 비율 (%)\n\n\nExamination\n군 징집 검사에서 최고 등급을 받은 지원자의 비율 (%)\n\n\nEducation\n징집 대상자 중 초등교육 이상(고등 교육 수준 포함)을 받은 비율 (%)\n\n\nCatholic\n가톨릭 인구 비율 (%) — 가톨릭 대 프로테스탄트 구분\n\n\nInfant_Mortality\n1년 미만 생존한 영아 비율(영아 사망률, %)\n\n\n\n다차원 척도법\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.manifold import MDS\n\n# 1. 데이터 불러오기\nswiss = pd.read_excel(\"/content/drive/MyDrive/eBook publish/swiss.xlsx\", index_col=0)  # 첫 열을 index로\n\n# 2. 변수 표준화\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(swiss.values)\n\n# 3. 유클리드 거리행렬 계산\nfrom sklearn.metrics import pairwise_distances\nD = pairwise_distances(X_scaled, metric=\"euclidean\")\n\n# 4. Metric MDS (sklearn)\nmds = MDS(\n    n_components=2,\n    dissimilarity=\"precomputed\",\n    random_state=42,\n    normalized_stress=False  # sklearn 버전에 따라 옵션\n)\ncoords = mds.fit_transform(D)\nstress = mds.stress_\nprint(\"Stress:\", stress)\n\nprint(\"\\nMDS 좌표 (2차원):\")\nfor name, (x, y) in zip(swiss.index, coords):\n    print(f\"{name:15s} : ({x:8.3f}, {y:8.3f})\")\n\n# 5. 결과 시각화\nplt.figure(figsize=(8, 6))\nplt.scatter(coords[:, 0], coords[:, 1])\n\nfor name, (x, y) in zip(swiss.index, coords):\n    plt.text(x + 0.02, y + 0.02, name, fontsize=8)\n\nplt.axhline(0, linewidth=0.5)\nplt.axvline(0, linewidth=0.5)\nplt.gca().set_aspect(\"equal\", adjustable=\"datalim\")\nplt.xlabel(\"Dimension 1\")\nplt.ylabel(\"Dimension 2\")\nplt.title(\"MDS of Swiss Provinces (Fertility & Socio-economic Variables)\")\nplt.tight_layout()\nplt.show()\nstress 값이 매우 높은 이유는 사회경제적 지표간 구조가 복잡하여(상관계수가 낮음) 2D로 재현이 어렵다는 것을 의미하며 3D MDS 또는 non-metric MDS가 더 적합할 가능성이 높다.\nstress: 263.25291828843683\nMDS 좌표 (2차원):  Courtelary : ( -1.896, 0.701)  Delemont : ( -1.213, -1.849)  Franches-Mnt : ( -1.151, -2.677)  Moutier : ( -1.267, -0.560)  (이하 생략)\n\n\n\n\n\n군집개수 4개 k-means 군집분석 결과 다차원 공간에 시각화\nfrom sklearn.cluster import KMeans\n\n# 1. K-means 군집분석 (4개 군집)\nkmeans = KMeans(n_clusters=4, n_init=20, random_state=42)\nclusters = kmeans.fit_predict(X_scaled)   # 표준화된 원자료 기준으로 군집화\n\n# (선택) swiss 데이터프레임에 군집번호 붙이기\nswiss['cluster'] = clusters  # 0,1,2,3\n# 2. 군집 결과를 MDS 좌표 위에 올려 그리기\nplt.figure(figsize=(8, 6))\n\nfor g in range(4):\n    mask = (clusters == g)\n    plt.scatter(\n        coords[mask, 0],\n        coords[mask, 1],\n        label=f\"Cluster {g+1}\",\n        alpha=0.7\n    )\n    # 각 군집의 중심(평균 좌표)을 MDS 평면에서 계산\n    center = coords[mask].mean(axis=0)\n    plt.scatter(\n        center[0], center[1],\n        marker='X', s=200, edgecolor='k', linewidth=1.5\n    )\n    plt.text(center[0] + 0.03, center[1] + 0.03,\n             f\"C{g+1}\", fontsize=11, weight='bold')\n\nfor name, (x, y, g) in zip(swiss.index, np.c_[coords, clusters]):\n    # 각 점 옆에 지명 + 군집번호도 표시 (원하면)\n    plt.text(x + 0.02, y + 0.02, f\"{name} (C{g+1})\", fontsize=7)\n\nplt.axhline(0, linewidth=0.5)\nplt.axvline(0, linewidth=0.5)\nplt.gca().set_aspect(\"equal\", adjustable=\"datalim\")\nplt.xlabel(\"Dimension 1\")\nplt.ylabel(\"Dimension 2\")\nplt.title(\"MDS of Swiss Provinces with K-means (k=4)\")\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n주성분 분석\n군집을 구성하는 변수가 모두 계량형으로 측정된 경우, 관측치 간의 구조를 2차원 공간에서 표현하기 위한 절차로는 다차원척도법보다 주성분분석을 적용하는 것이 방법론적으로 더 적합하다. 그 근거는 다음과 같다.\n첫째, MDS에서 도출되는 차원(예: Dimension 1, Dimension 2)은 단지 거리(dissimilarity) 보존을 목적으로 산출된 좌표축이며, 그 축 자체가 어떠한 통계적 속성이나 변수 조합을 반영한다는 근거가 존재하지 않는다. MDS의 축은 회전 불변성 또한 보장되지 않으며, 따라서 차원의 의미를 명확하게 정의하거나 해석하기 어렵다.\n반면, PCA는 원 변수를 선형결합하여 주성분을 구성하고, 각 주성분에 대한 적재값을 통해 해당 성분이 어떤 변수군의 변동을 주도하는지 해석할 수 있다. 이러한 구조적 해석 가능성 덕분에 PC1, PC2와 같은 차원에 대하여 명확한 개념적 명명이 가능하다.\n따라서 군집 분석 결과를 시각적으로 제시함과 동시에 차원의 통계적·개념적 속성을 해석해야 하는 목적을 가진 경우, PCA의 2개 주성분을 활용하여 2차원 공간을 구성하는 방법이 MDS보다 분명한 이점을 제공한다. 즉, PCA를 활용하면 군집 간 위치 관계뿐만 아니라 각 차원이 의미하는 바를 이론적·실증적으로 설명할 수 있어, 해석 가능성 측면에서 우수하다.\nfrom sklearn.decomposition import PCA\n\n# 1. PCA(2개 성분)\npca = PCA(n_components=2)\npcs = pca.fit_transform(X_scaled)   # 이미 X_scaled가 표준화된 상태\n\n# 2. 데이터프레임으로 보기 좋게\npcs_df = pd.DataFrame(pcs, columns=[\"PC1\", \"PC2\"], index=swiss.index)\nprint(pcs_df.head())\n\n# 3. 산점도 그리기\nplt.figure(figsize=(8,6))\nplt.scatter(pcs_df[\"PC1\"], pcs_df[\"PC2\"], alpha=0.7)\n\n# labels\nfor name, (x, y) in pcs_df.iterrows():\n    plt.text(x + 0.02, y + 0.02, name, fontsize=8)\n\nplt.axhline(0, color='gray', linewidth=0.5)\nplt.axvline(0, color='gray', linewidth=0.5)\nplt.xlabel(f\"PC1 ({pca.explained_variance_ratio_[0]*100:.1f}% var)\")\nplt.ylabel(f\"PC2 ({pca.explained_variance_ratio_[1]*100:.1f}% var)\")\nplt.title(\"PCA Biplot (PC1 vs PC2)\")\nplt.gca().set_aspect(\"equal\", adjustable=\"datalim\")\nplt.tight_layout()\nplt.show()\nPC1: ”전통적·농업 중심성 요인(Traditional–Agricultural Axis)“\nPC1은 Fertility와 Agriculture가 높은 값을 보이는 지역에서 양의 방향으로 나타나며, 반대로 Education과 Examination 수준이 높은 지역에서는 음의 방향을 보이는 구조를 보인다. 이는 출산율이 높고 농업 비중이 큰 전통적 사회경제 특성과 교육 및 군사시험 성취도가 높고 근대화된 지역적 특성 사이의 대립축을 반영한다. 따라서 PC1은 지역의 전통성–근대화 또는 농업 중심성–교육·도시화 축을 설명하는 성분으로 해석할 수 있다.\nPC2: ”종교적 구성 및 건강지표 요인(Religiosity–Health Axis)“\nPC2는 Catholic 비율과 Infant.Mortality(영아사망률) 변수의 기여도가 커서 가톨릭 신자 비중이 높고 영아사망률이 상대적으로 높은 지역에서 양의 방향을 보인다. 반면 이러한 특성이 낮은 지역은 음의 방향에 위치한다. 따라서 PC2는 종교적 구성(가톨릭 비율)과 기초 보건 상태(영아사망률)를 함께 반영하는 종교·보건 요인 또는 가톨릭성–보건수준 축으로 해석할 수 있다.\nPC1 PC2  province_name  Courtelary 0.363552 1.399420  Delemont -1.634175 1.026047  Franches-Mnt -2.104200 0.746024  Moutier -0.747608 0.595892  Neuveville 0.381528 0.448845 \n# PCA 좌표 위에 군집 결과 올리기\nplt.figure(figsize=(8,6))\n\nfor g in range(4):\n    mask = (clusters == g)\n    plt.scatter(\n        pcs[mask, 0],\n        pcs[mask, 1],\n        label=f\"Cluster {g+1}\",\n        alpha=0.7\n    )\n\n    # 각 군집 중심(PC1, PC2에서의 평균점)\n    center = pcs[mask].mean(axis=0)\n    plt.scatter(center[0], center[1],\n                marker='X', s=200, edgecolor='k', linewidth=1.5)\n    plt.text(center[0] + 0.05, center[1] + 0.05,\n             f\"C{g+1}\", fontsize=11, weight='bold')\n\n# 각 지역 이름 + 군집 표시\nfor name, (x, y, g) in zip(swiss.index, np.c_[pcs, clusters]):\n    plt.text(x + 0.03, y + 0.03, f\"{name} (C{g+1})\", fontsize=7)\n\nplt.axhline(0, color='gray', linewidth=0.5)\nplt.axvline(0, color='gray', linewidth=0.5)\nplt.xlabel(f\"PC1 ({pca.explained_variance_ratio_[0]*100:.1f}% var)\")\nplt.ylabel(f\"PC2 ({pca.explained_variance_ratio_[1]*100:.1f}% var)\")\nplt.title(\"PCA Scatter Plot with K-means Clusters (k=4)\")\nplt.legend()\nplt.gca().set_aspect(\"equal\", adjustable=\"datalim\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n(3) 리커트 척도 문항 다차원척도 non-metric\n리커트 척도는 본질적으로 순서척도이므로 non-metric MDS가 이론적으로 더 타당하다. 다만 실증 분석에서는 리커트 척도를 등간척도로 간주하여 metric MDS를 적용하는 경우도 흔하며, 분석 목적과 자료 특성에 따라 두 방식 모두 선택될 수 있다. 척도의 등간성을 엄밀히 보장하기 어렵다는 점을 고려하면, 비모수적 거리 변환을 허용하는 non-metric 접근이 보다 보수적이고 안정적인 분석 방법이라고 할 수 있다.\n사례데이터 만들기\n자신이 속한 조직에 대하여 응답자 20명이 다음 6개 항목에 대하여 5점 리커트 척도로 측정하였다고 하자.\n\n\n\n\n\n\n\n변수\n내용\n\n\nTrust\n조직을 신뢰한다\n\n\nWarmth\n조직이 따뜻하게 느껴진다\n\n\nReliability\n약속을 잘 지킨다\n\n\nEfficiency\n효율적으로 운영된다\n\n\nTransparency\n의사결정이 투명하다\n\n\nCommitment\n조직에 헌신하고 싶다\n\n\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.manifold import MDS\nfrom sklearn.metrics import pairwise_distances\nimport matplotlib.pyplot as plt\n\nnp.random.seed(7622)\n\natt = pd.DataFrame({\n    \"Trust\": np.random.randint(1, 6, 20),\n    \"Warmth\": np.random.randint(1, 6, 20),\n    \"Reliability\": np.random.randint(1, 6, 20),\n    \"Efficiency\": np.random.randint(1, 6, 20),\n    \"Transparency\": np.random.randint(1, 6, 20),\n    \"Commitment\": np.random.randint(1, 6, 20)\n})\n\n# 기존 att 행 수 파악\nn = att.shape[0]\n\n# Subject1 ~ Subject20 식 이름 만들기\natt.index = [f\"Subject{i+1}\" for i in range(n)]\n\nprint(att.head())\nTrust Warmth Reliability Efficiency Transparency Commitment  Subject1 4 4 1 3 4 4  Subject2 2 2 4 2 3 4  Subject3 1 3 3 4 4 4  Subject4 5 2 1 4 1 3  Subject5 1 5 5 3 5 3\nItem(variable)-level MDS\ncorr = att.corr()\nD_item = 1 - corr   # 상관 거리\n\nmds = MDS(n_components=2, dissimilarity=\"precomputed\", random_state=42)\ncoords_item = mds.fit_transform(D_item)\n\ncoords_item_df = pd.DataFrame(coords_item, index=corr.index, columns=[\"Dim1\", \"Dim2\"])\ncoords_item_df\nDim1 Dim2  Trust -0.563689 -0.370335  Warmth 0.536723 0.343738  Reliability 0.581360 -0.191912  Efficiency -0.755631 0.404456  Transparency -0.006990 0.485314  Commitment 0.208226 -0.671261\nplt.figure(figsize=(7,6))\nplt.scatter(coords_item[:,0], coords_item[:,1])\nfor name, (x,y) in coords_item_df.iterrows():\n    plt.text(x+0.03, y+0.03, name, fontsize=12)\nplt.axhline(0); plt.axvline(0)\nplt.title(\"Item-level MDS (Likert-scale attitude items)\")\nplt.show()\n\n\n\n\n\nSubject-level MDS\nfrom sklearn.metrics import pairwise_distances\nfrom sklearn.manifold import MDS\n\n# 개체 거리행렬\nD_subj = pairwise_distances(att.values, metric=\"euclidean\")\n\n# MDS\nmds2 = MDS(n_components=2, dissimilarity=\"precomputed\", random_state=42)\ncoords_subj = mds2.fit_transform(D_subj)\n\nprint(coords_subj)\n[[-0.63406539 3.36331313]  [-0.40346786 -4.1378684 ]  [-2.86082664 2.31117055]  [ 1.39073653 3.89379038]  (이하 생략)\nplt.figure(figsize=(8,6))\nplt.scatter(coords_subj[:,0], coords_subj[:,1], s=30, alpha=0.7)\n\n# subject 이름 붙이기\nfor name, (x, y) in zip(att.index, coords_subj):\n    plt.text(x + 0.02, y + 0.02, name, fontsize=9)\n\nplt.title(\"Subject-level MDS (Respondents)\")\nplt.xlabel(\"Dim1\")\nplt.ylabel(\"Dim2\")\nplt.axhline(0); plt.axvline(0)\nplt.show()\n\n\n\n\n\n\n\n(5) 교차빈도표 다차원척도 non-metric\n데이터 만들기\n5개 브랜드에 대한 4개 연령대 1997명의 선호도를 조사한 후 생성한 교차표이다.\nimport pandas as pd\n\ntable = pd.DataFrame({\n    \"BrandA\": [120, 210, 180, 60],\n    \"BrandB\": [ 85, 190, 240,125],\n    \"BrandC\": [ 40,  80, 110, 95],\n    \"BrandD\": [ 22,  75,  98,110],\n    \"BrandE\": [ 12,  20,  45, 80]\n}, index=[\"Teen\",\"20s\",\"30s\",\"40s\"])\nprint(table)\nBrandA BrandB BrandC BrandD BrandE  Teen 120 85 40 22 12  20s 210 190 80 75 20  30s 180 240 110 98 45  40s 60 125 95 110 80\n행 프로파일 MDS\n# 행(연령대)의 유클리드 거리\nfrom sklearn.metrics import pairwise_distances\nD = pairwise_distances(table, metric=\"euclidean\")\n\n# 행(연령대)의  MDS\nfrom sklearn.manifold import MDS\nmds = MDS(n_components=2, dissimilarity=\"precomputed\", random_state=42)\ncoords = mds.fit_transform(D)\n\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(7,6))\nplt.scatter(coords[:,0], coords[:,1])\n\nfor name, (x,y) in zip(table.index, coords):\n    plt.text(x+0.02, y+0.02, name, fontsize=12)\n\nplt.axhline(0); plt.axvline(0)\nplt.title(\"MDS of Age Groups based on Brand Purchase Pattern\")\nplt.xlabel(\"Dimension 1\")\nplt.ylabel(\"Dimension 2\")\nplt.show()\n빈도표를 이용하여 행·열 범주 간의 관계를 해석할 때, 행 프로파일과 열 프로파일을 각각 따로 살펴보는 것보다는 두 프로파일을 동시에 하나의 공간에 표현하는 것이 훨씬 효율적인 해석을 제공한다. 이는 두 범주 집합이 동일한 차원축에서 어떤 방향성과 거리를 갖는지를 직관적으로 확인할 수 있기 때문이다.\n다차원척도법(MDS) 역시 행 프로파일과 열 프로파일을 동일 좌표평면에 함께 배치할 수 있으며, 이러한 점에서는 대응분석(CA)의 Biplot과 유사한 해석 구조를 갖는다. 즉, 두 방법 모두 범주 간 상대적 거리와 패턴의 유사성을 시각적으로 표현한다는 공통점을 지닌다.\n그러나 두 방법이 사용하는 유사성(거리) 척도는 본질적으로 다르다.\n대응분석은 카이제곱(χ²) 거리 기반으로 범주형 자료의 빈도 구조를 해석하는 데 특화되어 있으며,\nMDS는 사용자가 정의한 거리(유클리드 거리, 상관 기반 거리 등)를 바탕으로 범주 간 구조를 구성한다.\n따라서 빈도표 기반 행·열 프로파일의 활용 측면에서는 두 방법이 유사하게 보이지만, 거리 정의의 차이로 인해 결과의 형태와 해석 방향은 서로 달라질 수 있음을 유념해야 한다.\n\n\n\n\n\n열 프로파일 MDS\n# 브랜드(열) 기준 거리행렬 계산\nfrom sklearn.metrics import pairwise_distances\nfrom sklearn.manifold import MDS\nimport matplotlib.pyplot as plt\n\n# table.T 는 행↔열 전치 (브랜드를 행으로)\nD_brand = pairwise_distances(table.T, metric=\"euclidean\")\n\n# MDS 실행\nmds_brand = MDS(n_components=2, dissimilarity=\"precomputed\", random_state=42)\ncoords_brand = mds_brand.fit_transform(D_brand)\n\n# 시각화\nplt.figure(figsize=(7,6))\nplt.scatter(coords_brand[:,0], coords_brand[:,1])\n\nfor name, (x, y) in zip(table.columns, coords_brand):\n    plt.text(x+0.02, y+0.02, name, fontsize=12)\n\nplt.axhline(0)\nplt.axvline(0)\nplt.title(\"MDS of Brands based on Age-Group Purchase Patterns\")\nplt.xlabel(\"Dimension 1\")\nplt.ylabel(\"Dimension 2\")\nplt.show()\n\n\n\n\n\n\n\n\n\nChapter 2. 대응분석\n\n1. 대응분석이란?\n개념\n대응분석은 범주형 변수의 범주 간 유사성을 시각적으로 표현하기 위한 탐색적 자료 분석 기법이다. 특히, 교차표(분할표) 형태로 정리된 자료에서 행 범주와 열 범주를 동시에 저차원 공간(주로 2차원)에 좌표로 나타내어 범주 간의 구조적 관계와 연관성을 파악하는 데 활용된다.\n복잡한 분할표의 정보를 직관적으로 해석 가능한 시각적 구조로 변환하여, 어떤 행 범주가 어떤 열 범주와 가까운지, 범주들 사이에 어떤 패턴이나 군집이 존재하는지를 쉽게 탐색할 수 있도록 해주는 기법이다. 즉, 대응분석은 범주형 데이터의 행·열 범주를 동시에 시각화하여 관계를 탐색하는 기법이다.\n기원\n대응분석의 이론적‧수리적 기반과 기하적 표현 방식은 서로 다른 학자들에 의해 독립적으로 발전해 왔다.\n수리적 기원: Hirshfeld(1930)는 상관구조와 분할표의 연관성을 해석하는 수리적 틀을 제시하였다. 이 연구가 이후 대응분석의 수학적 기반을 형성하는 중요한 출발점이 되었다.\n기하적 발전: 1960년대 프랑스 Jean-Paul Benzécrii와 그의 연구 그룹은 분할표 자료를 기하학적 공간에 배치하여 해석하는 방법을 체계화하고, 범주형 자료 분석 전반에 대응분석을 적극 활용함으로써 기법을 크게 확장시켰다. 오늘날 사용되는 대응분석의 좌표계 구성, 특이값 분해(SVD) 기반 해석 등은 이 시기에 정립되었다.\n일본의 발전: 1950년대 일본의 Chikio Hayashi는 범주형 자료 분석을 위한 수량화 제3방법을 개발했으며, 이는 대응분석과 유사한 목적과 구조를 지닌 기법으로 평가된다. 일본 학계에서는 이 접근을 통해 범주형 자료 분석이 활발히 발전하였다. 기법의 수리적 기반은 Hirshfeld(1930), 기하적·응용적 발전은 **Benzécri(1960s)**에 의해 확립되었다. 일본의 Hayashi의 수량화 이론 역시 대응분석과 유사한 분석적 맥락에서 발전하였다.\nRXC 분할표\n두 범주형 변수 X(R개 범주, 원인), Y(C개 범주, 결과변인)의 교차표는 다음과 같다.\n\n\n\n\n\n\n\n\n\n\n\nY\nX\n1\n2\n…\nC\nTotal\n\n\n1\nπ11\nπ12\n…\nπ1c\nπ1+\n\n\n2\nπ21\nπ22\n…\nπ2c\nπ2+\n\n\n…\n…\n…\n…\n…\n…\n\n\nR\nπr1\nπr2\n…\nπrc\nπr+\n\n\nTotal\nπ+1\nπ+2\n…\nπ+c\nπ++\n\n\n\n\\(\\pi_{ij}\\) : (X, Y) 결합밀도함수, \\(\\pi_{i +}\\): X 주변밀도함수, \\(\\pi_{+ j}\\) : Y 주변밀도함수\nHomogeneity (동질성) 검정\n각 행에 대해 열의 분포가 동일한가?\n귀무가설 : \\(H_{0}:\\pi_{ij} = \\pi_{kj}forallj = 1,2,.\\ldots,C\\)\nIndependence (독립성) 검정\n(X, Y)는 서로 독립인가? 두 변수가 독립이라면 (\\(P(X = x,Y = y) = P(X = x)P(Y = y)\\))이므로 귀무가설은 다음과 같다.\n귀무가설 : \\(H_{0}:\\pi_{ij} = \\pi_{i +}\\pi_{+ j}\\)\n검정통계량\n동질성, 독립성 검정 모두 검정통계량은 동일하다. (i, j) 셀의 관측빈도를 \\(O_{ij} = n_{ij}\\) 라 하자. i-행의 행 관측빈도 합을 \\(n_{i +}\\), j-열의 열 관측빈도 합을 \\(n_{+ j}\\), 총 관측빈도를 \\(n_{+ +}\\)라 정의하자.\n만약 귀무가설이 맞다면 (i, j) 셀의 기대빈도는 \\(E_{ij} = \\frac{n_{i +}n_{+ j}}{n_{+ +}}\\)이다.\n검정통계량\n\\(TS = \\frac{(O_{ij} - E_{ij})^{2}}{E_{ij}} \\sim \\chi^{2}(df = (R - 1)(C - 1))\\)\n*) Cochran Theorem : 셀의 기대빈도가 5이하인 셀이 전체 셀 중 20% 미만이면 교차검정통계량은 \\(\\chi^{2}\\)- 분포를 따른다. 만약 이를 위반하면 Fisher Exact 검정 방법을 적용한다.\n\n\n2. 기본 알고리즘\n\\((i,j)\\)셀의 빈도 \\(n_{ij} \\geq 0\\)의 i번째 행의 각 열빈도 \\((n_{i1},n_{i2},\\ldots,n_{iC})\\)은 총빈도가 \\(n_{i +}\\)이고 \\(C\\)개 범주를 갖는 다항(Multinomial) 분포이다.\n순서1: 다항분포의 \\((i,j)\\) 확률은 상대빈도 \\(f_{ij} = \\frac{n_{ij}}{n_{i +}}\\)을 계산한다. 이것을 행 프로파일이라 정의한다.\n순서2: 각 행의 상대빈도 \\(f_{i1},f_{i2},\\ldots,f_{iC}\\)를 선형계수로 (주성분 분석과 유사) 하여 좌표 계산한다.\n순서3: \\(R_{i} = (\\frac{f_{i1}}{f_{i +}},\\frac{f_{i2}}{f_{i +}},\\ldots,\\frac{f_{iC}}{f_{i +}})\\)가 C 차원 가중 유크리드 공간의 좌표이다. 가중 유클리드 공간이란에서는 두 개의  좌표 (\\(R_{i},R_{j}\\)) 사이의 거리를 다음과 같이 정의한다. \\(D(R_{i},R_{j}) = \\sqrt{\\frac{\\sum_{c = 1}^{C}(\\frac{f_{ic}}{f_{i +}} - \\frac{f_{jc}}{f_{j +}})^{2}}{f_{+ c}}}\\)\n순서4: 같은 방식으로 열 프로파일의 좌표 및 개체 거리 계산하고 행, 열 프로파일을 각각 2차원 공간에 표현한다.\n\n\n3. 사례분석\n\n(1) 집안일 x 담당 교차 빈도표\n데이터 만들기\nimport pandas as pd\n# 집안일x담당(아내, 남편, 공동, 교대) 교차표이다.\nhouse = pd.DataFrame({\n    \"Wife\":      [156, 124, 120, 45,  67],\n    \"Husband\":   [ 14,  11,  58, 135, 21],\n    \"Joint\":     [100, 120, 160,  80, 50],\n    \"Altering\":     [ 12,  20,  40,  18, 10]\n}, index=[\"Laundry\", \"Meals\", \"Shopping\", \"Finance\", \"Repairs\"])\n\nprint(house)\nWife Husband Joint Altering  Laundry 156 14 100 12  Meals 124 11 120 20  Shopping 120 58 160 40  Finance 45 135 80 18  Repairs 67 21 50 10\nfrom scipy.stats import chi2_contingency\n# 독립성 검정 χ² test\nchi2, p, dof, expected = chi2_contingency(house)\n\nprint(\"=== Chi-square Test for Independence ===\")\nprint(f\"Chi-square statistic : {chi2: .3f}\")\nprint(f\"Degrees of freedom   : {dof}\")\nprint(f\"P-value              : {p: .5f}\")\n\n# 행 퍼센트 (Row Percentages)\nrow_percent = house.div(house.sum(axis=1), axis=0) * 100\nprint(\"=== Row Percentages (%) ===\")\nprint(row_percent)\n통계적으로 매우 유의미하게, 가사 항목의 종류와 누가 그 항목을 수행하는지에 대한 책임 분담 방식은 서로 독립이 아니며, 강한 연관성이 있다고 결론 내릴 수 있습니다. 즉, 어떤 가사일을 수행하느냐에 따라 주된 수행 주체가 달라진다는 것을 의미한다.\n=== Chi-square Test for Independence ===  Chi-square statistic : 296.741  Degrees of freedom : 12  P-value : 0.00000\n세탁(Laundry)은 아내가 약 55%로 가장 큰 비중을 차지하고 공동 수행이 약 35%로 뒤를 이어 전형적인 아내 중심 혹은 공동 수행 중심의 작업으로 나타난다. 식사 준비(Meals) 역시 아내가 약 45%, 공동 수행이 약 44%로 두 방식이 거의 균형을 이루며 남편 참여는 4% 수준으로 낮다. 장보기(Shopping)는 공동 수행이 약 42%로 가장 높고 아내 단독이 32%, 남편이 15% 수준을 차지해 가족 구성원 간의 분담이 상대적으로 활발한 작업임을 보여준다. 반면 재정 관리(Finance)는 남편이 49%로 압도적으로 높은 참여율을 보이며, 아내는 16%, 공동 수행은 29%로 나타나 유일하게 남편 중심의 작업이라는 특징을 가진다. 수리 작업(Repairs)은 아내 45%, 공동 34%로 세탁이나 식사 준비와 유사하게 아내·공동 중심의 구조를 보이며, 남편의 비중은 14%로 낮아 일반적 기대와 다른 패턴을 보여준다.\n=== Row Percentages (%) ===  Wife Husband Joint Altering  Laundry 55.319149 4.964539 35.460993 4.255319  Meals 45.090909 4.000000 43.636364 7.272727  Shopping 31.746032 15.343915 42.328042 10.582011  Finance 16.187050 48.561151 28.776978 6.474820  Repairs 45.270270 14.189189 33.783784 6.756757\nprince 모듈 활용 대응분석\n# (1) prince 모듈 설치 (Colab)\n!pip install -q prince\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport prince\n\n# (2) CA 적합\nca = prince.CA(n_components=2,\n    n_iter=10,\n    copy=True,\n    check_input=True,\n    engine=\"scipy\",\n    random_state=42)\n\nca = ca.fit(house)\n\n# (3) 좌표 추출 (principal coordinates)\nrow_coords = ca.row_coordinates(house)     # 행(집안일)\ncol_coords = ca.column_coordinates(house)  # 열(담당자)\n\nprint(\"Row coordinates:\\n\", row_coords)\nprint(\"\\nColumn coordinates:\\n\", col_coords)\n\n# (4) 고유값/관성(= inertia)\neigenvalues = ca.eigenvalues_\n# 설명된 관성 비율\nexplained_inertia = eigenvalues / eigenvalues.sum()\n\nprint(\"Eigenvalues:\\n\", eigenvalues)\nprint(\"Explained inertia (ratio):\\n\", explained_inertia)\nprint(\"Explained inertia (%):\\n\", explained_inertia * 100)\n\n# (5) Biplot (직접 그리기)\nplt.figure(figsize=(8,7))\n\n# rows\nplt.scatter(row_coords[0], row_coords[1], marker=\"o\", label=\"Tasks\")\nfor name, (x, y) in row_coords.iterrows():\n    plt.text(x+0.02, y+0.02, name, fontsize=11)\n\n# cols\nplt.scatter(col_coords[0], col_coords[1], marker=\"s\", label=\"Persons\")\nfor name, (x, y) in col_coords.iterrows():\n    plt.text(x+0.02, y+0.02, name, fontsize=11, fontweight=\"bold\")\n\nplt.axhline(0, linewidth=0.7)\nplt.axvline(0, linewidth=0.7)\nplt.gca().set_aspect(\"equal\", adjustable=\"datalim\")\nplt.xlabel(\"Dim 1\")\nplt.ylabel(\"Dim 2\")\nplt.title(\"Correspondence Analysis Biplot (prince)\")\nplt.legend()\nplt.tight_layout()\nplt.show()\n대응분석 결과, 집안일 유형과 담당자 간의 관계는 2차원 공간에서 명확한 구조를 보인다. 제1차원(Dim 1)은 전체 관성의 약 91.7%를 설명하는 주된 축으로, 집안일 수행에서 성별 분화 정도를 가장 강하게 반영하는 차원으로 해석된다. 이 축에서 Finance와 Husband가 동일한 방향(양의 방향)으로 크게 떨어져 위치하고 있어, 재정 관련 집안일이 남편에게 강하게 귀속되는 경향을 보임을 시사한다. 반면 Laundry와 Meals는 Dim 1의 음의 방향에 위치하며, Wife와 공간적으로 근접해 있어 해당 활동들이 아내 중심적으로 수행되는 집안일임을 보여준다.\n제2차원(Dim 2)은 약 8.3%의 관성을 설명하는 보조적 차원으로, 집안일 수행 방식의 차이를 세분화하는 역할을 한다. 이 차원에서 Laundry–Wife는 상대적으로 상단에 위치하여 전형적인 단독 수행 성격을 나타내는 반면, Shopping, Joint, Altering는 하단부에 함께 분포하며 공동 수행 또는 교대 수행의 성격이 강한 활동으로 구분된다. 특히 Shopping은 원점 근처에 위치하면서 Joint와 가깝게 나타나, 특정 담당자에 귀속되기보다는 가족 구성원 간 분담이 비교적 유연한 집안일임을 시사한다.\n한편 Repairs는 Dim 1에서는 아내 쪽에 가깝지만, Dim 2에서는 중립적 위치를 보이며 다른 집안일과는 다소 분리된 양상을 나타낸다. 이는 수선·보수 활동이 특정 성별에 강하게 고정되기보다는 상황에 따라 담당이 달라지는 특성을 지닐 가능성을 시사한다.\n종합하면, 대응분석 결과는 집안일 수행이 단일한 성별 구도로 설명되기보다는, 아내 중심형(Laundry, Meals), 남편 중심형(Finance), 그리고 공동·혼합형(Shopping, Altering)으로 구조화되어 있음을 명확히 보여준다. 이러한 결과는 단순한 비율 비교를 넘어, 집안일 유형과 담당자 간의 관계를 다차원적 공간에서 직관적으로 파악할 수 있게 해 준다는 점에서 의미가 있다.\nRow coordinates:  0 1  Laundry -0.394625 0.178859  Meals -0.353594 -0.056338  Shopping -0.010353 -0.179201  Finance 0.828223 0.067011  Repairs -0.120338 0.095699\nColumn coordinates:  0 1  Wife -0.338852 0.128318  Husband 0.930497 0.079204  Joint -0.102391 -0.107858  Altering 0.033226 -0.296209\nEigenvalues:  [0.19924861 0.01810192]  Explained inertia (ratio):  [0.91671557 0.08328443]  Explained inertia (%):  [91.67155667 8.32844333]\n\n\n\n\n\n다차원척도법 행/열 프로파일 bipolar\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import pairwise_distances\nfrom sklearn.manifold import MDS\n\n# -----------------------------------------\n# 0. 교차표 (이미 house가 있다면 이 부분은 생략)\n# -----------------------------------------\nhouse = pd.DataFrame({\n    \"Wife\":      [156, 124, 120, 45,  67],\n    \"Husband\":   [ 14,  11,  58, 135, 21],\n    \"Joint\":     [100, 120, 160,  80, 50],\n    \"Altering\":  [ 12,  20,  40,  18, 10]\n}, index=[\"Laundry\", \"Meals\", \"Shopping\", \"Finance\", \"Repairs\"])\n\n# -----------------------------------------\n# 1. 행/열 프로파일 (비율)\n#    - 행 프로파일: 각 집안일을 100%로 했을 때 담당자 비율\n#    - 열 프로파일: 각 담당자를 100%로 했을 때 집안일 비율\n# -----------------------------------------\nrow_prof = house.div(house.sum(axis=1), axis=0)      # 5×4\ncol_prof = house.div(house.sum(axis=0), axis=1).T    # 4×5  (전치해서 열을 \"행\"으로 사용)\n\n# -----------------------------------------\n# 2. 행·열 프로파일별 거리행렬 (유클리드 거리)\n# -----------------------------------------\nD_row = pairwise_distances(row_prof.values, metric=\"euclidean\")\nD_col = pairwise_distances(col_prof.values, metric=\"euclidean\")\n\n# -----------------------------------------\n# 3. Metric MDS (2차원) – 행/열 따로\n# -----------------------------------------\nmds = MDS(n_components=2, dissimilarity=\"precomputed\", random_state=42)\n\nrow_coords = mds.fit_transform(D_row)   # (5×2)\ncol_coords = mds.fit_transform(D_col)   # (4×2)\n\nrow_coords_df = pd.DataFrame(row_coords, index=house.index, columns=[\"Dim1\",\"Dim2\"])\ncol_coords_df = pd.DataFrame(col_coords, index=house.columns, columns=[\"Dim1\",\"Dim2\"])\n\n# -----------------------------------------\n# 4. 스케일 맞추기\n#    - 두 해를 같은 그림에 올리기 위해\n#      열 좌표를 행 좌표의 분산에 맞춰 rescale\n# -----------------------------------------\nscale = row_coords_df.std(axis=0) / col_coords_df.std(axis=0)\ncol_coords_aligned = col_coords_df * scale\n\n# -----------------------------------------\n# 5. MDS-biplot 그리기\n# -----------------------------------------\nplt.figure(figsize=(8,7))\n\n# 행(집안일) – 파란 원\nplt.scatter(row_coords_df[\"Dim1\"], row_coords_df[\"Dim2\"],\n            marker=\"o\", color=\"tab:blue\", label=\"Tasks\")\nfor name, (x, y) in row_coords_df.iterrows():\n    plt.text(x+0.02, y+0.02, name, color=\"tab:blue\", fontsize=11)\n\n# 열(담당자) – 빨간 사각형 (스케일 조정된 좌표 사용)\nplt.scatter(col_coords_aligned[\"Dim1\"], col_coords_aligned[\"Dim2\"],\n            marker=\"s\", color=\"tab:red\", label=\"Persons\")\nfor name, (x, y) in col_coords_aligned.iterrows():\n    plt.text(x+0.02, y+0.02, name, color=\"tab:red\", fontsize=11, weight=\"bold\")\n\nplt.axhline(0, color=\"gray\", linewidth=0.7)\nplt.axvline(0, color=\"gray\", linewidth=0.7)\nplt.gca().set_aspect(\"equal\", adjustable=\"datalim\")\n\nplt.xlabel(\"Dimension 1 (MDS)\")\nplt.ylabel(\"Dimension 2 (MDS)\")\nplt.title(\"MDS Biplot of House Tasks × Person\\n(Profiles + Euclidean distance)\")\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n(2) 21대 대선 후보자 지역별 특표수\nimport pandas as pd\n\ndf = pd.DataFrame({\n    \"개혁신당 이준석\": [\n        78704, 816435, 161579, 116094, 62104, 135376, 94724,\n        168473, 655346, 25004, 63177, 176739, 60822, 67961,\n        36909, 111092, 86984\n    ],\n    \"국민의힘 김문수\": [\n        483360, 3504620, 1123843, 1159594, 79937, 1103913, 393549,\n        1146238, 2738405, 83965, 353180, 776952, 110624, 134996,\n        145290, 600108, 457065\n    ],\n    \"더불어민주당 이재명\": [\n        449161, 4821148, 851733, 442683, 844682, 379130, 470321,\n        895213, 3105459, 140620, 315820, 1044295, 1111941,\n        1023272, 228729, 661316, 501990\n    ],\n    \"무소속 송진호\": [\n        1137, 8356, 2678, 1788, 934, 1362, 1109,\n        2099, 5998, 235, 899, 2098, 2104, 1719,\n        528, 1519, 1228\n    ],\n    \"민주노동당 권영국\": [\n        9422, 84074, 21809, 13884, 8767, 12531, 9905,\n        18189, 83900, 2961, 9299, 20743, 9352,\n        10061, 6191, 12893, 10169\n    ]\n}, index=[\n    \"강원특별자치도\", \"경기도\", \"경상남도\", \"경상북도\", \"광주광역시\",\n    \"대구광역시\", \"대전광역시\", \"부산광역시\", \"서울특별시\",\n    \"세종특별자치시\", \"울산광역시\", \"인천광역시\", \"전라남도\",\n    \"전북특별자치도\", \"제주특별자치도\", \"충청남도\", \"충청북도\"\n])\nprint(df)\n개혁신당 이준석 국민의힘 김문수 더불어민주당 이재명 무소속 송진호 민주노동당 권영국  강원특별자치도 78704 483360 449161 1137 9422  경기도 816435 3504620 4821148 8356 84074  경상남도 161579 1123843 851733 2678 21809  경상북도 116094 1159594 442683 1788 13884  광주광역시 62104 79937 844682 934 8767  (이하 생략)\nimport prince\nimport matplotlib.pyplot as plt\n\n# 대응분석\nca = prince.CA(n_components=2,\n    n_iter=10,\n    copy=True,\n    check_input=True,\n    engine=\"scipy\",  #\n    random_state=42)\n\nca = ca.fit(df)\n\n# 차원 좌표 출력\nrow_coords = ca.row_coordinates(df)     # 지역 좌표\ncol_coords = ca.column_coordinates(df)  # 후보 좌표\n\nprint(row_coords, col_coords)\n\neigenvalues = ca.eigenvalues_\nexplained_inertia = eigenvalues / eigenvalues.sum()\n\n# (4) 고유값/관성(= inertia) 설병비율\nprint(\"Eigenvalues:\", eigenvalues)\nprint(\"Explained inertia (%):\", explained_inertia * 100)\n0 1  강원특별자치도 0.121599 -0.031163  경기도 -0.062447 0.018612  경상남도 0.218601 -0.041902  경상북도 0.520749 -0.092644  광주광역시 -0.723672 -0.034695  (이하 생략)\n0 1  개혁신당 이준석 0.056695 0.149126  국민의힘 김문수 0.333431 -0.018318  더불어민주당 이재명 -0.287291 -0.012566  무소속 송진호 -0.079156 -0.095681  민주노동당 권영국 0.011667 0.143204\nEigenvalues: [0.08681782 0.00228203]  Explained inertia (%): [97.43879148 2.56120852]\n# ===============================\n# Matplotlib 한글 폰트 설정 (Colab)\n# ===============================\n\n!apt-get -qq install fonts-nanum\n\nimport matplotlib.pyplot as plt\nimport matplotlib.font_manager as fm\n\nplt.rcParams['font.family'] = 'NanumGothic'\nplt.rcParams['axes.unicode_minus'] = False\n\n# 그래프 경고 출력 제거\nimport logging\nlogging.getLogger(\"matplotlib.font_manager\").setLevel(logging.ERROR)\nimport matplotlib.pyplot as plt\nimport matplotlib.font_manager as fm\n\n# -------------------------------------------------\n# 1. 한글 폰트: 파일 경로 기반 (findfont 우회)\n# -------------------------------------------------\nfont_path = \"/usr/share/fonts/truetype/nanum/NanumGothic.ttf\"\nfp = fm.FontProperties(fname=font_path)\n\n# -------------------------------------------------\n# 2. Figure\n# -------------------------------------------------\nplt.figure(figsize=(9, 8))\n\n# -------------------------------------------------\n# 3. 행 좌표 (지역)\n# -------------------------------------------------\nplt.scatter(\n    row_coords[0],\n    row_coords[1],\n    marker=\"o\",\n    color=\"tab:blue\",\n    label=\"Regions\"\n)\n\nfor name, (x, y) in row_coords.iterrows():\n    plt.text(\n        x + 0.01, y + 0.01,\n        name,\n        fontsize=9,\n        fontproperties=fp,\n        color=\"tab:blue\"\n    )\n\n# -------------------------------------------------\n# 4. 열 좌표 (후보)\n# -------------------------------------------------\nplt.scatter(\n    col_coords[0],\n    col_coords[1],\n    marker=\"s\",\n    color=\"tab:red\",\n    label=\"Candidates\"\n)\n\nfor name, (x, y) in col_coords.iterrows():\n    plt.text(\n        x + 0.01, y + 0.01,\n        name,\n        fontsize=11,\n        fontweight=\"bold\",\n        fontproperties=fp,\n        color=\"tab:red\"\n    )\n\n# -------------------------------------------------\n# 5. 기준선 및 축 설정\n# -------------------------------------------------\nplt.axhline(0, color=\"gray\", linewidth=0.7)\nplt.axvline(0, color=\"gray\", linewidth=0.7)\nplt.gca().set_aspect(\"equal\", adjustable=\"datalim\")\n\n# -------------------------------------------------\n# 6. 라벨·제목\n# -------------------------------------------------\nplt.xlabel(\"Dimension 1\", fontproperties=fp)\nplt.ylabel(\"Dimension 2\", fontproperties=fp)\nplt.title(\"Correspondence Analysis: Region × Candidate\", fontproperties=fp)\n\n# -------------------------------------------------\n# 7. 범례\n# -------------------------------------------------\nplt.legend(prop=fp)\n\nplt.tight_layout()\nplt.show()\n대응분석 결과, 지역별 득표 구조와 후보 간의 관계는 제1차원에서 매우 뚜렷한 분화 양상을 보인다. 제1차원(Dimension 1)은 전체 관성의 약 97.4%를 설명하는 지배적인 축으로, 한국 선거에서 전통적으로 관찰되는 지역 기반 정치 성향의 분화를 거의 대부분 반영하는 차원으로 해석된다.\n대응분석에서 지역과 후보가 공간적으로 가까이 위치한다는 것은, 해당 지역에서 그 후보의 상대적 득표 비중이 높다는 의미이며, 원점에 가까울수록 전국 평균적인 득표 패턴을 따른다고 해석한다. 본 분석 결과는 한국 선거에서 잘 알려진 영남–호남의 지역적 정치 분화가 여전히 강하게 작동하고 있음을 통계적으로 확인해 주며, 동시에 수도권과 충청권이 상대적으로 중도적·혼합적 성격을 띠고 있음을 보여준다.\n\n\n\n\n\n제1차원(Dimension 1): 지역 정치 성향 분화 축\n제1차원에서 광주광역시, 전라남도, 전북특별자치도는 음의 방향에 강하게 위치하며, 더불어민주당 이재명 후보와 공간적으로 근접해 있다. 이는 해당 지역들이 다른 지역에 비해 이 후보에게 상대적으로 높은 득표 비중을 보이는 구조임을 의미한다. 즉, 이들 지역은 전국 평균 대비 민주당 후보 지지가 뚜렷한 지역으로 분류된다.\n반대로 대구광역시, 경상북도는 제1차원의 양의 방향 끝에 위치하며, 국민의힘 김문수 후보와 가장 가까운 위치를 차지한다. 이는 영남권 일부 지역에서 보수 성향 후보에 대한 상대적 지지 집중 현상이 강하게 나타남을 시사한다. 이러한 배치는 단순한 득표수 크기가 아니라, 각 지역 내부에서의 후보 간 상대적 득표 구조가 크게 다르다는 점을 시각적으로 보여준다.\n수도권 및 충청권(서울특별시, 인천광역시, 경기도, 충청남·북도)은 제1차원에서 원점 부근에 비교적 가깝게 분포하며, 특정 후보에 대한 극단적인 쏠림보다는 전국 평균과 유사한 득표 구조를 보이는 지역군으로 해석된다.\n제2차원(Dimension 2): 군소·대안 후보의 차별화 축\n제2차원(Dimension 2)은 약 2.6%의 관성을 설명하는 보조적 차원으로, 전체 구조를 좌우하지는 않지만 군소 정당 및 대안 후보의 미묘한 지지 패턴을 구분하는 역할을 한다. 이 차원에서 민주노동당 권영국 후보는 상단에 위치하여, 특정 지역(예: 일부 수도권·특별자치시)과의 상대적 연관성을 나타낸다.\n한편 무소속 송진호 후보는 하단부에 위치하여, 주요 정당 후보들과는 다른 독립적인 득표 구조를 갖는 후보임을 보여준다. 다만 이들의 좌표는 원점에 비교적 가까워, 전국적으로는 제한적인 영향력을 갖는 후보임을 시사한다.\n\n\n(3) 다차원 대응분석 (삼차원 이상)\n데이터\nimport numpy as np\nimport pandas as pd\n\nnp.random.seed(42)\nn = 400  # 응답자 수\n\ntea = np.random.choice([\"black\", \"green\", \"flavored\"], size=n, p=[0.45, 0.35, 0.20])\n\n# tea에 따라 how가 조금 달라지도록(연관 구조를 일부러 넣음)\nhow = []\nfor t in tea:\n    if t == \"black\":\n        how.append(np.random.choice([\"alone\", \"w/milk\", \"w/lemon\", \"other\"], p=[0.25, 0.55, 0.10, 0.10]))\n    elif t == \"green\":\n        how.append(np.random.choice([\"alone\", \"w/milk\", \"w/lemon\", \"other\"], p=[0.50, 0.10, 0.25, 0.15]))\n    else:  # flavored\n        how.append(np.random.choice([\"alone\", \"w/milk\", \"w/lemon\", \"other\"], p=[0.35, 0.15, 0.15, 0.35]))\n\nhow = np.array(how)\n\n# sugar는 how에 따라 약간 연관\nsugar = []\nfor h in how:\n    if h in [\"w/milk\", \"other\"]:\n        sugar.append(np.random.choice([\"yes\", \"no\"], p=[0.65, 0.35]))\n    else:\n        sugar.append(np.random.choice([\"yes\", \"no\"], p=[0.35, 0.65]))\nsugar = np.array(sugar)\n\n# where는 tea에 따라 약간 연관\nwhere = []\nfor t in tea:\n    if t == \"green\":\n        where.append(np.random.choice([\"supermarket\", \"shops\", \"both\"], p=[0.25, 0.45, 0.30]))\n    else:\n        where.append(np.random.choice([\"supermarket\", \"shops\", \"both\"], p=[0.45, 0.25, 0.30]))\nwhere = np.array(where)\n\ndf_tea = pd.DataFrame({\n    \"Tea\": tea,\n    \"How\": how,\n    \"Sugar\": sugar,\n    \"Where\": where\n})\n\nprint(df_tea.head())\nTea How Sugar Where  0 black alone no both  1 flavored other yes supermarket  2 green w/milk yes supermarket  3 green w/lemon no shops  4 black w/milk yes shops  (이하 생략)\n# !pip -q install prince\n\nimport prince\n\nmca = prince.MCA(\n    n_components=3,     # ✅ 3차원\n    n_iter=10,\n    copy=True,\n    check_input=True,\n    engine=\"sklearn\",\n    random_state=42\n)\n\nmca = mca.fit(df_tea)\n\n# 범주(레벨) 좌표\ncat_coords = mca.column_coordinates(df_tea)\nprint(cat_coords)\n\n#설명력(관성)도\neigs = mca.eigenvalues_\nratio = eigs / eigs.sum()\nprint(\"Eigenvalues:\", eigs)\nprint(\"Explained inertia (%):\", ratio * 100)\n0 1 2  Tea__black -0.746388 -0.508416 -0.096227  Tea__flavored -0.016041 1.606115 -0.009891  Tea__green 0.988373 -0.275884 0.132016  How__alone 0.713766 -0.195411 -0.591800  How__other -0.176395 1.660154 0.078443  How__w/lemon 0.938484 -0.275501 1.233502  How__w/milk -1.058829 -0.600962 0.117182  Sugar__no 0.565563 -0.110402 -0.194316  Sugar__yes -0.600546 0.117231 0.206335  Where__both -0.037239 -0.011676 -1.277173  Where__shops 0.483176 -0.233871 0.703561  Where__supermarket -0.437806 0.234898 0.309031\nEigenvalues: [0.43805111 0.34071936 0.26249185]  Explained inertia (%): [42.06923674 32.72176029 25.20900298]\nimport matplotlib.pyplot as plt\nimport matplotlib.font_manager as fm\nfrom mpl_toolkits.mplot3d import Axes3D  # noqa\n\n# 한글 폰트\nfont_path = \"/usr/share/fonts/truetype/nanum/NanumGothic.ttf\"\nfp = fm.FontProperties(fname=font_path)\n\n# 3차원 좌표 (Dim1, Dim2, Dim3)\ncoords3 = cat_coords.iloc[:, :3].copy()\n\n# 범주별 색상\ncategory_colors = {\n    \"Tea\": \"tab:blue\",\n    \"How\": \"tab:orange\",\n    \"Sugar\": \"tab:green\",\n    \"Where\": \"tab:red\"\n}\n\n# ---------------------------\n# Figure & 3D axis\n# ---------------------------\nfig = plt.figure(figsize=(10, 8))\nax = fig.add_subplot(111, projection=\"3d\")\n\n# ---------------------------\n# 범주별 plotting\n# ---------------------------\nfor cat, color in category_colors.items():\n    mask = coords3.index.str.startswith(cat + \"__\")\n    \n    ax.scatter(\n        coords3.loc[mask, 0],\n        coords3.loc[mask, 1],\n        coords3.loc[mask, 2],\n        color=color,\n        s=60,\n        label=cat\n    )\n    \n    for name, (x, y, z) in coords3.loc[mask].iterrows():\n        ax.text(\n            x + 0.03, y + 0.03, z + 0.03,\n            name.replace(cat + \"__\", \"\"),\n            fontsize=9,\n            fontproperties=fp,\n            color=color\n        )\n\n# ---------------------------\n# 축 / 제목\n# ---------------------------\nax.set_xlabel(\"Dim 1\", fontproperties=fp)\nax.set_ylabel(\"Dim 2\", fontproperties=fp)\nax.set_zlabel(\"Dim 3\", fontproperties=fp)\n\nax.set_title(\n    \"MCA (3D): Tea × How × Sugar × Where\\n(Category Map)\",\n    fontproperties=fp\n)\n\nax.legend(prop=fp)\nplt.tight_layout()\nplt.show()\n본 다차원 대응분석(MCA)은 차의 종류(Tea), 마시는 방식(How), 설탕 첨가 여부(Sugar), 구입 장소(Where)라는 네 개의 범주형 변수가 차 소비 행태에서 어떻게 결합되어 나타나는지를 탐색하기 위해 수행되었다. 분석 결과, 전체 변동은 세 개의 차원으로 효과적으로 요약되었으며, 각 차원은 서로 다른 소비 성향과 맥락을 반영하는 구조적 의미를 갖는다.\n제1차원 (Dim 1): 기본적 차 소비 성향 축\n제1차원은 차 소비의 가장 핵심적인 대비를 설명하는 축으로, 전통적이고 풍미 중심적인 소비 성향과 담백하고 개인적인 소비 성향을 구분한다. 이 차원의 음(–)의 방향에는 black, w/milk, sugar=yes가 함께 위치하여, 홍차를 우유와 설탕을 곁들여 마시는 전통적 소비 유형이 결집되어 있음을 보여준다. 반면 양(+)의 방향에는 green, alone, sugar=no가 가까이 배치되어, 녹차를 혼자서 무설탕으로 마시는 건강·담백 지향적 소비 행태가 뚜렷이 분리된다. 따라서 제1차원은 차 소비의 기본적인 취향 차이를 나타내는 축으로 해석할 수 있다.\n제2차원 (Dim 2): 소비 방식과 변형 정도의 차이\n제2차원은 차를 마시는 방식의 다양성과 소비 상황의 맥락을 반영하는 축이다. 이 차원의 양(+)의 방향에는 flavored, other, w/lemon과 같은 범주가 위치하여, 기본적인 차 소비에서 벗어나 레몬을 첨가하거나 향이 가미된 차를 선택하는 변형된 소비 형태가 강조된다. 반대로 음(–)의 방향에는 green과 alone이 상대적으로 가까이 나타나, 단순하고 일상적인 소비 방식이 대비된다. 이는 차 소비가 단순한 음료 섭취를 넘어 상황과 취향에 따라 다양한 조합으로 확장된다는 점을 보여준다.\n제3차원 (Dim 3): 구매 채널과 보조적 선택 맥락\n제3차원은 앞선 두 차원에서 충분히 설명되지 않았던 구매 장소와 보조적 선택 요인을 분리하는 역할을 한다. 이 차원에서 shops와 w/lemon은 상대적으로 양의 방향에 위치하여 전문 매장이나 특정 취향 중심의 선택과 연결되는 반면, both와 alone은 음의 방향에 가까워 일상적이고 혼합적인 구매 행태를 반영한다. 제3차원은 전체 구조에서 차지하는 비중은 상대적으로 작지만, 소비자의 미세한 선택 차이를 보완적으로 설명하는 의미를 갖는다.\n종합적 해석\n종합하면, 차 소비 행태는 단일 변수에 의해 설명되기보다는 차의 종류, 마시는 방식, 설탕 첨가 여부, 구매 장소가 상호 연관된 다차원적 구조를 형성하고 있다. 제1차원은 소비자의 기본적 취향을, 제2차원은 소비 방식의 다양성과 상황적 맥락을, 제3차원은 구매 채널과 보조적 선택 요인을 각각 반영한다. 특히 3차원 분석을 통해 2차원에서는 겹쳐 보이던 범주들이 입체적으로 분리되면서, 차 소비의 복합적인 선택 구조가 보다 명확하게 드러난다.\n\n\n\n\n\nimport matplotlib.pyplot as plt\nimport matplotlib.font_manager as fm\n\n# 한글 폰트 (이미 설치된 상태)\nfont_path = \"/usr/share/fonts/truetype/nanum/NanumGothic.ttf\"\nfp = fm.FontProperties(fname=font_path)\n\ncoords2 = cat_coords.iloc[:, :2].copy()\n\n# ----------------------------------\n# 1. 범주별 색상 지정\n# ----------------------------------\ncategory_colors = {\n    \"Tea\": \"tab:blue\",\n    \"How\": \"tab:orange\",\n    \"Sugar\": \"tab:green\",\n    \"Where\": \"tab:red\"\n}\n\n# ----------------------------------\n# 2. Figure\n# ----------------------------------\nplt.figure(figsize=(9, 7))\n\n# ----------------------------------\n# 3. 범주별로 나누어 plotting\n# ----------------------------------\nfor cat, color in category_colors.items():\n    mask = coords2.index.str.startswith(cat + \"__\")\n    \n    plt.scatter(\n        coords2.loc[mask, 0],\n        coords2.loc[mask, 1],\n        color=color,\n        label=cat,\n        s=60\n    )\n    \n    for name, (x, y) in coords2.loc[mask].iterrows():\n        plt.text(\n            x + 0.02, y + 0.02,\n            name.replace(cat + \"__\", \"\"),  # 접두어 제거\n            fontsize=10,\n            fontproperties=fp,\n            color=color\n        )\n\n# ----------------------------------\n# 4. 기준선 및 레이아웃\n# ----------------------------------\nplt.axhline(0, linewidth=0.7, color=\"gray\")\nplt.axvline(0, linewidth=0.7, color=\"gray\")\nplt.gca().set_aspect(\"equal\", adjustable=\"datalim\")\n\nplt.xlabel(\"Dim 1\", fontproperties=fp)\nplt.ylabel(\"Dim 2\", fontproperties=fp)\nplt.title(\"MCA (2D): Tea × How × Sugar × Where\", fontproperties=fp)\n\nplt.legend(prop=fp)\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "notes/mda/mda_cluster.html",
    "href": "notes/mda/mda_cluster.html",
    "title": "다변량분석 5. 군집분석(계층적, 비계층적방법)",
    "section": "",
    "text": "Chapter 1. 군집분석 개요\n\n1. 군집분석 개념\n\n(1) 군집분석 목적 및 개념\n군집분석의 기본 목적은 자료 속에 잠재되어 있는 구조(structure)를 탐색하고, 변수들 간의 유사성에 기반하여 자연스럽게 형성되는 집단(clusters)을 발견하는 데 있다. 즉, 사전에 집단 정보가 주어지지 않은 상황에서 개체들 사이의 거리 또는 유사성을 분석하여, 내재된 패턴을 파악하고 데이터가 어떠한 방식으로 스스로 그룹화되는지를 밝히는 것이 군집분석의 핵심 목표이다.\n개체 간의 간격을 직관적으로 파악하기 위해 시각적 도구를 활용할 수 있다. 변수의 개수에 따라 사용되는 대표 시각화 기법은 다음과 같다.\n측정 변수가 2개일 때 → 산점도(Scatter Plot)\n측정 변수가 3개일 때 → 버블 플롯(Bubble Plot) x축, y축, 버블 크기(또는 색)를 이용하여 3차원 정보를 2D에서 표현\n측정 변수가 4개 이상일 때 → 주성분 분석(PCA)을 이용한 산점도, 고차원의 변수를 2~3개의 주성분으로 차원 축소한 뒤 시각적으로 군집 패턴 탐색한다.\n\n\n(2) 군집분석과 판별분석 비교\n군집분석(clustering)과 판별분석(discriminant analysis)은 모두 개체들을 여러 집단으로 나누거나 구분하는 데 사용되는 다변량 분석 기법이지만, 출발점과 목적, 그리고 사용되는 정보의 종류가 서로 다르다.\n군집분석(Clustering)\n군집분석은 사전에 집단 정보가 전혀 주어지지 않은 상태에서, 개체들 간의 유사성(similarity) 또는 거리(distance)를 기준으로 집단을 구성하는 방법이다. 즉, 데이터가 관측될 때는 모든 개체가 동일한 상태로 취급되며, 분석 과정에서 서로 가까운 개체끼리 묶어 자연스럽게 군집이 형성되는 비지도 학습(unsupervised learning) 절차이다.\n군집 형성 방식은 크게 두 가지로 나눌 수 있다.\n계층적 군집분석(Hierarchical clustering): 개체 간 거리가 가까운 것끼리 순차적으로 묶어 나가며, 트리 형태(dendrogram)의 구조를 형성하는 방법이다.\n비계층적 군집분석(K-means 등): 군집의 개수 K를 미리 정해두고, 중심점(centroid)을 기준으로 개체를 반복적으로 재배정하여 군집을 형성하는 방법이다.\n판별분석(Discriminant Analysis)\n판별분석은 군집분석과 달리, 자료 수집 단계에서 이미 집단(label)이 주어진 상태에서 출발한다. 즉, 각 개체가 어느 모집단에 속하는지 정보가 주어져 있으며, 이 정보를 이용하여 집단 간 차이를 가장 잘 설명하는 판별함수를 추정하는 분석이다.\n판별분석의 목표는 두 가지이다.\n집단 간 차이 설명: 어떤 변수들이 집단 구분에 기여하는지 파악한다.\n새로운 개체의 집단 판별: 추정된 판별규칙(classification rule)을 이용하여 새로운 표본이 어느 집단에 속할지를 예측한다.\n마케팅 담당자가 고객 세분화를 위해 나이, 학력, 소득, 결혼 상태, 자녀 수, 직업 등의 정보를 수집하였다면, 이와 같은 다변량 정보를 이용하여 고객들을 서로 유사한 특성을 가진 집단으로 묶는 방법이 바로 군집분석이다. 군집분석은 개체 간의 유사성 또는 거리 측정을 바탕으로, 자료 속에 내재된 구조를 찾아 자연스럽게 형성되는 고객 그룹을 탐색하는 비지도 학습(unsupervised) 기법이다.\n군집분석의 중요한 특징은 분석 전에는 어떤 개체가 어떤 그룹에 속하는지, 그리고 그룹이 몇 개인지조차 알려져 있지 않다는 점이다. 다시 말해, 집단의 이름과 개수는 분석이 수행된 후에야 결정된다. 이러한 이유로 군집분석은 grouping, classification, 또는 unsupervised classification이라고 불리기도 한다.\n반면, 판별분석(Discriminant Analysis)은 상황이 정반대이다. 판별분석에서는 자료 수집 단계에서 이미 개체가 속한 집단 변수가 포함되어 있으며, 이 집단 정보에 기반하여 집단을 가장 잘 구분하는 판별함수(discriminant function)를 찾는 것이 목표이다. 즉, 판별분석은 지도 학습(supervised learning)의 형태로, 새로운 개체가 주어졌을 때 어느 집단에 속할지를 예측하는 데 초점을 둔다.\n\n\n(3) 군집분석 장단점\n군집분석은 데이터에 내재된 구조를 탐색하여 서로 유사한 개체들을 그룹화하는 비지도 학습 기법으로, 다양한 분석 상황에서 널리 활용된다. 그러나 유연성이 큰 만큼 결과의 안정성과 해석 측면에서 주의해야 할 점도 많다. 아래에서는 군집분석의 주요 장점과 단점을 정리한다.\n장점\n1. 탐색적(Exploratory) 분석 기법: 군집분석은 사전에 집단 정보를 요구하지 않기 때문에, 자료 내부 구조에 대한 가정 없이 의미 있는 패턴이나 자연스러운 그룹을 탐색해낼 수 있다. 따라서 데이터의 숨겨진 구조를 파악하거나 시장세분화, 유형화, 패턴 탐색 등 초기 탐색 단계에서 매우 유용하다.\n2. 다양한 형태의 데이터에 적용 가능: 유사성을 정의할 수만 있다면, 수치형 데이터뿐 아니라 범주형, 이진형, 심지어 텍스트 데이터에도 적용할 수 있다.\n자연어 처리에서는 단어/문서 간 유사도 기반 군집\n마케팅에서는 고객 행동 데이터 기반 세분화\n생물정보학에서는 유전자 발현 패턴 군집화\n즉, ”거리(distance)” 또는 ”유사도(similarity)“가 정의되기만 하면, 자료의 형태에 큰 제약이 없다.\n3. 적용 용이성 및 직관성: 자료의 사전 정보가 없기 때문에 누구나 쉽게 적용할 수 있으며, 기초적인 산점도나 거리 개념을 통해 직관적으로 이해할 수 있다. 계층적 군집이나 K-means 알고리즘은 구현도 간단하고 계산 속도도 빠르기 때문에 실무에서 활용도가 높다.\n단점\n1. 거리 및 가중치 정의에 민감: 군집분석의 핵심은 개체 간 유사성을 측정하는 거리인데, 거리의 정의가 달라지면 군집 결과도 크게 달라진다. 또한 변수마다 가중치를 어떻게 설정하느냐에 따라서도 결과가 크게 변한다.\n표준화 유무에 따라 K-means 결과가 완전히 달라짐\nManhattan vs Euclidean 거리 선택에 따라 군집 경계 변화 → 즉, 메타 결정이 분석 결과에 매우 민감하다.\n2. 군집 수 결정의 어려움: K-means처럼 군집 수를 지정해야 하는 알고리즘에서는 적절한 군집 수(K)를 결정하는 것이 쉽지 않다. Elbow method, Silhouette score, Gap statistic 등 다양한 기준이 존재하지만, ”정답”은 존재하지 않는다.\n3. 결과 해석의 어려움: 군집은 데이터로부터 자동적으로 형성되므로,형성된 군집이 무엇을 의미하는지는 분석자가 별도로 해석해야 한다.종종 군집이 다음과 같은 문제를 가질 수 있다.\n군집이 의미하는 특성이 명확하지 않음\n군집 간 차이를 설명하기 어려움\n외부 변수(인구학적 특성 등) 없이는 군집의 특성을 파악하기 어려움\n이러한 경우 주성분 분석(PCA)으로 차원을 줄여 군집 특성을 시각화하거나, 군집별 외부 특성 요약을 통해 군집의 의미를 재해석해야 한다.\n\n\n(4) 군집분석 활용\n시장세분화 (마케팅)\n\n\n\n\n\n시장 세분화는 구매 태도, 구매 성향, 매체 사용 습관, 인구사회학적 특성 등에서 서로 유사한 성향을 지닌 소비자 집단을 찾아내고, 이러한 내재된 군집을 바탕으로 시장을 의미 있는 하위 단위로 나누는 과정이다. 예를 들어 나이, 성별, 직업, 학력, 거주지와 같은 변수들을 이용하면, 소비자들의 행동적·사회적 특성에 따라 자연스럽게 차별적인 그룹이 형성된다. 실제 예시 표에서는 전체 소비자를 ’추종자’, ’EB’, ’기본 기능’과 같은 세 집단으로 나누었으며, 각 집단은 서로 다른 인구학적 프로필을 가진다는 점을 보여준다.\n시장 세분화의 핵심 목적은 소비자를 분류함으로써 그들의 행동 패턴을 보다 정교하게 이해하는 데 있다. 즉, 특정 특성(측정 변수)에 따라 소비자를 군집화함으로써, 표면적으로는 드러나지 않는 내재적 특성을 도출할 수 있으며, 이러한 특성은 소비자 행동 연구나 타깃 마케팅 전략을 수립하는 데 매우 유용하다.\n또한 시장 세분화는 잠재적 신제품 기회 탐색에도 기여한다. 전체 시장을 하나의 동질적 집단으로 보지 않고 경쟁 상황이나 기업 특성에 따라 하위 시장을 구분하면, 각 세분시장에서 보다 명확한 수요와 니즈를 파악할 수 있게 된다. 이는 기업이 전략적으로 우위를 확보할 수 있는 영역을 발견하는 데 도움을 준다.\n더 나아가, 세분화된 집단은 **표본추출 설계(sample design)**에도 활용될 수 있다. 조사 대상 지역이나 소비자를 구분해야 하는 상황에서 군집분석을 통해 서로 유사한 특성을 가진 대상을 묶어 계층을 구성하면, 이후 표본추출 과정이 단순해지고, 표집의 정확성이 향상된다. 즉, 세분화는 마케팅 전략뿐 아니라 조사 방법론 측면에서도 실질적인 장점을 제공한다.\n고객 세분화\n\n\n\n\n\n고객 세분화는 기업이 보유한 고객들을 하나의 동질적 집단으로 보지 않고, 고객별 가치·행동·특성의 차이에 따라 하위 집단으로 나누는 과정이다. 예를 들어, 고객의 수입(기업에 대한 매출 기여도)과 브랜드 인지도(브랜드와의 친밀도 또는 충성도)를 두 축으로 고려하면, 그림과 같이 고객들이 자연스럽게 두 개 정도의 군집으로 나뉜다. 이 중 기업의 수익에 더 크게 기여하면서 브랜드 인지도도 높은 고객군은 마케팅 공략 대상(target segment)으로 선정될 수 있다.\n고객 세분화의 목적은 단순한 분류가 아니라, 기업의 수익 창출과 전략적 의사결정을 보다 정교하게 수행하기 위한 기반을 마련하는 데 있다. 다음은 대표적인 활용 방향들이다.\n1. 고객 기여도 기반 세분화\n고객이 기업의 수익에 얼마나 기여하는지를 기준으로 고객을 구분함으로써, 고가 제품 위주 구매 고객, 반복 구매 고객, 단발성 구매 고객과 같은 전략적 차이를 명확히 파악할 수 있다.\n이러한 세분화는 CRM(Customer Relationship Management)의 핵심으로, 각 고객군에 맞는 차별적 관리 전략 수립을 가능하게 한다.\n2. 우수 고객의 특성 및 생활패턴 파악\n특정 고객군이 높은 가치를 창출하는 것으로 확인되면, 그들의 인구통계적 특성, 소비습관, 생활패턴을 파악하여 더 정교한 개별고객 맞춤 관리가 가능해진다. 연령대별 맞춤 프로모션, 관심사 기반 추천 서비스, 구매 이력 기반 개인화 마케팅\n3. 표적 집단 구성(Target Group Design)\n신상품 판촉, 교차판매(cross-selling), 업셀링(up-selling) 등의 목적을 위해 특정 특성을 공유하는 고객을 표적 집단으로 구성한다. 이는 군집분석을 활용하여 유사한 특성을 가진 고객을 묶고, 해당 그룹을 대상으로 맞춤 전략을 실행하는 과정으로 이어진다.\n\n\n(5) 유사성과 비유사성 개념\n다변량 자료에서 개체들을 분류하거나 군집화하려면, 먼저 개체 간의 관계를 정량적으로 측정해야 한다. 이때 사용되는 가장 기본적 개념이 유사성(similarity)과 비유사성(dissimilarity)이다.\n유사성은 두 개체가 얼마나 비슷한지를 나타내고, 비유사성은 얼마나 다른지를 나타낸다. 유사성은 값이 클수록 두 개체가 닮았음을 의미하며, 비유사성은 값이 클수록 서로 다름을 의미한다. 군집분석에서 개체들은 보통 비유사성(거리)이 작은 개체들끼리 묶여 군집을 형성한다. 따라서 유사성과 비유사성의 정의는 군집 결과에 직접적인 영향을 미치는 핵심 개념이다.\n1. 유사성(Similarity)\n유사성은 두 개체가 서로 얼마나 비슷한지를 나타내는 개념이다. 유사성이 높다는 것은 두 개체가 속성값이 서로 가까우며, 행동·특성·측정값이 유사한 패턴을 보인다는 의미이다. 일반적으로 유사성은 값이 클수록 두 개체가 더 유사하다는 방식으로 표현된다.\n\n상관계수(correlation): ±1에 가까울수록 유사\nCosine similarity: 1에 가까울수록 유사\n\n2. 비유사성(Dissimilarity, Distance)\n반대로 비유사성은 두 개체가 서로 얼마나 다른지를 나타낸다. 이는 종종 거리(distance)라는 개념을 사용하여 정의된다. 비유사성은 값이 클수록 두 개체가 서로 더 다르다는 의미이다. 대표적인 비유사성 지표는 다음과 같다.\n\n유클리드 거리(Euclidean distance)\n맨해튼 거리(Manhattan distance)\n마할라노비스 거리(Mahalanobis distance)\n\n거리 기반 군집분석에서는 두 점이 가까울수록(= 비유사성이 작을수록) 같은 군집에 속할 가능성이 높다.\n3. 유사성과 비유사성의 관계\n유사성과 비유사성은 서로 반대개념처럼 보이지만, 실제로는 동일한 정보를 서로 다른 방식으로 표현한 것이다.\n\n두 개체의 유사성이 높다는 것은 → 두 개체 간 비유사성이 낮다는 의미\n두 개체의 비유사성이 크다는 것은 → 두 개체가 서로 유사하지 않다는 의미\n\n즉, 분석 목적에 따라 유사성 행렬, 비유사성 행렬 중 하나를 사용하면 되고, 수학적 변환이 가능한 경우가 많다.\n\n\n\n\nChapter 2. 유사성/비유사성 척도\n\n1. 유사성 척도\n군집분석에서는 개체들 간의 유사성이 얼마나 큰지 또는 작은지를 기준으로 군집을 형성한다. 유사성은 두 개체가 패턴·특성·수치 등이 얼마나 닮았는가를 수치로 표현한 값이며, 일반적으로 값이 클수록 유사성이 높다.\n유사성 척도는 자료의 형태(연속형, 범주형, 이진형, 텍스트 등)에 따라 선택해야 한다. 유사성이 정의되면, 필요할 때 이를 거리(비유사성)로 변환하여 군집 알고리즘에서 사용할 수 있다.\n\n(1) 연속형 데이터에서 사용하는 유사성 척도\n피어슨 상관계수 (Pearson Correlation Similarity)\n\\[\\text{corr}(x,y) = r_{xy}\\]\n상관계수는 두 개체가 시간이나 조건의 변화에 따라 얼마나 비슷한 패턴을 보이는지를 측정하는 대표적인 유사성 척도이다. 상관계수 값이 +1에 가까울수록 두 개체의 변화 양상이 거의 동일함을 의미하므로, 패턴 유사성을 평가하는 데 매우 적합하다. 이때 상관계수는 데이터의 절대적인 크기(scale)가 아니라, 증가·감소의 방향과 형태를 비교한다는 특징이 있다.\n이러한 특성 때문에 상관계수는 고차원 데이터나 시계열 자료, 혹은 소비자들의 행동 패턴처럼 변화의 형태가 중요한 분석에서 널리 사용된다. 군집분석에서도 상관 기반 유사성을 활용하는 경우가 많으며, 실질적인 군집 알고리즘에서 요구하는 거리 형식으로 활용하기 위해 상관계수를 단순히 \\(d = 1 - r\\) 로 변환하여 사용할 수 있다. 상관계수가 높을수록 거리 값이 작아지므로, 자연스럽게 유사한 개체끼리 가까운 위치를 갖게 된다.\n예를 들어, 고객의 월별 구매 패턴(12개월 데이터)을 이용하여 고객 세분화를 수행한다고 하자. 고객 간 총 구매액의 크기가 다르더라도, 월별 증감 패턴이 비슷하다면 상관계수는 높은 값을 갖게 된다. 이 경우 상관 기반 거리를 이용한 군집분석은 유사한 구매 패턴을 가진 고객들을 효과적으로 하나의 그룹으로 묶어낼 수 있으며, 특히 마케팅에서 행동 기반 고객 세분화에 매우 유용하다.\n코사인 유사도 (Cosine Similarity)\n\\[\\text{cos}(x,y) = \\frac{x \\cdot y}{\\parallel x \\parallel \\parallel y \\parallel}\\]\n코사인 유사도는 두 벡터가 이루는 방향이 얼마나 유사한가를 측정하는 유사성 척도이다. 즉, 두 개체가 가진 값의 절대 크기보다는 패턴의 모양이 어느 정도 같은 방향을 향하고 있는지가 핵심 비교 기준이 된다. 코사인 유사도의 값은 일반적으로 0에서 1 사이(또는 상황에 따라 -1에서 1)를 가지며, 1에 가까울수록 두 벡터가 거의 동일한 방향을 가진다는 것을 의미한다. 코시인 유사도는 \\(d = 1 - cos\\) 거리로 군집분석에 사용된다.\n이 척도는 특히 텍스트 마이닝이나 문서 분석처럼 고차원 희소(sparse) 데이터가 등장하는 분야에서 뛰어난 성능을 보인다. 단어 빈도 기반의 벡터 표현에서는 각 문서가 매우 높은 차원의 공간을 갖게 되는데, 이 경우 크기보다는 ”어떤 단어 조합을 사용하는가”라는 방향 정보가 더 의미 있는 유사성 기준이 된다. 따라서 코사인 유사도는 문서 간 의미적 유사성을 평가하는 데 적합하며, 실제로 많은 텍스트 군집분석에서 핵심 유사성 척도로 사용된다.\n예를 들어 뉴스 기사를 TF-IDF 벡터로 변환하여 각 기사를 고차원 벡터로 표현한 뒤, 코사인 유사도를 통해 서로 얼마나 비슷한 주제나 표현을 갖는지를 평가할 수 있다. 이 유사도를 기반으로 군집분석을 수행하면, 주제가 비슷한 기사끼리 자연스럽게 하나의 군집을 형성하게 되며, 이는 뉴스 자동 분류, 정보 검색, 주제 모델링과 같은 다양한 응용에 활용될 수 있다.\n\n\n(2) 범주형·이진형 데이터의 유사성 척도\n자카드 유사도 (Jaccard Similarity)\n\\[\\text{J}(A,B) = \\frac{|A \\cap B|}{|A \\cup B|}\\]\n자카드 유사도는 두 개체가 가진 이진형(0/1) 변수 또는 집합 정보가 얼마나 겹치는지를 측정하는 유사성 척도이다. 특히 두 개체가 공통으로 1을 가지는 항목의 비율을 중심으로 유사성을 평가한다는 점이 특징이다. 예를 들어 구매 여부, 보유 여부, 특정 관심 항목의 포함 여부처럼 ”있다/없다”로 표현되는 자료에 적합하며, 고객의 구매내역이나 SNS 태그처럼 집합 기반의 특성을 지닌 데이터를 분석할 때 유용하다.\n군집분석에서는 자카드 유사도를 거리로 변환하여 사용하며, 그 방식은 매우 간단하다. \\(d = 1 - \\text{Jaccard}\\) 자카드 유사도가 높을수록 두 개체가 공유하는 항목이 많다는 의미이므로, 자카드 거리는 작아지고 두 개체는 군집에서 서로 가까운 위치로 판단된다.\n예를 들어, 고객이 구매한 상품 목록을 0/1 형태로 기록한 데이터가 있다면, 같은 상품을 자주 함께 구매한 고객들끼리는 공통의 ’1’ 항목이 많아지므로 자카드 유사도가 높아지고, 자연스럽게 같은 군집으로 묶인다. 마찬가지로 SNS 사용자들이 사용하는 해시태그(Hashtag)를 집합으로 표현하면, 비슷한 태그를 사용하는 이용자들끼리 서로 높은 자카드 유사도를 띠게 되며, 이는 관심사 기반 이용자 군집화에 매우 효과적으로 활용된다.\n단순 일치계수(SMC: Simple Matching Coefficient)\n\\[\\text{SMC} = \\frac{\\text{일치 항목 수}}{\\text{전체 항목 수}}\\]\n단순 일치계수(SMC)는 두 개체가 가진 이진형(0/1) 정보가 얼마나 일치하는지를 측정하는 유사성 척도이다. 자카드 계수가 ”두 개체가 공통으로 1을 가지는 경우”만을 강조하는 것과 달리, SMC는 0과 1을 동일하게 중요하게 다루며, 두 개체가 어떤 값이든 서로 같은 값을 가지면 유사하다고 판단한다. 즉, SMC는 ”두 개체가 전체 항목 중 얼마나 많은 정보를 동일하게 가지고 있는가?“를 비율로 나타내는 지표이다.\n이러한 특성 때문에 SMC는 Yes/No 형태의 이진 설문 데이터를 다룰 때 매우 유용하다. 예를 들어 흡연 여부, 결혼 여부, 자동차 보유 여부처럼 단순한 소유·상태·행동을 나타내는 항목이 여러 개 있을 때, 두 응답자가 같은 응답을 많이 할수록 SMC 값은 높아지고 서로 유사한 개체로 판단된다.\n실제 군집분석 예를 들면, 여러 개의 Yes/No 설문 문항을 이용하여 고객을 분류하고자 할 때 두 고객이 설문 항목에 대해 비슷한 응답 패턴을 보이면 SMC가 높아지며, 이를 바탕으로 **유사한 생활양식 또는 행동 특성을 공유하는 고객군(cluster)**을 형성할 수 있다.\nDice 계수 (Dice Coefficient)\n\\[\\text{DICE}(A,B) = \\frac{2|A \\cap B|}{|A| + |B|}\\]\nDice 계수는 두 집합이 얼마나 많은 공통 요소를 가지고 있는가를 측정하는 유사성 척도로, 자카드 유사도와 유사하지만 교집합의 크기를 더 크게 반영한다는 점이 특징이다.\nDice 계수는 문자열 비교(string matching), 이미지 특징 유사성 분석, 자연어처리(NLP)와 같이 데이터 요소 간의 공통 패턴이 핵심적인 분야에서 널리 사용된다. 예를 들어 DNA 염기서열처럼 특정 패턴의 일치 여부가 중요하게 작용하는 데이터에서는 동일하거나 유사한 부분서열이 많을수록 Dice 계수가 크게 나타나므로, 서열 간 유사성을 직관적으로 파악할 수 있다. 마찬가지로 소비자 검색어 집합을 비교할 때, 공통으로 검색한 키워드가 많을수록 높은 Dice 값을 갖기 때문에, 검색어 기반 소비자 군집화를 수행할 때 효과적인 척도로 활용된다.\n\n\n\n2. 비유사성 척도\n군집분석에서 개체들을 분류하는 핵심 기준은 개체들 간의 비유사성이 얼마나 큰가 또는 작은가이다. 비유사성은 두 개체가 서로 얼마나 다른지를 수치로 표현하는 척도이며, 값이 클수록 두 개체가 더 멀리 떨어져 있음, 즉 서로 다른 특성을 가지고 있음을 나타낸다. 이 비유사성은 군집분석 알고리즘(K-means, 계층적 군집 등)의 입력으로 직접 사용되며, 실제로 군집 형성은 비유사성이 작은 개체들이 서로 가까이 묶이는 방식으로 이루어진다.\n비유사성 척도는 자료의 형태가 연속형인지, 범주형인지, 이진형인지 등에 따라 다양한 정의가 존재하지만, 가장 기본적인 개념은 두 벡터의 차이를 수학적으로 측정하는 것이다. 아래는 대표적인 비유사성 척도들의 개념과 공식을 정리한 것이다.\n유클리드 거리(Euclidean Distance)\n\\[d(x,y) = \\sqrt{\\overset{p}{\\sum_{i = 1}}(x_{i} - y_{i})^{2}}\\]\n유클리드 거리는 군집분석에서 가장 널리 사용되는 비유사성 척도로, 두 점 사이의 직선거리를 계산하는 방식이다. 연속형 변수로 이루어진 자료에서 개체 간 차이를 측정하는 가장 기본적인 방법이며, 거리가 클수록 두 개체가 서로 크게 다르다는 의미를 갖는다.\n다만 유클리드 거리는 각 변수의 규모에 민감하기 때문에, 변수의 단위나 범위가 서로 다를 경우 표준화를 수행한 뒤 사용하는 것이 일반적이다. 이러한 특성 때문에 K-means 군집분석에서는 표준화된 연속형 변수에 대해 유클리드 거리를 기본 거리 척도로 사용하여, 서로 가깝고 유사한 개체들끼리 하나의 군집으로 묶어 나간다.\n맨해튼 거리(Manhattan Distance)\n\\[d(x,y) = \\overset{p}{\\sum_{i = 1}}|x_{i} - y_{i}|\\]\n맨해튼 거리는 두 점 사이의 차이를 좌표별 절댓값의 합으로 계산하는 비유사성 척도로, 마치 격자(grid) 위를 직선과 수직 방향으로만 이동해 도달하는 거리와 같다고 하여 이러한 이름이 붙었다. 이 거리 척도는 각 변수의 차이를 단순한 절댓값으로 누적하기 때문에, 유클리드 거리보다 이상치(outlier)에 덜 민감한 특징을 지닌다.\n또한 차원이 높아질수록 유클리드 거리는 거리의 분산이 커지고 계산의 안정성이 떨어지는 반면, 맨해튼 거리는 절댓값 기반이기 때문에 고차원 공간에서도 비교적 안정적으로 개체 간 차이를 반영할 수 있다. 이러한 특성 덕분에 고차원 데이터나 잡음이 많은 데이터 환경에서 유용하게 활용되는 거리 척도이다.\n마할라노비스 거리(Mahalanobis Distance)\n\\[d(x,y) = \\sqrt{(x - y)'S^{- 1}(x - y)}\\]\n마할라노비스 거리는 변수 간 상관구조를 고려하여 두 점 사이의 비유사성을 측정하는 척도로, 다변량 자료에서 개체 간 차이를 평가할 때 매우 유용하게 사용된다. 일반적인 유클리드 거리가 각 변수 축을 독립적이고 동일한 스케일로 가정하는 데 반해, 마할라노비스 거리는 변수들 사이의 공분산 구조를 반영하여 거리 값을 계산한다.\n이러한 방식은 데이터가 변수 간 상관을 가지고 있거나, 변수들의 분포 폭이 서로 다른 경우에 특히 중요한데, 마할라노비스 거리는 이러한 정보까지 함께 고려하여 축 방향이 서로 다른 타원형 분포에서도 개체 간 차이를 정확하게 비교할 수 있게 한다.\n이런 이유로 마할라노비스 거리는 다변량 통계 분야에서 자주 등장하며, 특히 판별분석(LDA)에서는 서로 다른 집단의 공분산 구조를 고려한 판별함수를 구성하는 데 사용되고, 이상치 탐지에서는 집단 중심에서 얼마나 벗어나 있는지를 판단하는 지표로 활용된다.\n자카드 거리(Jaccard Distance) – 이진형 자료용\n\\[d = 1 - \\text{Jaccard}(A,B) = 1 - \\frac{|A \\cap B|}{|A \\cup B|}\\]\n자카드 거리는 이진형 자료나 집합형 자료에서 두 개체가 서로 얼마나 다른지를 측정하는 대표적인 비유사성 척도이다. 자카드 유사도가 두 개체가 공통으로 가진 항목의 비율을 강조하는 반면, 자카드 거리는 그 반대 개념으로, 두 개체가 서로 다르게 가진 항목이 얼마나 많은지를 반영하여 비유사성을 평가한다.\n이 방식은 항목의 존재 여부(1) 또는 부재(0)가 중요한 데이터, 예를 들어 고객의 구매내역, SNS 태그, 취미나 관심사와 같이 특정 항목의 포함 여부로 표현되는 자료를 비교할 때 유용하다. 공통으로 가진 항목이 적고 각자 다른 항목을 많이 가지고 있을수록 자카드 거리는 커지며, 두 개체가 서로 상당히 다른 특성을 가진다고 판단한다. 이러한 특성 때문에 자카드 거리는 집합 기반 자료의 군집분석에서 널리 활용된다.\n해밍 거리(Hamming Distance)\n\\[d(x,y) = \\overset{p}{\\sum_{i = 1}}I(x_{i} \\neq y_{i})\\]\n해밍 거리는 문자열 또는 이진 벡터에서 서로 다른 위치가 몇 개나 되는지를 세어 비유사성을 측정하는 거리 척도이다. 두 문자열을 같은 길이로 놓고 비교했을 때, 같은 위치의 문자가 서로 다르면 그만큼 해밍 거리가 증가하며, 값이 클수록 두 문자열이 서로 더 다르다는 의미가 된다.\n이 거리 척도는 특히 비트열처럼 0과 1로 표현된 데이터 비교에 적합하며, 단일 항목의 차이가 직접적인 의미를 갖는 상황에서 널리 사용된다. 예를 들어 데이터베이스에서 키 값의 오류 탐지, 통신 시스템에서 전송 오류 검출, 유전학에서 DNA 서열의 변이 위치 파악 등에서 해밍 거리는 중요한 역할을 한다. 각 위치의 일치·불일치만으로 비유사성을 판단하므로 계산이 단순하고 직관적이라는 장점이 있다.\n\n\n\nChapter 3. 계층적 군집분석\n\n1. 군집화 과정\n계층적 군집분석(hierarchical clustering)에서는 개체들 간의 유사성이 가장 높은 순서, 또는 비유사성이 가장 낮은 순서대로 개체들을 차례로 묶어 가며 군집을 형성한다. 여러 연결 방식(linkage method) 중에서 single-linkage clustering은 두 군집 간 거리를 정의할 때, 각 군집에서 가장 가까운 두 점 간 거리(최소 거리)를 기준으로 군집을 병합하는 방법이다.\n이 방식은 자료 구조를 자연스럽게 따라가며 가장 가까운 개체 또는 군집끼리 먼저 묶어 나가는 특성을 가지기 때문에, 전체 데이터 구조를 빠르게 파악할 수 있으며 계산 효율성이 높다는 장점이 있다. 이러한 이유로 single-linkage 방법은 효율성이 높고 구현이 간단하여, 계층적 군집 방법 중 가장 기본적이면서도 널리 사용되는 방식으로 알려져 있다.\n\n(1) 군집 간 거리 측정방법\nSingle-linkage 방식을 실제 군집화 절차로 구현한 방법 중 하나가 Neighbor Method이다. Neighbor Method는 개체들 간의 거리 행렬을 바탕으로, 가장 가까운 두 개체부터 순차적으로 묶어 가는 방식으로 군집 구조를 형성한다.\n첫째, 처음에는 개체의 수(n)만큼의 군집이 있다. 예를 들어 개체 6개가 있고 다음은 각 개체 간 Euclidean 거리(유사성)를 계산한 표이다. 처음에는 군집은 6개이다. 개체 (1, 2) 간 거리(유사성)은 0.1, (2, 3) 개체 유사성은 0.4이다. (2, 3) 개체 간 거리는 (3, 2) 개체 거리와 동일하므로 대각 행렬 형태이다.\n\n\n\n\n\n둘째, 유사성이 가장 가까운(거리가 가장 가까운) 개체를 군집으로 묶는다. 예제에서는 (1, 2)가 묶인다. 이제 (1, 2)와 3, 4간 거리를 어떻게 정의할 것인가?\n\n\n\n\n\n계층적 군집분석(hierarchical clustering)에서는 군집을 병합할 때 어떤 기준으로 두 군집 간 거리를 정의하느냐가 매우 중요하다. 개체 간 거리가 주어졌다고 하더라도, ”군집과 군집의 거리”는 여러 방식으로 정의할 수 있으며, 이러한 기준에 따라 군집 형성 방식과 결과가 달라진다. 대표적인 linkage 방식은 다음 네 가지이다.\nNearest Neighbor (Single Linkage): 두 군집에 속한 개체들 중 가장 가까운 두 개체 사이의 거리를 군집 간 거리로 정의한다. \\(D(A,B) = \\min_{i \\in A,j \\in B}d(i,j)\\)\nFurthest Neighbor (Complete Linkage): 두 군집에 속한 개체들 중 가장 멀리 떨어진 두 개체 사이의 거리를 군집 간 거리로 정의한다. \\(D(A,B) = \\max_{i \\in A,j \\in B}d(i,j)\\)\nCentroid Linkage (중심 연결법): 군집의 중심점(centroid) 간의 거리를 군집 간 거리로 사용한다. \\(D(A,B) = d({\\overline{x}}_{A},{\\overline{x}}_{B})\\)\nAverage Linkage (Group Average Method): 한 군집의 모든 개체와 다른 군집의 모든 개체 간의 거리 평균을 군집 간 거리로 정의한다. \\(D(A,B) = \\frac{1}{|A||B|}\\sum_{i \\in A}\\sum_{j \\in B}d(i,j)\\)\n\n\n\n\n\n다음은 Nearest neighbor 방법에 의해 개체를 군집화 하는 과정이다. 1과 3의 거리는 0.7, 2와 3의 거리는 0.4이므로 (1, 2)와 3의 거리는 0.4가 된다. 1과 4의 거리는 0.2이고 2와 4의 거리는 0.6이므로 작은 거리 0.2가 (1, 2)와 4의 거리이다.\n\n\n\n\n\n(1, 2)의 거리는 0.4이고 3과 4의 거리는 0.3이므로 (1, 2, 4)와 3의 거리는 0.3이 된다.\n\n\n\n\n\n\n\n(2) 군집 간 거리 측정방법 선택\n계층적 군집분석에서는 Nearest(단일 연결), Furthest(완전 연결), Centroid, Average, Ward 등 여러 방식으로 군집 간 거리를 정의할 수 있다. 그러나 어느 한 방법이 항상 우수하다고 말할 수는 없으며, 각 방법은 서로 다른 군집 형태를 만들어 내기 때문에 분석 목적과 자료의 특성을 고려해 선택해야 한다.\nNearest 방식(single linkage)은 군집 간 가장 가까운 두 점의 거리를 기준으로 병합하기 때문에 서로 가까운 개체들이 빠르게 묶여 군집의 수가 적게 형성되는 경향이 있고, 그 과정에서 사슬현상(chain effect)이 발생하여 긴 형태의 군집이 만들어지기 쉽다. 반대로 Furthest 방식(complete linkage)은 군집 간 가장 멀리 떨어진 개체의 거리를 기준으로 두 군집의 분리성을 크게 유지하려 하기 때문에 상대적으로 작은 군집이 많이 남는 경향이 있다.\nCentroid 방식은 각 군집의 중심점 간 거리를 이용하여 효율적으로 병합을 진행하지만, 군집이 병합되는 과정에서 거리 계산이 역전되는 문제가 발생해 다소 불안정할 수 있다. Average 방식(average linkage)은 두 군집에 속한 모든 개체 쌍의 평균 거리를 사용하여 Single과 Complete linkage의 극단성을 완화한 방법으로, 가장 안정적이고 실무에서 널리 활용되는 방식이다.\n또한 Ward 방법은 군집 병합 시 군집 내 제곱합(variance)의 증가를 최소화하는 방향으로 병합을 진행하여 가장 응집력 있고 통계적으로 해석 가능한 군집을 만들어 준다. 특히 연속형 변수로 구성된 자료에서는 매우 좋은 성능을 보이는 방식으로 알려져 있다.\n결국 각 방법에는 고유한 장단점이 있으므로, 실제 분석에서는 2~3개의 linkage 방법을 함께 적용해 보고, 자료 구조를 가장 잘 반영하며 해석 가능성이 높은 결과를 선택하는 것이 바람직하다. 이 중 Average linkage는 안정성과 실용성이 뛰어나 가장 널리 사용되는 방식이다.\n\n\n\n2. 군집 개수 결정\n군집의 개수를 결정하는 방법은 매우 다양하며, 단일 기준에 의존하기보다 여러 방법을 함께 사용하여 일관된 군집 수를 선택하는 것이 바람직하다. 계층적 군집에서는 덴드로그램, Mojena’s Rule, CCC가 주로 사용된다.\n\n(1) 계층적 나무 다이어그램 (Dendrogram)\n덴드로그램(dendrogram)은 개체 간의 유사성이 높은 순서대로 개체들이 어떻게 묶여 가는지를 단계적으로 보여주는 트리 구조의 그림이다. 덴드로그램에서 각 가지의 높이는 개체 또는 군집 간의 상대적 거리를 의미하며, 거리가 작을수록 서로 유사한 개체들이 먼저 결합된다.\n예를 들어, 그림에서는 가장 먼저 E와 F가 서로 가장 가까운 개체로 판단되어 하나의 군집으로 묶이고, 그다음 A와 B가 결합된다. 이후 (E, F) 군집이 D와 결합하고, 다시 C와 묶여 점차 큰 군집이 형성된다. 이러한 병합 과정을 거쳐 결국 모든 개체가 하나의 군집으로 연결되는 것이 계층적 군집분석의 흐름이다.\n\n\n\n\n\n덴드로그램을 활용하여 군집 수를 결정하는 방법은 매우 직관적이다. 그림에서 나무 형태의 수직선(가지)을 살펴보면, 높이의 급격한 증가가 나타나는 지점에서 트리를 자르는 것이 일반적인 기준이다. 예를 들어 군집의 개수를 2개로 설정하면, 덴드로그램에서 큰 분기점 위에서 수평으로 선을 그어보았을 때, (A, B)와 (C, D, E, F)처럼 자연스럽게 두 개의 큰 군집이 형성된다.\n이처럼 덴드로그램은 계층적 군집 형성 과정을 시각적으로 보여주기 때문에, 군집 내의 응집력과 군집 간 분리도를 동시에 고려하여 군집의 적절한 개수를 판단할 수 있는 유용한 도구이다. 분석자는 덴드로그램을 통해 어디에서 잘라야 가장 의미 있는 그룹이 형성되는가를 판단하여, 군집의 수를 결정할 수 있다.\n\n\n(2) Mojena’s Rule\n계층적 군집분석에서는 개체들이 병합되는 순서를 덴드로그램으로 표현하지만, 어디에서 트리를 잘라 군집 수를 정할지 판단이 모호할 수 있다. Mojena’s Rule은 이러한 문제를 해결하기 위해 고안된 고전적 기준으로, 덴드로그램에서 나타나는 병합 거리의 분포를 통계적으로 해석하여 cut height를 수치적으로 결정하는 방법이다.\nMojena는 군집 병합 과정에서 계산되는 병합 거리 \\(h_{1},h_{2},\\ldots,h_{n - 1}\\)을 하나의 데이터로 간주하고, 이 값들이 어떤 분포적 특성을 갖는지를 이용하여 ”비정상적으로 큰 병합 거리”가 나타나는 지점을 군집 경계로 판단할 수 있다고 보았다. 즉, 병합 높이가 갑자기 커지는 지점은 개체 간 유사성이 떨어지는 두 큰 군집이 합쳐지는 순간이므로, 이 이전 단계가 가장 자연스러운 군집 구조를 반영한다는 원리이다.\n이를 위해 Mojena는 다음과 같은 기준을 제안한다.\n\\(h_{c} &gt; \\overline{h} + ks_{h}\\), 여기서 \\(h_{c}\\)는 cut height 후보(어느 병합 단계의 height인지), \\(\\overline{h}\\)는 전체 병합 거리의 평균, \\(s_{h}\\)는 병합 거리의 표준편차, 그리고 \\(k\\)는 경험적 상수(보통 2.75 ~ 3.50 범위에서 사용됨)이다.\n즉, 덴드로그램에서의 병합 거리 중에서 평균보다 k배 이상 큰 병합 거리가 나타나는 지점이 자연스러운 분기점이 되며, 해당 height 위에서 트리를 자르면 적절한 군집 수가 결정된다. 이 기준은 단순히 그림을 보고 주관적으로 판단하는 것이 아니라, 병합 거리의 분포를 정량적으로 분석하여 군집 경계를 찾는다는 점에서 의미가 크다.\nMojena’s Rule은 계층적 군집분석이 처음 제안된 시기부터 꾸준히 사용되어 왔으며, 특히 덴드로그램이 복잡하거나 시각적으로 명확한 elbow가 보이지 않을 때 유용하다. 다만, 상수 k 의 선택에 어느 정도 경험적 요소가 포함되어 있으며, 자료 구조에 따라 최적의 k가 다를 수 있다는 점은 고려해야 한다. 그럼에도 불구하고, 덴드로그램 cut height를 명확한 수치 규칙으로 제시한다는 점에서 많은 교과서와 소프트웨어에서 소개되는 전통적인 기준이다.\n\n\n(3) Cubic Clustering Criterion (CCC)\nCubic Clustering Criterion(CCC)은 SAS에서 개발된 군집 타당성 지표로, 특히 계층적 군집분석에서 군집 수를 결정할 때 널리 사용되는 고전적 기준 중 하나이다. CCC는 특정 군집 수 K에 대해 실제 데이터에서 얻어진 군집화 구조가, 동일한 조건에서 생성된 무작위 데이터의 군집 구조보다 얼마나 더 명확하게 분리되는지를 수치화한 값이다. 다시 말해, 군집이 실제로 존재하는지, 혹은 단순히 우연히 생긴 구조인지 비교하는 지표라고 할 수 있다.\nCCC의 기본 원리는 다음과 같다.\n군집이 ”정말 존재한다면”, 동일한 차원의 난수 데이터를 군집화했을 때보다 집단 간 분리도가 훨씬 더 커야 한다. 따라서 CCC는 실제 군집 구조와 난수 데이터의 군집 분리도를 비교하여, 그 차이가 큰 경우에 높은 값을 갖는다. 일반적으로 CCC 값이 2 이상이거나 뚜렷한 피크를 보이는 군집 수는 실제 데이터에서 의미 있는 군집 구조가 존재함을 시사한다.\n계층적 군집분석에서 CCC는 각 병합 단계별로 계산되며, CCC 값을 K에 따라 플롯하면 데이터에서 군집 구조가 가장 또렷하게 드러나는 지점에서 샤프한 피크가 나타난다. 이 피크에 해당하는 K가 적절한 군집 수가 된다. 덴드로그램만으로 판단하기 모호한 경우, CCC는 수치적 기준을 제공한다는 점에서 실무와 연구에서 많이 활용된다.\n\n\n(4) Pseudo F / Pseudo t² 통계량\n계층적 군집분석에서는 각 병합 단계에서 군집의 응집력과 분리도가 어떻게 변하는지를 평가하여 적절한 군집 수를 결정할 수 있다. SAS의 PROC CLUSTER에서 제공하는 Pseudo F와 Pseudo t² 통계량은 바로 이러한 군집 구조 변화를 수치화한 지표로, 계층적 군집분석에서만 사용되는 특수한 타당성 기준이다.\nPseudo F는 K개 군집이 형성된 상태에서, 군집 간 분산과 군집 내 분산의 비율을 측정한다. 그 해석은 ANOVA의 F 비율과 유사하며, 군집이 잘 분리되어 있을수록 군집 간 분산이 커지고 군집 내 분산이 작아지므로 Pseudo F 값이 커진다.\n따라서, Pseudo F가 큰 K가 잘 분리된 군집 구조를 의미한다. K를 바꿔가며 계산했을 때 Pseudo F가 높게 나타나는 지점에서 군집 수를 선택할 수 있다.\nPseudo F는 군집 전체의 분리도를, Pseudo t²는 각 병합 단계의 안정성과 응집력 변화를 평가하는 지표이다. Pseudo t² 값이 급격히 증가하거나 감소하는 지점은 군집 경계가 바뀌는 중요한 단계로, 해당 지점을 중심으로 군집 수를 결정할 수 있다. 이 두 지표는 계층적 군집분석의 군집 수 판단을 수치적으로 보완하는 데 널리 사용된다.\n\n\n\n3. 사례 실습\n\n(1) 메타 정보\n이 데이터는 미국의 50개 주를 대상으로 수집된 자료로, 총 50개의 관측치로 이루어져 있다. 각 주에 대해 네 가지 연속형 변수가 기록되어 있으며, 이는 모두 강력범죄 발생률과 지역의 도시화 수준을 나타내는 지표들로 구성된다. 다시 말해, 살인·폭행·강간과 같은 범죄 발생률, 그리고 주별 도시 인구 비중을 함께 포함하고 있어, 범죄 패턴과 도시화 정도의 관계를 다변량적으로 살펴보기에 적합한 자료이다.\n\n\n\n\n\n\n\n\n변수명\n내용\n단위\n\n\nMurder\n살인 사건 발생률\n인구 10만 명당 발생 건수\n\n\nAssault\n폭행 사건 발생률\n인구 10만 명당 발생 건수\n\n\nUrbanPop\n도시 거주 인구 비율\n%\n\n\nRape\n강간 사건 발생률\n인구 10만 명당 발생 건수\n\n\n\nimport pandas as pd\nimport statsmodels.api as sm\n\ndf = sm.datasets.get_rdataset(\"USArrests\", package=\"datasets\").data\n\n# 인덱스를 컬럼으로 빼서 'State'라는 변수로 사용\ndf = df.reset_index().rename(columns={\"rownames\": \"State\"})\ndf.info()\n\n\n(1) 계층적 군집분석 및 군집 개수 결정\nimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy.cluster.hierarchy import linkage, dendrogram, fcluster\nimport matplotlib.pyplot as plt\n\n# ----------------------------------------\n# 1. 데이터 로드 & 표준화\n# ----------------------------------------\nraw = sm.datasets.get_rdataset(\"USArrests\", package=\"datasets\").data\n\n# State 이름 처리 불필요 (이미 인덱스가 주 이름)\nraw.index.name = \"State\"   # 선택 사항: 인덱스 이름만 붙이고 싶을 때\n\nX = raw.values\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\nn, p = X_scaled.shape\n\n# 전체 TSS (나중에 Pseudo F 계산에 사용)\nTSS = ((X_scaled - X_scaled.mean(axis=0)) ** 2).sum()\n\n# ----------------------------------------\n# 2. 계층적 군집 (Ward)\n# ----------------------------------------\nZ = linkage(X_scaled, method='ward')\n\n# ----------------------------------------\n# 3. Pseudo F (Calinski–Harabasz) 계산\n# ----------------------------------------\ndef compute_wss(X, labels):\n    wss = 0.0\n    for g in np.unique(labels):\n        Xg = X[labels == g]\n        mu = Xg.mean(axis=0)\n        wss += ((Xg - mu) ** 2).sum()\n    return wss\n\ndef pseudo_f_table(Z, X, max_k=15):\n    n = X.shape[0]\n    results = []\n    TSS = ((X - X.mean(axis=0)) ** 2).sum()\n    for k in range(2, min(max_k, n)):\n        labels = fcluster(Z, t=k, criterion='maxclust')\n        WSS = compute_wss(X, labels)\n        BSS = TSS - WSS\n        pf = (BSS / (k - 1)) / (WSS / (n - k))\n        results.append((k, pf))\n    return pd.DataFrame(results, columns=[\"k\", \"pseudo_F\"])\n\npf_df = pseudo_f_table(Z, X_scaled, max_k=10)\nprint(\"\\nCalinski–Harabasz Pseudo F:\")\nprint(pf_df)\n\n# ----------------------------------------\n# 4. Mojena’s Rule\n# ----------------------------------------\ndef mojena_rule(Z, c=2.75):\n    heights = Z[:, 2]              # 병합 높이\n    h_mean = heights.mean()\n    h_std = heights.std(ddof=1)\n    cutoff = h_mean + c * h_std\n\n    n = Z.shape[0] + 1  # 관측치 개수\n    for i, h in enumerate(heights):\n        if h &gt; cutoff:\n            # i번째 병합 이후 군집 수 = n - (i + 1)\n            k = n - (i + 1)\n            return {\n                \"step\": i + 1,\n                \"k\": k,\n                \"height\": h,\n                \"cutoff\": cutoff\n            }\n    return None\n\nmojena_res = mojena_rule(Z, c=2.75)\nprint(\"\\nMojena's Rule (c=2.75) 결과:\")\nprint(mojena_res)\n\n# ----------------------------------------\n# 5. Duda–Hart Je(2)/Je(1) 및 pseudo T²\n# ----------------------------------------\ndef duda_hart_pseudo_t2(Z, X):\n    n = X.shape[0]\n    clusters = {i: np.array([i]) for i in range(n)}\n    wss_dict = {}\n\n    def cluster_wss(idx):\n        if idx in wss_dict:\n            return wss_dict[idx]\n        members = clusters[idx]\n        Xg = X[members]\n        mu = Xg.mean(axis=0)\n        wss = ((Xg - mu) ** 2).sum()\n        wss_dict[idx] = wss\n        return wss\n\n    results = []\n\n    for step, row in enumerate(Z):\n        a, b, height, n_members = row\n        a, b = int(a), int(b)\n        new_id = n + step\n\n        # 새 클러스터 구성\n        members = np.concatenate([clusters[a], clusters[b]])\n        clusters[new_id] = members\n\n        # Je(2) = 합치기 전 두 군집의 WSS 합\n        wss_a = cluster_wss(a)\n        wss_b = cluster_wss(b)\n        Je2 = wss_a + wss_b\n\n        # Je(1) = 합친 후 WSS\n        Xg = X[members]\n        mu = Xg.mean(axis=0)\n        Je1 = ((Xg - mu) ** 2).sum()\n        wss_dict[new_id] = Je1\n\n        ratio = Je2 / Je1 if Je1 &gt; 0 else np.nan\n\n        N1 = len(clusters[a])\n        N2 = len(clusters[b])\n        if N1 + N2 - 2 &gt; 0 and Je1 &gt; 0:\n            T2 = (ratio - 1.0) * (N1 + N2 - 2)\n        else:\n            T2 = np.nan\n\n        num_clusters = n - (step + 1)\n\n        results.append({\n            \"step\": step + 1,\n            \"num_clusters\": num_clusters,\n            \"height\": height,\n            \"Je2_over_Je1\": ratio,\n            \"pseudo_T2\": T2\n        })\n    return pd.DataFrame(results)\n\ndh_df = duda_hart_pseudo_t2(Z, X_scaled)\nprint(\"\\nDuda–Hart Je(2)/Je(1) 및 pseudo T² (상위 10단계):\")\nprint(dh_df.head(10))\n\n# ----------------------------------------\n# 6. 덴드로그램 (Mojena 기준선 포함)\n# ----------------------------------------\nplt.figure(figsize=(12, 6))\ndendrogram(\n    Z,\n    labels=raw.index.tolist(),\n    leaf_rotation=90\n)\nplt.title(\"USArrests - Ward Hierarchical Clustering\")\n\n# Mojena 기준 높이에 수평선\nif mojena_res is not None:\n    plt.axhline(\n        y=mojena_res[\"height\"],\n        linestyle=\"--\",\n        linewidth=1,\n        label=f\"Mojena cutoff (k={mojena_res['k']})\"\n    )\n    plt.legend()\n\nplt.tight_layout()\nplt.show()\nCalinski–Harabasz Pseudo F 해석\nPseudo F는 군집 간 분산(BSS)이 크고 군집 내 분산(WSS)이 작을수록 큰 값을 가지므로 일반적으로 피크(최댓값) 근처의 k가 후보가 된다.\n\nk = 2일 때 최댓값 → 가장 뚜렷한 분리.\nk = 4는 두 번째로 높은 피크 → 4군집 구조도 상대적으로 해석 가능.\n\nCalinski–Harabasz Pseudo F:  k pseudo_F  0 2 41.894858  1 3 34.936527  2 4 36.533996  3 5 32.009982  4 6 29.648747  5 7 28.210572  6 8 27.475667  7 9 27.648135\nMojena's Rule\nMojena의 컷오프가 약 7.93인데, 실제 Ward 병합의 최종 병합 높이(13.65)가 이를 초과하여 마지막 병합에서 기준을 충족한다. 즉, ”더 이상 분리할 의미 있는 군집 구조가 없다”는 결론을 낸다. 하지만 이것은 Mojena의 한계로, Ward 거리의 분포 특성에 따라 매우 보수적으로 동작한다. 실무적으로는 이 값을 그대로 받아들이지 않고, 덴드로그램 형태와 다른 지표(Pseudo F 등)와 함께 고려한다.\nMojena's Rule (c=2.75) 결과:\n{'step': 49, 'k': 1, 'height': np.float64(13.653466603337856), 'cutoff': np.float64(7.930106044456396)}\nDuda–Hart Je(2)/Je(1) 및 pseudo T² 해석\nJe2/Je1 = 0 은 “합치기 전 두 군집 WSS가 0이었다”는 뜻 → 실제론 불가능\npseudo T²가 음수 → 수학적으로는 Je2/Je1 &lt; 1 에서만 발생하는데, Je2 &lt; Je1이라는 의미이며 비정상적 결과이다.\n이 데이터에서는 Duda–Hart 기준이 안정적이지 않게 동작한다.\nDuda–Hart Je(2)/Je(1) 및 pseudo T² (상위 10단계):\nstep num_clusters height Je2_over_Je1 pseudo_T2  0 1 49 0.207944 0.000000 NaN  1 2 48 0.353774 0.000000 NaN  2 3 47 0.433124 0.000000 NaN  3 4 46 0.499099 0.000000 NaN  4 5 45 0.540825 0.000000 NaN  5 6 44 0.559148 0.375011 -0.624989  6 7 43 0.599560 0.000000 NaN  7 8 42 0.662585 0.089663 -0.910337  8 9 41 0.710977 0.000000 NaN  9 10 40 0.718098 0.000000 NaN\n덴드로그램\n덴드로그램 최상단의 두 큰 가지가 매우 높은 높이(약 13.5)에서 합쳐지고 있다. 오른쪽(초록색 큰 가지), 왼쪽(주황색 큰 가지)\n즉, Ward 거리 기준으로 두 초대형 블록 간의 이질성이 가장 큼을 의미한다. → 이것은 Pseudo F가 k=2에서 최대값을 갖는 이유와 정확히 일치한다. 군집을 2개로 나누면 두 그룹 간 분리가 가장 크다는 뜻입니다.\n\n\n\n\n\nMojena 기준선(cutoff ≈ 7.93)이 덴드로그램에 표시된 수평선이다. 실제로 데이터에서는 마지막 병합 높이(13.6)가 이 cutoff를 넘는 유일한 지점 → 그래서 Mojena 알고리즘은 ”이 시점에서 처음 기준선을 넘었다”고 판단하여 k = 1을 반환한다. (군집개수 결정 활용 불가)\nimport numpy as np\nimport pandas as pd\nfrom scipy.cluster.hierarchy import linkage, fcluster\nfrom sklearn.preprocessing import StandardScaler\n\n# ---- 데이터 준비 ----\nimport statsmodels.api as sm\nraw = sm.datasets.get_rdataset(\"USArrests\", \"datasets\").data\nX = StandardScaler().fit_transform(raw.values)\n\nn, p = X.shape\n\n# 전체 TSS\nTSS = ((X - X.mean(axis=0)) ** 2).sum()\n\n# ---- 군집 결과 ----\nZ = linkage(X, method='ward')\n\n# ---- WSS 계산 함수 ----\ndef compute_wss(X, labels):\n    wss = 0.0\n    for g in np.unique(labels):\n        Xg = X[labels == g]\n        mu = Xg.mean(axis=0)\n        wss += ((Xg - mu) ** 2).sum()\n    return wss\n\n# ---- CCC 계산 ----\ndef compute_ccc(Z, X, max_k=10):\n    n, p = X.shape\n    TSS = ((X - X.mean(axis=0)) ** 2).sum()\n\n    results = []\n\n    for k in range(2, max_k + 1):\n        labels = fcluster(Z, k, criterion=\"maxclust\")\n\n        WSS = compute_wss(X, labels)\n        BSS = TSS - WSS\n\n        R2 = BSS / TSS\n\n        # Expected R2 under null (SAS 방식)\n        E_R2 = 1 - ((n - k) / (n - 1))**(2 / p)\n\n        # Variance approximation\n        V_R2 = (2 * E_R2**2) / (n - 1)\n\n        CCC = (R2 - E_R2) / np.sqrt(V_R2)\n\n        results.append([k, R2, E_R2, CCC])\n\n    return pd.DataFrame(results, columns=[\"k\", \"R2\", \"E(R2)\", \"CCC\"])\n\nccc_df = compute_ccc(Z, X, max_k=10)\nccc_df\nCCC 해석\nCCC ≥ 2 → 군집 구조 강함 → CCC가 높을수록 더 뚜렷한 군집 구조이므로 ”피크”(peak)를 찾는 것이 핵심이다.\n\nCCC가 갑자기 크게 증가하면 ”새로운 군집 구조 발견”\nCCC가 감소 or 완만해지면 ”추가 군집은 의미 없음”\n\nk=2에서 CCC = 219.96: 매우 높은 값으로 ”두 개로 나누면 군집 구조가 극도로 강함”, 보통 4~10 정도도 큰 값인데, 200대는 압도적이다.\nk=3에서 CCC = 138.56, 여전히 매우 큼, k=2보다는 낮지만 여전히 군집 구조 존재\nk=4에서 CCC = 107.17, 여전히 높지만 감소 추세 시작된다.\nk R2 E(R2) CCC  0 2 0.466043 0.010257 219.956775  1 3 0.597854 0.020621 138.557379  2 4 0.704374 0.031096 107.170943  3 5 0.739944 0.041685 82.912183  4 6 0.771124 0.052393 67.901102  5 7 0.797422 0.063223 57.480555  6 8 0.820765 0.074180 49.816855  7 9 0.843622 0.085268 44.021931  8 10 0.862420 0.096492 39.289737\n\n\n(2) 군집 결과 활용\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\nfrom scipy.cluster.hierarchy import fcluster\n\n# 이미 위에서 사용한 raw, X, Z를 그대로 활용한다고 가정합니다.\n# raw: USArrests 데이터 (index = 주 이름)\n# X  : 표준화된 데이터 (X = StandardScaler().fit_transform(raw.values))\n# Z  : linkage(X, method='ward')\n\n# 1. 군집 개수를 2개로 고정\nk = 2\nlabels_2 = fcluster(Z, k, criterion='maxclust')  # 1, 2 라벨\n\n# 2. PCA 2차원으로 차원 축소\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X)\n\n# 3. 시각화\nplt.figure(figsize=(10, 7))\n\nmarkers = ['o', 's']\nfor g, m in zip([1, 2], markers):\n    idx = (labels_2 == g)\n    plt.scatter(\n        X_pca[idx, 0],\n        X_pca[idx, 1],\n        marker=m,\n        label=f'Cluster {g}',\n        alpha=0.8\n    )\n    # 주(State) 이름 라벨 달기\n    for i in np.where(idx)[0]:\n        plt.text(\n            X_pca[i, 0] + 0.02,\n            X_pca[i, 1] + 0.02,\n            raw.index[i],\n            fontsize=8\n        )\n\nplt.axhline(0, linestyle=':', linewidth=0.5)\nplt.axvline(0, linestyle=':', linewidth=0.5)\nplt.xlabel('PC1')\nplt.ylabel('PC2')\nplt.title('USArrests - Ward Clustering (k=2, PCA 2D)')\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n# PC1, PC2 loading matrix\nloadings = pd.DataFrame(\n    pca.components_.T,\n    columns=['PC1', 'PC2'],\n    index=raw.columns)\n\nprint(\"PCA Loadings (부하값):\")\nprint(loadings)\nPC1의 부하값은 Murder(0.54), Assault(0.58), Rape(0.54)에서 모두 크고 양의 값을 보인다. 즉 세 가지 주요 강력범죄 지표가 모두 같은 방향으로 작용하고 있습니다. UrbanPop도 양의 부하값이지만 크기가 상대적으로 작다. → 따라서 PC1 점수가 높은 주(State)는 Murder·Assault·Rape가 모두 높은 곳이다. PC1은 살인, 폭행, 강간 등 강력범죄 발생률이 공통적으로 높아지는 방향을 가장 잘 설명하는 축으로, ’전반적 강력범죄 수준’을 나타내는 주성분으로 해석할 수 있다.\nPC2에서는 UrbanPop이 0.873으로 매우 큰 양의 부하값을 보이며, 다른 변수들은 양·음이 섞여 있고 절대값도 작다. PC2는 도시 거주 인구 비율이 가장 큰 비중으로 작용하는 축으로, 주의 도시화 수준과 관련된 패턴을 반영하는 주성분이다. 즉, ’도시화 기반 범죄 패턴’ 또는 ’도시화 수준 축’으로 이해할 수 있다.\nPCA Loadings (부하값):\nPC1 PC2  Murder 0.535899 -0.418181  Assault 0.583184 -0.187986  UrbanPop 0.278191 0.872806  Rape 0.543432 0.167319\n\n\n\n\nChapter 4. 비계층적 군집분석\n비계층적 군집분석은 군집 수 k를 사전에 정하고, 관측치를 반복적으로 재배정하여 최적의 군집 분할을 찾는 방식으로, 대표적인 전통적 방법은 k-means, k-medoids(PAM), CLARA, CLARANS, Fuzzy C-means 등이 있다. 이들 방법은 계층적 군집과 달리 대규모 자료에서도 적용 가능하며, 초깃값 설정과 거리 함수 선택이 군집 결과에 큰 영향을 준다.\n여기서는 전통적인 다변량 기법인 k-means와 k-means의 약점을 보완한 방법인 PAM 기법만을 다룬다. 다른 기법은 빅데이터 기법의 군집분석에서 다루기로 한다.\n\n1. K-means 방법\n군집화 방법\nK-means 방법은 사전에 정해진 군집 수 K 를 기준으로, 각 관측치를 가장 가까운 군집 중심에 배정하면서 군집 구조를 반복적으로 갱신해 나가는 비계층적 군집 방법이다. 알고리즘의 기본 절차는 다음과 같다.\n\n\n\n\n\n① 군집 수 결정: 우선 계층적 군집분석 결과나 군집 타당성 지표 등을 이용하여 적절한 군집 수 K를 결정한다.\n② 초기 중심값 설정: 결정된 군집 수 K에 대해 초기 군집 중심을 무작위로 선택하거나, 다른 기준에 따라 초기 중심을 설정한다.\n③ 관측치 배정(assignment step): 각 관측치는 현재의 군집 중심들 중에서 가장 가까운(center와의 거리 최소) 군집에 배정된다.\n④ 중심값 갱신(update step): 새롭게 배정된 군집을 기준으로 각 군집의 평균을 계산하여 군집 중심을 다시 업데이트한다.\n⑤ 수렴할 때까지 반복: 배정과 중심 갱신(단계 ③–④)을 반복하여, 더 이상 군집의 변화가 나타나지 않을 때 알고리즘을 종료한다.\n사례분석\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# ---- 데이터 준비 ----\nimport statsmodels.api as sm\nraw = sm.datasets.get_rdataset(\"USArrests\", \"datasets\").data\nX = StandardScaler().fit_transform(raw.values)\n\n# --- 1. K-means (k=2) ---\nk = 2\nkmeans = KMeans(n_clusters=k, n_init=20, random_state=42)\nlabels_km = kmeans.fit_predict(X_scaled)\n\n# --- 2. PCA 차원축소 ---\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X_scaled)\n\n# --- 3. 시각화 ---\nplt.figure(figsize=(10, 7))\n\nmarkers = ['o', 's']\ncolors = ['tab:blue', 'tab:orange']\n\nfor g, m, c in zip(range(k), markers, colors):\n    idx = (labels_km == g)\n    plt.scatter(\n        X_pca[idx, 0],\n        X_pca[idx, 1],\n        marker=m,\n        color=c,\n        label=f'Cluster {g+1}',\n        alpha=0.8\n    )\n    # 주 이름 라벨\n    for i in np.where(idx)[0]:\n        plt.text(\n            X_pca[i, 0] + 0.02,\n            X_pca[i, 1] + 0.02,\n            raw.index[i],\n            fontsize=8\n        )\n\nplt.axhline(0, linestyle=':', linewidth=0.5)\nplt.axvline(0, linestyle=':', linewidth=0.5)\n\nplt.xlabel(\"PC1: Violent Crime Level\")\nplt.ylabel(\"PC2: Urbanization Level\")\nplt.title(\"USArrests - K-means (k=2) on PCA Space\")\n\nplt.legend()\nplt.tight_layout()\nplt.show()\n계층적 군집분석 결과와 차이는 missouri 주가 ’전반적 강력범죄 수준’ 높은 집단으로 분류되었다.\n\n\n\n\n\n\n\n2. k-medoids(PAM)\nK-medoids 군집분석은 군집의 중심을 평균(mean) 대신, 데이터 중에서 실제 존재하는 관측치 하나(medoid)로 선택하는 분석법이다. 여기서 medoid는 군집 내에서 다른 모든 관측치들과의 거리 합이 최소가 되는 대표점 을 의미한다.\n즉, K-means의 ”중심(centroid)“이 가상의 점이라면 K-medoids의 ”medoid”는 실제로 존재하는 데이터 점이다.\nK-means 알고리즘은 단순하고 계산이 빠르다는 장점이 있지만, 군집의 중심을 각 군집 내 관측치들의 평균값으로 정의하는 방식 때문에 여러 한계를 갖는다. 우선 평균은 이상치에 민감하므로, 소수의 극단값이 존재할 경우 중심점이 크게 왜곡될 수 있다. 또한 변수들의 스케일에 매우 민감하여, 적절한 표준화가 이루어지지 않으면 특정 변수가 군집 구조를 과도하게 좌우하게 된다. 더불어 K-means는 일반적으로 유클리드 거리를 전제로 하기 때문에 비유클리드 거리 공간에서는 적용이 어렵고, 범주형 변수와 같이 평균값이 의미를 갖지 않는 자료에는 사용할 수 없다는 제한이 있다.\n이러한 단점을 보완하기 위해 고안된 방법이 K-medoids 알고리즘이다. K-medoids는 군집 중심을 평균이 아닌 실제 관측치(medoid) 중 하나로 선택함으로써 이상치의 영향을 크게 줄인다. 또한 군집 형성에 사용되는 거리 척도를 자유롭게 선택할 수 있어, 연속형뿐만 아니라 범주형 혹은 혼합형 데이터에도 적용할 수 있다는 유연성을 갖는다. 군집의 중심이 실제 존재하는 관측치이기 때문에 해석 또한 직관적이며, 이러한 이유로 의료 자료, 마케팅 고객 세분화, 생태학적 데이터와 같이 다양한 거리 구조를 가진 분야에서 널리 활용되고 있다.\n!pip install scikit-learn-extra\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import pairwise_distances\nfrom sklearn.decomposition import PCA\nimport statsmodels.api as sm\nfrom sklearn.preprocessing import StandardScaler\n\n# -----------------------------\n# 0. 데이터 준비 (USArrests + 표준화)\n# -----------------------------\nraw = sm.datasets.get_rdataset(\"USArrests\", \"datasets\").data\nX_scaled = StandardScaler().fit_transform(raw.values)\n\n# -----------------------------\n# 1. k-medoids(PAM) 구현\n# -----------------------------\ndef k_medoids(X, k, max_iter=100, random_state=42):\n    \"\"\"\n    간단한 PAM 알고리즘 구현 (Euclidean 거리)\n    X : (n, p) 데이터\n    k : 군집 수\n    \"\"\"\n    rng = np.random.default_rng(random_state)\n    n = X.shape[0]\n\n    # 전체 pairwise distance 미리 계산 (n이 작아서 가능)\n    D = pairwise_distances(X, metric=\"euclidean\")\n\n    # 초기 medoid를 임의로 선택\n    medoids = rng.choice(n, size=k, replace=False)\n\n    # 초기 라벨 부여\n    labels = np.argmin(D[:, medoids], axis=1)\n\n    for it in range(max_iter):\n        old_medoids = medoids.copy()\n\n        # 각 군집마다 medoid 갱신\n        for m in range(k):\n            idx = np.where(labels == m)[0]\n            if len(idx) == 0:\n                continue\n\n            # 군집 내 거리 합이 최소가 되는 점을 medoid로 선택\n            intra_D = D[np.ix_(idx, idx)]         # 군집 내 거리행렬\n            costs = intra_D.sum(axis=1)           # 각 점을 medoid로 했을 때 비용\n            best_idx = idx[np.argmin(costs)]\n            medoids[m] = best_idx\n\n        # medoid가 바뀐 후 새 라벨 부여\n        labels = np.argmin(D[:, medoids], axis=1)\n\n        # 수렴 체크\n        if np.all(medoids == old_medoids):\n            break\n\n    return medoids, labels\n\n# -----------------------------\n# 2. k-medoids 수행 (k=2)\n# -----------------------------\nk = 2\nmedoids, labels_pm = k_medoids(X_scaled, k=k, max_iter=100, random_state=42)\n\nprint(\"Medoid 인덱스:\", medoids)\nprint(\"Medoid 주 이름:\", raw.index[medoids].tolist())\n\n# -----------------------------\n# 3. PCA 2차원 축으로 시각화\n# -----------------------------\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X_scaled)\n\nplt.figure(figsize=(10, 7))\n\nmarkers = ['o', 's']\ncolors = ['tab:blue', 'tab:orange']\n\nfor g, m, c in zip(range(k), markers, colors):\n    idx = (labels_pm == g)\n    plt.scatter(\n        X_pca[idx, 0],\n        X_pca[idx, 1],\n        marker=m,\n        label=f'Cluster {g+1}',\n        alpha=0.8,\n        color=c\n    )\n    # 주 이름 라벨\n    for i in np.where(idx)[0]:\n        plt.text(\n            X_pca[i, 0] + 0.03,\n            X_pca[i, 1] + 0.03,\n            raw.index[i],\n            fontsize=8\n        )\n\n# medoid 위치 별도 표시 (★)\nplt.scatter(\n    X_pca[medoids, 0],\n    X_pca[medoids, 1],\n    s=200,\n    marker='*',\n    edgecolor='k',\n    facecolor='none',\n    linewidth=1.5,\n    label='Medoids'\n)\n\nplt.axhline(0, linestyle=':', linewidth=0.5)\nplt.axvline(0, linestyle=':', linewidth=0.5)\n\nplt.xlabel(\"PC1: Violent Crime Level\")\nplt.ylabel(\"PC2: Urbanization Level\")\nplt.title(\"USArrests - k-medoids (PAM, k=2) on PCA space\")\nplt.legend()\nplt.tight_layout()\nplt.show()\n계층적 군집분석 결과와 차이는 missouri 주가 ’전반적 강력범죄 수준’ 높은 집단으로 분류되었다. 이상 개체가 없어 k-means 군집분석 결과와는 동일하다."
  },
  {
    "objectID": "notes/survey/questionnaire.html",
    "href": "notes/survey/questionnaire.html",
    "title": "조사방법론. 4. 설문지",
    "section": "",
    "text": "chapter 1. 설문지 작성 개요\n설문조사는 응답자에 대한 다양한 정보를 수집하기 위해 여러 방식을 활용한다. 이 중 가장 일반적인 방법은 설문지를 사용하는 것이다. 설문지는 일정한 순서로 제시되는 표준화된 질문들로 구성되며, 대부분 고정된 선택지를 포함하고 있다. 이를 통해 응답자로부터 일관된 데이터를 얻을 수 있다.\n오늘날 설문지는 점점 전자적인 형태로 변화하고 있다. 컴퓨터 프로그램이 설문지를 조사원에게 제공하거나, 응답자에게 직접 보여주는 방식이 증가하고 있다. 그러나 설문 방식이 종이이든 전자이든, 조사원이 있든 없든 대부분의 설문조사는 여전히 응답자가 정해진 질문을 해석하고 그에 맞는 정보를 제공하는 구조에 의존하고 있다.\n모든 설문조사가 응답자에게 질문에 대한 답변을 직접 구성하도록 요구하는 것은 아니다. 예를 들어, 기업이나 기관을 대상으로 한 설문은 주로 기록에서 정보를 추출하며, 이 경우 설문지는 인터뷰 대본이라기보다 데이터 기록 양식에 가깝다. 면접자는 실제 응답자가 아닌 기록과 상호 작용하면서 필요한 정보를 수집할 수 있다.\n교육 관련 설문에서는 학생의 성적 기록이 설문 데이터를 보완할 수 있고, 건강 관련 조사는 의료 기록을 활용함으로써 응답자의 진술에만 의존하는 것을 피할 수 있다. 이러한 경우에도 응답자는 기록에 접근할 수 있도록 협조하거나, 조사 담당자가 필요한 정보를 찾도록 도와주는 역할을 한다.\n일부 조사는 사전에 관련 기록을 준비하도록 요청함으로써 보다 정확한 응답을 유도하기도 한다. 예를 들어, 건강 관련 조사를 앞두고 응답자에게 진료비 영수증이나 의료비 기록을 준비하도록 안내하면, 인터뷰 중 더 정확한 답변을 이끌어낼 수 있다. 다만 어떤 가정에서는 일상 지출을 체계적으로 기록하지 않기 때문에, 필요한 정보가 아예 존재하지 않을 수도 있다.\n설문지는 응답자로부터 일관된 정보를 수집하는 데 핵심적인 역할을 하며, 설문조사의 가장 기본적인 도구로 사용된다. 동일한 질문을 모든 응답자에게 같은 방식으로 제시함으로써 비교 가능성을 높이고, 결과의 신뢰성을 확보할 수 있다. 닫힌 질문과 열린 질문을 적절히 구성함으로써, 응답자의 사고방식을 반영하고 정량적 또는 정성적 분석이 가능하도록 돕는다.\n설문지는 면접 조사에서 조사원이 질문을 일관되게 제시할 수 있도록 안내 역할을 하며, 자기 기입식 조사에서는 응답자가 혼자서도 쉽게 이해하고 답변할 수 있도록 구성된다. 응답 내용을 체계적으로 기록하고 보관하는 기능도 수행하며, 디지털 설문지는 이 과정을 더욱 효율적으로 만들어준다.\n또한, 응답자가 과거 경험을 회상할 수 있도록 특정 사건이나 시점을 제시하는 등 기억을 보조하는 역할도 수행한다. 의료비 지출과 같은 주제를 다룰 때는, 관련 기록을 참고하게 함으로써 보다 정확한 응답을 유도할 수 있다. 이외에도 설문지는 사람들의 행동이나 태도, 의견을 정량적으로 측정하는 데 사용된다. 리커트 척도와 같은 방식은 응답자의 주관적인 평가를 수치화하여 비교 분석을 가능하게 한다.\n설문지는 면접 조사, 우편 설문, 온라인 조사, 전화 조사 등 다양한 방식으로 적용되며, 조사 방식에 따라 질문 순서나 난이도, 응답 인터페이스 등이 달라지기도 한다.\n결국 설문지는 단순한 질문지를 넘어, 응답자의 반응을 유도하고 연구자가 원하는 정보를 효과적으로 수집하는 도구로 기능한다. 설문지 설계의 질은 전체 조사 결과의 신뢰성과 타당성을 결정짓는 중요한 요소가 된다.\n\n\nchapter 2. 좋은 질문지 작성\n좋은 설문지 작성 지침은 응답자의 부담을 줄이면서도 정확하고 신뢰할 수 있는 데이터를 확보하는 데 큰 도움이 된다. 이러한 데이터는 이후 연구, 정책 수립, 비즈니스 의사결정 등에서 보다 효과적인 결과를 도출하는 기반이 된다.\n우선, 설문 조사는 응답 과정에서 다양한 오류가 발생할 수 있다. 질문이 모호하거나 복잡하면 응답자가 정확하게 이해하지 못하고 엉뚱한 답을 하거나, 기억에 의존해 부정확하게 응답하는 일이 생긴다. 때로는 사회적으로 바람직한 방향으로 응답하려는 경향도 나타난다. 좋은 설문지 지침은 이러한 오류를 줄이고, 응답의 일관성과 정확성을 높이는 데 기여한다.\n설문 설계의 품질이 높아지면 수집되는 데이터의 신뢰성과 타당성도 향상된다. 질문이 명확하고 체계적으로 구성되면, 응답자는 자신의 경험이나 생각을 보다 정확하게 반영할 수 있다. 이로 인해 연구자는 실제 현상을 왜곡 없이 분석할 수 있고, 결과 해석의 정확도도 높아진다.\n응답자의 입장에서 보면, 질문이 명확하고 간결할수록 설문을 보다 수월하게 작성할 수 있다. 반대로 질문이 길거나 복잡하면 응답 피로도가 높아지고, 설문을 끝까지 완료하지 않거나 무성의한 답변을 하게 될 위험이 커진다. 잘 구성된 설문지는 응답 부담을 낮추고, 자연스럽게 높은 응답률을 유도할 수 있다.\n응답률이 높아지면 결과의 대표성도 향상된다. 이는 곧 전체 모집단을 보다 정확하게 반영하는 데이터 확보로 이어진다. 반면, 난해하거나 민감한 질문이 많을 경우 응답자가 중도에 설문을 포기할 수 있으므로, 질문 구성과 배열에도 세심한 주의가 필요하다.\n설문으로 수집된 자료는 다양한 분석과 의사결정에 활용된다. 그러나 질문이 잘못 설계되면 데이터가 왜곡되고, 결과적으로 잘못된 판단이나 정책으로 이어질 수 있다. 따라서 질문은 연구 목적에 부합하도록 설계되어야 하며, 타당하고 신뢰할 수 있는 해석이 가능하도록 구성되어야 한다.\n또한 설문 문항의 성격에 따라 적절한 접근 방식이 필요하다. 예를 들어, 행동을 묻는 질문은 구체적인 시점이나 상황을 제시해야 하며, 태도를 측정하는 질문은 명확한 척도와 함께 제시되어야 한다. 민감한 주제를 다룰 경우에는 응답자의 익명성과 프라이버시를 충분히 고려해야 한다. 질문 유형에 따라 각각 다른 원칙과 기법을 적용해야 하며, 이를 위해 명확한 설문지 작성 지침이 필요하다.\n설문조사는 종종 연구 결과나 정책 제안서, 또는 기업 보고서에 인용된다. 따라서 설문 설계가 과학적이고 체계적이지 않다면 결과의 신뢰성이 떨어지고, 해당 결과를 활용하는 다양한 이해관계자들에게 부정적인 영향을 미칠 수 있다. 설문지 작성 지침은 단순히 문항을 구성하는 기술적 매뉴얼을 넘어, 설문 연구의 품질을 결정짓는 핵심 요소라 할 수 있다.\n\n1. 행동문항과 태도 문항\n개념적 차이\n\n\n\n\n\n\n\n\n구분\n행동 behavior 문항\n태도 attitude 문항\n\n\n정의\n응답자의 실제 경험이나 행동을 측정하는 질문\n응답자의 신념, 가치관, 감정, 의견 등을 측정하는 질문\n\n\n목적\n특정 행동을 수행한 빈도, 시기, 방식 등을 파악\n특정 주제에 대한 태도나 선호도를 측정\n\n\n측정 대상\n객관적이고 구체적인 행동(실제 경험)\n주관적인 인식, 감정, 의견\n\n\n기록 방식\n응답자가 직접 보고한 행동(예: 구매경험, 운동빈도 등)\n응답자의 심리적 상태, 선호도 등을 측정\n\n\n\n측정방법 차이\n\n\n\n\n\n\n\n\n측정 방법\n행동 문항\n태도 문항\n\n\n자기보고\n응답자가 직접 자신의 행동을 보고\n응답자가 자신의 의견을 보고\n\n\n행동 기록\n응답자가 일정 기간 동안 행동을 기록 (예: 음식섭취 일기)\n해당 없음\n\n\n관찰법\n연구자가 직접 응답자의 행동을 관찰 (예: 실제 투표 여부 확인)\n연구자가 응답자 태도를 직접 측정할 수 없음\n\n\n반응 척도\n행동 횟수, 빈도를 정량적으로 측정 (예: '한 달에 5회 이상')\nLikert 척도, 시각적 아날로그 척도 등을 사용하여 태도를 정량적으로 측정\n\n\n\n활용방안\n\n\n\n\n\n\n\n\n연구 목적\n행동 문항\n태도 문항\n\n\n실제 행동 측정\n응답자가 특정 행동을 수행했는지 확인할 때\n해당 없음\n\n\n정책 평가\n정책이 실제 행동 변화에 영향을 미쳤는지 측정할 때\n정책에 대한 인식을 평가할 때\n\n\n소비자 조사\n제품 구매 빈도, 사용 습관 측정\n브랜드 선호도, 만족도 평가\n\n\n건강 연구\n운동, 식습관, 흡연여부 조사\n건강에 대한 인식, 위험에 대한 태도 측정\n\n\n\n설문 조사에서 행동 문항과 태도 문항은 서로 다른 목적과 특성을 지니며, 응답 방식이나 인식에도 차이가 있다. 행동 문항은 응답자의 실제 경험이나 행위를 측정하는 데 중점을 두며, 객관적인 데이터를 수집하는 데 적합하다. 반면 태도 문항은 특정 주제에 대한 신념, 감정, 의견을 평가하는 데 활용되며, 응답자의 주관적인 인식을 파악하는 데 유리하다.\n행동 문항은 특정 행동이 언제, 얼마나 자주, 어떤 방식으로 이루어졌는지를 묻는 것이 일반적이다. 예를 들어, “지난 한 달 동안 커피를 몇 번 구매하셨습니까?“와 같은 질문은 실제 경험을 기반으로 한 응답을 이끌어내기 위한 전형적인 행동 문항이다. 이에 비해, “당신은 커피를 마시는 것이 건강에 좋다고 생각하십니까?“는 응답자의 생각이나 판단을 묻는 태도 문항에 해당한다.\n행동 문항은 실재 경험에 기초하기 때문에 응답의 객관성이 상대적으로 높을 수 있지만, 응답자가 모든 경험을 정확히 기억하지 못할 가능성도 고려해야 한다. 예를 들어, “지난 6개월 동안 술을 얼마나 자주 마셨습니까?“와 같은 질문에서는 과거나 현재의 행동을 과장하거나 축소해 응답하는 경향이 나타날 수 있다. 반대로, 태도 문항은 응답자의 감정 상태나 환경적 요인에 따라 동일한 질문에도 답변이 달라질 수 있다. “술을 마시는 것이 사회적 관계에 도움이 된다고 생각하십니까?“라는 질문은 개인의 가치관에 따라 매우 다른 반응이 나올 수 있다.\n두 문항 유형은 질문 구성 방식에서도 차이를 보인다. 행동 문항은 구체적인 시간 범위와 빈도를 포함해야 응답자가 명확하게 답할 수 있다. 예를 들어, “최근에 운동한 적이 있습니까?“는 다소 모호한 반면, “지난 7일 동안 30분 이상 운동한 날이 며칠입니까?“는 보다 구체적인 정보를 요구하여 응답의 정확성을 높일 수 있다. 태도 문항은 주로 Likert 척도와 같은 평가 척도를 활용하여 응답자의 생각이나 입장을 수치화할 수 있도록 설계된다. 예를 들어, “당신은 정부의 환경 보호 정책을 얼마나 지지하십니까?“라는 문항은 5점 또는 7점 척도를 통해 응답을 정량화할 수 있다.\n행동 문항을 설계할 때는 응답자가 기억하기 쉬운 시간 기준을 사용하는 것이 중요하다. “지난 1년 동안 해외여행을 몇 번 다녀오셨습니까?“보다 “2024년 1월 이후 해외여행을 몇 번 다녀오셨습니까?“와 같이 명확한 시점을 제시하면 보다 정확한 응답을 유도할 수 있다. 또한, 행동 문항의 신뢰도를 높이기 위해 영수증, 일정표 등 실제 자료를 참조하도록 안내하는 것도 좋은 방법이다.\n태도 문항에서는 사회적 바람직성 편향을 줄이는 것이 중요하다. 예를 들어, “당신은 환경 보호를 중요하게 생각하십니까?“와 같은 질문은 응답자가 사회적으로 바람직한 방향으로 답할 가능성이 높다. 이때, “최근 6개월 동안 환경 보호를 위해 어떤 행동을 하셨습니까?“와 같이 구체적인 행동을 확인할 수 있는 보조 문항을 추가하면 응답의 신뢰도를 높일 수 있다. 또한, 태도 문항은 앞뒤 문항의 영향을 받을 수 있으므로, 설문지의 흐름과 문항 배열에도 주의를 기울여야 한다.\n행동 문항과 태도 문항은 연구 목적에 따라 적절히 선택하여 사용해야 한다. 행동 문항은 실제 행위를 파악하는 데 적합하여 정책 효과 분석, 소비 패턴 연구, 건강 행동 연구 등에 활용된다. 예를 들어, “지난 선거에서 투표를 하셨습니까?“와 같은 문항은 실제 참여 여부를 측정할 수 있다. 반면, 태도 문항은 개인의 가치관이나 인식을 이해하는 데 유용하며, 사회 문제 인식 조사나 브랜드 이미지 조사 등에 자주 사용된다. 예를 들어, “당신은 선거 참여가 중요하다고 생각하십니까?“는 정치적 태도를 분석하는 데 적합한 문항이다.\n이처럼 행동 문항과 태도 문항은 각기 다른 역할을 수행하며, 상황에 맞게 잘 설계되었을 때 설문조사의 타당성과 신뢰도를 크게 높일 수 있다.\n\n\n2. 행동문항: 민감하지 않은 질문\n민감하지 않은 질문이라 하더라도, 응답자의 기억력에 한계가 있을 수 있기 때문에 보다 정확하고 신뢰성 높은 데이터를 얻기 위해서는 설문 설계 시 세심한 전략이 필요하다. 특히, 응답자가 질문을 혼동하지 않도록 질문을 구체적이고 명확하게 제시하고, 필요한 경우 과거의 기억을 떠올릴 수 있도록 돕는 자극 요소를 함께 제공하는 것이 효과적이다.\n예를 들어, 특정 시기나 사건을 기준으로 삼아 질문을 제시하거나, 응답자가 참고할 수 있는 일정표, 영수증, 가계부 등과 같은 보조 자료를 활용하도록 유도하면 기억에 의존한 응답의 정확도를 크게 높일 수 있다. 이처럼 응답자의 회상을 돕는 다양한 도구와 설계 기법을 적용함으로써, 사소하거나 잊기 쉬운 정보에 대해서도 신뢰할 수 있는 응답을 이끌어낼 수 있다.\n폐쇄형 질문에서는 모든 합리적인 가능성을 응답 보기에 포함할 것\n폐쇄형 질문(closed-ended questions)은 응답자가 자유롭게 서술하기보다, 미리 제시된 선택지 중에서 가장 적절하다고 생각되는 답을 고르도록 설계된 질문 유형이다. 이러한 질문은 응답 결과를 정량적으로 분석하기 용이하다는 장점이 있으며, 조사자 간의 해석 차이나 데이터 정제 과정의 오류를 줄이는 데도 유리하다.\n하지만 응답자가 자신의 경험과 가장 일치하는 선택지를 찾을 수 있도록, 가능한 모든 합리적 응답 옵션이 빠짐없이 포함되어야 한다. 예를 들어, “지난 한 달 동안 외식한 횟수는?”이라는 질문을 제시할 때, 보기로는 0회, 1–2회, 3–4회, 5회 이상과 같이 모든 응답 범주를 포괄할 수 있어야 하며, 누락되거나 겹치는 구간이 없어야 한다. 이렇게 구성된 질문은 응답자의 선택을 쉽게 하고, 결과 해석의 명확성도 높여준다.\n질문을 가능한 한 구체적으로 만들 것\n질문이 모호하게 제시되면, 응답자는 각자의 경험과 기준에 따라 질문을 다르게 해석하게 되고, 그 결과 응답의 일관성과 비교 가능성이 떨어질 수 있다. 예를 들어, “운동을 얼마나 자주 하십니까?”라는 질문은 매우 일반적이고 추상적인 표현이기 때문에, 어떤 사람은 일주일에 한두 번 하는 것을 ’자주’라고 생각할 수 있고, 또 다른 사람은 하루에 한 번 이상 해야 ’자주’라고 느낄 수 있다. 이처럼 해석의 기준이 응답자마다 다를 경우, 수집된 데이터의 정확성과 신뢰성이 저하될 수 있다.\n따라서 질문은 응답자가 같은 기준으로 이해하고 답변할 수 있도록 구체적이고 명확하게 설계하는 것이 중요하다. 예컨대, “지난 7일 동안 30분 이상 신체 활동을 한 날은 며칠입니까?”처럼 시간 범위와 활동 기준을 함께 제시하면, 응답자는 자신의 경험을 보다 정확하게 회상하고 일관된 기준에 따라 응답할 수 있다. 이와 같은 질문 설계는 응답 해석의 차이를 줄이고 데이터 품질을 높이는 데 효과적이다.\n거의 모든 응답자가 이해할 수 있는 단어를 사용할 것\n설문 문항에 전문 용어나 지나치게 어려운 단어가 포함되면, 응답자가 질문의 의미를 정확히 이해하지 못하고 의도와 다른 답변을 할 가능성이 높아진다. 이는 특히 일반 대중을 대상으로 한 조사에서 자주 발생할 수 있으며, 데이터의 신뢰도와 해석의 정확성을 크게 떨어뜨릴 수 있다.\n예를 들어, “귀하는 주간 단위로 주거 이동을 하나요?”라는 문장은 일부 응답자에게 생소하거나 난해하게 느껴질 수 있다. 반면, 같은 내용을 보다 쉽게 표현한 “한 주 동안 다른 곳에서 머문 적이 있나요?”라는 문장은 일상적인 언어를 사용해 응답자가 질문을 명확히 이해하고 답할 수 있도록 돕는다.\n또한, 문화적 배경에 따라 다르게 해석될 수 있는 단어나 표현은 가급적 피해야 한다. 특정 지역이나 계층에서만 통용되는 표현은 설문 대상자 전체에게 일관된 이해를 제공하기 어렵기 때문이다. 따라서 모든 응답자가 비슷한 방식으로 해석할 수 있도록, 쉬운 언어로 표현하고 문화적 중립성을 유지하는 것이 설문 문항 설계의 기본 원칙 중 하나이다.\n기억을 향상시키기 위해 기억 단서를 추가하여 질문을 길게 만들 것\n응답자가 과거의 특정 사건을 정확히 기억해내기 어려운 경우, 질문에 기억을 자극하는 단서를 함께 제공하면 응답의 정확성을 높일 수 있다. 사람들은 일상적으로 경험한 사건이라 하더라도 시간이 지나면 세부 내용을 잊어버릴 수 있으며, 특히 빈도가 낮거나 중요도가 높지 않은 사건일수록 그 경향이 뚜렷하다.\n예를 들어, “지난 6개월 동안 해외여행을 다녀온 적이 있습니까?”라는 질문은 포괄적으로 보일 수 있으나, 응답자가 기억하지 못한 경험을 빠뜨릴 가능성이 있다. 이때 “업무 출장, 가족여행, 친구와의 여행, 유학 등 포함”과 같이 다양한 유형의 여행 사례를 함께 제시하면, 응답자가 자신의 경험을 더 쉽게 회상할 수 있고, 누락 없이 답변할 가능성이 높아진다.\n이러한 방식은 질문을 보다 구체적이고 현실적인 맥락 안에서 이해하도록 도와주며, 결과적으로 설문 데이터의 질을 향상시키는 데 기여한다.\n잊어버릴 가능성이 높을 때는 도움 회상을 사용할 것\n사람들은 일상에서 다양한 경험을 하며, 그중 일부는 시간이 지나면서 자연스럽게 잊혀질 수 있다. 특히 자주 발생하지 않거나 특별히 주의를 기울이지 않았던 사건에 대해서는 기억이 흐릿해지기 쉽다. 이러한 경우, 설문에서 정확한 정보를 얻기 위해서는 응답자의 기억을 돕는 회상 기법을 적절히 활용하는 것이 필요하다.\n예를 들어, “지난 3개월 동안 병원에 방문한 적이 있습니까?”라는 질문에 응답자가 확실하게 기억하지 못하는 상황이 있을 수 있다. 이럴 때 “가족 병원 방문, 정기 건강검진, 치과 진료, 응급실 방문 포함”과 같이 구체적인 사례나 유형을 제시하면, 응답자가 해당 경험을 떠올리는 데 도움이 되며, 그 결과 더 정확하고 신뢰할 수 있는 응답을 얻을 수 있다.\n이러한 회상 자극은 응답 누락이나 부정확한 답변을 줄이는 데 효과적이며, 설문조사 데이터의 품질을 높이는 중요한 전략 중 하나로 활용될 수 있다.\n빈번하지만 크게 중요하지 않은 경우, 응답자가 일지를 작성 유도\n일상에서 자주 일어나지만 중요도가 낮아 쉽게 잊히는 행동이나 사건에 대해서는, 설문 시 응답의 정확성을 확보하기가 쉽지 않다. 예를 들어, 간식 섭취, 대중교통 이용, 소액 지출 등은 자주 일어나지만 응답자가 구체적으로 기억하지 못할 가능성이 크다. 이런 경우에는 사전에 기록을 남기도록 안내함으로써, 설문 시 보다 정확한 정보를 제공받을 수 있다.\n예를 들어, 조사 시작 전 일정 기간 동안 응답자에게 간단한 일지나 체크리스트를 작성하도록 요청하면, 특정 행동에 대한 기억을 체계적으로 보존할 수 있다. 매일 섭취한 음식, 사용한 교통수단, 지출한 항목 등을 기록하게 하면, 설문조사 시 응답자는 이를 참고해 보다 정확한 응답을 제공할 수 있으며, 조사 결과의 신뢰성과 정밀도도 함께 향상된다.\n이러한 사전 기록 활용은 조사 대상의 특성에 따라 유연하게 적용할 수 있으며, 특히 생활습관, 소비행동, 건강 관련 조사의 정밀도를 높이는 데 유용한 방법이다.\n긴 회상 기간이 필요한 경우, 생애 사건 달력을 사용 권장\n조사에서 긴 기간에 걸친 기억을 회상해야 할 때는, 응답자의 기억이 흐릿해져 정확한 응답을 기대하기 어려운 경우가 많다. 예를 들어, 지난 1년간의 의료 이용 횟수처럼 시간적 범위가 길고 사건의 발생 시점이 분산된 경우, 응답자는 정확한 시기를 떠올리기 어려울 수 있다. 이런 경우에는 생애 사건 달력(life event calendar)과 같은 도구를 활용하여 응답자의 기억을 자극하는 것이 효과적이다.\n예를 들어, “작년 초에 병원에 간 적이 있나요?”처럼 막연한 시기를 제시하기보다는 “설 연휴 이후, 여름휴가 기간, 추석 이후에 병원에 방문한 적이 있나요?”처럼 사회적으로 널리 인식되는 기준 시점을 활용하면, 응답자가 과거 경험을 특정 시점에 연결해 더 정확하게 회상할 수 있다.\n이러한 방식은 단순한 시간 기준보다 현실적인 단서를 제공하기 때문에, 장기 회상이 필요한 설문조사에서 응답 오류를 줄이고, 데이터의 정확성과 신뢰성을 높이는 데 효과적인 전략이 될 수 있다.\n망원경 오류를 줄이기 위해 응답자가 가계 기록을 사용\n망원경 오류는 사람들이 실제 사건이 발생한 시기를 정확히 기억하지 못하고, 과거의 일을 더 최근의 일로 기억하거나, 반대로 최근에 일어난 일을 오래전에 있었던 일로 착각하는 현상을 의미한다. 이러한 오류는 특히 회상 기간이 길거나 사건이 반복적으로 발생했을 때 자주 나타나며, 응답 데이터의 시점 정확도를 떨어뜨릴 수 있다.\n이러한 오류를 줄이기 위해서는 응답자가 자신이 경험한 사건의 시점을 보다 명확히 인식할 수 있도록 도와주는 전략이 필요하다. 예를 들어, 설문 응답 시 영수증, 일정표, 가계부 등과 같은 실제 기록을 참고하도록 유도하면, 응답자가 막연한 기억에 의존하지 않고 보다 구체적인 정보를 제공할 수 있게 된다.\n또한, 명확한 시간 기준을 제시하는 경계 회상 기법을 함께 활용하면 효과적이다. “올해 1월 1일 이후”나 “2024년 7월 이후”와 같은 기준점을 제공하면, 응답자는 해당 시점을 중심으로 기억을 정리하고 보다 정확한 시기 판단을 할 수 있어, 전체적인 응답 오류를 줄이는 데 도움이 된다.\n비용이 문제가 되는 경우, 대리 응답자를 활용할 것\n설문조사에서 응답자의 기억이 부정확하거나, 조사 비용과 시간을 절감할 필요가 있을 때는 해당 정보를 더 잘 알고 있는 대리 응답자(proxy respondent)를 통해 답변을 받는 방법을 사용할 수 있다. 이는 특히 가구 단위 조사나 조직 내부 조사의 경우에 효과적인 전략으로, 일부 응답자가 세부적인 내용을 정확히 기억하지 못할 때 유용하게 활용된다. 예를 들어, 가계 소비 조사에서 가족 구성원이 자신의 지출 내역을 잘 기억하지 못할 경우, 가계 예산을 실제로 관리하고 있는 가족 구성원이 다른 구성원을 대신해 응답함으로써 전체적인 소비 정보를 보다 정확하게 수집할 수 있다.\n하지만 대리 응답 방식은 정보의 정확성을 담보하지 않는 한계도 함께 내포하고 있다. 대리 응답자가 실제로 얼마나 정확한 정보를 알고 있는지를 검토하는 과정이 병행되어야 하며, 경우에 따라 보완 질문이나 확인 절차가 필요할 수 있다. 만약 대리 응답자가 본인의 추측에 근거해 답변하거나 일부 정보만 알고 있는 상태라면, 수집된 데이터의 신뢰성과 타당성이 낮아질 수 있다. 따라서 대리 응답은 신중하게 적용되어야 하며, 응답의 품질을 확보하기 위한 장치도 함께 고려해야 한다.\n\n\n3. 행동문항: 민감한 질문\n민감한 질문을 포함하는 설문조사에서는 응답자의 심리적 불편함이나 저항감을 최소화하는 것이 매우 중요하다. 응답자가 부담을 느끼거나 불안감을 갖게 되면 질문에 대해 회피하거나 왜곡된 정보를 제공할 가능성이 높아지기 때문이다. 이러한 문제를 줄이기 위해, 질문을 보다 중립적인 언어로 표현하거나, 간접적인 방식으로 묻는 전략을 사용할 수 있다. 또한, 응답의 익명성과 비밀 보장이 철저히 이루어진다는 점을 사전에 안내하면, 응답자가 보다 솔직하게 자신의 생각이나 경험을 공유할 가능성이 높아진다.\n이와 함께, 응답의 신뢰도를 높이기 위한 다양한 보완 기법을 활용하는 것도 중요하다. 예를 들어, 민감한 문항을 설문지 후반부에 배치하거나, 자기기입 방식 또는 전산화된 응답 시스템을 도입하면 응답자가 심리적 부담 없이 답할 수 있는 환경을 조성할 수 있다. 이러한 전략을 적절히 적용하면 민감한 주제에 대해서도 보다 정밀하고 신뢰할 수 있는 데이터를 수집할 수 있으며, 이는 궁극적으로 연구의 타당성과 정책 수립의 실효성을 높이는 데 기여할 수 있다.\n민감한 행동의 빈도를 파악할 때 개방형 질문을 사용할 것\n민감한 행동에 대해 질문할 때, 응답자에게 정해진 선택지를 고르게 하기보다 스스로 자유롭게 경험을 서술할 수 있는 기회를 제공하면, 응답의 진실성과 정확성을 높일 수 있다. 폐쇄형 질문은 응답자의 민감함을 자극하거나 방어적인 태도를 유발할 수 있지만, 개방형 질문은 보다 편안한 분위기에서 자신의 경험을 표현하게 하여 왜곡 가능성을 줄여준다.\n예를 들어, “지난 6개월 동안 몇 번 음주했습니까?”라고 직접적으로 횟수를 묻는 대신, “지난 6개월 동안 음주한 경험에 대해 설명해 주세요.”와 같이 질문을 보다 개방적이고 서술적으로 제시하면, 응답자는 스스로 기억을 떠올리며 자신의 상황에 맞게 보다 자연스럽고 솔직한 답변을 할 가능성이 높다. 이러한 방식은 특히 민감하거나 사적인 영역의 행동을 조사할 때 유용하며, 응답의 신뢰도를 높이는 데 도움이 된다.\n짧은 질문보다는 긴 질문을 사용할 것\n짧고 간단한 질문은 응답자의 이해를 돕는 데 효과적일 수 있지만, 경우에 따라 오해를 불러일으키거나 질문의 의도를 제대로 전달하지 못할 위험도 함께 존재한다. 특히 민감하거나 구체적인 행동을 묻는 경우, 질문이 지나치게 간단하면 응답자가 질문의 범위나 맥락을 정확히 이해하지 못해 부정확한 답변을 할 가능성이 높아진다.\n이런 경우에는 보다 긴 문장을 사용해 질문의 맥락과 내용을 구체적으로 설명하는 것이 바람직하다. 예를 들어, 단순히 “마약을 사용한 적이 있습니까?”라고 묻는 것보다, “지난 12개월 동안 의사의 처방 없이 마약(예: 코카인, 대마초, 헤로인 등)을 사용한 적이 있습니까?”처럼 질문의 범위, 기간, 예시를 명확히 제시하면, 응답자는 질문을 보다 정확하게 이해하고 신중하게 답할 수 있다. 이는 설문 데이터의 신뢰성과 해석 가능성을 높이는 데도 도움이 된다.\n민감한 행동을 설명할 때 익숙한 단어를 사용할 것\n설문 문항을 작성할 때는 응답자가 쉽게 이해할 수 있는 일상적인 단어를 사용하는 것이 기본이다. 전문 용어나 생소한 표현은 혼란을 줄 수 있고, 특히 민감한 주제의 경우 단어 선택이 응답자의 심리적 저항을 유발할 수 있다. 따라서 가능하면 응답자가 익숙하게 받아들일 수 있는 언어로 질문을 구성해야 한다.\n예를 들어, “음주 습관”이라는 표현은 다소 공식적이고 평가적인 뉘앙스를 줄 수 있지만, “술을 마신 경험”이라는 표현은 보다 자연스럽고 부담 없이 받아들여질 수 있다. 마찬가지로, “불법 약물 사용”이라는 표현은 응답자에게 심리적 압박을 줄 수 있는 반면, “기분 전환을 위한 약물 복용”처럼 완화된 표현을 사용하면 보다 솔직한 응답을 유도하는 데 도움이 된다. 이러한 언어적 배려는 응답자의 거부감을 줄이고, 설문 응답의 질을 높이는 데 중요한 역할을 한다.\n허위 응답을 줄이기 위해 의도적으로 질문을 구성할 것\n민감한 주제를 다룰 때는 응답자가 정직하게 답할 수 있도록 질문을 신중하게 구성하는 것이 매우 중요하다. 질문이 너무 직설적이거나 판단적인 어투를 담고 있으면, 응답자는 심리적으로 위축되거나 방어적인 태도를 보일 수 있으며, 이로 인해 정확한 정보를 얻기 어려워질 수 있다.\n예를 들어, “당신은 불법적으로 마약을 사용한 적이 있습니까?”처럼 직접적이고 비판적으로 들릴 수 있는 질문보다는, “많은 사람들이 스트레스 해소를 위해 마약을 사용합니다. 당신은 지난 6개월 동안 마약을 사용한 경험이 있습니까?”처럼 사회적 낙인을 완화하는 문구를 함께 제시하면, 응답자는 자기 행동에 대해 덜 비난받는다고 느끼고 보다 솔직하게 답변할 가능성이 높아진다. 이러한 접근은 민감한 정보의 응답률과 정확도를 높이는 데 효과적인 전략이 된다.\n과거의 먼 시점을 먼저 질문할 것\n민감한 주제를 다룰 때는 질문의 순서를 전략적으로 구성하는 것이 응답자의 부담을 줄이는 데 도움이 된다. 특히 최근의 행동이나 경험을 곧바로 묻는 것보다는, 비교적 덜 민감하게 느껴질 수 있는 과거의 경험부터 질문을 시작하는 것이 효과적이다. 이를 통해 응답자는 민감한 주제에 대해 심리적으로 준비할 시간을 갖고, 점진적으로 질문에 익숙해질 수 있다.\n예를 들어, “지난 1년 동안 도박을 한 적이 있습니까?”라고 바로 묻기보다는, “어릴 때 가족이나 친구들과 함께 카지노나 내기 게임을 한 경험이 있습니까?”와 같이 과거의 경험을 먼저 묻는 방식이 응답자의 방어심을 낮추는 데 도움이 된다. 이렇게 점진적인 질문 흐름을 구성하면, 이후 보다 민감한 질문에 대해서도 응답자가 부담을 덜 느끼고 정직하게 답변할 가능성이 높아진다.\n민감한 질문을 다른 민감한 항목들 사이에 포함\n민감한 질문을 설문에 포함시킬 때는, 응답자가 해당 질문을 특별히 부담스럽게 느끼지 않도록 질문의 배치 순서와 문맥을 신중하게 설계하는 것이 중요하다. 민감한 문항이 갑작스럽게 등장하면 응답자가 방어적인 태도를 보이거나 응답을 회피할 수 있으므로, 주변 질문들과의 자연스러운 연결을 통해 해당 문항이 눈에 띄지 않도록 하는 전략이 필요하다.\n예를 들어, 음주에 관한 질문을 직접적으로 제시하기보다 “평소에 즐겨 마시는 음료는 무엇인가요?”, “지난 한 달 동안 커피를 얼마나 자주 마셨습니까?”와 같은 일반적인 음료 소비 질문들과 함께 배치하고, 그 중 하나로 “지난 한 달 동안 술을 얼마나 자주 마셨습니까?”를 포함시키면, 응답자는 해당 질문을 덜 민감하게 느끼고 자연스럽게 답변할 가능성이 높아진다. 이러한 배치는 설문 흐름의 부드러움을 유지하면서도 응답의 신뢰도를 높이는 데 도움이 된다.\n자기기입 방식 또는 유사한 방법을 사용할 것\n민감한 주제를 다루는 설문에서는 응답 환경이 응답의 정확도에 큰 영향을 미친다. 특히 조사원이 직접 질문을 제시하는 면접 방식에서는, 응답자가 타인의 시선을 의식해 솔직하게 답변하지 않거나, 사회적으로 바람직한 방향으로 응답을 왜곡할 가능성이 높다. 이러한 경우 조사 방식 자체가 심리적 장벽이 되어 응답의 질을 저하시킬 수 있다.\n이러한 문제를 줄이기 위해, 응답자가 스스로 질문에 답하는 자기기입 방식이 효과적으로 활용된다. 예를 들어, 온라인 설문조사, 컴퓨터를 이용한 자기기입식 조사(CASI), 종이 설문지를 활용한 자기기입식 방식 등은 조사원과의 직접적인 대면이 없기 때문에, 응답자가 보다 편안한 환경에서 자신의 경험이나 생각을 솔직하게 표현할 수 있게 해준다. 결과적으로 이러한 방식은 민감한 문항에 대한 응답률과 정확도를 높이는 데 유리하다.\n데이터를 수집할 때 일기 형식을 고려할 것\n민감한 행동에 관한 질문은 응답자가 의도적으로 왜곡된 정보를 제공하거나, 정확히 기억하지 못해 잘못된 응답을 하는 경우가 많다. 특히 음주, 흡연, 약물 사용, 성행동 등과 같이 사회적 평가와 관련된 주제는 응답자가 자신의 행동을 축소하거나 과장해서 보고할 가능성이 높다. 이로 인해 설문 데이터의 정확성과 신뢰도가 떨어질 수 있으며, 결과 해석에도 한계가 생긴다.\n이러한 문제를 보완하기 위한 방법으로, 응답자에게 일기 형식의 기록을 요청하는 전략이 효과적이다. 예를 들어, 음주 행동을 조사할 때 설문 이전 일정 기간 동안 매일 마신 술의 종류와 양을 직접 기록하도록 안내하면, 단 한 번의 회상에 의존하는 방식보다 훨씬 더 정밀하고 신뢰도 높은 정보를 확보할 수 있다. 이러한 방식은 특히 반복적이고 일상적인 행동을 측정할 때 유용하며, 민감한 주제에 대한 응답의 왜곡 가능성을 줄이는 데도 큰 도움이 된다.\n설문지의 끝부분에 응답 민감 항목에 대한 추가 문항 구성\n응답자가 설문에 답하는 과정에서 어떤 문항을 특히 민감하게 느꼈는지를 확인하는 것은, 전체 응답의 신뢰도를 높이고 설문 도구의 완성도를 향상시키는 데 도움이 된다. 민감한 질문에 대해 응답자가 실제로 어떤 수준의 불편함을 느꼈는지를 파악함으로써, 향후 설문 설계 시 해당 문항의 표현 방식이나 위치, 방식 등을 개선할 수 있는 근거 자료를 확보할 수 있다.\n예를 들어, 설문 마지막에 “설문에서 가장 대답하기 어려웠던 질문은 무엇이었습니까?”, “이 설문에서 불편함을 느낀 부분이 있다면 자유롭게 적어주세요”와 같은 질문을 추가하면, 응답자의 심리적 반응에 대한 피드백을 직접 수집할 수 있다. 이러한 정보를 바탕으로 민감도를 높이는 요소를 조정하거나 응답 환경을 개선하면, 이후 조사에서는 보다 솔직하고 신뢰성 높은 응답을 이끌어낼 수 있다.\n검증 데이터를 수집할 것\n설문조사를 통해 수집된 응답이 실제 행동이나 사실을 정확하게 반영하고 있는지를 확인하기 위해, 추가적인 검증 데이터를 함께 수집하는 것이 중요하다. 응답자는 무심코 기억을 잘못하거나, 사회적 압박으로 인해 의도적으로 사실을 왜곡할 수도 있기 때문에, 단지 설문 응답에만 의존하는 것은 데이터의 신뢰성을 저하시킬 위험이 있다.\n예를 들어, 음주 습관에 관한 조사를 실시할 때 응답자가 보고한 음주 빈도나 종류와 실제 구입 내역(예: 술 구매 영수증)을 비교하면, 응답의 정확성을 검토할 수 있다. 이러한 검증 절차는 조사 과정에서 발생할 수 있는 응답 오류나 왜곡을 줄이는 데 도움이 되며, 전체 조사 결과의 신뢰도와 타당성을 한층 높여준다. 경우에 따라 기록 자료, 관찰 데이터, 제3자의 보고 등 다양한 방식으로 보완 자료를 확보하는 것도 효과적인 전략이 될 수 있다.\n\n\n4. 태도문항\n태도의 대상을 명확하게 지정할 것\n태도 문항을 설계할 때는 응답자가 어떤 대상이나 상황에 대해 자신의 의견을 표현하는지 명확하게 이해할 수 있도록 질문을 구체적으로 구성하는 것이 중요하다. 질문이 모호하거나 지나치게 포괄적이면, 응답자는 각기 다른 해석을 바탕으로 답하게 되어 응답 간 비교가 어려워지고, 설문 결과의 타당성이 낮아질 수 있다.\n예를 들어, “환경 보호 정책에 대해 어떻게 생각하십니까?”라는 질문은 환경 문제 전반에 대한 광범위한 해석을 유도할 수 있어 응답자의 인식 차이를 제대로 반영하기 어렵다. 대신, “정부의 일회용 플라스틱 사용 제한 정책에 대해 어떻게 생각하십니까?”처럼 구체적인 정책이나 행동을 명시하면, 응답자는 보다 명확한 판단 기준에 따라 의견을 표현할 수 있게 된다. 이는 태도 측정의 정확도와 응답 해석의 일관성을 높이는 데 도움이 된다.\n이중 질문을 피할 것\n이중 질문(double-barreled question)은 하나의 문항에 두 개 이상의 서로 다른 개념이나 주제가 포함되어 있어, 응답자가 하나의 명확한 답변을 하기 어렵게 만드는 질문을 의미한다. 이러한 질문은 응답자의 의도를 정확히 파악하기 어렵게 만들고, 결과적으로 데이터의 신뢰성과 해석력을 떨어뜨리는 요인이 된다. 특히 응답자가 두 개의 주제에 대해 상반된 태도를 가지고 있을 경우, 어떤 부분에 대해 긍정 또는 부정을 표시했는지 명확히 알 수 없게 된다.\n예를 들어, “귀하는 정부의 환경 보호 정책과 경제 성장 전략을 지지하십니까?”라는 질문은 환경 보호 정책에는 찬성하지만 경제 성장 전략에는 반대하는 응답자에게 혼란을 줄 수 있다. 이 경우, “귀하는 정부의 환경 보호 정책을 지지하십니까?”와 “귀하는 정부의 경제 성장 전략을 지지하십니까?”처럼 각 개념을 별도의 문항으로 분리해 묻는 것이 바람직하다. 이를 통해 응답자는 각 항목에 대해 보다 정확하게 자신의 입장을 표현할 수 있으며, 조사자는 더 신뢰도 높은 자료를 확보할 수 있다.\n필요하다면 개별 문항을 사용하여 태도의 강도를 측정할 것\n태도 조사를 보다 정밀하게 수행하려면, 단순히 찬성이나 반대 여부만을 묻는 이분법적인 질문 방식에서 벗어나, 응답자의 태도 강도를 세분화하여 측정할 수 있는 척도를 사용하는 것이 중요하다. 찬성과 반대는 방향만을 나타낼 뿐, 그 정도나 확신의 수준을 반영하지 못하므로, 응답자의 입장을 충분히 해석하는 데 한계가 있을 수 있다.\n예를 들어, “귀하는 이 정책에 찬성하십니까?”처럼 단순하게 묻는 대신, “귀하는 이 정책을 얼마나 강하게 찬성 또는 반대하십니까?”와 같은 질문을 제시하고, ‘매우 찬성’, ‘약간 찬성’, ‘중립’, ‘약간 반대’, ‘매우 반대’ 등으로 구성된 리커트 척도를 활용하면, 응답자의 태도를 더 구체적이고 정량적으로 파악할 수 있다. 이러한 방식은 정책 수립이나 여론 분석 시 보다 섬세한 해석과 비교를 가능하게 하며, 태도 연구의 질을 높이는 데 기여한다.\n주요 정보를 놓칠 가능성이 있는 경우 제외하고 양극 항목을 사용할 것\n양극 항목(bipolar scale)은 응답자가 긍정적인 평가와 부정적인 평가 중에서 자신의 생각에 가장 가까운 선택지를 고를 수 있도록, 양쪽 극단을 모두 포함하는 응답 척도를 의미한다. 이러한 척도는 응답자의 태도 강도와 방향을 함께 파악할 수 있다는 점에서 유용하며, 태도의 분포나 변화 정도를 정밀하게 분석하는 데 적합하다. 예를 들어, “귀하는 정부의 환경 보호 정책을 얼마나 효과적이라고 생각하십니까?”라는 질문에 대해 ‘매우 효과적이다’, ‘효과적이다’, ‘보통이다’, ‘효과적이지 않다’, ’전혀 효과적이지 않다’처럼 긍정에서 부정까지 연속적인 범위를 제공하면, 다양한 응답자의 입장을 반영할 수 있다.\n그러나 모든 상황에서 양극 항목이 반드시 적절한 것은 아니다. 특정 문항에서는 응답자가 명확하게 긍정적인 방향에서만 평가하도록 유도하거나, 중립적인 선택지가 불필요한 경우도 있을 수 있다. 반대로, 중립적 입장을 유지하고자 하는 응답자를 위해 ‘보통이다’ 혹은 ’잘 모르겠다’와 같은 응답 옵션을 제공하는 것이 적절할 때도 있다. 따라서 척도의 구성은 조사 목적과 문항의 성격에 따라 신중하게 조정되어야 하며, 응답자가 자신의 의견을 왜곡 없이 표현할 수 있도록 돕는 방향으로 설계해야 한다.\n응답 보기는 응답 가능한 선택을 모두 포함하고 있어야 한다.\n설문에서 제시하는 응답 옵션은 응답자의 선택에 직접적인 영향을 미치기 때문에, 옵션의 구성은 질문의 신뢰성과 타당성을 좌우하는 중요한 요소가 된다. 응답자가 자신의 상황에 맞는 선택지를 찾지 못하면, 임의로 가장 가까운 항목을 선택하거나 응답을 포기할 수 있으며, 이는 데이터의 왜곡이나 누락으로 이어질 수 있다. 따라서 응답 항목은 가능한 현실적인 대안을 충분히 반영하도록 신중하게 설계되어야 한다.\n예를 들어, “귀하는 주로 어떤 교통수단을 이용합니까?”라는 질문에 ’자전거 — 버스 — 지하철 — 자동차’만 제시된다면, 실제로 기차나 도보를 주로 이용하는 사람은 자신의 경험과 일치하는 응답을 선택하기 어렵다. 이런 경우를 대비해 가능한 교통수단을 포괄적으로 포함하거나, “기타 (직접 입력)” 항목을 함께 제공하면, 응답자의 다양성을 반영하고 데이터의 완전성을 높이는 데 도움이 된다. 응답 옵션 설계 시 응답자의 입장에서 선택 가능성을 충분히 고려하는 것이 바람직하다.\n시간에 따른 변화를 측정할 때는 매번 동일한 질문을 사용할 것\n시간의 흐름에 따른 태도 변화를 측정할 때는, 동일한 질문에 대해 시점만 다르게 반복해 물어야 비교가 가능한 데이터가 수집된다. 질문의 문구나 표현 방식이 달라지면, 응답자는 각 질문을 다르게 해석할 수 있고, 그로 인해 실제 태도 변화가 아닌 질문 형식의 차이로 인해 응답 결과가 달라질 수 있다. 이는 설문조사의 신뢰성과 해석 가능성을 떨어뜨리는 주요한 원인이 된다.\n예를 들어, “지난해 환경 보호 정책에 대한 귀하의 태도는 어땠습니까?”와 “올해 환경 보호 정책에 대한 귀하의 태도는 어떻습니까?”는 내용상 유사해 보이지만, 질문의 표현 방식이나 문장 구조가 달라지면 응답자가 미묘하게 다르게 이해할 수 있다. 따라서 시계열 조사나 반복 조사에서는 질문의 문구를 가능한 한 동일하게 유지해야 하며, 시점 외에는 다른 요소를 바꾸지 않도록 주의함으로써 신뢰도 높은 비교가 가능하도록 해야 한다.\n일반적인 질문을 구체적인 질문보다 먼저 제시할 것\n응답자의 인식을 보다 체계적으로 평가하기 위해서는 질문의 흐름을 논리적이고 점진적으로 구성하는 것이 중요하다. 특히 태도나 가치관과 관련된 질문에서는, 일반적인 개념에 대한 인식을 먼저 확인한 후 점차 구체적인 사례나 정책으로 좁혀가는 방식이 효과적이다. 이러한 구조는 응답자가 자신의 생각을 정리하고, 보다 일관된 기준에 따라 질문에 답하도록 유도하는 데 도움이 된다.\n예를 들어, “귀하는 환경 보호가 중요하다고 생각하십니까?”와 같은 일반적인 질문을 먼저 제시한 다음, “귀하는 정부의 일회용 플라스틱 제한 정책에 대해 어떻게 생각하십니까?”처럼 특정 정책이나 사례에 대한 질문으로 이어지면, 응답자는 자연스러운 사고 흐름에 따라 자신의 입장을 보다 명확히 표현할 수 있다. 이와 같은 질문 배치는 응답의 품질을 높이고, 조사 결과의 해석력과 타당성을 향상시키는 데 기여한다.\n여러 항목을 질문을 할 때는 가장 덜 인기 있는 항목부터 시작할 것\n응답자가 설문에서 여러 주제에 대해 연속적으로 답변해야 하는 경우, 항목의 제시 순서가 응답의 집중도와 내용에 영향을 줄 수 있다. 특히 항목 간 중요도나 관심도의 차이가 클 경우, 응답자는 처음 제시된 항목에만 주의를 집중하고 뒤에 나오는 항목에는 상대적으로 덜 신중하게 답할 수 있다. 이를 방지하고 전체 응답 품질을 높이기 위해서는 일반적으로 중요도가 낮거나 덜 알려진 항목을 먼저 제시하는 것이 바람직하다.\n예를 들어, “귀하는 환경 보호 정책 중 어떤 부분을 가장 중요하다고 생각하십니까?”라는 질문에서 응답 항목을 ‘재생 에너지 확대 — 일회용 플라스틱 금지 — 자동차 배출가스 규제’ 순서로 제시하면, 첫 번째 항목이 자동적으로 주목을 받을 가능성이 높다. 반면, 중요도가 상대적으로 낮은 항목부터 제시하면 응답자는 전체 선택지를 더 고르게 검토하게 되며, 이는 응답의 신중함과 정확도를 높이는 데 도움이 된다. 이처럼 항목 배열 순서 또한 설문 설계에서 고려해야 할 중요한 요소 중 하나이다.\n태도를 측정할 때는 폐쇄형 질문을 사용할 것\n설문조사에서 응답자의 의견을 보다 체계적으로 분석하고자 할 때는, 응답 내용을 정량화할 수 있는 폐쇄형 질문을 사용하는 것이 효과적이다. 폐쇄형 질문은 미리 정해진 선택지 중 하나를 고르게 함으로써, 응답 결과를 수치로 변환하고 통계적으로 처리하기 용이하다. 특히 응답 항목이 표준화되어 있기 때문에, 여러 응답자의 답변을 비교하거나 집단 간 차이를 분석하는 데 유리하다.\n예를 들어, “귀하는 정부의 환경 보호 정책을 어떻게 생각하십니까?”라는 질문을 자유롭게 서술하도록 하면 개별 응답의 맥락은 풍부하게 파악할 수 있지만, 이를 정리하고 분석하는 데 시간이 많이 들고 주관적인 해석이 개입될 수 있다. 반면, 같은 질문에 대해 ’매우 찬성 — 찬성 — 중립 — 반대 — 매우 반대’와 같은 리커트 척도를 활용한 폐쇄형 질문으로 구성하면, 응답을 정량적으로 수집할 수 있어 전체적인 태도 경향을 쉽게 파악하고, 연구 결과의 일관성과 분석 효율성을 높일 수 있다.\n5점/7점 척도의 응답 척도를 사용하고, 모든 척도에 라벨을 붙일 것\n태도 문항의 응답을 정량적으로 측정하기 위해서는 적절한 척도를 사용하는 것이 중요하며, 일반적으로 5점 또는 7점 척도가 가장 널리 사용된다. 이러한 척도는 응답자의 세부적인 입장을 반영할 수 있을 만큼 충분한 선택지를 제공하면서도, 혼란을 주지 않을 정도로 간결한 구조를 유지할 수 있다는 장점이 있다. 척도의 간격이 너무 적으면 태도의 강도를 충분히 반영하지 못하고, 너무 많으면 응답자가 선택에 어려움을 느낄 수 있다.\n또한 각 점수에 의미 있는 라벨(설명)을 명확히 제시하는 것이 매우 중요하다. 예를 들어, 1점에는 ‘전혀 동의하지 않음’, 7점에는 ’매우 동의함’이라는 식으로 각 점수의 의미를 구체적으로 표현하면, 응답자는 자신의 태도에 가장 가까운 선택지를 정확히 고를 수 있다. 라벨이 불분명하거나 생략될 경우 응답자가 척도를 자의적으로 해석할 가능성이 있어, 응답 일관성과 데이터의 신뢰도가 떨어질 수 있다. 따라서 태도 측정 척도는 구조뿐만 아니라 설명의 명확성까지 고려하여 설계해야 한다.\n인기 없는 문항을 먼저 배치하지만, 설문의 성격에 따라 조정이 필요\n설문지의 문항 배치 순서는 응답자의 몰입도와 응답 품질에 영향을 미칠 수 있기 때문에 전략적으로 설계할 필요가 있다. 일반적으로는 덜 인기 있거나 상대적으로 중요도가 낮은 문항을 앞부분에 배치하고, 응답자의 관심이 높은 인기 있는 문항은 후반부에 배치하는 것이 권장된다. 이러한 구성은 응답자가 점차 문항에 익숙해지도록 돕고, 후반부의 주요 문항에 더 집중할 수 있는 기반을 마련해 준다.\n그러나 반드시 모든 경우에 이 원칙이 적용되는 것은 아니다. 설문 초반에 흥미를 유도하거나 설문 참여를 이어가게 하기 위해서는 응답자가 관심을 가질 만한 쉬운 문항을 앞부분에 제시하는 것이 더 효과적일 수 있다. 특히 반드시 응답이 필요한 핵심 문항이 있는 경우에는, 응답 이탈 전에 해당 문항을 먼저 배치하는 것이 유리하다. 따라서 최적의 전략은 응답자의 흥미와 집중도를 고려하여, 초반에는 가볍고 직관적인 문항으로 시작하고, 점차 세부적이고 분석적인 문항으로 자연스럽게 전환하는 방식이 바람직하다.\n모든 대안을 한눈에 볼 수 있는 경우에만 순위 매기기를 사용할 것\n응답자가 여러 대안 중에서 우선순위를 판단하거나 순위를 매겨야 하는 경우에는, 모든 선택지를 한눈에 비교할 수 있도록 제시하는 것이 중요하다. 응답자가 전체 대안을 동시에 검토하지 못하면, 일부 항목에 편중되거나 앞에 나온 항목에 영향을 받아 일관되지 않은 응답을 할 가능성이 높다. 따라서 우선순위 판단을 요구하는 질문에서는 시각적으로 명확한 배열과 균형 있는 제시 방식이 응답의 신뢰도를 높이는 데 도움이 된다.\n만약 모든 대안을 한 화면이나 페이지에서 제시하기 어렵거나, 항목 간 비교가 복잡한 경우에는 쌍대 비교(pairwise comparison) 방식을 활용하는 것이 적절하다. 이 방법은 각 대안을 두 개씩 짝지어 반복적으로 비교하게 하여, 응답자가 상대적인 중요도를 더 명확하게 판단할 수 있도록 돕는다. 쌍대 비교는 특히 항목 수가 많거나 선택 기준이 모호할 때 유용한 접근 방식이며, 보다 정밀한 선호도 분석에 적합하다.\n다중 선택 문항보다는 개별문항 리커트 척도 평가방법 사용\n설문에서 여러 항목에 대해 응답자의 선호나 평가를 수집할 때, 단순히 체크박스 방식으로 해당 항목을 모두 선택하게 하는 방식은 응답의 정밀도가 떨어질 수 있다. 체크박스는 특정 항목의 선택 여부만을 파악할 수 있어, 어떤 항목을 더 선호하거나 중요하게 여기는지를 비교하기 어렵고, 항목 간 상대적인 차이를 반영하기에도 한계가 있다.\n이러한 한계를 보완하기 위해, 각 항목에 대해 개별적으로 평점을 부여하도록 하는 방식이 더 적절하다. 예를 들어, ‘관심도’, ‘중요도’, ‘만족도’ 등을 기준으로 각 항목에 1점에서 5점 또는 7점 사이의 점수를 매기도록 하면, 응답자는 자신의 인식을 보다 세밀하게 표현할 수 있다. 이 방식은 항목 간 우선순위나 차이를 정량적으로 분석할 수 있도록 해주며, 더 정밀하고 유용한 데이터를 수집하는 데 도움이 된다.\n\n\n\nchapter 3. 질문지 평가\n질문 평가는 크게 두 가지 핵심 요소로 구성된다. 첫 번째는 질문이 응답자에게 얼마나 잘 이해되는지, 그리고 답변하는 데 얼마나 어려움이 따르는지를 평가하는 것이다. 이러한 요소들은 질문이 응답 과정에서 인지적 부담을 얼마나 유발하는지를 판단하는 기준이 되며, 궁극적으로 측정의 질에 직접적인 영향을 미친다. 설문 조사 연구자들은 주로 사람들이 질문을 읽고 해석하며 답변하는 전 과정을 관찰하거나, 응답자의 반응을 정성적으로 분석함으로써 질문의 이해도, 기억 회상의 난이도, 표현 방식의 모호성 등을 평가한다. 기본적인 가정은, 명확하게 이해되고 인지적으로 부담이 적은 질문일수록 응답자가 더 정확하게 반응할 수 있으며, 그 결과 측정 오류가 줄어든다는 것이다.\n두 번째 요소는 질문을 통해 수집된 응답이 실제로 우리가 측정하고자 하는 개념을 얼마나 정확하게 반영하고 있는지를 평가하는 과정이다. 이는 곧 측정 오류를 직접적으로 추정하는 작업으로, 설문 문항이 연구자가 의도한 개념과 일치하는지를 검증하는 것이다. 이를 위해 설문 방법론자들은 설문 응답 결과를 외부의 객관적인 자료와 비교하거나 동일한 질문을 반복 측정하는 방식을 활용한다. 외부 자료와의 비교는 주로 타당성(construct validity)이나 응답 편향(social desirability bias)과 같은 문제를 분석하는 데 사용되며, 측정 반복은 신뢰성(reliability)이나 응답 분산(response variability)을 평가하는 데 활용된다. 이처럼 질문 평가는 설문조사의 품질을 높이기 위한 핵심 과정으로, 설계 단계부터 체계적으로 고려되어야 한다.\n모든 설문 질문은 다음의 세 가지 명확한 기준을 충족해야 한다.\n내용 content 기준: 적절한 정보를 수집하는지를 평가하는 기준\n\n분석 관점: 문항이 연구 목적에 부합하는 데이터를 수집하는지 확인해야 한다. 이를 위해 분석가나 주제 전문가의 평가가 필요하다.\n응답 가능성: 질문이 실제로 응답자가 답할 수 있는 내용인지 평가해야 한다. 응답자의 답변이 신뢰할 만한 수준의 정확성을 가지려면, 질문을 이해하고 일관되게 응답할 수 있어야 한다. 이를 확인하기 위해 포커스 그룹 및 인지 인터뷰가 유용하다. 초점 집단은 응답자들이 특정 주제에 대해 어떤 지식을 가지고 있는지 평가하는 데 효과적이며, 인지 인터뷰는 특정 질문이 일관되게 응답될 수 있는지 분석하는 데 활용된다.\n\n인지 cognitive 기준: 응답자가 문항을 올바르게 이해하고 답변할 수 있는지를 평가하는 기준\n\n포커스 그룹 인터뷰: 이해되지 않는 용어나 모호한 개념을 파악할 수 있다.\n전문가 검토: 응답자가 답변하기 어려운 문항을 사전 탐지할 수 있다.\n행동 코딩: 사전조사 과정에서 응답자가 혼란을 느끼는 문항을 식별할 수 있다.\n\n사용성 usability 기준: 설문 도구가 실제 조사에서 원활하게 활용될 수 있는지를 평가하는 기준\n설문조사를 본격적으로 실시하기 전에, 문항의 타당성과 사용성을 점검하기 위한 절차로 전문가 검토를 실시하는 것이 중요하다. 전문가들은 설문 문항이 응답자에게 혼란을 주거나, 면접관이 질문을 제시하고 응답을 수집하는 과정에서 문제가 될 수 있는 요소들을 사전에 식별할 수 있다. 이를 통해 문항의 표현 방식, 내용의 민감성, 질문 흐름상의 논리적 오류 등을 조정하여, 설문이 원활하게 작동할 수 있도록 한다.\n특히 자기기입식(self-administered) 설문에서는 사용성 테스트가 핵심적인 검토 절차 중 하나로 간주된다. 실험실 환경이나 실제 조사 환경에서 테스트를 진행하면, 응답자가 문항을 이해하고 처리하는 방식, 특정 문항에서 혼란을 겪는 지점, 응답 오류가 발생하는 가능성 등을 직접 관찰할 수 있다. 컴퓨터 기반 설문에서는 응답자의 키 입력 시간, 문항 간 이동 패턴, 비정상적 입력 감지 등 다양한 디지털 데이터를 활용하여 사용성을 평가할 수 있다. 물론 실험실 테스트가 현장의 복잡한 변수들을 모두 반영하지는 못하지만, 실제 조사가 진행되기 전에 발생 가능한 문제를 미리 식별하고 개선할 수 있는 매우 유용한 방법이다.\n\n1. 전문가 검토\n설문조사에서 질문의 질을 평가하는 과정에서, 주제 전문가와 설문지 설계 전문가의 사전 검토는 필수적인 절차로 간주된다. 설문지 설계 전문가는 질문의 형식, 흐름, 문장 구조 등 기술적인 요소를 정교하게 다듬는 역할을 수행하며, 조사 도구의 완성도를 높이는 데 핵심적인 기여를 한다. 동시에, 설문이 연구의 분석 목적과 이론적 틀에 부합하는 내용을 포함하고 있는지를 검토하기 위해서는 해당 분야의 주제 전문가가 반드시 참여해야 한다. 주제 전문가의 관점은 수집되는 데이터가 실제 연구 목적에 맞게 활용 가능한지를 판단하는 데 중요한 기준이 된다.\n전문가 검토는 문항의 표현 방식, 질문 순서, 응답 보기의 구성, 조사원을 위한 지침, 설문 흐름 규칙 등 설계 전반에 걸쳐 진행되며, 응답자가 질문을 명확히 이해하고 일관된 방식으로 응답할 수 있도록 돕는 데 중점을 둔다. 이러한 검토는 설문 도구의 오류를 사전에 발견하고 수정함으로써 전체 조사 과정의 신뢰성과 타당성을 높이는 데 기여한다. 다만, 전문가의 평가만으로는 응답자의 실제 이해 과정과 반응을 완전히 예측하기 어렵기 때문에, 이후 논의될 추가 평가 방법들과 병행하여 실시해야 한다. 따라서 전문가 검토는 설문 설계 초기 단계에서 문항의 질을 향상시키는 핵심적인 절차이며, 이를 체계적으로 수행하기 위한 다양한 도구와 방법론이 꾸준히 개발되고 있다.\n\n\n2. 포커스 그룹 focus group\n포커스 그룹은 설문 도구를 개발하기 전, 특정 주제에 대한 응답자들의 인식과 반응을 깊이 있게 이해하기 위해 활용되는 질적 조사 방법이다. 일반적으로 6~10명 정도의 소규모 집단을 구성하여, 중재자의 진행 하에 자유로운 토론이 이루어진다. 참여자들은 서로의 의견을 듣고 상호작용하면서 자신의 생각을 확장하게 되며, 이를 통해 연구자는 특정 주제에 대해 사람들이 어떻게 생각하고 있는지, 어떤 표현이 혼란을 줄 수 있는지, 문항의 구조나 개념이 응답자들에게 어떻게 해석되는지를 구체적으로 파악할 수 있다.\n포커스 그룹은 특히 설문 설계의 초기 단계에서 유용한 도구로, 응답자들이 주제에 대해 가진 배경지식, 용어에 대한 이해, 관심의 정도 등을 사전에 탐색하는 데 효과적이다. 이러한 과정을 통해 설문 문항을 보다 응답자 친화적으로 조정할 수 있으며, 향후 조사에서 발생할 수 있는 오류를 줄이는 데 도움이 된다. 다만 포커스 그룹의 결과는 참여자의 수가 적고 비확률 표본이기 때문에 일반화에 한계가 있으며, 이를 보완하기 위해 개별 심층 인터뷰나 파일럿 테스트 등 다른 평가 방법과 병행해 사용하는 것이 바람직하다.\n\n(1) 포커스 그룹의 역할과 장점\n응답자의 지식 및 인식 구조 분석\n연구자는 포커스 그룹을 통해 응답자들이 조사 주제에 대해 어떤 지식과 경험을 가지고 있는지, 그리고 그 정보를 어떤 방식으로 인식하고 조직화하는지를 심층적으로 탐색할 수 있다. 예를 들어, 건강보험 관련 조사를 준비할 경우, 포커스 그룹 논의를 통해 응답자들이 알고 있는 보험 유형은 무엇인지, 각 유형을 어떻게 이해하고 있는지를 파악할 수 있으며, 이 과정에서 용어나 개념에 대한 오해 가능성도 확인할 수 있다. 또한 응답자들이 어떤 요소를 중요하게 여기고, 어떤 요소는 상대적으로 관심이 적은지 구분하는 데에도 유용하여, 설문 문항 설계 시 초점을 어디에 둘지 결정하는 데 실질적인 도움을 준다.\n용어 및 개념 정교화\n포커스 그룹은 설문에서 사용될 단어나 표현이 응답자에게 어떻게 받아들여지는지를 분석하는 데 매우 효과적인 도구이다. 연구자는 이 과정을 통해 응답자들이 일상적으로 사용하는 용어나 개념, 특정 표현에 대한 선호도와 이해 수준을 직접 관찰할 수 있다. 이를 바탕으로 설문 문항의 언어를 보다 명확하고 직관적으로 조정할 수 있으며, 응답자의 혼란이나 오해를 최소화하는 데 기여한다. 결과적으로 포커스 그룹은 응답자 중심의 설문 설계를 가능하게 하여, 전체 조사의 품질과 응답의 신뢰도를 높이는 데 중요한 역할을 한다.\n질문 설계의 현실성 확보\n포커스 그룹은 연구자가 특정 개념에 대해 응답자가 어떻게 인식하고, 어떤 경험과 사고 과정을 바탕으로 답변을 구성하는지를 이해하는 데 유용한 방법이다. 이러한 과정을 통해 연구자는 설문 문항이 단순히 이론적 개념에 기반한 것이 아니라, 실제 응답자의 경험과 감각에 맞닿아 있는지를 점검할 수 있다. 결과적으로, 설문 내용이 응답자의 현실과 더 잘 연결되도록 문항을 조정함으로써, 보다 신뢰할 수 있고 의미 있는 데이터를 수집할 수 있는 기반이 마련된다.\n\n\n(2) 포커스 그룹의 운영 방식\n포커스 그룹은 일반적으로 조용하고 집중할 수 있는 환경에서 진행되며, 연구진이 관찰할 수 있도록 단방향 거울이 설치되거나 오디오·비디오 녹화가 이루어지기도 한다. 경우에 따라 필기 노트나 토론 기록지를 활용해 토론 내용을 분석하고, 녹화된 영상은 편집하여 주요 내용을 요약하는 데 쓰이기도 한다.\n포커스 그룹에서 중재자는 개방적이고 편안한 분위기를 조성하면서도 논의가 연구 주제에서 이탈하지 않도록 유도하는 핵심적인 역할을 맡는다. 중재자는 다음과 같은 원칙을 지켜야 한다. 첫째, 모든 참가자가 적극적으로 참여할 수 있도록 유도하고, 내성적인 참가자의 의견도 이끌어내며 특정인의 발언 독점을 방지해야 한다. 둘째, 참가자들의 의견을 경청하고, 새로운 아이디어가 등장하면 다른 참가자의 반응을 관찰하며 조율한다. 셋째, 미리 준비된 질문지(script)에만 의존하지 않고, 연구 목표를 고려한 유연하고 자연스러운 방식으로 논의를 이끌어야 한다.\n또한 포커스 그룹은 조사 주제에 따라 유사한 특성을 가진 응답자들로 구분해 운영할 수 있다. 예를 들어, 취업 관련 조사의 경우 정규직 근로자와 시간제 근로자를 나누어 별도로 논의하거나, 다양한 하위 집단을 가진 조사에서는 각 집단에 대한 포커스 그룹을 따로 구성하여 보다 정교한 분석이 가능하도록 한다.\n\n\n(3) 포커스 그룹의 한계점\n대표성 부족\n포커스 그룹은 소규모의 참여자들이 특정 주제에 대해 깊이 있는 의견을 나누는 데 중점을 두기 때문에, 참가자들이 전체 조사 모집단을 통계적으로 대표한다고 보기 어렵다. 이로 인해 포커스 그룹 결과만으로 일반적인 결론이나 전체 인구에 대한 추정을 도출하는 데는 한계가 있다. 따라서 포커스 그룹은 전반적인 경향을 파악하거나 문항 개발을 위한 탐색적 도구로 활용하고, 일반화 가능한 결론은 보다 대표성 있는 양적 조사에서 도출해야 한다.\n설문 문항 평가의 한계\n포커스 그룹은 응답자의 전반적인 인식과 개념에 대한 이해를 도모하는 데 유용하지만, 특정 설문 문항의 문구가 얼마나 명확한지, 또는 응답자가 질문을 어떻게 해석하고 답변을 구성하는지에 대한 정밀한 분석에는 적합하지 않다. 이러한 문항 수준의 타당성과 이해도 평가는 집단 토론보다는 개별 면담을 통해 수행되는 1:1 인지 면접(cognitive testing) 방식이 훨씬 효과적이다. 인지 면접은 응답자의 사고 과정을 추적할 수 있어, 문항 해석의 차이나 오해의 가능성을 더 정확하게 파악할 수 있다.\n결과의 신뢰성 문제\n포커스 그룹에서 얻어진 정보는 주로 질적 데이터에 기반하기 때문에, 표준화된 분석이 어렵고, 연구자의 해석에 따라 결과가 달라질 가능성이 크다. 같은 토론 내용을 두고도 해석이 다양할 수 있어, 객관성과 일관성을 확보하는 데 한계가 있다. 또한, 포커스 그룹은 참가자 구성이나 토론 흐름에 따라 결과가 달라질 수 있기 때문에 동일한 연구를 반복하더라도 일관된 결과를 얻기 어렵다는 재현성(replicability)의 문제도 수반된다. 이러한 이유로, 포커스 그룹은 설문 설계 초기의 보조적 수단으로 활용하되, 주요 결론 도출의 단독 근거로 삼기에는 신중함이 요구된다.\n\n\n\n3. 인지 인터뷰 cognitive interviewing\n인지 인터뷰(Cognitive Interviewing)는 설문 문항이 응답자에게 어떻게 이해되고 해석되는지를 분석하기 위한 질적 조사 기법이다. 이 방법은 설문 문항이 의도한 바와 실제 응답자가 받아들이는 방식 간의 불일치를 확인하고, 문항의 개선 가능성을 탐색하는 데 중점을 둔다.\n인지 인터뷰의 핵심 방법론은 프로토콜 분석(protocol analysis)으로, 응답자가 문제를 해결하는 과정을 소리 내어 말하도록 요청하는 방식이다. 이는 원래 심리학에서 문제 해결 과정을 분석하기 위해 개발된 기법이며, 이후 설문 문항의 인지적 처리 과정을 이해하는 데 활용되었다. 대표적인 기법은 다음과 같다.\n\n동시적 사고 구술 (Concurrent Think-Alouds): 응답자가 질문을 읽고 답하는 과정에서 자신의 사고 과정을 실시간으로 소리 내어 설명하도록 유도한다. 회고적 사고 구술 (Retrospective Think-Alouds): 응답자가 질문에 답변한 이후, 그 응답을 도출하는 과정에서 어떤 생각을 했는지를 설명하게 한다.\n자신감 평가 (Confidence Rating): 응답자가 자신의 답변에 대해 어느 정도 확신이 있는지를 표현하게 한다.\n패러프레이징 (Paraphrasing): 응답자가 주어진 질문을 자신의 말로 다시 진술하게 함으로써, 문항 이해도를 평가한다.\n정의 제공 (Definition Probing): 질문 내 핵심 용어나 개념에 대해 응답자가 자신의 이해를 설명하도록 유도한다.\n추가 질문 (Probing Questions): 응답자가 어떤 전략이나 기준으로 답을 결정했는지 파악하기 위한 후속 질문을 제시한다.\n\n이러한 기법들은 설문 문항이 응답자에게 의도한 바대로 정확하고 일관되게 전달되는지를 점검하는 데 활용되며, 설문지의 신뢰성과 타당성을 높이는 데 매우 유용한 자료를 제공한다.\n인지 인터뷰는 일정한 절차가 고정된 정형화된 방식이라기보다는, 연구 목적과 상황에 따라 유연하게 설계되는 질적 접근 방법이다. 일반적으로 응답자는 일정한 보상을 제공받는 자원봉사자 형태로 모집되며, 인터뷰는 조사 연구자뿐만 아니라 인지 심리학자, 설문 조사 방법론 전문가 등 다양한 배경을 가진 면접자가 수행할 수 있다.\n연구 기관이나 조사 목적에 따라 인지 인터뷰의 운영 방식은 다소 상이할 수 있다. 예를 들어, 어떤 기관은 면접자가 사전 구성된 후속 질문(probes)을 중심으로 진행하는 반면, 다른 기관은 응답자의 자연스러운 사고 구술(thinking aloud)에 보다 초점을 맞추어 인터뷰를 이끈다. 또한, 인터뷰 결과는 비디오 녹화, 오디오 녹음, 필기 노트 등 다양한 방식으로 기록되며, 분석 방법 역시 연구 환경이나 목적에 따라 차이가 있다.\n이처럼 인지 인터뷰는 설문 문항의 인지적 적합성(cognitive validity)을 평가하는 핵심 도구로 자리 잡고 있으며, 특히 문항이 응답자에게 어떻게 해석되고 반응되는지를 실제로 검증할 수 있다는 점에서 중요한 역할을 한다. 다만, 이러한 기법의 결과를 보다 체계적으로 활용하고, 연구자 간 해석의 일관성을 확보하기 위해서는 방법론적 표준화와 추가적인 검증 연구가 지속적으로 요구된다.\n\n\n4. 사전조사와 행동코딩\n사전조사(pretests)는 본격적인 설문 조사에 앞서 소규모로 실시되는 리허설로, 설문 도구의 설계와 데이터 수집 절차의 적절성을 검토하는 데 목적이 있다. 주로 소수의 응답자와 면접관을 대상으로 진행되며, 설문 문항이 의도대로 작동하는지, 응답 과정에서 어떤 문제가 발생하는지를 사전에 점검할 수 있는 표준적인 절차로 널리 활용된다.\n사전조사에서 활용되는 대표적인 방법 중 하나는 면접관 피드백 수집이다. 사전조사에 참여한 응답자들을 대상으로 포커스 그룹을 운영하거나 인터뷰를 실시해, 응답자가 설문을 수행하면서 느낀 불편이나 이해하기 어려웠던 부분을 수집한다. 이 과정에서 면접관 역시 설문 절차를 단순화하거나 문항 구성을 개선할 수 있는 실질적인 제안을 제공할 수 있다.\n또한, 사전조사를 통해 수집된 응답 데이터를 정량적으로 분석하여 문항의 품질을 평가할 수도 있다. 연구자들은 누락된 응답의 비율, 논리적 불일치, 범위 밖의 응답 등 다양한 지표를 통해 설문 문항의 문제점을 진단하고, 분산이 지나치게 낮아 유의미한 구분이 어려운 문항은 삭제하거나 수정할 수 있다. 이러한 정량 분석은 문항의 타당성과 응답 가능성을 높이는 데 중요한 역할을 한다.\n행동 코딩(behavior coding)은 면접관과 응답자 간의 상호작용을 분석하여 설문 문항의 이해도와 전달 효과를 평가하는 기법이다. 면접 과정을 녹음한 후, 면접관이 질문을 반복하거나 수정하는 빈도, 응답자의 주저나 ‘모름’ 응답 비율 등을 코드화하여 분석함으로써 문항이 실제로 어떻게 작동하는지를 평가할 수 있다. 이는 응답자가 문항을 이해하는 데 어려움이 있었는지를 간접적으로 파악하는 데 매우 유용하다.\n실제로 행동 코딩의 신뢰도는 비교적 높은 편이다. 예를 들어, 동일한 질문을 서로 다른 조사팀이 평가했을 때, 행동 코딩 결과 간의 상관계수가 0.75에서 0.90에 이를 정도로 일관성이 있다는 연구도 있다. 이는 설문 문항의 구조적 문제가 특정 응답자나 면접관에 국한되지 않고 보다 보편적으로 발생할 수 있음을 보여주는 결과이며, 설문 도구를 수정하는 근거로 활용될 수 있다.\n\n\n\nchapter 4. 질문지 평가 통계적 접근\n설문 조사 연구에서 ’품질’을 측정할 때 사용되는 용어는 아직까지 완전히 표준화되어 있지 않지만, 전통적으로 두 가지 주요 접근법이 존재한다. 하나는 심리측정(psychometrics)의 관점이고, 다른 하나는 표본통계(survey statistics)의 관점이다.\n첫 번째 접근법인 심리측정은 개별 응답자의 반응에 초점을 맞추며, 문항이 측정하고자 하는 개념을 얼마나 정확하게 반영하는지를 뜻하는 타당성(validity)과, 동일한 조건에서 일관된 결과를 도출할 수 있는지를 의미하는 신뢰성(reliability)이라는 개념을 중심으로 품질을 평가한다.\n반면, 두 번째 접근법은 설문 응답을 종합하여 전체 집단을 대표하는 통계치를 도출하는 과정에서의 편향(bias)과 분산(variance)을 중심으로 설문 품질을 평가한다. 이는 각 응답의 정합성보다는 추정치의 정확성과 일관성에 중점을 둔 통계적 접근이라고 할 수 있다.\n이처럼 설문 조사 품질을 평가하는 개념은 분석의 초점에 따라 서로 다른 용어와 기준이 사용되며, 각각의 접근은 조사 목적과 분석 수준에 따라 적절히 선택되어야 한다.\n\n1. 타당도\n타당도는 설문 문항이 측정하고자 하는 구성 개념(construct)을 얼마나 정확하게 반영하고 있는지를 나타내는 개념이다. 즉, 응답자가 문항에 답한 결과가 연구자가 의도한 속성이나 태도를 실제로 잘 측정하고 있는지를 평가한다.\n타당도는 설문 조사에서 매우 중요한 개념으로, 다음과 같은 주요 유형으로 나뉜다: - 내용 타당도 (Content Validity): 문항이 측정하고자 하는 개념의 모든 중요한 측면을 충분히 포괄하고 있는지를 평가한다. 전문가 평가 등을 통해 판단된다. - 기준 타당도 (Criterion Validity): 설문 결과가 외부의 객관적 기준(예: 행동, 실적 등)과 얼마나 잘 부합하는지를 본다. 예컨대, 직무 적성 검사의 점수가 실제 업무 성과와 상관관계가 높다면 기준 타당도가 높다고 할 수 있다. - 구성 타당도 (Construct Validity): 문항이 이론적으로 정의된 개념을 제대로 측정하고 있는지를 평가한다. 주로 요인분석, 상관분석 등을 통해 통계적으로 검증된다.\n따라서, 타당도는 단순히 문항이 잘 만들어졌는지를 넘어서, 이 문항이 과연 내가 측정하고자 하는 ’그것’을 제대로 측정하고 있는가에 대한 핵심적인 질문이라 할 수 있다.\n\n(1) 내용 content 타당도\n전문가 검토(Expert Review)\n내용(content) 타당도는 설문 문항이 측정하고자 하는 개념의 모든 핵심 요소를 포괄하고 있는지를 평가하는 개념이다. 가장 일반적인 평가 방법은 전문가 검토(Expert Review)로, 해당 분야의 전문가들이 설문 문항을 검토하여 설문이 연구 목적과 개념을 충분히 반영하고 있는지를 판단한다. 이 과정에서 전문가들은 문항이 적절한지를 평가하고, 누락된 내용이나 불필요한 요소가 있는지를 지적하며 수정 사항을 제안한다. 이러한 절차를 통해 문항이 연구 주제를 충분히 반영하고 있는지를 확인하고, 설문의 내용 타당도를 향상시킬 수 있다.\n내용 타당도 지수 content validity index, CVI\n내용 타당도 지수(Content Validity Index, CVI)는 설문 문항이 측정하고자 하는 연구 개념을 얼마나 잘 반영하는지를 정량적으로 평가하는 지표이다. 이 방법은 전문가 패널이 각 문항의 적절성을 4점 척도(예: 1 = 적절하지 않음, 2 = 다소 적절하지 않음, 3 = 적절함, 4 = 매우 적절함)로 평가한 후, 문항별 또는 전체 문항의 평균 점수를 산출하는 방식으로 진행된다. 일반적으로 CVI 값이 0.80 이상이면 해당 문항 또는 설문 전체가 높은 내용 타당도를 가진 것으로 간주한다. 이 지수는 설문지의 내용 타당성을 수치적으로 나타내어 문항의 적절성 여부를 보다 명확하게 판단할 수 있도록 도와준다.\n내용 타당도 비율 content validity ratio, CVR\nLawshe(1975)가 제안한 정량적 평가 방법으로, 설문 문항이 연구 개념 측정에 필수적인지를 전문가 집단을 통해 판단하는 데 사용된다. 이 방법에서는 각 전문가에게 특정 문항이 “필수적이다”, “유용하지만 필수는 아니다”, “불필요하다” 중 하나로 평가하게 한 뒤, “필수적이다”라고 응답한 전문가의 수를 기준으로 다음과 같은 공식을 사용해 CVR을 계산한다:\n\\(\\text{CVR} = \\frac{n_{e} - (N/2)}{N/2}\\)\n여기서 \\(n_e\\) 는 해당 문항을 “필수적”이라고 평가한 전문가 수, N은 전체 전문가 수이다.\n계산된 CVR 값은 Lawshe가 제시한 임계값 이상이어야 해당 문항이 내용 타당성을 갖춘 것으로 인정된다. 예를 들어, 전문가 수가 8명일 경우 CVR 값이 최소 0.75 이상이어야 하며, 전문가 수가 10명이라면 0.62 이상이어야 한다. 이 방식은 문항의 타당성을 보다 객관적으로 판단할 수 있는 기준을 제공한다.\n문항 분석\n파일럿 테스트(사전조사)를 통해 응답자들이 각 문항에 어떻게 반응하는지를 체계적으로 분석하여, 설문지의 품질을 향상시키기 위한 과정이다. 이 분석은 문항의 이해도, 반응 일관성, 응답 분포 등을 종합적으로 고려하여 수행된다.\n주로 문항 간 상관관계, 문항-전체 점수 상관(item-total correlation), 응답 분산 등을 통해 특정 문항이 전체 개념 측정에 기여하는 정도를 평가하며, 그 결과 타당도가 낮거나 혼란을 유발하는 문항은 제거하거나 수정된다. 이러한 분석은 문항의 내용 타당도를 간접적으로 평가하고, 설문 구성의 논리성과 신뢰성을 강화하는 데 매우 유용하다.\n포커스 그룹\n설문 문항을 본격적으로 적용하기 전에, 잠재적 응답자들을 소규모 집단(보통 6~10명)으로 모아 자유롭게 토론하도록 하여 문항의 적절성과 이해도를 평가하는 질적 조사 방법이다. 이 과정에서 연구자는 응답자들이 특정 문항을 어떻게 해석하는지, 문항에 대해 어떤 반응을 보이는지, 혼란을 느끼거나 불편함을 표현하는 부분이 있는지를 파악할 수 있다.\n포커스 그룹을 통해 수집한 피드백은 설문 문항을 보다 명확하고 응답자 친화적으로 다듬는 데 중요한 근거가 된다. 이 방법은 특히 민감하거나 복잡한 개념을 다루는 설문에서, 질문이 응답자들에게 의도한 의미로 전달되는지를 사전에 확인하고 개선할 수 있는 유용한 절차로 활용된다.\n\n\n(2) 기준 criterion 타당도\n기준 타당도(Criterion Validity)는 설문 문항이 측정하고자 하는 속성이 실제 외부의 객관적 기준과 얼마나 밀접하게 관련되어 있는지를 평가하는 개념이다. 다시 말해, 특정 문항이나 척도가 이미 검증된 외부 지표(기준 변수)와 어느 정도의 상관관계를 가지는지를 검토함으로써 그 타당성을 확인한다.\n예를 들어, 스트레스 수준을 측정하는 설문이 있다면, 해당 설문 점수와 생리적 지표(예: 코르티솔 수치) 혹은 임상 평가 결과와의 상관관계를 분석하여 기준 타당도를 평가할 수 있다. 기준 타당도는 일반적으로 동시 타당도(concurrent validity)와 예측 타당도(predictive validity)로 나뉘는데, 전자는 현재 시점에서의 상관성을, 후자는 미래 행동이나 성과를 예측하는 능력을 평가한다.\n동시 concurrent 타당성\n설문 도구가 현재 시점에서 존재하는 외부 기준 변수(객관적인 지표)와 얼마나 강한 상관관계를 가지는지를 평가하는 방식이다. 이는 측정 도구의 정확성을 검증하는 방법 중 하나로, 설문 결과와 이미 검증된 외부 평가나 측정 지표를 동시에 비교함으로써 평가된다.\n예를 들어, 새로 개발한 우울증 척도가 기존에 널리 사용되며 검증된 우울증 척도와 높은 상관관계를 보인다면, 해당 척도는 동시 타당성이 높다고 볼 수 있다. 또 다른 예로, 한 기업에서 직무 만족도를 측정하는 설문을 실시했을 때, 그 결과가 현재의 상사 평가나 인사 고과 점수와 밀접하게 연관되어 있다면, 해당 설문 도구는 높은 동시 타당성을 가진 것으로 평가된다. 이처럼 동시 타당성은 기존의 신뢰할 수 있는 기준과 비교하여 새로운 측정 도구의 신뢰성을 간접적으로 검증하는 데 유용하다.\n예측 predictive 타당성\n설문을 통해 측정된 값이 미래의 특정 결과나 행동을 얼마나 정확하게 예측하는지를 평가하는 방식이다. 이는 설문 도구가 단순히 현재 상태를 반영하는 것을 넘어, 장기적으로 의미 있는 결과와 연결될 수 있는지를 검증하는 데 사용된다.\n예를 들어, 대학 입학 시험 점수가 입학 이후의 평균 학점(GPA)과 유의미한 상관관계를 보인다면, 해당 시험은 학업 성취도를 예측하는 데 타당한 도구로 간주된다. 마찬가지로, 어떤 기업에서 실시한 성격 검사가 향후 직원의 업무 성과와 높은 연관성을 나타낸다면, 해당 검사는 예측 타당성이 높은 것으로 평가할 수 있다. 이처럼 예측 타당성은 설문 결과가 미래 행동이나 성과를 얼마나 잘 설명하고 예측할 수 있는지를 판단하는 데 필수적인 기준이 된다.\n상관 분석\n기준 타당성을 평가할 때 가장 기본적으로 사용되는 통계 기법이다. 이는 설문 문항이 측정하고자 하는 개념이 실제 외부 기준 변수와 얼마나 일관되게 연관되어 있는지를 확인하는 데 초점을 둔다.\n가장 일반적으로 사용되는 지표는 피어슨 상관계수(Pearson’s r)로, 두 변수 간의 선형 상관관계를 수치로 표현한다. 예를 들어, 직무 만족도 설문 점수와 실제 이직률 사이에 r 값이 -0.70이라면, 만족도가 높을수록 이직률이 낮다는 강한 음의 상관관계를 의미한다. 일반적으로 r 값이 0.70 이상이면 강한 상관관계, 0.50 이상이면 중간 정도의 상관관계로 해석된다. 이러한 분석 결과는 설문 도구가 외부의 객관적인 기준과 얼마나 밀접하게 연결되어 있는지를 판단하는 데 유용한 근거를 제공한다.\n회귀 분석\n설문 문항의 기준 타당성, 특히 예측 타당성을 평가하는 데 매우 유용한 통계 기법이다. 이 방법에서는 외부의 기준 변수를 종속 변수로, 설문 문항 또는 척도 점수를 독립 변수로 설정하여, 설문 결과가 기준 변수를 얼마나 잘 설명하거나 예측할 수 있는지를 분석한다.\n회귀 분석의 결과에서 회귀 계수(coefficient)가 통계적으로 유의미하고, 결정계수(R²) 값이 높을수록 설문 문항의 예측력이 뛰어나다고 판단할 수 있다. 예를 들어, 직무 만족도 설문 점수가 향후 이직 여부(0=재직, 1=이직)를 유의하게 예측할 수 있다면, 해당 설문의 기준 타당성이 높다고 평가된다. 이처럼 회귀 분석은 설문 도구의 실제적인 예측 가능성과 설명력을 계량적으로 평가하는 데 핵심적인 역할을 한다.\nROC 곡선 Receiver Operating Characteristic Curve\n설문 문항이나 척도가 기준 변수에 따라 이분형 결과(예: 질병 유무, 합격 여부)를 얼마나 잘 분류할 수 있는지를 평가하는 데 사용된다. 이 방법은 특히 진단 도구나 예측 척도의 기준 타당성을 평가할 때 널리 활용된다.\nROC 곡선은 민감도(sensitivity)와 특이도(1-specificity)의 관계를 시각화한 곡선이며, 이 곡선 아래 면적인 AUC (Area Under the Curve) 값을 통해 분류 성능을 정량적으로 평가할 수 있다.\n\nAUC = 0.5이면 무작위 추측 수준과 동일한 성능을 의미하고,\nAUC ≥ 0.80이면 높은 분류 정확도, 즉 높은 기준 타당성을 의미한다.\n\n예를 들어, 우울증 설문 점수를 기준으로 우울증 진단 여부를 정확히 예측할 수 있는지를 ROC 분석을 통해 확인할 수 있으며, AUC가 0.85라면 해당 설문은 높은 기준 타당성을 가진다고 평가할 수 있다.\n카이제곱 검정\n설문 결과와 기준 변수가 범주형(categorical) 데이터일 때, 이들 간의 독립성 또는 연관성을 검증함으로써 기준 타당성(criterion validity)을 평가하는 데 사용된다. 예를 들어, 설문에서 수집한 직무 만족도 수준(예: “만족”, “보통”, “불만족”)과 실제 직무 성과 평가 결과(예: “우수”, “보통”, “미흡”)가 서로 독립적인지 아닌지를 카이제곱 검정을 통해 확인할 수 있다.\n\n검정 결과가 유의미하면(p &lt; .05), 두 변수 간 통계적으로 유의한 연관성이 있다고 판단하며,\n이는 해당 설문 문항이 기준 변수(실제 성과)를 잘 설명하거나 예측한다는 점에서 기준 타당성이 높다고 평가할 수 있다.\n\n이 검정은 범주형 변수 간의 관계를 분석하는 데 적합하며, 특히 명목 척도 또는 순서 척도 문항의 기준 타당성을 검증할 때 효과적이다.\n\n\n(3) 구성 construction 타당성\n설문 문항이 이론적으로 정의된 추상적인 개념(구성 개념, construct)을 얼마나 잘 측정하고 있는지를 평가하는 개념이다. 각 설문 측정은 개념적으로 무한한 실험(trials)의 집합 중 하나의 실현으로 볼 수 있다. 즉, 특정 응답자가 특정 질문에 대해 제공하는 응답은 이론적으로 가능한 수많은 측정 중 하나에 해당하며, 동일한 구성 개념에 대한 측정이 반복적으로 이루어진다고 가정할 수 있다.\n이러한 맥락에서 구성 타당성은 단순히 문항의 적절성 여부를 넘어서, 측정하고자 하는 개념과 설문 문항 간의 이론적 연관성을 종합적으로 검증하는 절차를 포함한다. 예를 들어, 설문을 통해 ’사회적 불안’을 측정하고자 할 때, 그 문항들이 실제로 사회적 불안이라는 심리적 구성 개념을 반영하는지를 검증하는 것이 바로 구성 타당성을 평가하는 과정이다.\n구성 타당성을 확보하기 위해서는 통계적 분석뿐 아니라 이론적 정당성과 관련 문헌 기반의 논리적 타당성 확보가 병행되어야 하며, 일반적으로 요인분석, 수렴 및 판별 타당도 분석, 가설 검증 등을 통해 평가된다.\n단일 문항 구성 타당성 척도\n\\(\\mu_{i}\\): \\(i\\) 번째 응답자의 실제 구성 개념 값\n\\(Y_{it}\\): \\(i\\) 번째 응답자가 \\(t\\) 번째 실험에서 제공한 응답\n\\(\\epsilon_{it}\\): \\(i\\) 번째 응답자의 실제 개념 값과 응답 값 간의 편차\n설문에서 특정 구성 개념, \\(\\mu_{i}\\)에 대한 질문이 \\(i\\) 번째 응답자에게 주어졌을 때 응답자는 실제 값 \\(\\mu_{i}\\)를 그대로 응답하는 것이 아니라 다음과 같이 표현되는 측정값 \\(Y_{it} = \\mu_{i} + \\epsilon_{it}\\)을 응답하게 된다. 편차 \\(\\epsilon_{it}\\)는 측정 과정에서 발생할 수 있는 변동성이다.\n구성 타당성은 위 식에서 나타낸 측정값, \\(Y_{it}\\)과 실제 개념 값, \\(\\mu_{i}\\) 간의 상관계수로 측정된다. 구성 타당성은 0.0에서 1.0 사이의 숫자로 표현되며, 값이 클수록 타당성이 높음을 의미한다.0.7 이상이면 높은 타당성을 가진다고 판단한다.\n구성 타당도: \\(\\text{Validity}(Y) = \\frac{\\sum_{i,t}(Y_{it} - \\overline{Y})(\\mu_{i} - \\overline{\\mu})}{\\sqrt{\\sum_{i,t}(Y_{it} - \\overline{Y})^{2}\\sum_{i}(\\mu_{i} - \\overline{\\mu})^{2}}}\\)\n응답자의 실제 값 \\(\\mu_{i}\\)은 기록 데이터나 외부 자료를 활용하거나 응답자에게 다른 질문을 통하여 얻은 참 값을 사용한다. 일반적으로 설문 문항의 응답을 외부 자료와 비교하는 것이 이상적이다.\n외부 기준이 없는 경우\n외부 기준이 없는 경우에도 구성 타당성을 평가할 수 있는 방법들이 있다. 첫째, 설문 응답 간 상관관계 분석은 이론적으로 관련이 있어야 할 문항들이 실제로 높은 상관관계를 보이는지를 통해 설문이 제대로 구성 개념을 측정하고 있는지를 평가한다. 예를 들어, “나는 경제가 앞으로 좋아질 것이라고 생각한다”는 문항이 “나는 소비 지출을 늘릴 계획이다”와 높은 상관을 보인다면, 해당 문항들이 동일한 구성 개념(예: 경제에 대한 낙관적 기대)을 측정하고 있을 가능성이 높다고 판단할 수 있다.\n둘째, 집단 간 비교를 통해 구성 타당성을 간접적으로 검증할 수 있다. 측정하고자 하는 구성 개념이 실제로 다른 집단 간에 응답 차이가 유의미하게 나타나는지를 평가하는 방식이다. 예를 들어, 고소득층과 저소득층은 경제 전망에 대해 다른 인식을 가질 가능성이 있으므로, 두 집단의 응답 차이를 비교하여 문항이 해당 구성 개념을 잘 반영하고 있는지 검토할 수 있다.\n셋째, 대체 문항 또는 자료 수집 방법 간 비교도 활용된다. 동일한 응답자를 대상으로 문항 형식을 약간 바꾸거나 다른 방식으로 자료를 수집한 후 결과를 비교함으로써 일관성 있는 측정이 이루어졌는지를 평가한다. 예컨대, “향후 12개월 동안 경제 상황이 나아질 것이라고 생각하십니까?”와 “향후 12개월 동안 본인의 가계 경제가 나아질 것이라고 생각하십니까?”라는 문항을 비교하면, 유사하지만 범위가 다른 문항에 대한 반응을 분석함으로써 구성 개념이 일관되게 측정되고 있는지를 점검할 수 있다.\n하위 문항 구성 타당도\n\n\n\n\n\n탐색적 요인 분석에서는 요인 적재값이 0.40 이상이면 수용 가능, 0.70 이상이면 강한 구성 타당성을 가진다고 평가한다.\n확인적 요인 분석에서는 구조 방정식 모델의 적합도 지수를 활용하여 구성 타당성을 평가한다. (수용가능 지표) 비교 적합 지수 (Comparative Fit Index)-≥ 0.90 (권장: ≥ 0.95), 표준화된 SRMR (Standardized Root Mean Square Residual)-≤ 0.08 (이상적: ≤ 0.05), 튜커-루이스 지수(Tucker-Lewis Index)-≥ 0.90 (권장: ≥ 0.95)\n\n\n(4) 응답 편향 response bias\n설문 문항과 관련된 오류와 관련하여 가장 흔히 발생하는 개념적 혼란 중 하나는 ”타당성”과 ”편향” 간의 관계이다. 타당성은 응답과 실제 값 간의 상관관계의 함수이다. 따라서, 타당성은 개별 응답의 속성으로 정의될 수 있다. 그러나, 응답이 실제 값에서 체계적으로 벗어나는 경우는 어떻게 될까?\n예를 들어, 사회적으로 바람직하지 않은 특성에 대한 응답이 체계적으로 과소 응답되는 현상을 보인다. 이러한 체계적인 과소 응답과 실제 값 간의 상관관계를 반드시 감소시키는 것은 아니다. 예를 들어, 모든 응답자가 실제 체중보다 5파운드 적게 보고한다면, 응답 값과 실제 값 간의 상관관계는 1.0으로 유지될 것이다. 그러나, 이 경우 응답 값의 평균은 실제 값의 평균보다 5파운드 낮게 나타나는 편향이 발생하게 된다.\n스플릿-발롯 split-ballot 접근법을 활용한 편향 검증\n\n비교된 두 가지 질문, Sudman과 Bradburn(1982)\n술을 마시는 날, 보통 몇 잔을 마십니까? 1잔, 2잔, 3잔 이상?\n술을 마시는 날, 보통 몇 잔을 마십니까? 1-2잔, 3-4잔, 5-6잔, 7잔 이상?\n실험 절차 및 결과\n\n\n응답자들은 위 두 질문 중 무작위로 하나를 배정받아 응답했다.\n연구 결과, 질문 (b)에 응답한 사람들이 질문 (a)에 응답한 사람들보다 3잔 이상 마신다고 응답할 가능성이 훨씬 높았다.\n연구자들은 응답자들이 실제 음주량을 과소 응답하는 경향이 있다고 확신했다.\n이 가정을 바탕으로, 연구자들은 두 번째 질문(b)의 응답이 첫 번째 질문(a)보다 더 타당하다고 결론지었다.\n\n설문 통계에서의 편향\n\\(Y_{it} = \\mu_{i} + \\epsilon_{it}\\). 즉, 응답 값(\\(Y_{it}\\))은 실제 값(\\(\\mu_{i}\\))과 오차(\\(\\epsilon_{it}\\))의 합으로 구성된다. 만약 오차 항(\\(\\epsilon_{it}\\))이 체계적인 성분을 포함하고 있다면, 오차의 기대값이 0이 아닐 것이다.\n편향은 설문응답(\\(Y_{it}\\)) 기대값과 실제 값(\\(\\mu_{i}\\))이 다를 때 발생한다. 즉, 응답 값과 실제 값 간의 체계적인 편차가 존재할 때 편향이 발생한다.\n편향: \\(Bias(Y_{it}) = E_{i}\\left\\lbrack E_{t}(Y_{it}) - \\mu_{i} \\right\\rbrack\\)\n편향 추정치: \\(Bias(\\overline{Y}) = E_{t}\\left( \\frac{\\sum_{i}^{}Y_{it}}{N} - \\frac{\\sum_{i}^{}\\mu_{i}}{N} \\right)\\) 응답 값의 평균이 실제 값의 평균에 대한 편향된 추정치임을 의미한다. 첫 번째 항(\\(\\sum_{i}Y_{it}/N\\))은 모든 실험에서 응답 값의 기대값을 나타낸다. 편향은 응답 값의 평균이 실제 값의 평균과 체계적으로 다를 때 발생한다.\n타당성 평가와 편향 측정\n타당성 평가와 편향 측정의 유사성\n\n타당성 평가와 유사하게 연구자가 ”진실”에 대해 일정한 가정을 해야만 편향을 측정할 수 있음을 의미한다.\n기록 시스템을 활용해 설문 응답을 비교하는 경우, 연구자는 기록 데이터가 오류 없이 측정되었다는 가정을 하게 된다.\n\n타당성과 편향의 차이\n\n편향의 개념은 실제 값(\\(\\mu_{i}\\))의 존재 여부에 따라 정의된다.\n타당성은 \\(\\mu_{i}\\)의 변동성에 따라 달라진다.\n즉, 편향은 절대적인 기준(진실된 값)이 존재해야 정의될 수 있지만, 타당성은 응답 값이 얼마나 일관되게 측정되는지를 평가하는 개념이다.\n\n\n\n\n2. 신뢰도\n신뢰도란 반복적인 개념적 실험에서 응답 값의 변동성을 측정하는 개념이다. 신뢰도는 응답자가 일관되게 또는 안정적으로 응답하는지 여부를 평가한다. 따라서, 신뢰도는 응답 분산 성분을 기준으로 정의되며 이는 응답 값(\\(Y_{it}\\))의 오차 항(\\(\\epsilon_{it}\\)) 변동성을 포함한다.\n\\[Reliability(Y_{it}) = \\frac{E_{i}(\\mu_{i} - \\overline{\\mu})^{2}}{E_{i}(\\mu_{i} - \\overline{\\mu})^{2} + E_{i,t}(\\epsilon_{it} - \\overline{\\epsilon})^{2}}\\]\n\\[= \\frac{\\text{실제 값의 분산 (Variance of true values)}}{\\text{보고된 값의 분산 (Variance of reported values)}}\\]\n\n(1) 신뢰도의 해석\n응답 오차의 분산(\\(E_{i,t}(\\epsilon_{it} - \\overline{\\epsilon})^{2}\\))이 작으면 신뢰도 계수는 1에 가까워진다. 신뢰도는 어떤 설문 문항이나 측정 도구가 일관된 결과를 제공하는 정도를 나타낸다. 통계적으로는 전체 변동성 중에서 진짜 값(실제 측정하고자 하는 개념)의 분산이 차지하는 비율로 해석되며, 이 값이 1에 가까울수록 신뢰도가 높다는 의미이다.\n응답 오차의 분산이 작을수록, 즉 반복된 측정에서 응답 결과가 일정할수록 신뢰도는 높아진다. 이때 신뢰도 계수는 1에 가까워지며, 해당 측정이 안정적이고 일관된 정보를 제공한다고 판단할 수 있다. 반대로, 반복된 실험이나 조사에서 응답 결과가 크게 달라진다면, 이는 응답 오차의 분산이 크다는 뜻이며, 신뢰도 계수는 0에 가까워진다.\n결론적으로 신뢰도는 측정된 값의 총 변동성 중 실제 값이 차지하는 비율을 나타낸다. 신뢰도가 높다는 것은, 동일한 응답자에게 반복해서 질문했을 때 측정값이 재현 가능하고 변동이 적다는 의미이며, 이는 좋은 설문 도구의 핵심 요건 중 하나이다.\n\n\n(2) 개별 문항 신뢰도 측정\n참값을 알고 있을 때\n\\(R = \\frac{\\sigma_{T}^{2}}{\\sigma_{T}^{2} + \\sigma_{E}^{2}}\\), 여기서 \\(\\sigma_{T}^{2}\\)은 실제 값의 분산, \\(\\sigma_{E}^{2}\\)은 오차 분산, \\(\\sigma_{T}^{2} + \\sigma_{E}^{2}\\)은 총 분산이다. 이 식을 사용하려면 ”참값”을 직접 알아야 한다. 그러나, 현실에서는 참가자들의 ”실제 심리 상태”나 ”실제 능력”을 정확히 측정할 수 없기 때문에 현실에서는 참 값을 모르는 경우가 대부분이므로 계산할 수 없다.\n참값을 알지 못하는 경우\n같은 응답자를 대상으로 반복 면접을 실시하는 방식은 설문 문항의 신뢰도(일관성)를 평가하는 데 효과적인 방법이다. 이 접근법에서는 동일한 문항을 일정한 시간 간격을 두고 동일한 응답자에게 다시 제시하여, 두 응답 간의 차이를 분석함으로써 측정의 안정성을 평가한다.\n이러한 반복 측정이 유의미하려면 몇 가지 기본적인 가정이 전제되어야 한다.\n첫째, 응답자의 실제 속성 또는 태도(구성 개념)가 두 번의 면접 사이에서 변하지 않아야 한다. 즉, 측정 간에 실제 변화가 없어야 오차만 분석할 수 있다. 둘째, 조사 환경과 측정 조건이 동일해야 한다. 질문의 표현, 면접 방식, 응답 방식 등이 같아야 비교가 가능하다. 셋째, 기억 효과(memory effect)가 없어야 한다. 응답자가 첫 번째 면접 내용을 기억하고 두 번째에 영향을 받으면, 독립적인 응답이 되지 않기 때문이다.\n이러한 조건이 충족되면, 반복 면접을 통해 얻은 결과는 응답의 일관성과 측정 도구의 안정성을 신뢰성 있게 평가하는 데 사용될 수 있다.\n검사 재검사 신뢰도 Test-Retest Reliability\n같은 응답자에게 시간 간격을 두고 동일한 설문을 반복 측정하여 신뢰도를 평가한다. 두 번의 측정 값 간 상관관계를 계산하여 신뢰도를 추정한다.\n설문 도구의 시간에 따른 일관성을 평가하는 대표적인 방법이다. 동일한 응답자에게 동일한 문항을 시간 간격을 두고 두 번 제시하고, 그 두 번의 측정값 간의 상관계수를 계산함으로써 신뢰도를 추정한다.\n수식으로 표현하면 다음과 같다. \\(R = r(\\text{Time1, Time2})\\), 여기서 r은 첫 번째 측정값과 두 번째 측정값 간의 피어슨 상관계수를 의미한다.\n이 신뢰도가 높다는 것은, 시간의 흐름에도 불구하고 설문 문항이 응답자의 태도나 속성을 안정적으로 측정하고 있다는 의미이며, 일반적으로 r 0.70이면 수용 가능한 수준으로 본다. 단, 시간 간격이 너무 짧으면 기억 효과가 생기고, 너무 길면 실제 태도 변화가 반영될 수 있으므로 적절한 간격 설정이 중요하다.\n평행 검사 신뢰도 Parallel Forms Reliability\n동일한 개념을 측정하지만 문항의 구성이나 표현이 서로 다른 두 형태(Form A, Form B)의 설문지를 개발하여, 두 검사의 결과 간 상관관계를 통해 신뢰도를 평가하는 방법이다. 이 방법의 핵심은 두 검사가 내용적으로 동등하고 통계적으로 유사한 속성을 가져야 한다는 점입니다. 즉, A형과 B형이 서로 다른 문항을 사용하더라도, 측정하고자 하는 심리적·사회적 개념이 같아야 한다.\n\\(R = r(\\text{Form A}, \\text{Form B})\\)\n여기서 r은 동일한 응답자 집단이 두 형태의 설문에 응답했을 때의 점수 간 상관계수를 의미한다. 높은 상관계수는 두 형태 모두 일관된 측정 결과를 낸다는 것을 의미하며, 이는 설문 도구의 신뢰도를 뒷받침 한다. 이 방법은 기억 효과를 최소화할 수 있는 장점이 있지만, 문항을 두 세트로 개발해야 하므로 설계와 타당성 확보에 더 많은 노력이 필요하다.\n\n\n(3) 다수 하위 문항 신뢰도 측정\n다수의 하위 문항을 이용한 신뢰도 측정에서는 몇 가지 중요한 가정이 전제되어야 한다. 첫째, 모든 문항은 동일한 구성 개념을 반영하는 지표로서, 기대값이 동일해야 한다. 둘째, 모든 문항의 응답 편차는 일정해야 하며, 즉 단순 응답 분산이나 신뢰도가 일정해야 한다. 셋째, 각 문항의 측정값은 독립적이어야 하며, 하나의 문항에 대한 응답이 다른 문항의 응답에 영향을 미쳐서는 안 된다.\n이러한 가정을 만족하는 경우, 동일한 개념을 측정하는 여러 문항을 통해 신뢰도를 보다 정밀하게 평가할 수 있으며, 이는 문항 간의 내적 일관성(internal consistency)을 측정하는 데 중요한 역할을 한다.\n대표적인 신뢰도 측정 방법으로는 크론바흐 알파(Cronbach’s Alpha)가 있으며, 이는 다중 문항 척도의 신뢰도를 평가하는 데 널리 사용된다. 일반적으로 알파 값이 0.70 이상이면 수용 가능한 수준의 신뢰도를 가진 것으로 간주되며, 값이 높을수록 문항 간 일관성이 강하다는 것을 의미한다.\n\\(\\alpha = \\frac{k}{k - 1}\\left( 1 - \\frac{\\sum\\sigma_{Y_{i}}^{2}}{\\sigma_{Y}^{2}} \\right)\\), 여기서 \\(k\\)는 문항 개수, \\(\\sigma_{Y_{i}}^{2}\\)은 각 문항의 분산, \\(\\sigma_{Y}^{2}\\)은 총 점수의 분산이다.\n\\(\\alpha = \\frac{k\\overline{r}}{1 + (k - 1)\\overline{r}}\\), \\(\\overline{r}\\)은 문항 간 평균 상관관계이다.\n문항 간 상관이 높거나 문항 수가 많을수록 신뢰도는 증가하는 경향이 있다. 이는 크론바흐 알파 계수의 수학적 특성이며, 문항 간 일관성이 클수록 전체 척도의 내적 일관성 또한 높아진다는 것을 의미한다.\n알파 값이 0.90을 초과하는 경우, 신뢰도가 지나치게 높은 것으로 간주되어 주의가 필요하다. 이는 문항들이 거의 동일한 내용을 반복하여 측정하고 있을 가능성을 시사하며, 중복성이 높아져 설문지의 효율성을 저해할 수 있다. 이런 경우 일부 문항을 제거함으로써 척도를 간결하고 효율적으로 다듬는 것이 바람직하다.\n알파 값이 0.70에서 0.90 사이에 위치할 경우, 일반적으로 적절한 신뢰도를 가진 것으로 평가된다. 특히 심리학이나 교육학 분야에서는 0.80 이상이면 양호한 신뢰도를 나타내는 것으로 간주한다.\n다만, 문항 수가 적은 경우에는 알파 계수가 낮게 나올 수 있으며, 이는 척도의 신뢰도가 낮기 때문이 아니라 문항 수가 적어서 나타나는 현상일 수 있다. 반대로, 문항 수가 많더라도 문항 간 상관관계가 낮다면 신뢰도는 높아지지 않는다. 따라서 신뢰도를 높이기 위해서는 단순히 문항 수를 늘리는 것보다는 문항 간의 개념적 일관성과 상관관계를 높이는 것이 더 효과적이다."
  },
  {
    "objectID": "notes/survey/psm.html",
    "href": "notes/survey/psm.html",
    "title": "조사방법론. 8. 성향점수매칭(Propensity Score Matching, PSM)",
    "section": "",
    "text": "1. psm 개념\n\n(1) 성향점수매칭(Propensity Score Matching, PSM) 배경\n정책 평가, 경제학 및 사회과학 분야에서 개입(처치, treatment)의 효과를 정량적으로 추정하는 것은 인과적 관계를 분석하는 데 있어 핵심적인 연구 과제이다. 이상적인 실험 설계는 무작위 배정을 통해 처치 그룹과 통제 그룹 간의 동질성을 보장하는 것이지만, 현실적으로 RCT를 수행하는 것은 윤리적·경제적·실무적 제약으로 인해 어려운 경우가 많다.\n성향점수매칭은 관찰 연구에서 사용되는 준실험적 통계 기법으로, 무작위 대조군 연구의 수행이 불가능하거나 비현실적인 경우, 특정 처치나 중재의 효과를 추정하는 데 활용된다. PSM의 핵심 목표는 처치 그룹과 통제 그룹 간의 비교 가능성을 높여, 처치 효과의 인과적 추론을 보다 엄밀하게 수행하는 것이다. 이를 위해, 처치 여부에 영향을 미칠 수 있는 변수들(즉, 교란 변수)에 의해 발생하는 선택 편향을 최소화한다.\nPSM은 개체가 처치를 받을 확률을 성향 점수로 추정한 후, 해당 점수가 유사한 개체들 간의 매칭을 수행하여 처치 효과를 분석한다. 이를 통해, 관찰 연구에서도 실험 연구와 유사한 조건을 조성함으로써 처치 효과가 결과 변수(예: 질병 발생, 사망률 등)에 미치는 영향을 보다 신뢰성 있게 평가할 수 있도록 한다. 다만, PSM은 관찰된 변수에 의한 선택 편향만을 조정할 수 있으며, 관찰되지 않은 변수에 의한 편향은 여전히 문제로 남을 수 있다는 점에서 한계를 가진다.\nPSM의 핵심은 성별, 나이 등과 같은 관측된 특성을 기반으로 특정 개체가 치료 또는 중재를 받을 확률인 성향 점수를 추정하는 것이다. 이를 계산하기 위해 일반적으로 로지스틱 회귀분석이 활용되며, 개체별 성향 점수가 산출되면, 해당 점수를 기준으로 치료를 받은 집단과 받지 않은 집단 간의 매칭이 이루어진다. 이러한 과정은 두 집단 간의 관찰된 차이를 최소화하여 보다 유사한 비교군을 형성하고, 이후 치료나 중재가 결과 변수에 미치는 인과적 효과를 보다 정확하게 평가할 수 있도록 한다.\n\n\n(2) PSM 원리\nPSM은 개입을 받은 개체(처치 그룹)와 개입을 받지 않은 개체(통제 그룹)를 유사한 특성을 가진 개체끼리 매칭하여 비교하는 방법이다. 하지만, 모든 변수의 값이 동일한 개체를 찾는 것은 현실적으로 불가능하므로 성향점수라는 개념을 활용한다.\n성향점수는 특정 개체가 개입을 받을 확률을 관찰된 변수의 함수로 추정한 값을 의미한다. 즉, \\(P(D = 1|X)\\), 여기서 \\(D = 1\\)은 개입을 받은 여부, \\(X\\)는 관찰된 개체의 특성 벡터를 의미한다.\n각 개체의 성향 점수를 계산한 후, 성향 점수가 유사한 처치 그룹과 통제 그룹의 개체를 짝지어 매칭한다. 이후, 매칭된 개체들 간의 평균 결과 차이를 비교하여 개입 효과를 추정한다.\n\n\n(3) 주요 가정\nPSM이 올바른 인과적 추론을 수행하기 위해서는 다음 중요한 가정이 필요하다.\n조건부 독립성 가정\n개입 여부는 관찰된 변수 \\(X\\)를 통제하면 결과와 독립적이어야 한다. 즉, \\(Y(1),Y(0)\\bot D|X\\) 여기서, \\(Y(1)\\)과 \\(Y(0)\\)은 각각 개입을 받은 경우와 받지 않은 경우의 잠재적 결과이다. 이 가정이 성립해야 개입 여부가 결과에 미치는 순수한 효과를 확인할 수 있다. 만약 관찰되지 않은 요인이 개입과 결과를 동시에 결정한다면, PSM은 편향된 결과를 초래할 수 있다.\n공통 지원 영역\n처치 그룹과 통제 그룹이 비교 가능한 영역에서 존재해야 한다. 즉, \\(0 &lt; P(X) &lt; 1\\). 이 가정은 처치 그룹과 통제 그룹이 성향 점수의 분포에서 겹치는 부분이 있어야 한다는 의미를 가진다. 만약 성향 점수가 처치 그룹과 통제 그룹 간에 크게 차이가 난다면, 비교할 수 있는 개체가 없어 매칭이 어려워지고, 결과의 신뢰성이 낮아질 수 있다.따라서, 성향 점수 분포가 겹치는 공통 지원 영역을 설정하여 분석을 수행해야 한다.\n\n\n\n2. 성향점수매칭 절차\n\n(1) 샘플 선정 및 데이터 확보\nPSM을 수행하기 위해서는 먼저 측정할 개입 효과가 무엇인지 명확히 정의하고, 해당 효과를 분석할 수 있는 적절한 데이터 샘플을 선정해야 한다.\n효과 평가 변수 결정\n분석의 핵심은 특정 개입이 결과 변수에 미치는 영향을 추정하는 것이다. 예를 들어, 특정 약물 사용 여부가 환자의 질병 발생률이나 사망률에 미치는 영향을 평가하고자 할 경우, 약물 사용 여부를 기준으로 처치 그룹과 통제 그룹을 구분한 후 매칭을 수행해야 한다. 즉, 개입 변수를 명확히 정의한 후, 그 효과를 분석하기 위한 데이터 구조를 설계해야 한다.\n개입 그룹과 비교 가능한 통제 그룹 확보\n처치 그룹과 통제 그룹을 비교하기 위해 충분한 샘플 크기를 확보하는 것이 필수적이다. 데이터는 성향 점수를 예측할 수 있도록 개체의 주요 특성(예: 성별, 연령, 건강 상태, 사회경제적 요인 등)을 포함해야 한다. 가능하다면 동일한 데이터 출처를 활용하여 두 그룹 간 측정 편향을 최소화하는 것이 바람직하다.\n이러한 데이터 확보 과정을 거쳐야 PSM이 정확한 인과적 효과를 추정할 수 있으며, 분석 결과의 신뢰성을 높일 수 있다.\n\n\n(2) 성향 점수 추정\n로짓(Logit) 또는 프로빗(Probit) 모델을 사용하여 개입을 받을 확률(성향점수)을 추정한다. \\(P(Y = 1) = \\frac{e^{\\beta_{0} + \\beta_{1}X_{1} + \\ldots + \\beta_{k}X_{k}}}{1 + e^{\\beta_{0} + \\beta_{1}X_{1} + \\ldots + \\beta_{k}X_{k}}}\\), 여기서 \\(X_{i}\\)는 개체의 관찰된 특성이다.\n\n\n(3) 매칭 수행\n성향 점수가 가장 가까운 개체를 매칭하는 방법을 사용한다. 매칭 개념은 다음과 같다.\n\n각 개체에 대해 성향 점수, \\(P(X)\\)를 추정한다.\n처치군(개입 그룹)의 개체를 하나 선택한다.\n통제군(비개입 그룹)에서 해당 처치군 개체와 가장 가까운 성향 점수를 가진 개체를 찾아 매칭한다.\n모든 처치군 개체에 대해 동일한 과정을 반복한다.\n매칭이 완료되면 처치군과 통제군의 결과를 비교하여 개입 효과를 추정한다.\n\n\n\n\n\n\n공통지지영역(Region of Common Support)이란, 처치집단과 비교집단의 성향점수 분포가 실제로 겹치는 구간을 의미한다. 이 영역은, 주어진 공변량에 대해 처치를 받은 개체와 받지 않은 개체가 모두 존재하는 성향점수 범위를 말한다. 따라서 이 영역 내에서는 서로 유사한 특성을 지닌 처치자와 비처치자를 짝지을 수 있으며, 이를 통해 공정한 비교가 가능해진다.\n반면, 공통지지영역을 벗어난 사례들—즉 성향점수가 지나치게 낮거나 높아 비교 가능한 상대가 존재하지 않는 경우—에는 적절한 매칭이 불가능하므로, 분석 대상에서 제외하는 것이 일반적이다. 이러한 사례를 억지로 포함할 경우, 매칭의 정밀도가 떨어지고 처치효과 추정의 신뢰성 역시 손상될 수 있다.\n결과적으로, 공통지지영역의 존재 여부와 그 폭은 성향점수 매칭의 적용 가능성과 결과의 타당성을 판단하는 데 있어 핵심적인 요소라 할 수 있다.\n처치 그룹을 기준으로 비처치 그룹을 매칭하므로, 비처치 그룹의 크기가 충분히 커야 한다. 일반적으로 비처치 그룹의 크기는 처치 그룹의 최소 3배 이상이 되는 것이 바람직하다. 이는 매칭할 수 있는 후보 개체를 충분히 확보하여 매칭 품질을 높이고, 매칭되지 않는 처치군 개체의 발생을 최소화하기 위함이다.\n그러나, 반드시 3배수를 원칙으로 해야 하는 것은 아니며, 연구 설계와 데이터 가용성에 따라 다를 수 있다. 비처치 그룹의 크기가 작을 경우, 복원 매칭을 고려할 수도 있지만, 특정 개체에 대한 과도한 의존이 발생할 수 있으므로 주의해야 한다.\n최근접 이웃 매칭(Nearest Neighbor Matching)\n최근접 이웃 매칭은 처치 그룹(개입을 받은 그룹)의 개체를 기준으로, 비처치 그룹(개입을 받지 않은 그룹)에서 가장 가까운 성향 점수를 가진 개체를 매칭하는 방법이다. 이 방법은 PSM에서 가장 널리 사용되는 매칭 기법 중 하나로, 처치군과 통제군 간의 비교 가능성을 높이고, 선택 편향을 최소화하는데 유용하다.\n1:1 최근접 이웃 매칭 (One-to-One Nearest Neighbor Matching)\n\n처치군 개체 1개당 비처치군 개체 1개를 매칭하는 방식.\n성향 점수 차이가 가장 작은 개체를 찾아 매칭함.\n비복원 매칭: 한 개체는 한 번만 매칭됨 (더 공정한 비교 가능).\n복원 매칭: 한 개체가 여러 번 매칭될 수 있음 (표본크기 유지 가능하나 권고하지 않음).\n\n칼리퍼 매칭(Caliper Matching)\n처치군(개입 그룹)과 통제군(비개입 그룹) 간의 성향 점수 차이가 일정한 임계값(Threshold, 칼리퍼) 이하인 경우에만 매칭을 허용하는 방법이다. 즉, 처치군 개체와 가장 가까운 성향 점수를 가진 통제군 개체를 찾되, 성향 점수 차이가 너무 큰 경우에는 매칭을 허용하지 않는 방식이다. 칼리퍼 매칭은 잘못된 매칭을 방지하여 분석 결과의 신뢰도를 높이는 효과가 있지만, 매칭될 수 있는 개체 수가 줄어들 수 있다는 단점도 있다.\n처치군 개체 \\(i\\)의 성향 점수를 \\(P(X_{i})\\), 비처치군 개체 \\(j\\)의 성향 점수를 \\(P(X_{j})\\)라고 하면, \\(|P(X_{i}) - P(X_{j})| &lt; C\\), 여기서 C 는 칼리퍼 값(허용 임계값)이다. 만약 칼리퍼 값 = 0.05 → 성향 점수 차이가 0.05 이하인 경우에만 매칭 허용한다. 일반적으로 칼리퍼 값은 성향 점수 표준 편차의 0.2~0.5배 사이에서 설정하는 것이 권장된다.\n\n\n(4) 공변량 균형성 확인\n성향점수 매칭의 핵심은, 비실험자료에서도 마치 무작위 실험과 유사한 비교가 가능하도록, 처치집단과 비교집단 간의 공변량 분포를 정렬하는 데 있다. 따라서 매칭 이후에는 두 집단의 공변량 분포가 실제로 유사해졌는지를 검토하는 절차가 필수적으로 따라야 한다. 이러한 검토 과정을 공변량 균형성 확인(balance checking)이라 한다.\n매칭 이전의 관찰자료에서는 일반적으로 처치집단과 비교집단의 특성이 서로 다르다. 예컨대, 고소득자일수록 특정 프로그램에 참여할 가능성이 높다면, 단순한 평균 비교만으로는 프로그램의 순수한 효과를 분리해낼 수 없다. 이에 따라, 성향점수를 이용하여 유사한 특성을 가진 비교 대상을 짝지은 후, 그 효과를 비교하려는 것이 PSM의 기본 원리이다. 그러나 이 짝짓기 과정이 얼마나 효과적으로 작동했는지를 판단하지 않으면, 매칭의 신뢰성은 담보될 수 없다.\n공변량 균형성은 주로 다음과 같은 방법으로 평가된다. 첫째, 가장 널리 사용되는 지표는 표준화 평균 차이(Standardized Mean Difference, SMD)이다. 이는 두 집단의 각 공변량 평균 차이를 표준편차로 나눈 값으로, 표본 크기나 단위에 영향을 받지 않으므로 비교가 용이하다. SMD 값이 0에 가까울수록 두 집단의 분포가 유사하다는 것을 의미하며, 일반적으로 |SMD|가 0.1 미만이면 ‘양호한 균형’, 0.1 이상 0.2 미만이면 ‘수용 가능’, 0.2 이상이면 ’균형 부족’으로 해석된다.\n둘째, 공변량의 분포를 시각적으로 확인하는 방법도 중요하다. 예를 들어, 각 공변량에 대해 처치집단과 비교집단의 밀도곡선(density plot)이나 박스플롯(box plot)을 중첩하여 나타냄으로써, 두 집단 간 분포의 유사성을 직관적으로 확인할 수 있다. 또한 QQ-플롯이나 누적분포함수(CDF) 비교도 유용한 시각적 도구로 활용된다.\n한편, 일부 연구에서는 매칭 전후의 공변량에 대해 통계적 유의성 검정(t-test, χ² 검정 등)을 시행하기도 하나, 이는 권장되지 않는다. 그 이유는, 매칭 후 표본 크기가 작아짐에 따라 통계적 검정력이 급격히 낮아지거나 반대로, 표본 수가 매우 크면 사소한 차이도 유의하게 나타날 수 있기 때문이다. 따라서 공변량 균형성 평가는 주로 SMD와 시각적 분석을 병행하는 것이 바람직하다.\n\n\n(5) 처치효과 추정\n공변량 균형성이 확보되면, 다음 단계는 처치가 결과 변수에 미치는 영향을 추정하는 것이다. 성향점수 매칭을 통해 짝지어진 처치자와 비처치자의 결과 변수를 비교함으로써, 우리는 해당 처치의 인과적 효과(causal effect)를 도출할 수 있다.\n처치효과 추정은 일반적으로 두 가지 방식으로 구분된다.\n\nATT (Average Treatment Effect on the Treated) 실제로 처치를 받은 집단에 대해, 해당 처치를 받지 않았을 경우의 반사실적 결과(counterfactual outcome)를 추정하고 그 차이를 계산한 것이다. \\(ATT = E[Y(1) - Y(0) | D=1]\\) 실무에서는, 매칭된 쌍(pair) 간 결과값의 평균 차이로 ATT를 계산한다.\nATE (Average Treatment Effect) 전체 집단(처치자 + 비처치자)에 대해 처치가 있을 경우와 없을 경우의 결과 차이를 추정하는 것으로, 보다 강한 가정을 필요로 한다. 특히 모든 개체가 처치와 비처치 상태 모두에 노출될 수 있었던(overlap) 가능성을 전제해야 한다. ATE는 종종 성향점수 기반 가중치(weighting)를 통해 추정된다.\n\n처치효과 추정 시 고려해야 할 중요한 점은 다음과 같다.\n\n공통지지영역 밖의 표본 제거: 성향점수가 지나치게 높은 혹은 낮은 사례는 매칭이 불가능하므로 분석에서 제외한다.\n매칭 방법의 선택: 최근접 이웃 매칭(nearest neighbor), 다대일 매칭(many-to-one), 칼리퍼 매칭(caliper matching) 등 다양한 방법 중 선택해야 하며, 매칭 방식은 결과의 편향과 분산에 영향을 미친다.\n추론의 방법: 매칭된 데이터는 독립 표본이 아니므로, 표준 오차와 신뢰구간 추정 시 부트스트랩 등의 방법을 사용하는 것이 적절하다.\n\n\n\n\n3. psm 사례연구\n\n(1) 연구 목적\nJalan & Ravallion(2003)은 인도의 농촌 지역에서 파이프 수돗물 공급이 아동의 건강(특히 설사병 발병률)에 미치는 영향을 평가하기 위해 연구를 수행하였다.\n\n수돗물이 공급된 가구의 아동이 설사병 발생 위험이 낮은가?\n소득수준이나 부모교육수준이 수돗물의 효과에 영향을 미치나?\n수돗물 혜택이 특정 사회경제적 계층에 따라 다르게 나타나나?\n\n무작위 배정이 불가능한 상황에서, 연구자들은 성향점수 매칭을 사용하여 관찰된 데이터에서 최대한 인과적 효과를 추정하고자 하였다.\n\n\n(2) 데이터 및 성향 점수 추정\n연구에서는 1993-1994년 인도 국가 대표 가구 조사 데이터를 활용하였다. 총 33,000개 농촌 가구(1,765개 마을)에 대한 데이터를 분석하였으며, 다음과 같은 변수를 이용하여 성향 점수를 추정하였다.\n개입 변수(Treatment Variable)\n개입(처치)은 가구가 파이프 수돗물을 이용하는지 여부로 정의되었다. 따라서, 파이프 수돗물을 이용하는 가구(처치 그룹) vs. 이용하지 않는 가구(통제 그룹)을 비교 대상으로 삼았다.\n성향 점수 추정을 위한 독립 변수(관찰된 특성, Covariates)\n파이프 수돗물 공급 여부에 영향을 미칠 수 있는 변수를 고려하여, 로지스틱 회귀분석을 사용해 성향 점수를 추정하였다.\n① 마을 수준 변수(Village-Level Variables)\n\n마을 규모(인구)\n관개된 농지 면적\n공공 인프라(학교, 병원, 도로, 철도역, 버스 정류장 등)\n\n② 가구 수준 변수(Household-Level Variables)\n\n사회경제적 지위 (자산 보유: 자전거, 라디오, 탈곡기 등)\n가구주의 교육 수준 (문맹 여부, 초등·중등 교육 여부)\n종교 및 카스트 (힌두교, 이슬람교 등)\n가구원 수 및 연령 구조\n\n이러한 변수들을 활용하여, 개별 가구가 파이프 수돗물을 사용할 확률(성향 점수)을 추정하였다.\n\n\n(3) 매칭 방법\n성향 점수가 계산된 후, 연구자들은 처치 그룹(파이프 수돗물 이용 가구)과 통제 그룹(이용하지 않는 가구)을 유사한 성향 점수를 가진 개체끼리 매칭하였다. 연구에서는 최근접 이웃 매칭 및 칼리퍼 매칭을 사용하였다.\n최근접 이웃 매칭: 성향 점수가 가장 가까운 처치군-통제군 개체를 1:1로 매칭한다.\n칼리퍼 매칭: 성향 점수 차이가 일정 임계값 이하인 경우에만 매칭한다.\n매칭이 수행된 후, 공통 지원 영역(Common Support)을 확인하여 성향 점수가 지나치게 차이 나는 개체들은 분석에서 제외하였다.\n\n\n(4) 결과 분석 및 개입 효과 추정\n매칭이 완료된 후, 연구자들은 처치 그룹과 통제 그룹 간의 아동 설사병 발생률 차이를 분석하였다.\n파이프 수돗물을 이용하는 가구의 아동은 설사병 발생률이 21% 낮았다. 즉, 파이프 수돗물이 없었다면, 설사병 발생률이 21% 더 높았을 것으로 추정됨.\n소득 수준이 높은 가구와 부모의 교육 수준이 높은 가구에서 더 큰 건강 개선 효과가 나타났다. 고소득층 가구일수록 파이프 수돗물을 통한 혜택을 더욱 크게 누림. 교육 수준이 높은 부모일수록 위생 관리가 철저하여 추가적인 건강 효과를 봄.\n저소득층 가구에서는 파이프 수돗물이 오히려 설사병 발생 증가와 연관됨. 연구자들은 물 저장 방식의 차이가 주요 원인일 것으로 해석함. 저소득층 가구에서는 깨끗한 물을 공급받더라도 올바르게 저장하지 못하면 오염 위험이 증가할 수 있음."
  },
  {
    "objectID": "notes/survey/survey_intro.html",
    "href": "notes/survey/survey_intro.html",
    "title": "조사방법론. 1. 조사방법론 개요",
    "section": "",
    "text": "chapter 1. 개요\n\n1. 조사란?\n조사는 특정 집단, 즉 표본으로부터 정보를 수집하여, 그 집단이 속한 더 큰 모집단의 특성을 수치적으로 설명하고자 하는 체계적인 방법이다. 조사를 통해 얻어진 통계는 특정 요소 집합에 대한 관찰 결과를 요약한 수치적 표현이며, 이는 크게 두 가지 유형으로 구분된다. 이러한 통계는 사회의 다양한 모집단이 가진 특성이나 경험을 이해하고 설명하는 데 중요한 도구로 활용된다.\n첫째, 기술통계는 모집단 내 다양한 특성의 수준과 분포를 설명한다. 예를 들어, 사람들의 평균 교육 연수, 병원에 있는 총 환자 수, 대통령을 지지하는 사람들의 비율 등이 이에 해당한다.\n둘째, 분석통계는 두 개 이상의 변수 간 관계를 측정한다. 예를 들어, 소득 수준과 교육 연수 간의 관계를 설명하는 회귀계수, 혹은 지난 1년 동안 읽은 책의 수와 교육 수준 간의 상관관계 등이 이에 속한다.\n조사는 사회과학에서 사회의 작동 방식이나 행동 이론을 검증하는 데 가장 널리 사용되는 방법 중 하나이며, 다양한 형태로 수행된다. 본 강의에서는 그중 특정 유형의 조사를 중심으로 다룰 예정이며, 정보는 주로 사람들에게 질문을 통해 수집된다.\n정보 수집 방식은 조사자가 직접 질문하고 응답을 기록하거나, 응답자가 스스로 질문을 읽거나 들은 뒤 답을 작성하는 방식으로 이루어진다. 일반적으로 정보는 모집단 전체가 아닌, 기술된 모집단의 일부인 표본으로부터 수집된다.\n조사방법론은 조사 과정에서 발생하는 다양한 오류의 원인을 분석하고, 조사 결과로 얻어진 수치가 가능한 한 정확하게 모집단을 반영하도록 하기 위한 연구 분야이다. 여기서 ’오류’란 원하는 결과에서 벗어난 편차나 이탈을 의미하며, 통계적 오류는 단순한 실수가 아닌, 모집단의 실제 값과 조사로 얻은 추정값 사이의 차이를 설명하기 위해 사용된다.\n\n\n2. 조사목적\n조사는 특정 현상을 이해하고 분석하기 위해 체계적으로 데이터를 수집하고 해석하는 활동으로, 다양한 목적에 따라 설계된다. 조사 과정에서 가장 중요한 두 가지 질문은 “무엇을 발견할 것인가?“와 “가장 효과적인 방법은 무엇인가?“이다. 이러한 질문을 바탕으로, 대부분의 조사는 다음 세 가지 주요 목적을 중심으로 수행된다.\n첫째, 탐구는 관심 있는 집단이나 현상을 보다 깊이 이해하기 위한 예비적 조사로, 잘 알려지지 않은 영역이나 새로운 주제를 다룰 때 활용된다. 이는 조사 설계와 연구 방향 설정의 기초를 마련하며, 예를 들어 새로운 사회적 트렌드나 특정 인구 집단의 행동 패턴을 파악하기 위한 사전 연구가 이에 해당한다. 탐구적 조사는 주로 인터뷰나 포커스 그룹과 같은 정성적 방법을 사용하여 초기 데이터를 수집한다.\n둘째, 서술은 조사 대상 집단의 특성을 수치적 또는 질적 데이터로 기술하는 데 목적을 둔다. 이는 대상 집단의 상태를 명확히 규명하고, 그 결과를 일반화할 수 있는 기반을 제공한다. 예를 들어 실업률, 인구 구조, 산업 동향과 같은 국가 통계나 시장 조사 등이 이에 해당한다. 서술적 조사에서는 데이터의 품질과 일반화 가능성이 핵심이며, 이를 위해 엄격한 표본추출과 신뢰도 검증이 요구된다.\n셋째, 설명은 특정 현상의 원인과 결과를 실증적으로 밝히는 것을 목표로 한다. 이는 변수 간의 관계를 탐구하거나, 특정 행동이나 태도의 원인을 설명하기 위해 설계된다. 예를 들어 “왜 노년층은 보수적인 정치 성향을 보이는가?“와 같은 질문에 답하기 위해 설문조사와 통계 분석을 통해 인과 관계를 추론하는 방식이 이에 해당한다. 설명적 조사는 대체로 정량적 데이터를 바탕으로 가설 검정과 통계 분석을 수행한다.\n조사의 목적을 명확히 하기 위해서는 누구를 대상으로, 어떤 방법으로, 어떤 내용을 조사할 것인지가 분명히 정의되어야 한다. 이를 위해 조사 대상 집단을 규정하고 적절한 표본 프레임을 설정한 뒤, 수집하고자 하는 정보에 기반하여 설문 항목을 신중히 설계해야 한다.\n조사 목적은 연구의 가설 설정과 직결되며, 이 가설을 바탕으로 설문지가 구성된다. 단, 설문조사 결과를 분석한 후에 가설을 설정하거나, 조사의 목적을 설정할 때부터 결론을 미리 정하는 것은 바람직하지 않다. 이러한 방식은 조사 결과를 왜곡시킬 가능성이 있으므로 피해야 한다.\n조사 목적이 분명히 설정된 이후에는 이해관계자 또는 관련 전문가로 구성된 포커스 그룹이나 컨센서스 패널을 통해 설문지를 검토하고 개선할 필요가 있다. 또한, 유사한 주제를 다룬 기존 조사 문헌을 분석함으로써 설문지의 완성도를 높이는 것이 바람직하다.\n\n\n\nchapter 2. 조사에서의 추론과 오류\n조사는 설계 단계에서 출발하여 실행 과정을 거쳐, 궁극적으로 모집단의 통계적 특성을 설명하는 데 목적을 둔다. 조사의 출발점은 응답자가 제시하는 답변이며, 이 답변을 바탕으로 응답자의 개별적인 특성을 추론하게 된다. 이후 이러한 개인 수준의 정보는 통계적 계산을 통해 표본의 특성으로 통합되며, 다시 이를 기반으로 전체 모집단의 특성을 추론하는 과정으로 이어진다.\n즉, 조사는 응답자의 답변에서 시작하여 개인의 특성을 도출하고, 이를 표본 수준으로 확장한 뒤, 다시 모집단 수준으로 일반화하는 일련의 추론 과정을 포함한다. 이 과정에서 두 가지 핵심 조건이 충족되어야 한다.\n첫째, 응답자가 제공한 답변이 실제로 그 사람의 특성을 정확하게 반영해야 한다.\n둘째, 조사에 참여한 표본이 모집단 전체의 특성을 대표할 수 있어야 한다.\n이 두 조건 중 하나라도 충족되지 않으면 오류가 발생할 수 있다. 여기서 말하는 오류는 단순한 실수가 아니라, 의도한 결과와 실제 결과 사이의 편차를 의미한다. 예를 들어, 측정 오류는 질문에 대한 응답이 실제 측정하고자 하는 속성과 일치하지 않을 때 발생하며, 비관찰 오류는 표본으로부터 추정한 통계량이 모집단의 실제 값과 차이를 보일 때 나타난다.\n조사방법론은 이러한 오류를 체계적으로 분류하고 분석하며, 오류를 최소화하기 위해 조사 설계, 표본추출, 자료 수집 등 모든 단계에서 신중한 계획이 요구된다.\n\n1. 조사 주기(조사 설계 관점)\n조사를 바라보는 데에는 두 가지 주요 관점이 있다. 하나는 설계 관점이며, 다른 하나는 품질 관점이다. 설계 관점에서는 조사 설계를 추상적인 아이디어를 구체적인 실행 단계로 전환하는 과정으로 이해한다. 반면, 품질 관점에서는 조사 설계가 조사 통계에 영향을 미치는 다양한 오류의 근원으로 작용할 수 있음을 강조한다.\n조사는 설계 단계에서 출발하여 실행 단계로 이어진다. 적절한 설계 없이는 신뢰할 수 있는 조사 결과를 얻기 어렵다. 설계에서 실행으로 초점이 이동함에 따라 조사 작업은 추상적인 구상에서 실제적인 실행으로 전환된다. 이후 조사 결과를 해석하고 모집단에 대한 추론을 수행하는 과정에서는 다시 추상적인 수준의 사고가 요구된다.\n조사의 핵심은 측정 차원과 표현 차원이라는 두 가지 틀을 통해 설명할 수 있다. 측정 차원은 표본 내 관찰 단위에서 수집되는 데이터, 즉 “무엇에 관한 조사인가?“에 해당하며, 표현 차원은 조사에서 다루는 모집단, 즉 “누구에 관한 조사인가?“에 초점을 둔다.\n\n측정 과정\n조사의 측정 과정은 먼저 조사에서 측정하고자 하는 개념이나 구성 요소를 정의하는 것으로 시작된다. 이를 바탕으로 구체적인 측정 도구와 질문이 설계되고, 응답자는 이에 대한 답변을 제공한다. 수집된 응답은 검토 및 편집 과정을 통해 오류나 불일치가 수정되며, 정제된 데이터를 기반으로 통계가 산출된다.\n\n\n표현 과정\n표현 과정은 조사 대상이 되는 모집단을 명확히 정의하는 것으로 시작된다. 이후, 해당 모집단의 특정 부분을 대상으로 하는 표본 프레임이 설정되고, 이로부터 표본이 추출된다. 표본으로 선정된 응답자가 실제로 조사에 참여하게 되며, 조사 이후에는 필요에 따라 보정 작업이 이루어진다. 이렇게 수집된 자료는 전체 모집단을 대표하는 통계로 일반화된다.\n\n\n(1) 구성 요소 constructs\n구성 요소는 연구자가 조사에서 얻고자 하는 정보의 내용을 의미한다. 예를 들어, 고용 통계 조사는 특정 월의 근로자 수나 일자리 개수를 측정하고자 하며, 교육 성취도 평가는 학생들의 지식을 평가하는 데 목적이 있다. 전국 범죄 피해 조사는 지난 1년 동안 발생한 범죄 피해 사건의 수를 파악하려는 조사이다. 이처럼 구성 요소는 조사 목적에 따라 다양하지만, 종종 추상적이며 정확하게 측정하기 어려운 특성을 지닌다.\n예를 들어, 범죄 피해자의 정체성을 명확히 정의하는 것은 경우에 따라 모호할 수 있다. 공공장소에 낙서가 그려진 경우, 피해자를 누구로 간주할 수 있는가? 특정 수준의 범죄가 실제로 처벌의 대상이 되는 기준은 어디에 있는가? 이러한 질문들은 단순한 서술 수준의 개념을 실제 측정 가능한 항목으로 전환하는 과정에서 발생한다.\n구성 요소의 추상성은 주제에 따라 달라질 수 있다. 예를 들어, 소비자 신뢰도 조사는 개인의 재정 상태에 대한 단기적인 낙관적 태도를 측정하는데, 이는 매우 주관적이며 사람마다 인식의 차이가 크다. 반면, 전국 약물 사용 및 건강 조사는 지난달의 맥주 소비량처럼 비교적 구체적이고 관찰 가능한 행동을 측정한다. 이 경우에는 맥주로 간주되는 음료의 범위를 어떻게 정의할 것인지와 같은 실질적인 문제를 해결하는 것이 중요하다. 이처럼 소비자 신뢰도는 맥주 소비량에 비해 훨씬 더 추상적인 구성 요소라 할 수 있다.\n\n\n(2) 측정 measurement\n측정은 구성 요소보다 한층 더 구체적인 개념이다. 조사에서 ’측정’은 특정 구성 요소에 대한 정보를 수집하는 방법을 의미한다. 조사의 측정 방식은 매우 다양하며, 조사 주제에 따라 물리적 측정, 행동 관찰, 또는 질문을 통한 정보 수집 등 여러 형태로 나타난다.\n예를 들어, 유독물질 오염에 관한 조사에서는 표본 가구의 마당에서 흙 샘플을 채취할 수 있고, 건강 조사에서는 혈압을 측정할 수 있으며, 교통 조사에서는 센서를 활용해 차량 흐름을 전자적으로 기록할 수 있다. 한편, 많은 조사는 응답자에게 질문을 던져 그들의 인식이나 행동에 대한 정보를 수집하는 방식으로 이루어진다. 예를 들어, “지난 6개월 동안 본인이 범죄라고 생각한 사건과 관련해 경찰에 신고한 적이 있습니까?“와 같은 질문이 포함될 수 있다.\n측정 과정에서 가장 중요한 과제는 측정하고자 하는 구성 요소를 충실히 반영할 수 있는 질문을 설계하는 것이다. 질문이 부정확하거나 모호할 경우, 수집된 정보는 실제 구성 요소를 제대로 대변하지 못할 수 있다.\n이러한 질문은 전화 인터뷰나 대면 조사 방식으로 제시될 수 있으며, 종이 설문지나 컴퓨터를 이용한 자가 응답 방식으로도 제공될 수 있다. 경우에 따라서는 조사자가 직접 관찰을 통해 정보를 수집해야 하는 상황도 있다.\n\n\n(3) 응답 response\n조사에서 생성된 데이터는 조사 측정을 통해 수집된 정보에서 비롯되며, 응답의 성격은 사용된 측정 방법에 따라 달라진다. 질문이 측정 도구로 사용되는 경우, 응답자는 기억을 되살리거나 주관적 판단을 통해 답을 생성하거나, 기록을 참고하거나, 때로는 타인의 도움을 받아 응답할 수 있다.\n예를 들어, 소비자 신뢰도 조사에서는 “앞으로 1년 후 본인과 가족의 경제적 상황이 더 나아질 것 같은지, 더 나빠질 것 같은지, 아니면 비슷할 것 같은지”와 같은 질문이 제시된다. 반면, 고용 통계 조사에서는 고용주가 직원 기록을 확인하여 특정 주간의 비관리직 직원 수를 보고하는 방식으로 이루어진다.\n측정 방식에 따라 응답 형식도 달라진다. 어떤 경우에는 선택지가 제공되어 응답자가 해당 범주 중 하나를 선택하면 되며, 다른 경우에는 질문만 주어지고 응답자가 자신의 언어로 자유롭게 답을 작성해야 하는 경우도 있다.\n\n\n(4) 편집된 응답 edited response\n일부 데이터 수집 방식에서는 초기 측정 데이터를 다음 단계로 넘기기 전에 사전 검토 과정을 거친다. 컴퓨터를 이용한 조사에서는 정량적 응답에 대해 범위 검사를 수행하여, 허용된 한계를 벗어난 답변을 자동으로 감지하고, 후속 질문을 통해 응답의 정확성을 확인한다. 예를 들어, 출생 연도를 묻는 질문에 1890년 이전의 숫자가 입력되었거나, 한 응답자가 자신의 나이를 14세라고 답하면서 동시에 다섯 명의 자녀가 있다고 응답한 경우, 이러한 불일치를 확인하고 수정을 유도하는 후속 질문이 제시된다.\n종이 설문지의 경우에는 조사자가 설문지를 수기로 검토하여, 읽기 어려운 답변이나 누락된 항목을 찾아 보완하는 작업이 이루어진다. 이는 현장 조사자가 수행하는 1차적인 오류 점검 단계라 할 수 있다.\n모든 응답자의 답변이 수집된 이후에도 데이터에 대한 추가적인 편집 과정이 진행될 수 있다. 이 과정에서는 전체 응답 분포를 검토하고, 비정상적인 응답 패턴이나 불일치 사례를 탐지하여 이상치를 식별한다. 경우에 따라 특정 설문지나 응답자의 응답을 보다 면밀히 검토해야 할 수도 있다.\n이러한 데이터의 검토와 편집 과정은 조사 결과의 신뢰성과 정확성을 확보하기 위한 핵심적인 절차로 간주된다.\n\n\n(5) 조사 대상 모집단 target population\n조사 대상 모집단은 조사의 대상이 되는 단위들의 집합을 의미한다. 예를 들어, 가구 조사의 경우 성인을 조사 대상 모집단으로 정의할 수 있다. 그러나 이와 같은 정의는 몇 가지 세부 사항이 명확히 설정되지 않으면 해석의 여지를 남기게 된다. 조사 시점을 특정하지 않거나, 전통적인 가구 형태에 속하지 않는 사람들(예: 노숙인, 시설 거주자 등)을 포함할지 여부를 명시하지 않는다면 모집단의 범위는 모호해질 수 있다. 또한, 최근 성인이 된 사람들을 포함할 것인지, 국내 거주 상태를 어떤 기준으로 판단할 것인지 등이 명확히 정의되지 않으면, 모집단의 일관성과 재현 가능성에 문제가 발생할 수 있다.\n조사 대상 모집단은 일반적으로 유한한 규모의 개인들로 구성되며, 이들은 조사의 분석 대상이 된다. 예를 들어, 경제활동인구조사는 만 15세 이상이면서 현재 군 복무를 하지 않고, 병원·교도소·기숙사와 같은 시설이 아닌 일반 주거지에 거주하는 사람들을 모집단으로 정의한다. 모집단의 시간적 범위는 보통 특정 월이나 주로 고정되며, 이 시점에 표본으로 선정된 사람이 해당 거주지에 실제로 거주하고 있어야 한다.\n\n\n(6) 표본 프레임 모집단 frame population\n표본 프레임 모집단은 조사 표본에 선택될 가능성이 있는 대상 모집단의 구성원 집합을 의미한다. 단순한 경우에는 표본 프레임이 대상 모집단의 모든 단위(예: 개인, 고용주 등) 목록으로 구성된다. 그러나 현실에서는 표본 프레임이 대상 모집단과 완전히 일치하지 않거나, 일부 단위만을 포함하는 경우도 많다.\n예를 들어, 소비자 신뢰도 조사의 대상 모집단은 성인 가구이며, 이때 전화번호 목록이 표본 프레임으로 사용될 수 있다. 이는 각 사람을 자신이 속한 가구의 전화번호와 연결할 수 있다는 전제에 기반한다. 그러나 실제로는 전화가 없는 가구도 존재하며, 하나의 가구가 여러 개의 전화번호를 보유하고 있는 경우도 있어, 이러한 전제가 항상 성립하지 않으며 표본 프레임의 복잡성이 발생할 수 있다.\n건강 조사의 경우, 행정구역별 거주지 목록이 표본 프레임으로 활용된다. 이 목록은 각 주택 단위를 특정 시군구와 연결시키며, 일반적으로 15세 이상의 성인이 거주하는 주택을 조사 대상으로 설정한다. 그러나 고정된 거주지가 없는 사람, 혹은 복수의 거주지를 가진 사람과 같이 표본 프레임에서 다루기 어려운 사례도 존재한다.\n\n\n(7) 표본 sample\n표본은 표본프레임에서 선택된다. 이 표본은 측정을 통해 데이터를 수집할 대상 그룹이다. 표본은 표본 프레임의 매우 작은 부분만을 차지한다.\n\n\n(8) 응답자 respondents\n대부분의 조사에서는 선택된 표본 사례를 모두 성공적으로 측정하기 어렵다. 조사에 성공적으로 응답한 사례는 ‘응답자’로 분류되며, 반대로 전혀 응답하지 않은 사례는 ‘무응답자’ 또는 ’단위 무응답’으로 간주된다.\n그러나 어떤 사례를 응답자로 분류할지, 무응답자로 간주할지를 판단하는 일은 종종 명확하지 않다. 일부 응답자는 조사에서 요구되는 정보 중 일부만을 제공하는 경우가 있으며, 이 경우 응답 상태의 구분이 애매해질 수 있다. 이러한 사례에 대해 데이터를 구축할 때는, 불완전한 응답을 포함할지 아니면 해당 응답자를 분석 파일에서 제외할지를 결정해야 한다.\n한편, ‘항목 결측’ 또는 ’항목 무응답’이라는 용어는 응답자가 전체적으로는 조사에 참여했지만, 특정 질문에 대한 응답이 누락된 경우를 지칭한다. 즉, 단위 무응답이 전체 사례에 대한 응답 실패를 의미한다면, 항목 무응답은 일부 질문에 대한 응답 누락을 의미한다.\n\n\n(9) 조사 후 조정 (Postsurvey Adjustments)\n모든 응답자가 데이터를 제공하고 해당 데이터 기록이 완성된 이후에도, 조사로부터 도출된 추정치의 품질을 향상시키기 위해 추가적인 절차가 진행되는 경우가 많다. 이는 무응답 문제나, 표본 프레임과 실제 대상 모집단 간의 불일치와 같은 커버리지 문제로 인해, 응답자 기반 통계가 전체 모집단의 통계와 차이를 보일 수 있기 때문이다.\n이러한 차이를 이해하기 위해, 조사 단계에서는 서로 다른 하위 집단에 대한 단위 무응답 패턴을 분석한다. 예를 들어, 도시 지역의 응답률이 농촌 지역보다 낮은 경우, 도시 거주자가 표본에서 과소 대표되고 있을 가능성을 시사한다. 이와 유사하게, 표본 프레임 자체에 포함되지 않은 단위 유형에 대한 정보를 분석하면, 모집단 내 특정 유형이 아예 조사 대상에서 누락되었음을 확인할 수 있다.\n이후 과정에서, 과소 대표된 집단의 영향을 보정하기 위해 가중치를 조정함으로써 전체 추정치를 개선할 수 있다. 또한, 응답하지 않은 항목은 ’결측 대체(imputation)’라 불리는 절차를 통해 추정된 값으로 대체할 수 있다.\n이러한 가중 조정과 결측 대체 방법은 다양하게 존재하며, 모두 조사 후 조정(post-survey adjustment)에 해당하는 절차로 분류된다.\n\n\n\n2. 조사 주기(조사 품질 관점)\n조사 방법론에서 흔히 사용하는 품질 개념을 추가적으로 표시한 타원형을 볼 수 있다. 이 개념들은 조사 과정의 연속적인 단계 사이에서 발생하는 불일치를 나타내며, 대부분 \"오류\" 라는 단어를 포함하고 있다. 조사 설계자의 역할은 설계 및 추정 선택을 통해 이러한 오류를 최소화하여 조사 통계의 품질을 높이는 것이다.\n\n\n\n\n\n\n\\(\\mu_{i}\\): 모집단에서 \\(i\\) 번째 사람의 참 값\n\\(Y_{i}\\): \\(i\\) 번째 표본 사람의 측정값\n\\(y_{i}\\): \\(i\\) 번째 표본 측정 응답값(조사 질문에 대한 응답)\n\\(y_{ip}\\): 편집 및 추가 처리 과정을 거친 \\(i\\) 번째 표본 데이터값\n\n결론적으로, 측정하려는 기본 목표 속성은 \\(\\mu_{i}\\)이지만, 실제로는 측정 오류로 인해 목표에서 벗어난 불완전한 지표 \\(y_{ip}\\)를 사용한다.\n\n(1) 타당성 validity\n구성 요소의 타당성 validity은 측정값이 근본적인 구성 요소와 관련된 정도를 나타낸다. 반면, 타당하지 않음은 타당성이 달성되지 않은 정도를 설명하는 데 사용되는 용어다. 통계적으로, 타당성의 개념은 개별 응답자 수준에서 적용된다. 이는 구성 요소가 관찰되지 않거나 쉽게 관찰될 수 없는 경우에도, 모집단에서 \\(i\\) 번째 사람과 관련된 일부 값을 가지며, 이는 전통적으로 \\(\\mu_{i}\\)(구성 요소의 참값)로 표시된다. 특정 측정값 \\(Y_{i}\\)의 결과는 \\(\\mu_{i}\\)가 아닌 \\(Y_{i} = \\mu_{i} + \\varepsilon_{i}\\)가 된다.\n이 식에서 \\(\\varepsilon_{i}\\)는 진짜 값에서의 편차를 나타내며, 타당성 개념의 기초를 형성한다. 또한, 측정 타당성을 이해하려면 한 번의 측정이 아닌 여러 번의 반복적인 측정을 고려해야 한다. 같은 측정이 \\(i\\) 번째 사람에게 여러 번 적용될 경우 각 결과는 달라질 수 있으며, 이를 고려해 식이 \\(Y_{it} = \\mu_{i} + \\varepsilon_{it}\\)로 확장된다. 여기서 \\(t\\)는 측정 시도의 번호를 나타낸다. 결국, 타당성은 측정값과 참값 간의 상관 관계로 정의된다. 이는 측정값 \\(Y\\)와 구성 요소 \\(\\mu\\)간의 관계가 높을수록 타당성이 높은 것으로 간주된다.\n\n\n(2) 측정 오류 measurement error\n측정 오류는 측정값이 실제 값, 즉 참값에서 벗어나는 현상을 의미한다. 예를 들어, 국민건강조사에서 “코카인을 한 번이라도 사용한 적이 있습니까?“라는 질문이 있다고 가정해보자. 연구에 따르면, 응답자가 부정적으로 인식하는 행동에 대해서는 과소 보고하는 경향이 있다. 따라서 실제로는 “예”라고 응답해야 하는 사람이 자신의 약물 사용 사실이 드러나는 것을 우려해 “아니요”라고 응답할 수 있다. 이처럼 특정 질문에 대한 응답이 반복적으로 왜곡된다면, 응답값의 평균과 모집단의 실제 평균 사이에 차이가 발생하게 된다.\n통계적으로는 특정 응답자 i에 대해, 측정값과 참값 사이의 체계적 편차를 \\((y_₁ - Y_₁)\\) 로 나타낼 수 있다. 이러한 차이가 일정한 방향으로 발생하면 이를 응답 편향이라고 한다. 예를 들어, 응답자가 자신의 약물 사용이나 범죄 피해 경험을 일관되게 과소 보고하는 경우가 이에 해당한다. 이 편향은 조사 결과가 실제보다 낮거나 높게 나타나는 경향을 유발한다.\n한편, 응답 행동의 불안정성은 또 다른 유형의 오류를 야기할 수 있다. 예를 들어, “현재 사업 환경이 1년 전보다 나아졌습니까, 아니면 나빠졌습니까?“와 같은 질문에 대해 응답자는 질문의 내용뿐 아니라 앞선 질문의 맥락이나 측정 환경의 자극 등 다양한 요소에 영향을 받아 응답할 수 있다. 이러한 자극은 예측하기 어려우며, 동일한 질문을 여러 차례 반복해도 응답이 일관되지 않을 수 있다. 다시 말해, 기대값 \\(E(y_i)\\) 가 참값 \\(Y_\\) 와 일치하지 않을 수 있다.\n응답 분산은 동일한 사람에게 동일한 질문을 여러 번 했을 때 매번 다른 응답이 나타나는 현상을 의미한다. 이는 응답 편향과 구별되며, 보통 신뢰도가 낮은 측정에서 발생한다. 설문 조사에서는 이러한 응답 분산이 추정값의 불안정성을 높이는 주요 원인 중 하나로 간주된다.\n\n\n(3) 처리 오류 processing error\n데이터가 수집된 이후, 추정 단계에 들어가기 전까지 발생할 수 있는 오류에는 여러 가지가 있다. 대표적인 예로 편집 오류와 코딩 오류가 있다.\n편집 단계에서는 응답값의 신뢰성과 일관성을 검토하면서, 명백히 이상해 보이는 값을 누락 처리하거나 수정할 수 있다. 예를 들어, 한 응답자가 “매일 여러 차례 폭행을 당했다”고 보고한 경우, 이는 직관적으로 믿기 어려운 보고로 간주되어 자동 편집 규칙에 따라 결측 처리될 수 있다. 그러나 만약 해당 응답자가 술집의 보안요원이라는 추가 정보가 제공된다면, 응답 내용이 실제 상황을 반영하고 있을 가능성이 높아진다. 이처럼 측정하려는 구성 요소의 맥락에 따라 응답을 수정하거나 유지할지 여부를 판단하는 과정에서 처리 오류가 발생할 수 있다.\n또한, 코딩 단계에서도 오류가 발생할 수 있다. 텍스트 응답을 분류할 때, 동일한 응답이라 하더라도 코딩하는 사람에 따라 다르게 해석될 수 있다. 이러한 차이는 결과의 변동성을 유발하며, 코딩 시스템의 구조나 코더 간 일관성 부족에서 기인한다. 이를 흔히 코딩 분산이라 부른다. 특히 훈련이 부족한 코더는 모호한 언어나 응답자의 설명을 일관되게 해석하지 못해 잘못된 범주로 분류할 수 있으며, 이는 코딩 편향을 초래하게 된다.\n결국, 편집과 코딩 단계 모두에서 발생하는 오류는 측정 이후 데이터 품질에 영향을 미치며, 추정 단계에서의 통계적 결과에도 왜곡을 가져올 수 있다.\n통계적 표기법으로 표현하면, 예를 들어 소득과 같은 변수를 고려할 때, 처리 효과는 제공된 응답과 편집된 응답 간의 차이로 정의될 수 있다. \\(y_{i}\\)는 조사 질문에 대한 응답을, \\(y_{ip}\\)는 편집된 응답을 나타낸다. 따라서, 처리 편차는 \\((y_{ip} - y_{i})\\)로 나타낼 수 있다.\n\n\n(4) 포함오류 coverage error\n포함 오류는 모집단과 표본 프레임 간의 차이에서 발생한다. 예를 들어, 표본 프레임이 모집단의 일부를 포함하지 못한 경우를 포함 부족(undercoverage)이라고 하며, 반대로 표본 프레임에 모집단에 속하지 않는 요소가 포함된 경우는 과잉 포함(overcoverage)이라고 한다.\n통계적으로 볼 때, 표본 평균에서 발생하는 포함 편향은 두 가지 요소에 의해 결정된다. 첫째는 표본 프레임에 포함되지 않은 모집단 구성원의 비율이고, 둘째는 프레임에 포함된 구성원과 포함되지 않은 구성원 간의 특성 차이이다. 즉, 포함되지 않은 비율이 높고, 포함된 구성원과 포함되지 않은 구성원 간에 측정하고자 하는 변수의 평균 차이가 클수록 포함 편향은 커지게 된다.\n\\({\\overline{Y}}_{C} - \\overline{Y} = \\frac{U}{N}({\\overline{Y}}_{C} - {\\overline{Y}}_{U})\\), 여기서 \\(\\overline{Y}\\)는 목표 모집단 전체의 평균, \\({\\overline{Y}}_{C}\\)는 표본 프레임에 포함된 모집단의 평균, \\({\\overline{Y}}_{U}\\)는 표본 프레임 밖 모집단의 평균을 나타낸다. \\(N\\)은 목표 모집단의 총 구성원 수, \\(C\\)는 표본 프레임에 포함된 적격 구성원의 총수, 그리고 \\(U\\)는 표본 프레임에 포함되지 않은 적격 구성원의 총수이다.\n예를 들어, 전화 조사를 통해 가구의 평균 교육 연수를 측정한다고 가정하자. 전화가 없는 가구는 표본에서 제외되므로 이들의 평균 교육 연수는 낮아질 가능성이 있다. 전화가 있는 가구의 평균 교육 연수가 14.3년이고, 전화가 없는 가구의 평균 교육 연수가 11.2년이라면, 전체 모집단 평균에 대한 편향은 다음과 같이 계산될 수 있다(단, 전화 없는 가구 비율을 5%라 가정하자).\n\\({\\overline{Y}}_{C} - \\overline{Y} = 0.05(14.3 - 11.2) = 0.16\\text{년}\\).\n즉, 전화가 없는 가구를 포함하지 않은 표본 프레임은 모집단 평균보다 약 0.16년 더 높은 평균 교육 연수를 보여줄 것이다. 결론적으로, 표본 프레임의 포괄 오류는 표본 평균 추정값에 영향을 미치며, 이는 모집단 평균이 아닌 표본 프레임 평균을 반영하게 된다.\n\n\n(5) 표본 오류 sampling err0r\n표본 설문조사에서는 비용과 시간의 제약으로 인해, 표본 프레임 내 모든 구성원을 조사하는 것이 현실적으로 어렵다. 따라서 전체 중 일부만을 선택하여 조사하고, 나머지는 제외하는 방식이 일반적으로 채택된다. 이러한 선택적 측정으로 인해 발생하는 통계적 차이를 표본 오류라고 한다.\n표본 오류는 크게 두 가지 유형으로 구분된다. - 표본 편향(sampling bias)은 표본 프레임의 일부 구성원이 표본으로 선택될 기회를 갖지 못하거나, 선택될 가능성이 상대적으로 낮을 때 발생한다. 예를 들어, 특정 표본 설계가 체계적으로 일부 그룹을 항상 제외하도록 구성되어 있다면, 그 결과로 도출된 통계치는 실제 프레임 모집단의 통계와 차이를 보일 수 있다. - 표본 분산(sampling variance)은 동일한 방법으로 표본을 반복 추출할 경우, 각 표본이 서로 다른 응답을 포함하게 되어 조사 통계가 반복마다 달라질 수 있는 현상을 의미한다. 이는 표본 선택이 확률적일 때 자연스럽게 발생하는 오차로, 표본의 수나 분포에 따라 크기가 달라질 수 있다. 표본 선택의 여러 가능성을 개념적으로 반복한 결과, 표본 분산의 개념이 만들어진다. 표본 분산은 동일한 설계에서 얻어진 여러 표본 평균이 얼마나 변동하는지를 나타낸다.\n\n\\({\\overline{Y}}_{s}\\): 특정 표본 추출의 표본 평균, 표본 \\(s;s = 1,2,\\ldots,S\\)\n\\({\\overline{Y}}_{C}\\): 표본 프레임에서의 모든 요소의 총평균\n\\(\\overline{Y}_s = \\frac{\\sum_{i=1}^{n_s} y_{si}}{n_s},\\ \\overline{Y}_C = \\frac{\\sum_{i=1}^{C} Y_i}{C}\\)\n표본 분산: \\(\\frac{\\sum_{s = 1}^{S}({\\overline{Y}}_{s} - {\\overline{Y}}_{C})^{2}}{S}\\)\n\n\n\n(6) 응답률 오류 nonresponse error\n모든 표본 구성원을 설문조사에서 완전히 측정하는 것은 현실적으로 어렵다. 특히 사람을 대상으로 하는 조사에서는 이러한 상황이 자주 발생한다. 이로 인해 발생하는 오류를 응답률 오류라고 하며, 이는 실제 응답한 사람들의 통계 값이 전체 표본을 기준으로 했을 때의 통계 값과 다를 때 나타난다.\n예를 들어, 수행평가 당일 결석한 학생들이 수학적 또는 언어적 능력이 낮은 경향이 있다면, 이들이 측정에서 제외됨으로써 전체 수행평가 점수가 과대평가될 수 있다. 즉, 응답자의 평균 점수가 전체 표본의 진정한 평균보다 체계적으로 높아지는 결과가 나타난다. 이러한 오류는 응답률이 낮을수록 그 영향이 커지며, 조사 결과의 왜곡 가능성도 더욱 심각해질 수 있다.\n응답률 편향: \\({\\overline{y}}_{r} - {\\overline{y}}_{s} = \\frac{m_{s}}{n_{s}}({\\overline{y}}_{r} - {\\overline{y}}_{m})\\)\n\n\\({\\overline{y}}_{s}\\): 선택된 특정 표본의 전체 평균,\n\\({\\overline{y}}_{r}\\): 𝑠번째 표본의 응답자 평균, \\({\\overline{y}}_{m}\\): 𝑠번째 표본의 비응답자 평균\n\n𝑛𝑠: 𝑠번째 표본의 총 구성원 수, 𝑟𝑠: 𝑠번째 표본의 응답자 수, 𝑚𝑠: 𝑠번째 표본의 비응답자 수\n따라서 표본 평균에 대한 응답률 편향은 응답률(데이터가 수집되지 않은 표본 구성원의 비율)과 응답자와 비응답자 평균 간의 차이의 곱으로 나타난다. 이는 높은 응답률만으로는 반드시 품질 지표가 아님을 나타낸다. 응답률이 높은 설문조사에서도 비응답자가 조사 변수에서 매우 독특할 경우, 높은 응답률 편향이 나타날 수 있다. 이 문제를 방지하는 가장 좋은 방법은 높은 응답률을 유지하여 응답률 편향의 위험을 줄이는 것이다.\n\n\n(7) 보정 오류 adjustment error\n조사에서 발생하는 비관측 오류를 줄이기 위한 마지막 단계는 조사 후 보정이다. 이 보정은 표본 추정치를 개선하기 위해 시행되며, 포함 오류, 표본 오류, 무응답 오류와 같은 오류를 줄이는 것을 목표로 하며, 보정 과정은 개별 응답에 대한 수정 단계와 유사한 역할을 한다.\n보정은 대상 모집단 또는 표본 프레임에 대한 정보와 응답률 데이터를 활용하여 과소 대표된 표본 사례에 더 큰 가중치를 부여함으로써 데이터의 균형을 맞춘다. 예를 들어, A지역의 응답률이 85%인 경우 해당 지역 응답자에게 \\(w_{i} = 1/0.85\\)의 가중치를 부여하여 특정 응답자의 영향을 평균 계산에서 확대한다.\n조정된 평균은 이러한 가중치를 적용해 계산되며, 조정된 표본 평균(\\(\\overline{y}nw = \\frac{\\sum_{i = 1}^{r}w_{i}y_{si}}{\\sum_{i = 1}^{r}w_{i}}\\))과 모집단 평균과 간의 차이는 \\(({\\overline{y}}_{nw} - \\overline{Y})\\) 로 나타난다. 이러한 보정은 통계적 편향을 줄이는 데 기여하지만, 경우에 따라 오류를 오히려 증가시킬 수도 있으므로 설계와 실행 단계에서 세심한 주의가 필요하다.\n\n\n\n\nchapter 3. 목표모집단, 표본프레임, 포함오류\n표본 설문조사는 명확히 정의된 모집단을 설명하거나, 그로부터 통계적 추론을 도출하는 과정이다. 이때 모집단을 구성하는 기본 단위는 ‘요소’ 또는 ’조사단위’로 불리며, 이 요소들이 전체 모집단을 형성한다. 예를 들어, 가구 조사의 경우 요소는 개별 가구원일 수 있으며, 학교 조사의 경우에는 학생이, 비즈니스 조사의 경우에는 사업체나 시설이 요소가 된다. 하나의 설문조사 내에서도 다양한 유형의 요소가 존재할 수 있다. 가구 조사의 경우, 조사 대상은 사람일 수도 있지만, 주거 단위나 거주 커뮤니티 등과 같은 더 넓은 단위로 확장될 수도 있다.\n설문조사에서 모집단 정의는 조사 설계와 결과 해석의 출발점이자 핵심이다. 모집단을 어떻게 정의하느냐에 따라 데이터의 대표성과 신뢰성이 결정되며, 이는 추론의 정확도와 직결된다. 다음은 설문조사에서 모집단 정의가 중요한 이유를 설명하는 네 가지 측면이다.\n설문의 주요 목적\n설문조사의 핵심 목적은 특정 모집단의 특성을 통계적으로 설명하거나, 그 모집단에 대해 일반화된 결론을 도출하는 것이다. 모집단이 명확히 정의되지 않으면, 조사 결과의 정확성과 대표성이 저하될 수 있다. 모집단 정의는 조사 결과가 어떤 집단을 설명하고 있는지를 분명히 밝히는 역할을 한다.\n조사의 설계 및 표본 추출\n설문조사는 모집단으로부터 표본을 추출하고, 이를 통해 모집단 전체에 대한 추정치를 산출한다. 모집단 정의가 불명확하면 표본 추출 과정에 왜곡이 생기고, 결과적으로 표본이 모집단을 제대로 대표하지 못하게 된다. 따라서 모집단 정의는 표본 설계의 출발점으로서 반드시 선행되어야 한다.\n다양한 단위와 요소 처리\n하나의 조사에서도 사람, 가구, 기업 등 다양한 요소들이 존재할 수 있다. 이러한 요소들의 범위와 성격이 명확히 정의되지 않으면, 데이터 해석 과정에서 혼란이 발생할 수 있으며, 잘못된 결론으로 이어질 위험도 존재한다.\n다른 연구와의 차별성\n실험 연구와 같이 자극과 반응 간의 인과관계를 규명하는 데 초점을 맞춘 연구에서는 모집단 정의가 상대적으로 부차적인 요소일 수 있다. 반면, 설문조사는 모집단 전체의 특성을 설명하고 해석하는 것을 궁극적인 목표로 삼기 때문에, 모집단 정의는 조사 전 과정에서 가장 중요한 요소 중 하나로 간주된다.\n\n1. 모집단과 프레임\n목표 모집단(target population)은 조사 결과를 일반화하고자 하는, 즉 표본 통계를 통해 추론하고자 하는 요소들의 집합을 의미한다. 목표 모집단은 다음과 같은 조건을 충족해야 한다:\n\n크기가 유한해야 한다. 이론적으로라도 개별 요소들을 셀 수 있어야 한다.\n시간적으로 정의되어야 한다. 특정 시점 또는 시기 내에서 존재하는 집단이어야 한다.\n관찰 가능해야 한다. 즉, 실제로 접근하여 조사할 수 있어야 한다.\n\n목표 모집단을 정의할 때는 두 가지 요소를 명확히 해야 한다. 첫째, 어떤 단위를 모집단의 요소로 간주할 것인가(예: 사람, 가구, 시설). 둘째, 어떤 시간적 범위를 설정할 것인가. 예를 들어, 가구 조사의 경우 조사 대상은 일반적으로 경제활동인구에 해당하는 만 15세 이상의 성인으로, 주택 단위에서 거주하는 사람들을 포함한다. 이때 ’가구’는 집, 아파트, 이동식 주택, 방 그룹 또는 독립된 방처럼 거주를 목적으로 마련된 공간을 포함한다.\n모든 사람이 반드시 성인이거나 주택에 거주하는 것은 아니므로(예: 교도소 수감자, 군부대 거주자, 장기 요양시설 입소자 등), 이들이 목표 모집단에 포함될지 여부는 조사의 목적과 범위에 따라 달라질 수 있다. 어떤 조사는 특정 지역(예: 특광역시)으로 모집단을 한정하기도 하며, 어떤 조사는 시설 거주자를 포함하기도 한다.\n모집단은 고정된 것이 아니라 시간에 따라 변할 수 있기 때문에, 조사 시점 역시 모집단 정의의 중요한 요소이다. 예를 들어, 가구 조사가 며칠 혹은 몇 주에 걸쳐 이루어질 경우, 그 조사 기간 중 해당 주택에 거주하는 사람이 모집단에 포함된다. 실무에서는 첫 번째 접촉 시점에서 거주자가 누구인지를 기준으로 모집단을 “고정”하는 방식을 자주 사용한다.\n그러나 실제로 조사 데이터를 수집할 때는, 조사 방법 자체가 모집단을 더 좁은 범위로 제한하는 경우가 많다. 예를 들어, 목표 모집단이 ’대한민국에 거주하는 만 18세 이상 성인’이라 하더라도, 전화 조사는 실제로 전화번호를 가진 사람들만 조사할 수 있다. 이처럼 실질적으로 조사가 이루어진 집단을 조사 모집단(survey population)이라 하며, 이는 원래 목표 모집단과 다를 수 있다.\n조사를 설계하기 위해서는 표본 프레임(sample frame)이 필요하다. 표본 프레임은 모집단 요소를 식별할 수 있는 자료 집합으로, 요소들의 명부(예: 회원 명단, 주소록)나 요소가 존재하는 지역·시설·시점의 목록일 수 있다. 예를 들어, 특정 전문 협회 회원 명단, 특정 지역 내 사업체 목록 등이 이에 해당한다.\n그러나 실제 조사에서는 단일한 표본 프레임이 존재하지 않는 경우도 많다. 예를 들어, 서울 지역 모든 초중고등학생의 명단이나, 교정시설 수감자 전체 명단은 현실적으로 존재하지 않거나 접근이 어렵다. 이 경우 조사자는 두 가지 선택지를 고려할 수 있다.\n\n표본 프레임에 맞게 목표 모집단을 재정의한다.\n원래 모집단을 유지하되, 포함 오류(coverage error)의 가능성을 인정한다.\n\n예를 들어, 전화 조사를 설계할 때 목표 모집단이 미국의 모든 성인이라 하더라도, 실제 표본 프레임은 전화번호를 보유한 성인으로 제한될 수 있다. 모집단을 전화가 있는 성인으로 재정의하는 것은 모집단과 프레임 간의 불일치를 해결하는 방식이지만, 원래 모집단을 그대로 유지할 경우에는 전화가 없는 성인이 누락되어 포함 오류가 발생한다.\n마지막으로, 표본 프레임 없이 조사를 수행해야 하는 경우도 있다. 이때는 확률 표집이 어려우므로, 눈덩이 표집(snowball sampling)이나 특정 지역을 설정한 현장 조사와 같은 비확률 표집(nonprobability sampling) 방식이 활용되기도 한다.\n\n\n2. 표본프레임의 포함 이슈\n조사에서 연구자들에게 중요한 통계적 관심은 표본 프레임(표본을 선택하기 위해 사용 가능한 자료)이 실제로 목표 모집단을 얼마나 잘 포함하고 있는가이다. 표본 프레임과 목표 모집단 간의 일치는 포함, 미포함(목표 모집단에 포함되어야 하지만 표본 프레임에 포함되지 않거나 포함될 수 없는 요소), 비적격(목표 모집단에 속하지 않지만 표본 프레임에 포함된 단위) 단위를 포함한 세 가지 잠재적 결과를 초래할 수 있다.\n표본 프레임이 완벽하다는 것은 프레임 요소와 목표 모집단 요소 간에 일대일 매핑이 있다는 것을 의미한다. 실제로는 완벽한 프레임이 존재하지 않으며, 일대일 매핑을 방해하는 문제가 항상 발생한다.\n프레임의 적합성을 논의할 때 포함 오류와 비적격 단위 문제이외 프레임 단위가 존재하고 목표 모집단의 요소와 매핑되지만 그 매핑이 고유하지 않을 때 발생하는 문제도 논의 되어야 한다. ”중복”은 여러 프레임 단위가 목표 모집단의 단일 요소에 매핑될 때 사용된다.\n”군집화”는 여러 목표 모집단 요소가 동일한 단일 프레임 요소와 연결될 때 사용되는 용어다. 표본 크기(요소 수로 측정)는 선택된 군집에 따라 클 수도 있고 작을 수도 있다. 또한 여러 프레임 단위가 여러 목표 모집단 요소와 매핑되는 경우(다대다 매핑)도 있다.\n\n(1) 미포함 undercoverage\n미포함 정의\n미포함는 조사 통계에서 특정 모집단 부분이 포함되지 않아 발생하는 오류를 뜻한다. 예를 들어, 전화 가구 조사는 모든 가구의 사람들을 대상으로 하지만, 전화 프레임에는 전화가 없는 가구가 포함되지 않아 미포함이 발생한다. 세계 여러 국가에서 전화 사용이 지속적인 비용을 요구하기 때문에 경제적으로 어려운 계층이 비율적으로 더 많이 제외된다. 또한, 휴대전화가 고정전화 서비스를 대체하는 국가에서는 젊은 사람들이 새로운 기술을 더 빨리 수용하기 때문에 고정전화 기반 프레임에서 제외될 가능성이 높다.\n미포함 문제의 원인\n미포함 문제는 표본 프레임 생성 과정에 따라 발생한다. 이 과정은 조사 설계에 의해 통제될 수도 있고, 외부 출처에서 프레임을 얻을 때는 조사 외부 요인에 의해 결정될 수도 있다. 예를 들어, 가구 조사의 경우, 조사 표본은 초기 지역 목록(시군구 등)에서 시작하여 시군구 내의 주택 목록으로 세분화되고, 최종적으로 가구 내 거주자 목록으로 연결된다. 이러한 샘플링 과정은 지역 샘플로 불린다.\n문제의 수준\n\n지리적 경계: 도로, 강, 철도 등 물리적 경계는 상대적으로 쉽게 구별되지만, 자연 경계선(산등성이, 능선 등)은 해석에 따라 차이가 발생할 수 있다. 경계 해석 오류로 인해 특정 가구가 목록에서 누락될 가능성이 있다.\n가구 정의: 가구는 독립된 입구를 갖춘 물리적 구조로 정의되지만, 추가가구나 숨겨진 입구가 있는 경우 누락될 가능성이 있다.\n특수 사례: 공동체 생활(공동 주방 사용 등)이나 제도적 시설(교도소, 병원 등)의 경우, 거주 단위를 정의하고 포함 여부를 결정하는 규칙이 필요하다.\n\n주민 등록 규칙의 문제\n조사에서 거주자는 일반적으로 ”일반 거주” 규칙에 따라 정의된다. 이 규칙은 거주 단위에서 통상적으로 거주하는 사람을 포함하도록 한다. 하지만, 여행하는 직업(트럭 운전사, 항공 조종사 등)을 가진 사람들의 경우 거주지 정의가 모호할 수 있다. 또한, 부모와 떨어져 살거나 복잡한 가족 구조를 가진 아동도 미포함이 발생할 수 있다.\n사업체 조사에서의 Undercoverage\n사업체 조사는 사업체의 생성, 병합, 종료로 인해 미포함이 발생할 가능성이 높다. 특히 대규모 또는 소규모 사업체는 표본 프레임에 포함되지 않을 수 있다. 새로운 사업체는 행정적 기록의 지연으로 프레임에서 누락되거나, 복잡한 사업체 구조는 데이터 정리 과정에서 오류를 일으킬 수 있다.\n\n\n(2) 부적격 단위 ineligible units\n표본 프레임에는 때로 목표 모집단에 속하지 않는 요소들이 포함될 수 있다. 예를 들어, 전화번호 프레임에는 작업 또는 비거주 전화번호가 많이 포함될 수 있는데, 이는 가구 모집단을 대상으로 하는 프레임의 사용을 복잡하게 만든다. 지역 확률 조사에서는 종종 지도 자료에 목표 지리적 영역 외부의 단위가 포함될 수 있다. 조사원이 주택 단위를 나열하기 위해 표본 영역을 방문할 때, 때때로 점유되지 않았거나 주택 단위로 보이는 사업장 구조물을 포함시킬 수 있다.\n조사원이 주택 단위에서 가구 구성원의 목록을 작성할 때, 응답자가 생각하는 ”가구”의 개념과 조사에서 요구하는 정의가 다를 수 있다. 예를 들어, 집을 떠나 학교에 다니는 학생의 부모는 여전히 그들을 가구의 일부로 여길 수 있지만, 대부분의 조사에서는 이들을 대학생으로 분류하여 별도로 다룬다. 또한, 응답자는 같은 주택 내에서 방을 임대해 거주하는 사람이나 친족 관계가 없는 사람들을 가구 구성원으로 포함하지 않을 가능성이 높다. 이는 조사 결과에서 특정 가구 유형이나 가족 구성원의 불균형을 초래할 수 있다.\n프레임에서 선택 시작 전에 외부 단위가 식별되면 적은 비용으로 제거될 수 있다. 외부 단위의 비율이 소수라면 표본 크기를 줄이는 것과 같은 스크리닝 단계에서 이를 식별하고 표본에서 제외할 수 있다. 외부 단위의 발생률이 대략적으로라도 사전에 알려진 경우, 일부 외부 단위를 스크리닝할 것을 예상하며 추가 단위를 선택할 수 있다. 예를 들어, 전국 전화번호 명부 리스트의 약 15%가 더 이상 존재하지 않는 번호임을 알고 있는 경우, 100개의 전화 가구 표본을 얻기 위해 디렉토리에서 100/(1 - 0.15) = 118개의 항목을 선택할 수 있으며, 그 중 18개가 유효하지 않는 번호일 것이다.\n\n\n(3) 프레임 요소 내에서 목표 모집단 요소의 클러스터링\n프레임에서 모집단으로, 또는 모집단에서 프레임으로의 다중 매핑(클러스터링 또는 중복)은 표본 선택에서 문제를 일으킬 수 있다. 전화번호부를 표본 프레임으로 사용해 전화 가구에 거주하는 성인(목표 모집단)을 표본으로 삼는 경우 전화번호부에 나열된 가구에는 하나의 프레임 요소(전화번호)로 여러 성인이 포함될 수 있다.\n클러스터링 문제의 예\n홍길동 가족(홀길동, 홍길동 아내, 홀길동 부모)은 같은 가구에 살며 동일 전화번호를 공유한다. 이 전화번호는 표본 프레임 요소로 사용되며, 홀길동 가족 모두가 동일한 프레임 요소와 연결됩니다. 그러나 이들은 목표 모집단의 4 요소를 구성한다.\n클러스터링 문제 해결 방법\n클러스터링 문제를 해결하는 한 가지 방법은 선택된 프레임 요소(예: 전화번호)에 속한 모든 자격 요소(목표 모집단 요소)를 포함하는 것이다. 이러한 설계에서는 클러스터 내의 모든 요소에 동일한 선택 확률이 적용된다.\n클러스터링 문제의 중요성\n클러스터링은 종종 클러스터를 부분적으로 표본화 하게 되는 중요한 문제를 제기한다.\n\n첫째, 일부 경우 클러스터의 모든 요소에서 성공적으로 정보를 수집하기 어려울 수 있다. 예를 들어, 전화 가구 조사에서는 한 가구에서 여러 번 인터뷰를 시도하면 무응답 비율이 증가하는 경우가 있다.\n둘째, 인터뷰가 여러 시간대에 걸쳐 진행되어야 할 경우 초기 응답자가 나중 응답자에게 설문 내용을 논의하면서 답변에 영향을 줄 수 있다.\n셋째, 클러스터 크기가 다를 경우 표본 크기 통제가 어려워질 수 있다.\n\n클러스터 크기와 표본 왜곡\n큰 클러스터의 요소는 작은 클러스터 요소에 비해 선택될 확률이 낮다. 예를 들어, 전화번호가 표본으로 선택되었을 때 두 명의 자격 요소를 포함한 가구에서는 한 사람이 선택될 확률이 50%인 반면, 네 명의 자격 요소를 포함한 가구에서는 각각 25%의 확률을 갖게 된다. 이러한 샘플링의 결과로 소규모 가구의 구성원이 목표 모집단에 비해 과대표될 가능성이 있다. 예를 들어, 범죄 피해 조사에서 소규모 가구의 구성원이 범죄 피해를 입을 확률이 더 높은 경우, 클러스터 크기와 변수 간의 관계로 인해 표본 결과는 편향된 추정치를 제공할 수 있다.\n해결 방법\n이러한 편향을 제거하기 위해 분석 과정에서 보상 조치를 취해야 한다. 클러스터 내 자격 요소 수에 따라 가중치를 적용해 표본 추정치를 수정할 수 있다.\n\n\n(4) 표본 프레임에서 목표 모집단 요소의 중복\n프레임과 목표 모집단 사이의 또 다른 중복 매핑 유형은 ”중복”이다. 중복은 단일 목표 모집단 요소가 여러 프레임 요소와 연관된 경우를 의미한다. 전화 설문조사 예를 들어, 단일 전화 가구가 전화번호부에 여러 번 나열되는 경우가 있다. 홍길동 목표 모집단 구성원은 전화번호 두 개를 등록하여 두 개의 프레임 요소와 연관되어 있다고 하자. 이러한 프레임 문제는 클러스터링과 유사하다. 여러 프레임 단위를 가진 목표 모집단 요소는 선택될 확률이 높아져 모집단에 비해 과대 대표된다. 중복과 관심 변수 간의 상관관계가 있는 경우, 설문조사 추정치는 편향될 수 있다. 문제는 중복 여부와 중복과 조사 변수 간의 상관관계가 종종 알려지지 않는다는 점이다.\n중복으로 인한 편향 문제는 다양한 방식으로 해결할 수 있다. 첫 번째 방법은 표본 선택 전에 프레임에서 중복 항목을 제거하는 것이다. 예를 들어, 전자 전화번호부를 정렬하여 동일한 번호의 중복 항목을 삭제하는 방식이다. 두 번째 방법은 표본 선택 과정이나 데이터 수집 중에 중복된 프레임 단위를 식별하는 것이다. 이 경우, 간단한 규칙을 적용하여 중복 항목을 처리할 수 있다. 예를 들어, 디렉토리에 여러 항목이 있을 경우, 첫 번째 항목만 유효하다고 간주하는 규칙을 사용할 수 있다. 데이터 수집 중에는 조사원이 가구에 여러 디렉토리 항목이 있는지 확인한 뒤, 확인된 중복 항목 중 첫 번째 항목만 선택하고 나머지는 ”외부 단위”로 분류하여 제외할 수 있다. 이러한 접근 방식은 중복으로 인한 표본 편향을 줄이는 데 효과적이다.\n또 다른 해결책은 가중치를 부여하는 방법이다. 클러스터링의 경우와 유사하게, 중복된 프레임 요소의 개수를 기반으로 역수를 사용하여 가중치를 계산한다. 예를 들어, 한 전화 가구가 두 개의 전화선을 보유하고 있으며 디렉토리에 총 세 개의 항목(한 개 번호는 홍길동 처의 이름으로 중복 등록)이 등재되어 있다면, 이 가구는 표본 내에서 가중치 1/3을 받게 된다. 반면, 무작위 숫자 다이얼(RDD) 프레임에서는 해당 가구가 가중치 1/2을 받게 된다. 이러한 가중치 계산은 표본 내의 중복 문제를 보정하여 통계적 편향을 최소화하는 데 기여한다.\n표본 프레임과 목표 모집단 요소 간의 복잡한 매핑\n다수의 프레임 단위가 여러 모집단 요소에 매핑될 가능성도 있습니다. 예를 들어, 성인을 대상으로 한 전화 가구 조사에서는 디렉토리에 여러 항목이 포함된 여러 성인이 있는 가구를 만날 수 있습니다. 이러한 다대다 매핑 문제는 클러스터링과 중복의 조합입니다.\n예를 들어, 홍길동 가구는 두 개의 전화번호 프레임 요소를 가지고 있으며, 이는 두 개의 표본 프레임 요소에 매핑된 세 개의 목표 모집단 요소를 나타낼 수 있습니다. 이 문제에 대한 일반적인 해결책은 조사 결과에 가중치를 부여하여 두 문제를 동시에 처리하는 것입니다. 개별 수준 통계를 위한 보정 가중치는 가구의 성인(또는 적격자) 수를 해당 가구의 프레임 항목 수로 나눈 값으로 정의됩니다. 예를 들어, 홍길동 가구의 구성원에게 부여되는 가중치는 1/2이 됩니다.\n\n\n\n3. 목표모집단과 표본프레임 이슈\n\n(1) 가구와 개인\n가구를 대상으로 한 일반적인 표본 프레임에는 지역 프레임(인구조사 구역 또는 카운티와 같은 지역 단위 목록), 전화번호, 전화목록, 그리고 우편목록이 있다. 지역 프레임은 지리적 단위를 기반으로 하기 때문에, 사람이 해당 지역에 속한다는 것을 거주 연결 규칙을 통해 연관지어야 한다. 이러한 프레임은 개인을 표본으로 선택할 때 여러 단계를 요구한다. 먼저 지역 단위의 일부를 선택한 후, 해당 구역의 주소 목록을 작성해야 한다. 우수한 지도나 항공사진이 있는 경우, 이 프레임은 이론적으로 주거지의 완전한 범위를 제공할 수 있다. 그러나 선택된 지역 단위 내 주거지 목록에 일부 누락된 단위가 존재할 경우, 프레임은 불포함 오류를 겪게 된다. 한 사람이 두 개 이상의 거주지를 가지고 있는 경우에는 중복 문제가 발생하며, 여러 사람이 동일한 거주지에 거주하는 경우에는 개인을 표본으로 선택할 때 클러스터링 문제가 발생한다.\n또 다른 가구 모집단 프레임은 주택 내 유선전화 번호를 기반으로 한 프레임이다. 일부 가구는 여러 개의 전화번호를 보유하고 있어 과포함 문제가 발생할 수 있다. 이 프레임에는 사용되지 않는 전화번호와 비주거용 번호가 포함되어 있기 때문에, 이를 개인 수준의 표본으로 활용하려면 제거해야 한다.\n주거용 전화번호 목록 프레임은 전화번호 프레임보다 범위가 좁지만, 비작동 번호와 비주거용 번호가 대부분 제거되어 있어 가구 조사에서는 더 효율적이다. 이 목록은 상업적 기업이 전자 및 인쇄된 전화번호 디렉토리에서 얻으며, 대량 발송업자와 조사기관에 판매한다. 그러나 상당수의 주거용 번호가 디렉토리에 포함되지 않으며, 특히 도시 지역 거주자나 일시적인 거주자의 번호가 빠질 수 있다. 같은 번호가 여러 이름으로 등재되는 경우도 많아 중복 문제가 발생하기도 한다.\n웹 설문조사에 대한 관심이 높아지면서, 이메일 주소를 기반으로 한 가구 모집단 프레임 개발에 주목이 쏠리고 있다. 그러나 이메일 프레임은 전체 가구 모집단을 충분히 포함하지 못하며, 한 사람이 여러 이메일 주소를 보유하거나 여러 사람이 하나의 이메일을 공유하는 등의 이유로 중복 및 클러스터링 문제가 존재한다.\n모바일 또는 휴대전화는 많은 국가에서 유선 전화를 빠르게 대체하고 있다. 예를 들어, 핀란드에서는 1990년대 중반부터 유선 전화 가입자가 감소하고 휴대전화 가입자가 급격히 증가하였다. 이는 기존의 유선 전화 기반 프레임에서 휴대전화 번호가 누락됨에 따라, 프레임 손실이 발생했음을 의미한다. 특히 젊은 세대 중 독립적인 가구를 처음 형성하는 층에서 이러한 손실이 두드러졌다.\n더욱이 휴대전화는 유선전화와 달리 한 사람과 직접 연결되며, 전체 가구 단위와 연결되지 않는다. 따라서 전화 설문조사는 휴대전화 번호를 표본으로 사용할 수밖에 없으며, 이는 프레임과 표본 단위가 가구에서 개인으로 분리되는 것을 요구하게 된다. 현재로서는 유선전화와 휴대전화 번호의 병용에서 비롯된 클러스터링과 중복 문제 등 여러 프레임 관련 이슈가 해결되지 않은 상태이다.\n\n\n(2) 고객, 고용주 또는 조직 구성원\n표본 프레임은 전자 파일 또는 인쇄물 형식으로 구성된 개인 기록일 수 있으며, 이러한 시스템은 주기적인 업데이트 지연으로 인해 불포함 오류가 발생하거나, 조직을 이미 떠난 인물이 빠르게 제거되지 않아 부적격 요소를 포함할 수 있다. 예를 들어, 마지막 거래가 오래전에 이루어진 고객이 여전히 목록에 남아 있는 경우가 있으며, 계약직 직원처럼 조직과의 소속이 모호한 경우도 존재한다. 고객 기반 프레임에서는 거래 단위별로 동일한 고객이 여러 차례 기록되어 중복이 발생할 수 있으며, 이때 설문조사 연구자는 목표 모집단이 ’사람’인지, ’거래’인지, 혹은 둘 다인지를 신중히 판단해야 한다.\n설문조사 연구자는 대체 가능한 프레임을 평가할 때, 해당 목록이 어떤 방식으로 생성되고 수정되는지 파악해야 하며, 예컨대 급여 목록이나 보안 시스템 기록이 특정 직원 집단을 포함하거나 제외할 가능성도 함께 검토해야 한다. 특히 월급제와 주급제의 차이, 임시 결근, 장기 병가 등은 프레임의 포괄성과 대표성을 더욱 복잡하게 만들 수 있다. 따라서 각 설문조사에서는 프레임에 포함될 대상의 기준과 선택 절차를 명확히 정의하고, 그 적절성을 면밀히 검토하는 과정이 반드시 필요하다.\n\n\n(3) 기관\n기관을 대상으로 한 표본 프레임은 일반적으로 단위 목록으로 구성되며, 이 중 기업체는 아마도 설문조사에서 가장 빈번하게 설정되는 목표 모집단일 것이다. 기업 모집단은 고유한 프레임 문제를 수반한다.\n첫째, 기업 모집단의 중요한 특성 중 하나는 규모의 차이가 매우 크다는 점이다. 예를 들어, 소프트웨어 판매업체를 조사할 경우, 연매출이 매우 큰 NC소프트와 소규모 소매점을 모두 프레임에 포함해야 한다. 많은 기업 설문조사는 산업 내 전체 고용 규모나 매출과 같은 크기 관련 변수를 측정하기 때문에, 프레임의 포괄성 문제는 일반적으로 가장 작은 기업보다 가장 큰 기업을 포함하는 데 더 많은 주의를 기울이게 된다.\n둘째, 기업 모집단은 매우 역동적이다. 소규모 기업은 빠르게 설립되거나 폐업되며, 대규모 기업은 다른 회사를 인수하거나 합병하기도 하고, 반대로 분할되기도 한다. 따라서 프레임 모집단은 새로운 기업을 포함하고, 더 이상 존재하지 않는 기업을 제거함으로써 그 포괄성을 유지하기 위해 지속적인 업데이트가 필요하다.\n셋째, 기업 모집단은 법적으로 정의된 실체와 물리적 위치 간의 구분을 내포한다. 예를 들어, 다지점 또는 다국적 기업은 전 세계에 여러 개의 사업장을 운영하지만 본사는 하나뿐이다. 이에 따라 설문조사는 ‘기업’(법적 실체)을 대상으로 할 수도 있고, ‘시설’(물리적 단위)을 대상으로 할 수도 있다. 일부 기업은 물리적 위치 없이 운영되기도 하며(예: 재택 근무 기반의 컨설팅 회사), 반대로 여러 기업이 하나의 물리적 위치를 공유하기도 한다. 이러한 구조는 표본 프레임 설계 시 조사 단위의 정의를 더욱 중요하게 만든다.\n\n\n(4) 사건\n설문조사는 사건 모집단을 대상으로 하며, 여기에 포함되는 사건의 예로는 서비스나 제품 구매, 결혼, 임신, 출생, 실직, 우울증 사례, 범죄 피해 등이 있다. 이러한 사건에 대한 설문조사는 일반적으로 사람들을 대상으로 한 프레임에서 시작되며, 일부 사람들은 여러 사건을 경험하면서 사건 요소 간의 집단을 형성하게 된다.\n사건 표본추출의 또 다른 접근 방식은 시간 단위를 프레임으로 사용하는 것이다. 예를 들어, 동물원 방문 사례를 조사할 때 방문 시간을 기준으로 프레임을 구성하고, 일정한 시간 간격(예를 들어 5분 블록)을 선택하여 해당 시간에 방문한 사람들을 대상으로 질문하는 방식이 사용될 수 있다.\n일부 시간 사용 설문조사는 무작위로 선택된 시점에 전자 호출기를 통해 신호음을 발생시키는 방식을 사용한다. 신호가 울리면, 응답자는 그 시점에 자신이 무엇을 하고 있었는지를 보고하도록 되어 있다. 예를 들어, 사무실에서 일하고 있었는지, 텔레비전을 시청하고 있었는지, 혹은 쇼핑을 하고 있었는지를 기록하게 된다.\n사건을 대상으로 하는 설문조사는 경우에 따라 여러 모집단을 동시에 포함할 수 있다. 이러한 조사는 사건 자체에 대한 통계뿐만 아니라 그 사건을 경험한 사람들에 대한 통계에도 관심을 가진다. 이처럼 목적이 이중적인 경우, 표본 설계 과정에서 클러스터링과 중복과 같은 문제가 발생할 수 있다. 예를 들어, 가족에 의한 자동차 구매 사건을 조사하는 경우, 사건 요소는 구매 행위이지만, 사건을 경험한 사람으로는 법적 소유자, 모든 가족 구성원, 또는 차량을 운전할 가능성이 있는 사람 등 다양한 해석이 가능하다.\n\n\n(5) 희귀 모집단\n희귀 모집단은 연구자가 관심을 갖는 소규모 대상 집단을 지칭하는 용어로, 이들이 희귀하다고 판단되는 이유는 절대적인 규모보다는 사용 가능한 프레임 내에서의 상대적 크기 때문이다. 예를 들어, 전체 인구가 5천만 명이고 이 중 100만 명이 노인 복지 혜택을 받고 있다면, 이는 전체 인구의 약 2%에 해당하므로 희귀 모집단으로 간주될 수 있다. 이러한 모집단을 조사 대상으로 설정할 경우, 적절한 표본 프레임을 식별하는 데 여러 가지 어려움이 따른다.\n희귀 모집단을 위한 표본 프레임을 구축하는 방식에는 크게 두 가지 접근이 있다. 첫째는 희귀 모집단에 속하는 요소들의 목록을 직접 구성하는 방법이다. 예를 들어, 복지 수급자의 목록을 복지 사무소의 행정기록에서 얻을 수 있다. 다만 이러한 자료는 종종 기밀로 취급되거나, 단일 목록이 전체 모집단을 포괄하지 못하는 경우가 많아 여러 출처의 목록을 조합해야 할 수도 있다.\n둘째는 보다 일반적인 모집단 프레임을 설정하고, 그 안에서 희귀 모집단에 해당하는 요소들을 선별하는 방식이다. 예를 들어, 일반 가구 모집단을 대상으로 하여 그 안에서 복지 수급 가구를 찾아내는 방식이 여기에 해당한다. 만약 희귀 모집단의 모든 구성원이 더 큰 프레임 모집단의 하위 집합으로 포함된다면, 희귀 모집단에 대한 완전한 포괄이 가능하다.\n\n\n\n4. 포함 오류\n불포함은 해결하기 어려운 문제이며, 설문조사에서 중요한 포함 오류의 원인이 될 수 있다. 포함 오류는 표본 통계나 설문조사에서 도출된 추정치의 특성에 영향을 미친다. 하나의 통계는 포함 오류로 인해 크게 왜곡될 수 있는 반면, 동일한 설문조사에서 얻어진 다른 통계는 같은 오류에 거의 영향을 받지 않을 수도 있다. 설문조사 방법론에서는 불포함, 중복, 클러스터링 등을 포함 오류를 유발하는 표본 프레임의 구조적 문제로 본다. 포함 오류란 이러한 문제들이 조사 결과에 미치는 영향을 지칭하는 개념이다.\n\n포함 오류: \\({\\overline{Y}}_{C} - \\overline{Y} = \\frac{U}{N}({\\overline{Y}}_{C} - {\\overline{Y}}_{U})\\), 여기서 \\(\\overline{Y}\\)는 목표 모집단 전체의 평균, \\({\\overline{Y}}_{C}\\)는 표본 프레임에 포함된 모집단의 평균, \\({\\overline{Y}}_{U}\\)는 표본 프레임 밖 모집단의 평균을 나타낸다. \\(N\\)은 목표 모집단의 총 구성원 수, \\(C\\)는 표본 프레임에 포함된 적격 구성원의 총수, 그리고 \\(U\\)는 표본 프레임에 포함되지 않은 적격 구성원의 총수이다.\n\n따라서 프레임에 포함되지 않은 \\((N - c)\\) 개의 단위로 인해 발생하는 오류는, 전체 모집단에서 포함되지 않은 비율과 포함된 단위와 포함되지 않은 단위 간의 평균 차이에 따라 결정된다. 설문조사는 표본 크기와 무관하게 포함된 단위의 평균 \\({\\overline{Y}}_{C}\\) 만을 추정할 수 있다. 이때, 포함되지 않은 단위 U의 규모가 크거나, 포함된 단위와 포함되지 않은 단위 간의 특성 차이가 클수록 편향, 즉 포함 오류의 크기는 커지게 된다.\n포함되지 않은 비율은 모집단의 하위 계층에 따라 달라질 수 있다. 전체 모집단에서의 불포함 비율보다 특정 하위 그룹에서의 불포함 비율이 더 높을 수도 있다. 또한 포함 오류는 포함된 단위와 포함되지 않은 단위 간의 추정치 차이에 따라 결정되므로, 동일한 적격 단위 하위 계층을 기준으로 계산된 통계라 하더라도 각 통계별로 포함 오류의 정도는 다를 수 있다."
  },
  {
    "objectID": "notes/survey/survey_scale.html",
    "href": "notes/survey/survey_scale.html",
    "title": "조사방법론. 6. 문항 척도",
    "section": "",
    "text": "chapter 1. 문항 척도 개요\n설문조사는 연구자가 특정 주제에 대해 구조화된 방식으로 정보를 수집하고 분석하는 주요 연구 방법 중 하나이다. 이 과정에서 척도(scale)는 응답자의 태도, 인식, 행동을 수치화하여 측정하는 핵심 도구로 작용하며, 연구 결과의 신뢰도(reliability)와 타당도(validity)에 직접적인 영향을 미친다. 따라서 연구 목적에 부합하는 적절한 척도를 선택하고 이를 정확하게 활용하는 것은 설문 데이터의 품질을 높이고, 분석 결과의 객관성과 재현성을 확보하는 데 필수적인 절차라 할 수 있다.\n\n1. 척도 개념\n척도는 설문 응답을 수치화하여 체계적으로 정리하는 방식으로, 응답 결과를 정량적으로 비교하고 분석할 수 있도록 돕는다. 설문조사에서는 다양한 유형의 척도가 사용되며, 이는 단순한 의견 수집을 넘어 분석 가능하고 일관성 있는 데이터를 생성하는 데 기여한다.\n첫째, 응답의 표준화 기능을 통해 모든 응답자가 동일한 기준에서 문항을 해석하고 응답하도록 유도할 수 있다. 이는 결과의 객관성을 높이고, 응답 간의 비교 가능성을 확보하는 데 필수적이다.\n둘째, 신뢰성 향상에도 기여한다. 동일한 질문을 반복 측정했을 때 유사한 결과가 나타나는지를 확인할 수 있으며, 예를 들어 리커트 5점 또는 7점 척도와 같이 일관된 방식의 척도는 태도나 인식의 변화를 비교하는 데 유리하다. 신뢰도 높은 척도는 연구 결과의 재현성을 높이고 후속 연구에서도 동일한 틀을 유지할 수 있게 한다.\n셋째, 타당성 확보에 있어서는 연구자가 측정하고자 하는 개념을 정확히 반영하는 척도를 선택하는 것이 핵심이다. 예를 들어 ’소비자 만족’을 단순한 이분법(만족/불만족)으로 측정하는 것보다, 5점 또는 7점 척도를 사용하는 것이 미묘한 차이를 반영하는 데 유리하다.\n마지막으로, 척도는 통계적 분석 가능성을 확장시킨다. 명목 및 서열 척도는 빈도 분석이나 카이제곱 검정에 적합하며, 등간 및 비율 척도는 평균, 표준편차, 상관 및 회귀분석 등 다양한 수치 분석 기법을 적용할 수 있다. 이처럼 척도의 적절한 활용은 조사 데이터의 질적 수준을 높이고, 과학적 분석을 가능하게 한다.\n\n\n2. 척도의 유형과 선택의 중요성\n\n\n\n\n\n\n\n\n\n척도 유형\n설명\n예시\n주요 분석 방법\n\n\n명목 척도\n(Nominal Scale)\n단순한 분류와 명칭을 부여하는 방식\n성별(남/여), 종교(기독교/불교)\n빈도분석, 카이제곱 검정\n\n\n서열 척도\n(Ordinal Scale)\n순서를 구별할 수 있으나 간격이 일정하지 않음\n교육 수준(초등/중등/고등), 만족도(높음/보통/낮음)\n순위 분석, 카이제곱 검정\n\n\n등간 척도\n(Interval Scale)\n간격이 일정하지만 절대적인 0점이 없음\n온도(°C), 지능지수(IQ)\n평균, 표준편차, 상관분석\n\n\n비율 척도\n(Ratio Scale)\n절대적인 0점을 가지며, 비율 비교가 가능\n나이(세), 소득(만원), 키(cm)\n평균, 분산 분석, 회귀 분석\n\n\n\n설문 조사에서 척도를 적절히 설정하는 것은 매우 중요하다. 척도가 부적절하게 설계되면 응답자의 반응이 일관성을 잃고, 수집된 데이터의 해석에도 오류가 발생할 수 있다. 이는 궁극적으로 연구 결과의 신뢰도를 저하시켜, 의미 있는 분석과 일반화된 결론 도출을 어렵게 만든다.\n예를 들어, ’매우 만족–만족–보통–불만족–매우 불만족’과 같은 5점 척도를 사용하면 응답자의 태도 강도를 세분화해 파악할 수 있다. 반면, 이를 단순히 ’예/아니오’와 같은 이분법적 응답 방식으로 대체하면 태도 변화의 미묘한 차이를 반영하기 어렵고, 해석의 폭도 좁아질 수 있다. 따라서 질문의 목적과 분석 수준에 맞는 척도를 설정하는 것이 설문 설계의 핵심 요소라 할 수 있다.\n\n통계적 분석 가능성: 척도는 통계적 분석의 기반이 되며, 설문 응답을 정량화할 수 있도록 돕는다. 만약 모든 문항이 개방형으로 구성된다면, 응답 내용의 수치화가 어렵고 체계적인 분석이 제한될 수 있다. 반면, 폐쇄형 질문에 적절한 척도를 결합하면 응답을 수치로 변환하여 다양한 통계 기법을 적용할 수 있다. 예를 들어, 마케팅 조사에서 소비자의 브랜드 인식을 등간 척도로 측정하면, 분산 분석(ANOVA)이나 회귀 분석을 통해 보다 정교한 통계적 해석이 가능하다.\n결과의 재현성 확보: 또한, 척도가 명확하게 정의되어 있다면 동일한 연구를 다른 시점이나 환경에서 반복해도 유사한 결과를 도출할 수 있다. 이는 연구의 재현성을 확보하고, 결과의 일반화 가능성을 높이는 데 기여한다. 이러한 점에서 척도는 단순한 응답 수단을 넘어서 과학적 연구 설계의 핵심 요소로 작용한다.\n\n설문조사에서 척도는 단순한 응답 선택지를 넘어, 수집된 데이터의 품질과 연구 결과의 신뢰도를 좌우하는 핵심 요소이다. 적절한 척도를 선택하면 연구자는 응답자의 태도나 행동을 정밀하게 측정할 수 있으며, 통계적으로 타당한 분석을 통해 보다 신뢰성 있는 결론을 도출할 수 있다. 따라서 설문 설계 단계에서 연구 목적에 부합하는 척도를 신중히 선택하는 것이 중요하며, 이는 데이터의 일관성과 해석의 정밀도를 높여 의미 있는 연구 성과로 이어진다.\n\n\n\nchapter 2. 전통적인 척도\n\n\n\n\n\n\n\n\n\n\n구분\n명목 척도\n서열 척도\n등간 척도\n비율 척도\n\n\n측정값 간 순서\nX\nO\nO\nO\n\n\n측정값 간 차이 일정성\nX\nX\nO\nO\n\n\n덧셈, 뺄셈 연산 가능 여부\nX\nX\nO\nO\n\n\n비율(나눗셈) 계산 가능 여부\nX\nX\nX\nO\n\n\n예시\n성별, 혈액형, 국적\n학년, 학점, 만족도\n온도, IQ 점수\n키, 몸무게, 연봉, 나이\n\n\n\n\n1. 명목 척도\n명목 척도는 가장 기초적인 수준의 측정 척도로, 대상을 단순히 분류하거나 명명하는 역할을 한다. 이 척도에서 사용되는 숫자는 단순한 식별자 역할을 할 뿐, 수치적 의미를 가지지 않는다. 즉, 명목 척도에서 1과 2, 또는 A와 B는 단순히 서로 다른 범주를 의미할 뿐, 크기나 순서의 개념이 없다.\n예를 들어, 응답자의 성별을 측정하는 문항에서 \"남성\"을 1, \"여성\"을 2로 표시할 수 있다. 그러나 이는 단순히 남성과 여성을 구분하기 위한 코드일 뿐, '1이 2보다 크다'는 의미를 갖지 않는다.\n\n(1) 명목 척도 특징\n측정값 간 서열이 없음\n명목 척도는 측정값 간 서열이나 크기의 차이를 나타내지 않는다. 이는 단순히 각 범주를 구별하기 위한 코드로 사용될 뿐, 그 숫자 간에 어떤 순서나 우열도 존재하지 않는다. 예를 들어, 종교 항목에서 “기독교=1, 불교=2, 이슬람교=3, 무교=4”와 같이 숫자를 부여한다고 해도, 이는 편의상 구분을 위한 부호일 뿐 “무교(4)”가 “기독교(1)”보다 크거나 뒤에 있다는 의미는 없다. 따라서 명목 척도는 범주 간 비교는 가능하되, 서열 비교는 허용되지 않는다.\n값의 차이를 측정할 수 없음\n명목 척도는 범주 간의 차이나 간격을 수치적으로 해석할 수 없다. 예를 들어, 성별을 “남성=1, 여성=2”로 부호화했을 때, 이들 사이의 차이를 “2 - 1 = 1”이라고 수치적으로 해석하는 것은 의미가 없다. 이 숫자는 단순히 범주를 구분하기 위한 상징일 뿐, 수학적 연산(합, 차, 평균 등)에 사용될 수 있는 값이 아니다. 따라서 명목 척도에서는 범주 간의 차이나 비율을 측정하거나 비교하는 것은 부적절하며, 빈도나 비율 같은 비수치적 분석이 주로 활용된다.\n기본적인 연산이 불가능함\n명목 척도는 단순히 범주를 구분하는 역할만 하므로, 수치 간의 의미 있는 간격이나 순서가 존재하지 않는다. 이로 인해 평균이나 표준편차처럼 수학적 계산을 필요로 하는 연산은 수행할 수 없다. 예를 들어, “기독교=1, 불교=2, 천주교=3”으로 부호화된 종교 변수의 평균을 구하는 것은 의미가 없다. 대신, 명목 척도에서는 각 범주에 속하는 응답자의 빈도 수, 전체 응답자 대비 백분율, 가장 많이 선택된 범주인 최빈값(Mode) 등을 활용한 기술 통계 분석이 가능하다. 이러한 방식으로 집단의 특성을 요약하고 비교할 수 있다.\n데이터의 범주가 명확해야 함\n명목 척도를 설계할 때는 범주 간의 구분이 명확하고, 모든 가능한 선택지가 포함되어야 한다. 응답자가 자신의 속성이나 의견을 올바르게 선택할 수 있도록 서로 겹치지 않는 범주(disjoint categories) 를 제공해야 하며, 중요한 응답 선택지가 누락되지 않도록 포괄성도 확보해야 한다. 예를 들어, “좋아하는 색깔”을 묻는 문항에서 “빨강, 파랑, 노랑”만 제시하고 “녹색”이나 “기타” 선택지를 제공하지 않는다면, 응답자의 진의를 반영하지 못해 데이터의 왜곡이 발생할 수 있다. 따라서 명확하고 완전한 범주 구성이 필수적이다.\n\n\n\n\n\n\n\n변수\n예시 값\n\n\n성별\n남성(1), 여성(2)\n\n\n국적\n한국(1), 미국(2), 영국(3)\n\n\n혈액형\nA형(1), B형(2), O형(3), AB형(4)\n\n\n종교\n기독교(1), 불교(2), 이슬람교(3), 무교(4)\n\n\n학과\n경영학과(1), 심리학과(2), 공학과(3)\n\n\n\n\n\n(2) 명목 척도 분석 방법\n명목 척도는 기본적인 범주형 데이터 분석을 수행하는 데 적합하다. 주요 분석 방법은 다음과 같다.\n빈도 분석 frequency analysis\n명목 척도 데이터를 요약하는 데 가장 기본적이고 효과적인 방법이다. 각 범주별로 응답자의 수(빈도)를 세고, 전체 응답자 수 대비 비율(%)을 계산함으로써 자료의 분포와 구성 비율을 직관적으로 파악할 수 있다.\n예를 들어, 설문조사에서 혈액형을 묻는 질문에 대해 다음과 같은 결과가 나왔다면: A형: 30명 (30%), B형: 25명 (25%), O형: 35명 (35%), AB형: 10명 (10%) 이처럼 빈도와 백분율을 함께 제시하면 각 범주의 상대적 크기와 특성을 명확하게 비교할 수 있어, 조사 대상의 특성을 이해하는 데 유용하다. 막대그래프(bar chart)나 원형그래프(pie chart)로 시각화하면 더 효과적인 설명이 가능하다.\n최빈값 mode 분석\n명목 척도 데이터를 요약할 때 대표값을 제시하는 가장 적절한 방법이다. 명목 척도에서는 숫자 간의 순서나 간격이 없기 때문에 평균이나 중앙값을 계산하는 것이 무의미하며, 가장 많이 나타난 응답(최빈값)을 통해 데이터를 대표할 수 있다.\n예를 들어, 학과별 학생 수를 조사한 결과가 다음과 같다면: 경영학과: 50명, 심리학과: 30명, 컴퓨터공학과: 40명, 이 경우 최빈값(mode)은 ’경영학과’이며, 이는 해당 집단에서 가장 흔한(대표적인) 특성으로 해석된다. 명목형 자료에서는 이러한 최빈값 분석이 실제 현장의 특성을 단순하고 명확하게 설명하는 데 매우 유용하다.\n카이제곱 검정 Chi-Square test\n두 개 이상의 명목형 변수 간의 통계적 연관성을 분석할 때 사용된다. 이 검정은 관측된 빈도와 기대 빈도의 차이를 비교하여, 두 변수 간에 우연 이상의 연관성이 있는지를 판단한다.\n예를 들어, 설문조사에서 응답자의 성별(남성/여성)과 선호하는 브랜드(브랜드 A/B/C) 간의 관계를 알아보고자 할 때, 다음과 같은 질문이 가능하다: “성별에 따라 선호 브랜드가 달라지는가?”\n이때 실제 응답에서 나타난 성별-브랜드 분포(관측값)와, 성별과 브랜드가 독립이라고 가정했을 때의 기대값 간의 차이를 분석하여 유의미한 차이가 있는지를 검정한다.\n백분율 분석 percentage analysis\n백분율 분석은 각 범주가 전체 응답자 중에서 차지하는 비중을 계산하여, 데이터의 분포를 직관적으로 파악할 수 있도록 해주는 방법이다. 주로 명목 척도에서 사용되며, 다양한 범주 간의 상대적인 크기를 비교하거나, 특정 집단이 전체에서 얼마나 중요한 비율을 차지하는지를 나타낼 때 유용하다.\n예를 들어, 종교에 대한 설문 조사에서 응답자의 40%가 무교, 35%가 기독교, 15%가 불교, 10%가 기타 종교라고 응답한 경우, 백분율 분석을 통해 각 종교의 상대적인 분포를 명확하게 확인할 수 있다. 이처럼 백분율은 단순한 빈도 수치를 상대적인 수치로 변환하여, 다양한 집단 간 비교를 용이하게 하고, 조사 결과를 시각적으로 설명할 수 있도록 돕는다.\n\n\n(3) 명목 척도의 장점과 단점\n명목 척도는 조사 대상의 특성을 구분하거나 분류할 때 가장 기본적으로 사용되는 측정 도구로, 실생활에서 쉽게 관찰되는 성별, 종교, 국적 등과 같은 범주형 정보를 효과적으로 수집할 수 있다. 이 척도는 응답자가 쉽게 이해하고 선택할 수 있는 형태이기 때문에, 데이터 수집이 용이하고 해석도 간단하다는 장점이 있다. 또한 인구통계학적 조사, 시장 조사, 심리학 연구 등 다양한 분야에서 폭넓게 활용되며, 현실에서 사용되는 범주를 그대로 반영할 수 있다는 점에서 응답의 실효성을 높인다.\n그러나 명목 척도는 수치 간의 크기나 순서를 나타내지 않기 때문에 평균이나 표준편차와 같은 수학적 연산이 불가능하다. 단순 빈도나 백분율과 같은 기초적인 분석만 수행할 수 있으며, 그 이상의 정교한 통계 분석은 어렵다는 한계가 있다. 또한, 범주 설정이 명확하지 않거나 중복될 경우, 응답자의 혼란을 초래하고 데이터 해석에도 오류가 발생할 수 있다. 이러한 점에서 명목 척도는 정보량이 상대적으로 적으며, 분석의 깊이를 확보하는 데 제한이 있다.\n\n\n\n2. 서열 척도\n서열 척도는 조사 대상 간의 순서를 비교할 수 있도록 해주는 측정 방식으로, 예를 들어 어떤 대상이 더 높거나 낮은지, 더 선호되거나 덜 선호되는지를 나타낼 수 있다. 응답자의 태도, 만족도, 선호도와 같은 심리적 또는 주관적 평가를 측정할 때 주로 사용되며, 명목 척도보다 더 많은 정보를 담고 있다는 점에서 분석의 정밀도를 높여준다.\n서열 척도의 핵심 특징은 순위를 부여할 수 있다는 점이다. 예를 들어 대학 성적에서 A, B, C, D, F와 같은 등급이 있다면, A가 B보다 우수하다는 순위는 명확하지만 A와 B의 점수 차이와 C와 D의 점수 차이가 동일하다고 볼 수는 없다. 즉, 순서 정보는 제공하지만 간격 정보는 제공하지 않는다는 것이 서열 척도의 중요한 제한점이다.\n이러한 특성으로 인해 서열 척도는 빈도분석, 백분율, 중앙값, 사분위수와 같은 순위 기반 분석에는 적합하지만, 평균이나 표준편차를 사용하는 분석에는 주의가 필요하다. 평균을 계산하는 것이 불가능한 것은 아니지만, 간격이 일정하지 않다는 점에서 그 해석에 신중함이 요구된다. 따라서 서열 척도는 정보량이 많고 해석이 직관적이지만, 정량적인 분석에는 다소 제약이 따를 수 있다.\n\n(1) 서열 척도 예시\n설문조사 및 심리 척도\n\n만족도 조사: ”매우 만족 - 만족 - 보통 - 불만족 - 매우 불만족”\n중요도 평가: ”매우 중요 - 중요 - 보통 - 중요하지 않음 - 전혀 중요하지 않음”\n\n경쟁 및 순위 관련 데이터\n\n고객 선호 브랜드 순위: 1위, 2위, 3위…\n스포츠 경기 성적: 금메달, 은메달, 동메달\n\n교육 및 시험 결과\n\n학점: A, B, C, D, F\n시험 성적 등급: 상, 중, 하\n\n소득 및 계층 구분\n\n고소득 &gt; 중소득 &gt; 저소득\n사회계층: 상류층 &gt; 중산층 &gt; 서민층\n\n건강 및 의료 평가\n\n병의 심각도: 경증 &gt; 중등도 &gt; 중증\n환자의 통증 수준: 심한 통증 &gt; 중간 통증 &gt; 약한 통증 &gt; 없음\n\n\n\n(2) 서열 척도 특징\n순위(서열) 부여 가능: 서열 척도에서는 대상 간의 비교가 가능하며, 순서를 정할 수 있다. (예) 소득 수준: 고소득 &gt; 중소득 &gt; 저소득\n순위 간의 간격(interval)은 균등하지 않음: 측정값 사이의 차이가 동일한 크기를 의미하지 않는다. (예) 영화 평점(별점 1~5점)에서 5점과 4점 차이가 2점과 1점 차이보다 크거나 작을 수 있음.\n(예) 만족도 조사(매우 만족, 만족, 보통, 불만족, 매우 불만족)의 경우, ’보통(3)’에서 ’만족(4)’으로 변하는 것이 ’불만족(2)’에서 ’보통(3)’으로 변하는 것과 동일한 차이를 가진다고 보기 어려움.\n덧셈, 뺄셈 등의 수치적 연산이 불가능: 서열 척도에서는 측정값이 숫자로 표현될 수 있으나, 그 숫자는 단순히 순서를 나타낼 뿐 절대적인 수치적 의미를 가지지는 않는다. 예를 들어, 만족도 조사에서 “매우 만족 = 5”, “만족 = 4”, “보통 = 3”, “불만족 = 2”, “매우 불만족 = 1”처럼 숫자를 부여할 수 있지만, 이들 간의 간격이 동일하다고 볼 수 없다.\n따라서 평균을 계산하여 분석하는 것보다는, 중위값(Median) 또는 최빈값(Mode)처럼 순위를 기반으로 한 통계량을 사용하는 것이 더 적절하다. 이러한 방식은 서열 척도의 핵심인 순서 정보를 보존하면서도, 간격이 일정하지 않다는 특성을 고려한 해석을 가능하게 한다.\n비율 계산 불가능: ”A의 점수가 B의 점수의 2배다”와 같은 해석이 불가능하다. (예) 선호도 조사에서 1위(고객 100명)와 2위(고객 50명)의 차이가 2배라고 해서, 1위 브랜드가 2위보다 정확히 2배 더 선호된다고 볼 수 없음.\n\n\n(3) 서열 척도 통계 분석 방법\n서열 척도 데이터는 평균이나 표준편차 계산이 적절하지 않으며, 대신 다음과 같은 기법이 주로 사용된다.\n최빈값 및 중위값 사용: 평균은 서열 척도에 적절하지 않기 때문에 중위값이나 최빈값을 사용하여 데이터의 중심 경향을 분석한다.\n순위 기반 통계 기법 적용: 순위합 검정(Rank-Sum Test), 윌콕슨 부호순위 검정(Wilcoxon Signed-Rank Test), 크루스칼-월리스 검정(Kruskal-Wallis Test) 등. 이는 서열 척도의 특성을 고려하여 두 그룹 이상 간의 차이를 비교하는 비모수적 방법이다.\n상관분석 시 순위 기반 기법 사용: Spearman의 순위 상관계수 또는 Kendall의 순위 상관계수(Kendall’s tau)를 사용한다.\n로지스틱 회귀(Logistic Regression) 사용 가능: 예측변수(종속변수)가 서열 척도인 경우 서열 로지스틱 회귀(Ordinal Logistic Regression, Proportional Odds Model) 를 사용한다.\n\n\n(5) 서열 척도의 한계와 보완 방법\n서열 척도는 측정 대상 간의 순서를 알 수 있다는 점에서 유용하지만, 각 순위 간의 간격이 균등하지 않다는 한계를 가지고 있다. 예를 들어, 어떤 응답자가 ’매우 만족(5점)’을 선택하고, 다른 응답자가 ’만족(4점)’을 선택했다 하더라도, 그 차이가 ’보통(3점)’과 ‘불만족(2점)’ 간의 차이와 같다고 보장할 수는 없다. 이처럼 간격이 일정하지 않기 때문에 평균이나 표준편차 등의 수치 연산을 적용하기에는 주의가 필요하다.\n그러나 실제 연구에서는 서열 척도 중 일부, 특히 Likert 척도(예: 5점 척도, 7점 척도 등)에 대해, 응답 간격이 대체로 균등하다고 간주할 수 있는 경우에는 등간 척도로 변환하여 사용하는 방식이 널리 활용된다. 이 경우, 평균, 표준편차 등의 통계 분석을 정당화할 수 있으며, 보다 정밀한 수치 분석이 가능해진다. 단, 이러한 접근은 연구 목적과 응답자의 인식 특성에 대한 충분한 고려를 바탕으로 이루어져야 한다.\n\n\n(6) 리커트 척도는 서열척도\n리커트 척도(Likert Scale)는 1932년 렌시스 리커트(Rensis Likert)가 박사학위 논문에서 처음 고안한 측정 도구로, 응답자의 태도나 의견을 보다 정밀하고 객관적으로 수량화하기 위해 개발되었다.\n기존의 설문조사에서는 “예/아니오”나 “찬성/반대”와 같은 단순한 명목 척도 방식이 주로 사용되었으나, 이는 사람들의 복합적인 감정이나 태도를 충분히 포착하기 어려웠다. 리커트는 이러한 한계를 보완하고자, 단순한 선택을 넘어서 다양한 정도의 동의 수준을 표현할 수 있도록 서열형 응답 구조를 제안했다.\n리커트 척도는 일반적으로 “매우 그렇다”부터 “전혀 그렇지 않다”까지의 다섯 점 또는 일곱 점 척도를 사용하여, 응답자가 자신의 의견을 보다 세밀하게 표현할 수 있도록 돕는다. 이러한 방식은 설문 응답의 신뢰성과 민감도를 높이며, 통계적으로도 평균, 분산 등 다양한 분석을 가능하게 하는 장점이 있다.\n단순한 이분법적 질문의 한계 극복\n기존의 설문조사 방식은 “찬성/반대”와 같은 단순한 이분법적 질문 형식을 주로 사용해 왔지만, 이는 응답자의 태도나 감정을 정밀하게 측정하는 데 한계가 있었다. 이러한 방식에서는 의견의 강도나 미묘한 차이를 반영하기 어렵기 때문에, 조사 결과가 응답자의 실제 생각을 충분히 드러내지 못하는 문제가 발생한다.\n리커트 척도는 이러한 한계를 극복하기 위해, 단순한 찬반 여부를 넘어 응답자의 태도 강도를 함께 측정할 수 있도록 설계되었다. 예를 들어, “이 정책을 찬성합니까?“라는 기존의 질문이 단순히 “찬성” 또는 “반대”만을 선택하게 한다면, 리커트 방식은 “이 정책에 대해 어떻게 생각하십니까?“라는 질문에 대해 “매우 찬성(5)”, “찬성(4)”, “보통(3)”, “반대(2)”, “매우 반대(1)“와 같은 응답 선택지를 제공한다. 이를 통해 응답자는 자신의 태도를 보다 섬세하게 표현할 수 있고, 연구자는 응답자의 의견 강도를 수치화하여 분석할 수 있다. 이처럼 리커트 척도는 태도 측정의 정밀도를 높이는 데 중요한 역할을 한다.\n사회과학 연구에서 태도 및 인식 연구를 위한 표준화된 도구 제공\n사회과학 연구에서는 인간의 태도, 인식, 감정 등과 같은 주관적 요소를 정량적으로 측정하는 것이 매우 중요하지만 동시에 어려운 과제이기도 하다. 이러한 비가시적이고 주관적인 특성들은 단순한 숫자나 범주로 쉽게 표현되기 어렵기 때문이다. 리커트 척도는 이러한 측정의 어려움을 해결하기 위한 표준화된 도구로 널리 활용된다.\n예를 들어 “이 제품을 다시 구매할 의향이 있습니까?”라는 질문에 대해, 단순히 ‘예’ 또는 ’아니오’로 응답하는 것이 아니라, “매우 그렇다(5) – 그렇다(4) – 보통이다(3) – 아니다(2) – 전혀 아니다(1)”와 같이 응답자의 태도 강도를 표현할 수 있는 서열화된 선택지를 제공함으로써, 보다 정밀하고 신뢰도 높은 데이터 수집이 가능해진다. 이처럼 리커트 척도는 태도 및 인식 연구에서 객관성과 일관성을 높이기 위한 핵심적인 도구로 기능하며, 사회과학 연구 전반에서 중요한 표준 측정 방식으로 자리 잡고 있다.\n리커트 척도는 서열 척도이다.\n리커트 척도는 응답자의 태도나 의견을 측정할 때 널리 사용되는 도구로, 기본적으로 서열 척도(ordnal scale)로 간주된다. 그 이유는 각 응답 항목 사이에 순위가 존재하지만, 그 간격이 반드시 균등하다고 보장할 수 없기 때문이다.\n예를 들어, “매우 만족(5) - 만족(4) - 보통(3) - 불만족(2) - 매우 불만족(1)”이라는 5점 척도에서 응답자들은 각 항목을 순서대로 인식하지만, ‘만족’과 ‘보통’ 사이의 거리와 ‘불만족’과 ‘매우 불만족’ 사이의 거리가 동일하다고 단정할 수는 없다. 즉, 수치는 서열을 나타내는 데에는 유용하지만, 수치 간 간격이 심리적으로나 실제로 일정하다는 보장은 없다.\n따라서 리커트 척도는 원칙적으로 서열 척도로 분류되며, 평균이나 표준편차 같은 등간 척도 기반의 분석을 수행할 경우에는 연구 목적에 따라 간격의 동일성을 전제해야 함을 유의해야 한다. 이 같은 전제를 정당화하기 어렵다면, 중위값, 빈도, 범주별 분포 등 서열 데이터에 적합한 분석 기법을 사용하는 것이 바람직하다.\n등간 척도로 활용하는 이유\n리커트 척도는 본래 서열 척도로 분류되지만, 심리학·사회과학 분야의 연구에서는 등간 척도로 간주하여 활용하는 경우가 많다. 그 주된 이유는 분석의 실용성과 통계 지표의 활용도 때문이다.\n서열 척도로 엄격히 해석하면 평균, 표준편차와 같은 연산은 적절하지 않다. 그러나 현실의 연구에서는 응답 항목 간의 간격이 대체로 균등하다는 가정을 받아들이고, 리커트 척도를 등간 척도로 취급한다. 이를 통해 예를 들어 “매우 만족(5) - 만족(4) - 보통(3) - 불만족(2) - 매우 불만족(1)” 같은 5점 척도에서 응답 평균이 3.6점이라면, 비교적 긍정적인 평가로 해석할 수 있다.\n또한, 실험적 연구나 집단 비교 분석에서 평균, 표준편차, t-검정, 분산분석(ANOVA) 등 통계 분석 기법을 적용할 수 있어 분석의 폭이 넓어진다. 실제로 많은 연구자가 리커트 점수의 차이가 응답자의 태도나 인식의 차이를 어느 정도 반영한다고 보고 있으며, 등간 척도로서의 활용은 연구의 실용성과 해석력을 높이는 데 기여하고 있다.\n리커트 척도를 등간 척도로 간주할 때 척도의 신뢰성 검토 필요\n리커트 척도를 등간 척도로 간주하여 평균이나 표준편차 등의 통계 분석을 수행하려면, 그에 앞서 척도의 신뢰성과 일관성에 대한 검토가 반드시 필요하다. 모든 리커트 척도가 등간척도처럼 해석 가능한 것은 아니기 때문이다.\n이는 각 응답자가 척도 간의 간격을 동일하게 인식하지 않을 수 있기 때문이다. 예를 들어, 어떤 사람은 ’보통(3)’에서 ’만족(4)’으로의 차이를 크게 느끼는 반면, 다른 사람은 ’만족(4)’에서 ’매우 만족(5)’으로의 차이를 더 중요하게 여길 수 있다. 이처럼 응답 간 간격에 대한 인식 차이는 등간척도로서의 가정을 위협할 수 있다.\n따라서, 척도의 신뢰성과 구조적 타당성을 검토하는 과정이 선행되어야 하며, 대표적인 방법으로는 다음과 같은 분석이 활용된다.\n\n신뢰도 분석(Cronbach’s Alpha): 동일한 척도를 구성하는 문항 간의 일관성을 평가하여, 응답자들이 각 문항에 대해 일관되게 반응하는지를 검토한다.\n탐색적 요인 분석(EFA): 여러 문항이 하나의 요인(개념)을 측정하고 있는지를 분석하여 척도의 구조적 타당성을 확인한다.\n\n이러한 검토를 통해 척도의 내적 일관성과 구성 타당성이 확보된다면, 해당 리커트 척도를 등간 척도로 간주하여 통계 분석에 활용하는 것이 보다 타당하고 신뢰로운 해석으로 이어질 수 있다.\n\n\n\n3. 등간 척도\n등간척도는 변수 간의 차이를 수치적으로 측정할 수 있도록 설계된 척도로, 각 값들 사이의 간격이 일정하다는 점이 핵심적인 특징이다. 예를 들어, 섭씨 온도는 10도에서 20도로 올라간 변화와, 20도에서 30도로 올라간 변화가 동일한 간격(10도 차)을 의미하므로 등간척도로 간주된다.\n그러나 절대적 기준점(진정한 0점)이 존재하지 않는다는 점에서 비율척도와는 다르다. 예를 들어, 섭씨 0도는 ’온도의 부재’를 의미하지 않으며, 20도가 10도의 ’두 배 더 덥다’라고 해석하는 것도 옳지 않다. 이처럼 등간척도는 간격(차이)을 비교할 수는 있지만, 비율 비교나 배수 해석은 불가능하다.\n\n(1) 등간척도 특징\n순서 정보 제공: 변수 값의 크고 작음을 비교할 수 있다. (예) 온도(섭씨, 화씨), IQ 점수, 표준화 점수(Z-score)\n일정한 간격 유지: 측정값들 사이의 차이가 동일한 간격을 유지한다. (예) 섭씨 온도에서 20°C와 30°C 사이의 차이(10°C)는, 30°C와 40°C 사이의 차이(10°C)와 같다.\n절대적 0이 없음: 등간척도는 임의적으로 정한 0점을 사용하므로, ”절대적인 없음”을 의미하는 0점이 아니다. 예를 들어, 섭씨 0°C는 온도의 완전한 부재가 아니라, 물의 어는점을 기준으로 설정된 값이다. 따라서, 40°C가 20°C의 ”두 배”라고 할 수 없다.\n덧셈과 뺄셈이 가능하지만, 곱셈과 나눗셈(비율 비교)은 불가능: 차이(증감량)를 분석하는 데 유용하지만, 비율(몇 배 큰지) 분석은 의미가 없다. 예를 들면, IQ 점수 140이 70보다 두 배 높은 지능을 의미하지 않는다.\n\n\n(2) 등간척도 예시\n\n온도(섭씨, 화씨): 20°C와 30°C 사이의 차이는 10°C, 하지만 40°C가 20°C의 2배 더 뜨겁다고 할 수 없음.\nIQ 점수: 140점이 70점의 두 배의 지능을 의미하지 않음.\n시험 점수(표준화 점수): 예를 들어, 수능 점수가 300점에서 400점으로 증가한 것과 400점에서 500점으로 증가한 것은 동일한 차이를 나타냄.\n신용평점: 신용평점 800이 400보다 정확히 두 배의 신용도를 의미하는 것은 아님.\n리커트 척도: ”1=매우 불만족, 2=불만족, 3=보통, 4=만족, 5=매우 만족” 등 숫자가 등간성을 가질 수 있는 경우.\n\n\n\n(3) 등간 척도 통계 분석 방법\n산술평균과 표준편차: 등간척도는 산술평균을 계산할 수 있으며, 자료의 분산과 표준편차 분석이 가능하다. (예) 특정 시험에서 학생들의 평균 점수를 계산.\nt-검정 및 분산분석: 두 집단 이상에서 평균 차이를 비교할 때 사용된다. (예) 새로운 교육 방법이 기존 방법보다 효과적인지 확인할 때.\n상관관계 분석: 등간척도 데이터 간의 선형 관계를 분석하는 데 사용한다. (예) 학생들의 IQ와 학업 성취도 간의 상관관계 분석.\n회귀분석: 예측 모델을 만들기 위해 사용된다. (예) 기온이 소비자 구매 행동에 미치는 영향을 분석한다.\nZ-점수 변환: 등간척도 데이터는 표준화하여 비교가능하도록 변환할 수 있다. (예) 여러 시험에서 점수를 비교할 때 표준화 점수를 사용한다.\n\n\n(4) 등간척도의 한계와 고려 사항\n비율 비교 불가능: 섭씨 온도에서 40°C가 20°C의 두 배라고 할 수 없다. (해결 방법) 비율척도로 변환 가능할 경우 변환하여 분석 (예: 절대적 0이 있는 켈빈 온도를 사용하는 방법).\n정확한 간격 유지 여부 확인 필요: 일부 척도(예: 심리학 설문지)는 등간척도를 가정하고 있지만, 실제로는 완전한 등간성이 없는 경우가 많다.\n정규성 검정 필요: 등간척도 자료가 정규분포를 따르는지 여부에 따라 사용할 수 있는 통계 분석 기법이 달라진다.\n\n\n\n4. 비율 척도\n비율척도는 측정값들 간의 절대적 0이 존재하며, 덧셈·뺄셈뿐만 아니라 곱셈·나눗셈을 포함한 모든 산술 연산이 가능한 척도이다. 즉, 값들 간의 비율(배수 개념)이 의미를 갖는 척도이다.\n\n(1) 비율척도 특징\n순서 정보 제공: 값들의 크고 작음을 비교할 수 있다. (예) 키(170cm &gt; 160cm), 몸무게(70kg &gt; 50kg)\n일정한 간격 유지: 측정값들 사이의 간격이 일정하다. (예) 10kg와 20kg 사이의 차이(10kg)는, 30kg와 40kg 사이의 차이(10kg)와 같다.\n절대적 0 존재: 값이 ’0’일 때 해당 속성이 완전히 없는 상태를 의미한다. (예) 키 0cm, 몸무게 0kg, 연령 0세(출생 이전 없음), 수입 0원\n비율(배수 비교) 가능: 40kg이 20kg의 두 배임을 의미하며, 100cm는 50cm의 두 배 길이임을 의미함. 이는 등간척도(Interval Scale)에서는 불가능했던 비교이다. (예) ”50세는 25세의 두 배 오래 살았다”는 논리가 성립.\n모든 산술 연산 가능: 덧셈, 뺄셈, 곱셈, 나눗셈 연산이 모두 가능하다. (예) 평균, 비율, 표준 편차, 변동 계수 등 계산 가능.\n\n\n(2) 비율척도 예시\n\n신체 측정값: 키(cm), 몸무게(kg), 허리둘레(cm)\n시간 측정값: 반응 시간(초), 나이(년), 학습 시간(시간)\n소득 및 비용: 월급(원), 저축액(원), 소비액(원)\n거리 및 속도: 거리(km, m), 속도(km/h), 주행거리(km)\n수량 데이터: 판매량(개), 학생 수(명), 인구 수(명)\n\n\n\n(3) 비율척도 통계 분석 방법\n산술평균, 중앙값, 최빈값 계산 가능: 모든 기술통계가 적용 가능하다. (예) 평균 소득, 중앙값 소득, 최빈값 분석.\n표준편차와 분산 계산 가능: 데이터의 변동성을 분석할 수 있다. (예) 신체검사에서 키의 표준편차 분석.\nt-검정 및 분산분석: 집단 간 평균 차이 검정 가능하다. (예) 남성과 여성의 평균 키 차이 분석.\n회귀분석 및 상관분석: (1) 독립변수와 종속변수 간의 관계를 예측하는 데 사용된다. (예) 학습 시간과 시험 성적 간의 관계 분석. (2) 두 변수 간의 관계를 측정할 수 있다. (예) 소득과 소비 간의 관계 분석.\n비율 비교 가능: 데이터 간 비율(배수)을 이용한 분석이 가능하다. (예) A 지역의 평균 소득이 B 지역보다 1.5배 높다.\n\n\n(4) 비율척도의 한계와 고려 사항\n데이터의 정규성 검토 필요: 대부분의 통계 분석 기법(t- 검정, 분산분석, 회귀분석 등)은 정규분포를 가정하므로 데이터가 정규성을 만족하는지 확인해야 한다.\n이상치(Outliers) 문제: 비율척도 데이터는 절대적인 크기가 의미를 가지므로, 이상치(극단값)에 민감하다. (예) 평균 소득 분석 시, 억대 연봉자가 포함되면 평균값이 왜곡될 수 있음.\n비율척도의 단위 고려: 단위 변환 시 비율 비교가 달라질 수 있다. (예) 키를 cm에서 m로 변환하면 수치가 100배 차이가 나므로, 단위 해석에 주의해야 함.\n\n\n\n\nchapter 3. 심리적 행동적 척도\n\n1. 거트만 Guttman 척도\n\n(1) 거트만 척도 개요\n거트만 척도(Guttman Scale)는 누적척도의 한 형태로, 응답자의 태도나 행동이 위계적으로 구성된 문항에 대해 일관된 응답 패턴을 보이는지를 측정하는 척도이다. 이 척도는 상위 문항에 동의한 응답자는 반드시 그보다 낮은 수준의 문항에도 동의한다고 가정한다. 다시 말해, 특정 진술에 ’그렇다’고 응답한 사람은 그보다 덜 강한 진술에도 ’그렇다’고 응답해야 논리적으로 일관된 것으로 간주한다.\n거트만 척도는 응답자 간 점수의 단순 비교보다, 응답 패턴의 누적성과 일관성에 초점을 두는 방식이다. 응답 결과가 이상적인 누적 형태를 따를수록, 해당 척도의 신뢰성과 설명력이 높다고 평가한다.\n이 척도는 사회적 태도, 신념 구조, 행동 수용 정도 등과 같이 위계적 순서가 존재한다고 판단되는 개념을 측정하는 데 적합하다. 따라서 다음과 같은 분야에서 주로 활용된다.\n\n사회과학 연구: 성평등 인식, 정치적 이념 수용도, 편견 수준 등\n교육 연구: 학습 태도, 교육 내용 수용도 등\n마케팅 조사: 제품 구매 의향, 브랜드 충성도 등\n\n예를 들어, 정치적 참여에 대한 척도를 구성할 때 다음과 같이 문항을 배열할 수 있다. 1. 정치 뉴스를 읽는다. &lt; 2. 정치 토론에 참여한다. &lt; 3. 집회에 참석한 적이 있다. &lt; 4. 정치 기부를 한다. &lt; 5. 정당에 가입했다.\n이 경우, ’정당에 가입했다’는 응답자는 앞선 모든 문항에도 동의할 것으로 예상되며, 이러한 응답 구조가 거트만 척도의 전형적인 예시이다.\n\n\n(2) 거트만 척도 원리\n거트만 척도는 응답자의 태도나 행동이 계층적으로 배열된 문항을 통해 일관된 패턴을 보이는지 측정하는 방법이다. 이를 위해 거트만 척도는 네 가지 핵심 원칙을 따른다.\n(1) 위계적 구조(hierarchical structure)\n문항들은 강도의 순서에 따라 배열되며, 상위 문항에 동의한 사람은 하위 문항에도 반드시 동의해야 한다. 예를 들어, “난민을 이웃으로 받아들일 수 있다”는 문항에 동의한 사람은 “같은 국가에 사는 것은 괜찮다”는 문항에도 동의해야 한다는 구조이다.\n(2) 누적성 (Cumulativeness)\n척도에 포함된 문항들은 특정 주제나 태도에 대한 강도의 위계를 가지며 응답자는 자신의 태도 강도에 따라 논리적으로 일관된 답변을 해야 한다. 즉, 응답자가 가장 강한 수준의 문항(상위 문항)에 동의했다면, 그보다 약한 수준의 문항(하위 문항)에도 반드시 동의해야 한다. 반대로, 특정 문항에 동의하지 않는다면, 그보다 강한 수준의 문항에도 동의하지 않는 것이 일반적인 응답 패턴이 된다.\n예를 들어, 성평등에 대한 태도를 측정하는 거트만 척도에서 응답자가 ”여성도 전투병이 될 수 있다”는 문항에 동의했다면, 그보다 낮은 수준인 ”여성도 CEO가 될 수 있다”와 ”여성도 직업을 가질 수 있다”는 문항에도 동의해야 한다. 만약 이러한 논리가 지켜지지 않는다면(예: ”여성도 전투병이 될 수 있다”에 동의하지만, ”여성도 CEO가 될 수 있다”에 동의하지 않는 경우), 척도 구성의 오류가 발생할 수 있다.\n(3) 단일 차원성 (Unidimensionality)\n거트만 척도는 측정하고자 하는 태도나 속성이 단일 차원으로 구성되어야 한다. 응답자의 태도를 하나의 연속된 척도로 설명할 수 있어야 하며, 문항 간의 위계적 관계가 유지되어야 한다. 만약 문항들이 여러 개의 차원을 포함하고 있다면, 거트만 척도를 적용하기 어려워진다.\n예를 들어, ”정치적 이념”을 측정하는 경우, 경제적 이념(자유시장과 정부 개입)과 사회적 이념(개인주의와 공동체주의)이 동시에 고려된다면, 이 두 가지는 서로 다른 차원의 속성이므로 거트만 척도로 일관된 위계를 형성하기 어렵다.\n(4) 예측 가능성 (Predictability)\n거트만 척도에서는 응답자의 답변 패턴이 예측 가능해야 한다. 즉, 특정 문항에 대한 응답을 보면 그보다 낮은 수준의 문항에 대한 응답도 예측할 수 있어야 한다. 거트만 척도는 규칙적인 응답 패턴을 기반으로 하기 때문에, 만약 응답자가 예상되는 논리를 따르지 않는다면, 해당 응답이 척도의 신뢰성을 떨어뜨릴 가능성이 크다.\n예를 들어, 교육 수준에 따른 학력 태도를 측정할 때 다음과 같은 문항이 있다고 가정하자.\n1. 초등학교 교육은 모든 사람이 받아야 한다.\n2. 중학교 교육은 모든 사람이 받아야 한다.\n3. 고등학교 교육은 모든 사람이 받아야 한다.\n4. 대학교 교육은 모든 사람이 받아야 한다.\n만약 한 응답자가 ”대학교 교육은 모든 사람이 받아야 한다”고 응답했음에도 불구하고 ”고등학교 교육은 모든 사람이 받아야 한다”에 동의하지 않는다면, 이는 논리적으로 모순된 응답이 된다. 거트만 척도에서는 이러한 불일치가 최소화되어야 하며, 응답자가 한 문항에 동의하면 그보다 낮은 강도의 문항에도 동의할 것이라는 전제가 성립해야 한다.\n\n\n(3) 거트만 척도 사례\n거트만 척도에서는 응답자가 동의한 마지막(가장 높은) 문항의 번호를 기준으로 척도 점수를 부여한다. 즉, 응답자가 어느 수준까지 동의했는지를 측정하여 그 강도를 수량화한다. 이는 연구자가 응답자의 태도 수준을 정량적으로 비교할 수 있도록 도와준다. 예를 들어, 아래와 같은 거트만 척도가 있다고 가정하자.\n\n\n\n\n\n\n\n\n\n\n문항\n응답자 A\n응답자 B\n응답자 C\n응답자 D\n\n\n1. 여성도 일해야 한다\nO\nO\nO\nX\n\n\n2. 여성도 CEO가 될 수 있다\nO\nO\nX\nX\n\n\n3. 여성도 국방의 의무를 가져야 한다\nO\nX\nX\nX\n\n\n4. 여성도 전투병이 될 수 있다\nX\nX\nX\nX\n\n\n척도 점수\n3\n2\n1\n0\n\n\n\n\n\n\n(4) 거트만 척도 구성방법\n(1) 연구 목적 설정\n거트만 척도를 적용하기 전에 가장 먼저 고려해야 할 점은 연구 목적이 명확한가와 측정하고자 하는 태도나 행동이 단일 차원인지를 확인하는 것이다. 거트만 척도는 단일 차원의 위계적 구조를 기반으로 하기 때문에, 측정 대상이 여러 차원의 요소를 포함하는 경우에는 거트만 척도가 적절하지 않을 수 있다.\n예를 들어, 사회적 평등에 대한 태도를 측정하고자 할 때, 경제적 평등(소득 재분배)과 성별 평등(여성의 권리)과 같은 서로 다른 차원의 개념이 포함되면, 하나의 일관된 위계를 형성하기 어렵다. 연구자는 한 가지 차원에서 일관되게 정의될 수 있는 주제를 선정해야 한다.\n적절한 연구 주제: ”성평등에 대한 태도”, ”환경 보호에 대한 인식 수준”, ”정치적 개혁에 대한 수용도”\n부적절한 연구 주제: ”사회적 평등 (경제 + 성별 + 인종 차별)” → 여러 차원을 포함함\n(2) 문항 선정 및 계층적 배열\n연구 주제를 설정한 후에는, 해당 태도를 측정할 수 있는 위계적인 문항을 개발해야 한다. 거트만 척도의 가장 중요한 특징은 문항이 논리적인 강도(위계, Hierarchy)를 가지며, 응답자가 특정 문항에 동의하면 그보다 낮은 강도의 문항도 동의해야 한다는 점이다.\n이를 위해 연구자는 문항을 만들 때 경도에서 난도로 정렬해야 한다. 즉, 가장 많은 사람이 동의할 만한 일반적인 문항에서 시작하여, 점차 강한 입장을 나타내는 문항으로 발전해야 한다.\n\n\n\n\n\n\n\n문항 번호\n문항 내용\n\n\n1\n나는 재활용을 실천한다.\n\n\n2\n나는 환경 보호를 위해 일회용품 사용을 줄인다.\n\n\n3\n나는 대중교통을 적극적으로 이용한다.\n\n\n4\n나는 환경 보호 단체에 기부하거나 활동에 참여한다.\n\n\n5\n나는 환경 보호를 위해 추가적인 세금을 부담할 의향이 있다.\n\n\n\n이러한 문항 배열에서는 응답자가 5번 문항(세금 부담)에 동의한다면, 1~4번 문항(재활용, 일회용품 줄이기, 대중교통 이용, 환경 단체 활동)에도 동의할 가능성이 높다. 반대로, 1번 문항(재활용 실천)에조차 동의하지 않는다면, 상위 문항에도 동의하지 않을 가능성이 크다.\n이처럼, 연구자는 문항을 개발할 때 위계적으로 정렬된 문항을 만들고, 응답자가 강한 수준의 문항에 동의하면 약한 수준의 문항도 포함될 수 있도록 설계해야 한다.\n(3) 문항의 적합성 검증\n거트만 척도의 문항이 연구 목적에 맞게 구성되었는지 확인한 후에는, 척도가 실제로 누적적 구조를 따르는지 검증하는 과정이 필요하다. 이를 위해 연구자는 응답 데이터를 분석하여 응답 패턴이 논리적인 위계를 따르는지 평가해야 한다.\n가장 일반적인 검증 방법은 척도 재현성 계수(Scale Reproducibility Coefficient, R)를 계산하는 것이다. 이 계수는 응답자들의 패턴이 거트만 척도의 논리를 얼마나 충실히 따르는지를 평가하는 지표로 사용된다.\n척도 재현성 계수 계산 공식: \\(R = 1 - \\frac{\\sum e}{(n \\times k)}\\)\n\\(e\\): 오류 응답 개수 (예: 낮은 강도의 문항에 동의하지 않으면서 높은 강도의 문항에 동의한 경우)\n\\(n\\)= 응답자 수, \\(k\\)= 문항 수\n만약 응답자 10명, 문항 5개로 구성된 척도에서 오류 응답(예측 불가능한 응답)이 4개라면, \\(R = 1 - \\frac{4}{(10 \\times 5)} = 1 - \\frac{4}{50} = 0.92\\)\n\\(R \\geq 0.90\\) → 척도의 일관성이 매우 높음, 신뢰할 수 있음\n\\(0.80 \\leq R &lt; 0.90\\) → 척도 개선 가능성 있음\n\\(R &lt; 0.80\\) → 척도의 신뢰성이 낮음, 재구성이 필요\n만약 R 값이 낮다면, 이는 척도 문항이 적절한 위계를 형성하지 못하고 있거나, 응답자들이 일관된 논리를 따르지 않는다는 의미가 될 수 있다. 이러한 경우, 연구자는 문항을 재구성하거나, 응답 패턴을 분석하여 비일관적인 데이터를 보정해야 한다.\n\n\n\n2. 서스톤 Thurstone 척도\n서스톤 척도는 응답자의 태도를 정량적으로 평가하는 방법 중 하나로, 전문가 패널이 사전에 각 문항의 강도를 평가하여 점수를 부여한 후, 이를 바탕으로 응답자의 태도 점수를 계산하는 방식이다. 일반적으로 태도 측정에서는 응답자가 자신의 태도를 직접 수치화하는 방식(예: 리커트 척도)이 많이 사용되지만, 서스톤 척도는 전문가가 문항의 강도를 미리 점수화하기 때문에 보다 객관적이고 정밀한 태도 측정이 가능하다는 장점이 있다.\n이 척도는 사회과학 연구, 마케팅 조사, 심리학 연구 등에서 활용되며, 예를 들어 환경 보호, 정치적 성향, 종교적 신념, 제품 선호도 등과 같은 태도를 측정하는 데 사용될 수 있다.\n\n(1) 서스톤 척도 원리\n서스톤 척도는 세 가지 핵심 원칙을 따른다.\n전문가 패널을 통해 문항의 강도를 미리 평가한다.\n문항별 강도를 등간척도(Interval Scale) 형태로 점수화한다.\n응답자의 태도를 문항 점수를 기반으로 계산한다.\n먼저 연구자는 특정 태도를 측정하기 위한 다양한 문항을 개발한다. 이때 문항은 응답자의 태도 강도를 점진적으로 측정할 수 있도록 다양하게 구성되어야 한다. 그런 다음, 전문가 패널이 각 문항이 나타내는 태도의 강도를 평가하여 점수를 부여한다. 이 과정에서 전문가들은 일반적으로 1점(매우 약함)에서 11점(매우 강함)까지의 척도를 사용하며, 문항별로 점수를 매긴 후 중앙값이나 평균값을 산출하여 최종 문항 점수로 사용한다.\n이후 응답자는 문항을 읽고 자신이 동의하는 문항을 선택한다. 연구자는 응답자가 동의한 문항들의 점수를 평균 또는 중앙값으로 계산하여 응답자의 태도 점수를 결정한다. 이 태도 점수는 등간척도 값이므로, 평균 비교나 통계적 분석이 가능하다.\n\n\n(2) 서스톤 척도의 구성 방법 (Construction of Thurstone Scale)\n(1) 연구 목적 설정\n서스톤 척도를 적용하기 위해서는 먼저 연구의 목적을 명확하게 설정해야 한다. 이는 측정하고자 하는 태도나 신념이 하나의 연속적인 차원에서 설명될 수 있는지 확인하는 과정이다. 서스톤 척도는 태도의 강도를 측정하는 방법이므로, 연구자가 다차원적인 개념을 포함할 경우 척도의 적용이 어려울 수 있다.따라서, 단일 차원을 유지하는 것이 중요하다.\n예를 들어, 연구자가 환경 보호에 대한 태도를 측정한다고 가정하자. 이 경우, 환경 보호에 대한 태도를 긍정적 태도와 부정적 태도의 연속적인 개념으로 설정할 수 있다. 즉, 응답자가 환경 보호를 얼마나 중요하게 생각하는지를 강도에 따라 평가할 수 있도록 환경 보호에 대한 긍정적인 태도를 하나의 차원으로 정의할 수 있다.\n반면, ”환경 보호 태도”를 측정할 때 정책적 태도(환경 규제 찬성/반대), 행동적 태도(재활용 실천 여부), 감정적 태도(환경 문제에 대한 관심도) 등 여러 가지 차원이 혼합된다면, 서스톤 척도를 적용하는 것이 어려워질 수 있다.\n따라서, 연구 목적을 설정할 때는 응답자의 태도가 하나의 연속적인 척도로 측정될 수 있도록 명확한 개념 정의가 필요하며, 측정하고자 하는 속성이 단일 차원으로 구성되어 있는지 검토해야 한다.\n(2) 문항 개발 및 전문가 평가\n서스톤 척도를 구성하기 위해 연구자는 먼저 측정하고자 하는 태도나 신념과 관련된 다양한 강도의 문항을 개발해야 한다. 이때 문항은 응답자의 태도 강도를 점진적으로 측정할 수 있도록 구성되어야 하며, 너무 유사하거나 중복된 표현이 포함되지 않도록 주의해야 한다.\n문항이 개발된 후, 연구자는 전문가 패널(판정단)을 구성하여 각 문항이 나타내는 태도의 강도를 평가하도록 한다. 전문가들은 보통 11점 척도(또는 9점, 7점 등)를 사용하여 문항의 강도를 판단하며, 점수가 클수록 더 강한 태도를 나타내는 것으로 평가한다. 즉, 가장 낮은 점수의 문항은 태도가 가장 약한 진술을 의미하고, 가장 높은 점수의 문항은 태도가 가장 강한 진술을 의미하게 된다.\n이러한 전문가 평가 결과를 바탕으로 연구자는 각 문항에 대해 평균값 또는 중앙값을 계산하여 최종 문항 점수로 설정한다. 이 과정에서 특정 문항에 대한 전문가들의 의견이 크게 분산되는 경우, 연구자는 해당 문항을 수정하거나 제외할 수도 있다. 예를 들어, 환경 보호에 대한 태도를 측정하는 서스톤 척도의 경우, 전문가 패널이 다음과 같은 문항을 평가할 수 있다.\n”나는 가끔 재활용을 한다.” → 평균 점수: 2.5점 (낮은 수준의 환경 보호 태도)\n”나는 환경 보호를 위해 일회용품 사용을 줄인다.” → 평균 점수: 5.2점 (중간 수준의 환경 보호 태도)\n”나는 환경 보호 단체에 기부하거나 활동에 참여한다.” → 평균 점수: 7.8점 (높은 수준의 환경 보호 태도)\n”나는 환경 보호를 위해 추가적인 세금을 부담할 의향이 있다.” → 평균 점수: 10.3점 (매우 높은 수준의 환경 보호 태도)\n이처럼, 문항이 전문가 패널에 의해 점수화되면, 연구자는 점수 분포를 분석하여 강도가 균등하게 분포된 문항을 선정하고, 최종적인 서스톤 척도를 구성할 수 있다. 이후 응답자는 자신이 동의하는 문항을 선택하게 되며, 연구자는 해당 문항의 점수를 바탕으로 응답자의 태도를 정량적으로 평가할 수 있다.\n(3) 최종 문항 선정\n전문가 패널이 모든 문항을 평가한 후, 연구자는 응답자의 태도를 효과적으로 측정할 수 있도록 최종적으로 사용할 문항을 선정해야 한다. 이때 가장 중요한 것은 문항의 점수 분포가 균형 있게 배치되는지 확인하는 것이다.\n서스톤 척도의 핵심 원리는 태도 강도를 연속적인 척도로 측정하는 것이므로, 최종적으로 선택된 문항들이 너무 한쪽에 치우치지 않도록 강도가 균등하게 분포되도록 조정해야 한다. 예를 들어, 전문가 평가를 거친 후 점수가 1점, 3점, 5점, 7점, 9점으로 균등하게 분포된 문항을 선택하면, 응답자의 태도 강도를 보다 정밀하게 측정할 수 있다.\n만약 문항 점수가 특정 구간(예: 5점~9점)에 집중되어 있다면, 연구자는 1점~3점 범위의 문항을 추가하거나, 특정 문항을 수정하여 보다 균형 잡힌 척도를 구성할 필요가 있다. 이 과정을 통해 최종적으로 선택된 문항들은 응답자의 태도를 단계적으로 측정할 수 있는 기준이 되며, 이후 태도 점수를 계산하는 데 활용된다.\n(4) 응답자의 태도 점수 측정\n최종적으로 선정된 문항을 바탕으로 연구자는 응답자의 태도를 측정한다. 이 과정에서 응답자는 제공된 문항을 읽고, 자신이 동의하는 문항을 선택하게 된다.\n응답자가 선택한 문항들은 연구자가 사전에 설정한 강도 점수를 가지고 있으며, 연구자는 이를 기반으로 응답자의 태도 점수를 계산할 수 있다. 태도 점수는 일반적으로 응답자가 동의한 문항들의 평균 또는 중앙값으로 측정된다.\n예를 들어, 한 응답자가 환경 보호 태도를 측정하는 서스톤 척도에서 다음과 같은 문항에 동의했다고 가정하자.\n”나는 환경 보호를 위해 일회용품 사용을 줄인다.” (평균 점수: 5.2)\n”나는 대중교통을 적극적으로 이용한다.” (평균 점수: 6.5)\n이 경우, 응답자의 태도 점수는 \\((5.2 + 6.5)/2 = 5.85\\)이다. 즉, 응답자는 환경 보호에 대한 태도를 5.85점 수준으로 가지고 있다고 해석할 수 있다.\n이러한 방식으로 연구자는 응답자의 태도를 정량적으로 평가할 수 있으며, 개별 응답자뿐만 아니라 집단 간 태도를 비교하거나 통계적 분석을 수행할 수도 있다. 예를 들어, 연령대별 환경 보호 태도 차이를 분석하거나, 기업과 일반 소비자의 태도를 비교하는 연구에서도 서스톤 척도를 활용할 수 있다. 이와 같이, 서스톤 척도는 전문가 패널을 통해 사전에 평가된 문항 점수를 활용하여 응답자의 태도를 보다 정밀하게 측정할 수 있는 방법을 제공한다.\n\n\n(3) 서스톤 척도 장단점\n(1) 장점 (Advantages)\n첫째, 객관적인 태도 측정이 가능하다. 일반적인 태도 측정 방법(예: 리커트 척도)은 응답자가 자신의 태도를 직접 평가하는 방식이지만, 서스톤 척도는 전문가가 사전에 문항의 강도를 점수화하기 때문에 보다 객관적인 평가가 가능하다. 즉, 응답자의 주관적 해석이 개입될 여지가 줄어들며, 연구자가 미리 설정한 기준에 따라 태도를 수량화할 수 있다.\n둘째, 정확한 태도 점수를 제공할 수 있다. 응답자가 단순히 ”동의” 또는 ”비동의”와 같은 선택을 하는 것이 아니라, 전문가가 평가한 문항 점수를 기반으로 태도 강도를 측정할 수 있다. 따라서, 응답자의 태도를 정밀하게 측정할 수 있으며, 비교적 작은 차이도 감지할 수 있다. 예를 들어, 환경 보호에 대한 태도를 평가할 때, ”나는 가끔 재활용을 한다”와 ”나는 환경 보호를 위해 추가적인 세금을 부담할 의향이 있다”는 명확히 다른 수준의 태도를 나타낸다. 서스톤 척도는 이러한 차이를 정량화하여 태도 점수를 제공할 수 있다.\n셋째, 등간척도로 활용할 수 있어 통계적 분석이 용이하다. 서스톤 척도는 문항이 사전에 점수화되어 있어 등간척도로 간주될 수 있다. 따라서, 환경 보호 태도를 조사한 후, 특정 연령대별 평균 태도 점수를 비교하거나, 기업과 일반 소비자의 태도 차이를 분석하는 데 사용할 수 있다. 이러한 분석이 가능하기 때문에 사회과학 연구나 마케팅 조사에서 활용도가 높다.\n(2) 단점 (Disadvantages)\n첫째, 전문가 패널을 구성해야 하는 번거로움이 있다. 서스톤 척도를 사용하려면, 연구자가 문항을 개발한 후 해당 분야의 전문가 집단을 구성하여 문항을 평가받아야 한다. 이는 상당한 시간과 노력이 필요하며, 전문가 의견을 조정하는 과정에서 추가적인 논의와 검토가 필요할 수 있다. 특히, 전문가들의 평가가 일관되지 않을 경우 추가적인 조정이 필요하므로 연구 과정이 더욱 복잡해질 수 있다.\n둘째, 문항 선정 및 점수화 과정이 복잡하고 시간이 많이 소요된다.리커트 척도처럼 ”매우 동의 ~ 매우 반대”의 선택지를 제공하는 방식과 달리, 서스톤 척도는 문항을 개발한 후 각 문항의 강도를 평가하고 점수화하는 과정이 필요하다. 이 과정에서 연구자는 전문가 패널의 평가 점수를 수집하고, 이를 평균 또는 중앙값으로 변환하여 최종 문항 점수를 결정해야 한다. 따라서, 서스톤 척도는 문항 개발과 평가 과정이 복잡하여 연구를 신속하게 진행하기 어려운 경우가 많다.\n셋째, 응답자가 특정 문항을 선택하지 않으면 태도 점수 계산이 어려울 수 있다. 서스톤 척도는 응답자가 동의하는 문항을 선택하는 방식이므로, 만약 응답자가 어떤 문항도 선택하지 않는다면 그 응답자의 태도 점수를 계산하기 어렵다. 또한, 응답자가 중간 강도의 문항만 선택하거나, 극단적인 문항만 선택할 경우, 연구자가 의도한 대로 태도 강도가 균형적으로 측정되지 않을 가능성이 있다. 이러한 문제를 방지하기 위해 연구자는 문항을 신중하게 구성해야 하며, 응답자의 선택 패턴을 분석하여 연구 결과의 신뢰성을 평가할 필요가 있다.\n\n\n\n3. 의미분화 Semantic Differential 척도\n의미분화 척도는 응답자의 태도, 감정, 인식을 측정하기 위한 방법으로, 서로 반대되는 형용사 쌍을 이용하여 특정 개념을 평가하는 방식이다. 이 척도는 1950년대 찰스 오스굿(Charles Osgood)에 의해 개발되었으며, 주로 브랜드 이미지 평가, 제품 선호도 조사, 감정 분석, 사회적 태도 연구에 활용된다.\n응답자는 주어진 개념(예: 특정 브랜드, 제품, 서비스, 정치적 이슈 등)에 대해 양극적인 의미를 가지는 형용사 쌍(예: 혁신적이다 – 전통적이다, 친절하다 – 무뚝뚝하다, 고급스럽다 – 저렴하다 등)을 기준으로 평가한다. 이 척도는 보통 5점 또는 7점 척도를 사용하여, 응답자가 두 개의 형용사 중 어느 쪽에 더 가깝다고 느끼는지를 선택하게 한다. 이를 통해 연구자는 특정 개념에 대한 응답자의 인식과 감정을 수량화하여 비교 및 분석할 수 있다.\n\n(1) 의미분화 척도 구성 방법\n(1) 연구 목적 설정\n먼저, 연구자는 측정하려는 개념을 명확히 정의해야 한다. 이 척도는 인지적 평가(브랜드 이미지), 감정적 반응(감성 평가), 태도(사회적 이슈에 대한 인식) 등을 측정하는 데 효과적이다.\n예를 들어, 연구자가 소비자의 브랜드 인식을 조사하려는 경우, 혁신성, 신뢰성, 감성적 요소 등 다양한 측면에서 브랜드를 평가할 수 있도록 형용사 쌍을 선정해야 한다.\n(2) 적절한 형용사 쌍 선정\n의미분화 척도의 핵심은 양극적인 의미를 가지는 형용사 쌍을 선정하는 것이다. 형용사는 측정 대상과 관련성이 있어야 하며, 응답자가 쉽게 이해할 수 있어야 한다. 예를 들어, 스마트폰 브랜드를 평가하는 경우 다음과 같은 형용사 쌍을 사용할 수 있다.\n디자인 측면: 세련된 – 투박한\n성능 측면: 강력한 – 느린\n가격 대비 가치: 가성비 좋다 – 비싸기만 하다\n브랜드 이미지: 혁신적인 – 전통적인\n(3) 응답 척도 설정\n보통 5점 척도 또는 7점 척도를 사용하여 응답자가 각 형용사 쌍 사이에서 자신의 입장을 선택할 수 있도록 한다. 예를 들어, ”이 브랜드는 세련된가?” 라는 질문이 주어졌을 때, 응답자는 아래와 같이 선택할 수 있다.\n세련된 (1) – (2) – (3) – (4) – (5) 투박한\n이러한 방식으로 응답자는 특정 개념에 대해 자신이 인식하는 정도를 선택하게 된다.\n\n\n(2) 의미분화 척도 응답 결과 해석\n응답자가 평가한 점수를 분석하여 특정 개념(브랜드, 제품, 서비스 등)이 어떤 속성을 가지고 있는지 해석할 수 있다. 이를 통해 연구자는 브랜드 간 비교, 소비자 인식 분석, 마케팅 전략 수립 등에 활용할 수 있다. 예를 들어, 브랜드 이미지 평가에서 애플과 삼성을 비교한 결과가 다음과 같다고 가정하자.\n\n\n\n\n\n\n\n\n형용사 쌍\n애플\n삼성\n\n\n혁신적이다 – 전통적이다\n2.1\n4.3\n\n\n고급스럽다 – 저렴해 보인다\n2.5\n3.9\n\n\n감성적이다 – 실용적이다\n3.8\n2.2\n\n\n\n이 결과를 해석하면 다음과 같다.\n\n애플은 혁신적이고 고급스러우며 감성적인 브랜드로 인식된다.\n삼성은 실용적이지만 비교적 전통적이고 저렴한 브랜드로 평가된다.\n\n이러한 분석을 통해 기업은 자신들의 브랜드 포지셔닝 전략을 조정하거나, 소비자의 인식 변화를 모니터링하는 데 활용할 수 있다.\n\n\n(3) 의미분화 척도 장단점\n(1) 장점\n감성적·정성적 데이터를 수량화할 수 있음: 브랜드 이미지나 소비자 감정처럼 측정하기 어려운 개념을 숫자로 변환할 수 있다.\n심층적인 분석이 가능함: 다양한 속성을 동시에 비교할 수 있어 브랜드 전략, 소비자 인식 분석에 효과적이다.\n다양한 연구 분야에서 활용 가능: 마케팅, 심리학, 사회과학 등 여러 분야에서 적용할 수 있으며, 정량적 분석과 정성적 분석을 함께 수행할 수 있다.\n(2) 단점\n형용사 쌍 선정이 어렵다: 연구자가 적절한 형용사 쌍을 선정하지 못하면 신뢰성 있는 결과를 얻기 어려울 수 있다.\n주관성이 개입될 수 있음: 응답자가 동일한 척도를 다르게 해석할 가능성이 있으며, 개인적인 경험이나 문화적 배경에 따라 답변이 달라질 수 있다.\n통계적 분석이 어려울 수 있음: 개별 속성마다 다른 점수를 얻기 때문에, 전체적인 인식을 종합적으로 분석하는 것이 복잡할 수 있다.\n\n\n\n4. 보가더스 Bogardus Social Distance 척도\n보가더스 척도는 사회적 거리를 측정하기 위한 방법으로, 특정 집단에 대한 개인의 사회적 수용도를 평가하는 척도이다. 이 척도는 1925년 에멜 보가더스(Emory S. Bogardus)가 개발했으며, 주로 인종, 계층, 문화적 차이 등에 대한 태도를 분석하는 데 사용된다.\n\n(1) 보가더스 척도 개요\n보가더스 척도는 응답자가 특정 집단(예: 인종, 종교, 성별 등)에 대해 어떤 수준까지 사회적 관계를 허용할 수 있는지를 평가하는 방식이다. 이 척도는 사회적 거리의 개념을 기반으로 하며, 응답자의 태도 강도를 점진적으로 측정한다.\n보가더스 척도는 보통 다음과 같은 7단계(또는 변형된 5~10단계)의 진술문으로 구성된다. 응답자는 특정 집단에 대해 각 단계에서 자신의 수용 가능 여부를 표시해야 한다.\n\n\n(2) 보가더스 척도 구성 방법\n(1) 연구 목적 설정\n보가더스 척도는 특정 집단에 대한 사회적 거리감을 측정하는 것이 목적이다. 예를 들어, 연구자가 다문화 사회에서 특정 민족 집단에 대한 태도를 조사하려는 경우, 이 척도를 활용하여 응답자들이 해당 집단을 어느 정도까지 수용할 수 있는지를 측정할 수 있다.\n(2) 7단계 문항 구성\n보가더스 척도는 일반적으로 사회적 관계의 밀도(친밀성)에 따라 7단계로 구성된다. 응답자는 특정 집단(예: 난민, 외국인 노동자, 특정 인종 등)에 대해 다음의 관계 수준에서 수용 가능 여부를 선택한다.\n\n가족 구성원으로 받아들일 수 있다.\n개인적인 친구로 받아들일 수 있다.\n이웃으로 받아들일 수 있다.\n동료(직장, 학교)로 받아들일 수 있다.\n시민으로 받아들일 수 있다.\n관광객으로 받아들일 수 있다.\n완전히 거부한다.\n\n이러한 문항을 바탕으로 응답자는 자신이 허용할 수 있는 가장 높은 단계까지만 ’동의’를 선택하게 된다. 즉, 응답자의 최종 선택된 단계가 사회적 거리 점수로 계산된다.\n\n\n(3) 보가더스 척도 응답 결과 해석\n(1) 태도 강도 측정 및 사회적 거리 점수 계산\n보가더스 척도에서는 응답자가 허용할 수 있는 가장 높은(즉, 가장 친밀한) 단계를 기준으로 점수를 계산한다. 응답자가 특정 집단에 대해 가까운 관계를 허용할수록 사회적 거리는 짧고, 거부할수록 사회적 거리는 길어진다.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n응답자\n가족 구성원\n친구\n이웃\n직장 동료\n시민권 허용\n관광객 허용\n완전 거부\n사회적 거리 점수\n\n\nA\nO\nO\nO\nO\nO\nO\nX\n6\n\n\nB\nX\nX\nO\nO\nO\nX\nX\n5\n\n\nC\nX\nO\nX\nO\nX\nX\nX\n4\n\n\n\n(2) 응답 집단 간 비교 분석\n보가더스 척도는 집단 간 태도를 비교하는 데 유용하다. 예를 들어, 연령별, 성별, 지역별로 특정 집단에 대한 사회적 거리를 비교할 수 있다.\n\n\n\n\n\n\n\n\n연령대\n평균 점수\n해석\n\n\n20대\n5.8\n난민 수용에 비교적 개방적\n\n\n40대\n4.2\n중립적 태도\n\n\n60대\n3\n난민 수용에 보수적\n\n\n\n\n\n\n(4) 보가더스 척도 장단점\n장점\n보가더스 척도는 특정 집단(예: 난민, 성소수자, 특정 인종 등)에 대한 사회적 거리감을 직관적으로 측정할 수 있는 장점을 지닌다. 이 척도는 응답자가 해당 집단을 얼마나 가까운 사회적 관계 속에서 수용할 수 있는지를 단계별로 평가하도록 설계되어 있어, 연구자는 응답자의 사회적 거리감과 수용도를 명확히 분석할 수 있다. 예를 들어, 어떤 응답자가 특정 집단을 직장 동료로는 허용하지만 가족 구성원으로는 받아들이기 어렵다고 응답했다면, 이는 해당 집단에 대해 일정한 거리감을 갖고 있음을 의미한다. 이러한 구조를 통해 보가더스 척도는 응답자의 태도를 체계적으로 측정하고, 사회적 거리 개념을 쉽게 해석할 수 있도록 한다.\n또한, 이 척도는 태도 변화 분석에도 유용하다. 난민, 이민자, 성소수자 등 논쟁이 되는 집단에 대한 사회적 거리 점수의 변화를 시계열적으로 분석함으로써, 사회적 태도가 더 개방적으로 변화했는지(점수 증가) 혹은 보수적으로 변화했는지(점수 감소)를 파악할 수 있다. 예컨대, 이민자 수용 정책이 바뀐 이후 대중의 사회적 거리 점수를 비교·분석함으로써, 해당 정책이 대중의 태도에 미친 영향을 측정할 수 있다.\n더 나아가, 보가더스 척도는 다양한 문화 및 사회 연구에 폭넓게 적용될 수 있다. 단순한 인종 차별 연구를 넘어, 성소수자(LGBTQ+), 난민, 이민자 등 특정 집단에 대한 사회적 거리 평가를 통해 차별과 수용의 정도를 정량적으로 분석할 수 있다. 예를 들어, 인종 차별 연구에서는 특정 인종 집단에 대한 거리감을 측정하고, 성차별 연구에서는 여성 및 성소수자에 대한 수용도를 평가하며, 장애인 연구에서는 장애인과 비장애인 간의 사회적 거리 차이를 분석할 수 있다. 이러한 방식으로 보가더스 척도는 다양한 사회적 관계 속에서 형성되는 거리감을 수치화하고, 사회적 통합과 차별을 이해하는 데 유용한 도구로 활용된다.\n단점\n보가더스 척도를 사용할 때 주의해야 할 한계 중 하나는 응답자의 사회적 바람직성 편향(social desirability bias) 가능성이다. 이는 응답자가 실제 태도보다 더 개방적이고 긍정적인 방향으로 답하려는 경향을 의미한다. 특히 인종, 성소수자, 난민과 같은 민감한 주제를 다룰 때 이러한 편향이 두드러지게 나타날 수 있다. 예를 들어, 공개된 설문조사 상황에서는 응답자가 인종차별적인 사람으로 보일까봐 특정 인종에 대해 실제보다 더 수용적인 태도를 보일 수 있으며, 성소수자에 대한 수용 태도 역시 사회 분위기를 의식한 긍정적인 응답으로 나타날 수 있다. 이러한 편향을 완화하기 위해 연구자는 익명성을 보장하거나, 보조 질문을 추가하여 응답의 일관성을 검토하는 등의 방법을 활용할 수 있다.\n또한 보가더스 척도는 응답자가 특정 집단에 대해 어느 정도 수용적인지를 수량화하는 데에는 효과적이지만, 그 이유나 동기를 설명하는 데에는 한계가 있다. 예를 들어, 어떤 응답자가 난민에 대해 높은 사회적 거리 점수를 보였다 하더라도, 그것이 정치적 신념 때문인지, 개인적인 경험이나 교육 수준 때문인지는 이 척도만으로는 파악할 수 없다. 따라서 연구자가 보가더스 척도를 사용할 경우, 응답자의 태도 형성 배경을 보다 깊이 이해하기 위해 개방형 질문, 면접(interview) 등의 질적 조사 방법을 병행하는 것이 바람직하다.\n마지막으로, 현대 사회의 다양한 사회적 관계를 보가더스 척도가 충분히 반영하지 못할 수 있다는 점도 고려해야 한다. 기존의 7단계 문항 구조는 ‘이웃’, ‘동료’, ‘결혼 상대’ 등 전통적인 관계 유형을 중심으로 구성되어 있다. 그러나 현대 사회에서는 SNS 친구, 온라인 커뮤니티 멤버, 동호회 파트너 등 다양한 비대면·비공식적 관계가 존재하며, 이러한 새로운 관계 유형은 기존 척도로는 포착하기 어렵다. 또한, 국제화된 사회에서는 ’관광객 허용’과 같은 문항이 구체적인 의미를 갖기 어렵고, 단지 관광객으로 오는 것을 허용한다고 해서 해당 집단을 실질적으로 수용하는 것은 아닐 수 있다. 따라서 연구자는 보가더스 척도를 활용할 때, 시대 변화에 맞춰 문항을 수정하거나 보완하여 보다 현실적이고 정교한 분석이 가능하도록 해야 한다.\n\n\n\n5. 기타 척도\n\n(1) 스테이플 척도\n스테이플 척도는 설문 응답자가 특정 속성에 대해 긍정적 또는 부정적인 정도를 평가할 수 있도록 설계된 단극형 척도이다. 이 척도는 R. Stapel에 의해 개발되었으며 마케팅 조사 및 소비자 태도 연구에서 자주 사용된다.\n스테이플 척도를 구성하기 위해서는 먼저 측정하려는 속성을 명확하게 정의하고, 이를 평가할 수 있는 단일 형용사를 선정해야 한다. 일반적으로 긍정적 또는 부정적 감정을 유발하는 형용사가 사용되며, 응답자가 해당 속성에 대해 얼마나 긍정적이거나 부정적인지 평가할 수 있도록 한다.\n스테이플 척도는 중립값(0)을 중심으로 -5에서 +5까지의 연속적인 점수를 제공하여, 응답자가 특정 속성에 대한 평가를 강한 부정(-5)부터 강한 긍정(+5)까지 표현할 수 있도록 한다. 0은 중립적인 태도를 의미하며, 응답자가 특정 속성에 대해 특별한 의견이 없거나 긍정과 부정이 동등하게 느껴질 경우 선택할 수 있다.\n응답자는 각 문항에서 제시된 형용사가 평가 대상(예: 제품, 브랜드, 서비스 등)에 얼마나 적절한지를 선택하며, 이를 통해 연구자는 특정 대상의 이미지나 특성을 수량화할 수 있다.\n예를 들어, 특정 스마트폰 브랜드에 대한 평가를 조사할 경우, ”고급스러운”, ”혁신적인”, ”신뢰할 수 있는” 등의 형용사를 제시하고 응답자가 -5에서 +5 사이에서 자신의 인식을 선택하도록 한다. 이때 응답자의 선택은 브랜드에 대한 감정적 태도를 직접적으로 반영하며, 평균 점수를 통해 특정 속성에 대한 전반적인 인식을 파악할 수 있다.\n\n\n\n\n\n\n\n\n\n척도 유형\n측정 방식\n주요 특징\n활용 예시\n\n\n리커트 척도\n5~7점 동의 수준 측정\n쉬운 응답 방식, 보편적으로 사용됨\n고객 만족도 조사\n\n\n의미분화 척도\n양극단의 형용사 선택\n감정적 태도 측정, 브랜드 이미지 분석\n브랜드 인식 조사\n\n\n스테이플 척도\n단일 형용사 긍정/부정 평가\n-5~+5 연속적 측정, 긍정/부정 감정 동시에 분석 가능\n제품 평가, 광고 효과 분석\n\n\n서스톤 척도\n전문가 패널이 문항 가중치 부여\n등간척도로 활용 가능, 신뢰도 높음\n감정 평가, 브랜드 선호도 조사\n\n\n\n\n\n(2) 맥콜스키 척도\n맥콜스키 McClosky 척도는 개인의 정치적 태도 및 민주주의에 대한 신념을 측정하기 위해 개발된 척도로, 보가더스 척도와 유사하게 사용되지만, 주로 정치적 태도를 평가하는 데 초점을 맞춘다는 점에서 차별성이 있다. 이 척도는 정치적 자유주의와 보수주의 사이에서 개인이 어디에 위치하는지를 분석하는 도구로 활용되며, 정치적 신념, 정부 개입에 대한 견해, 시민권 보호, 언론 자유, 법과 질서 등 다양한 정치적 개념을 평가하는 데 유용하다.\n리커트 척도 5점/7점 척도를 사용하여 응답자가 각 진술에 대해 ”강하게 반대(1)”에서 ”강하게 동의(5)”까지의 수준을 선택할 수 있도록 구성된다. 예를 들어, ”정부는 경제에 적극적으로 개입해야 한다.”라는 문항에 대해 응답자가 강하게 동의(5)를 선택한다면, 이는 해당 응답자가 정부 개입을 선호하는 자유주의적 경제 정책을 지지하는 경향이 있음을 나타낸다.\n반면, ”국가 안보를 위해 시민의 일부 자유를 제한할 수 있다.”라는 문항에 대해 강하게 반대(1)를 선택했다면, 이는 응답자가 개인 자유를 우선시하는 성향을 보인다고 해석할 수 있다. “언론의 자유는 어떠한 경우에도 제한되어서는 안 된다.”, “법과 질서를 유지하기 위해 강력한 정부 권력이 필요하다.” 문항도 동일하게 평가한다.\n이러한 방식으로 리커트 척도를 활용하면 개별 응답자의 정치적 태도를 수량적으로 평가할 수 있으며, 특정 집단(예: 연령별, 지역별, 교육 수준별)의 정치적 경향성을 비교하는 데 활용될 수 있다. 또한, 평균 점수를 계산하여 집단 간 차이를 분석하거나, 시간에 따른 태도 변화를 추적하는 연구에도 유용하게 적용할 수 있다.\n\n\n(3) 사전 확률 척도\n사전 확률 척도는 응답자가 특정 사건이 발생할 가능성을 직접 확률로 평가하도록 하는 척도이다. 이 척도는 0%에서 100%까지의 범위를 사용하여, 응답자가 얼마나 확신하는지를 수량적으로 표현할 수 있도록 설계되었다.\n전통적인 리커트 척도나 의미분화 척도와 달리, 사전 확률 척도는 응답자가 단순히 동의/반대 또는 강도를 선택하는 것이 아니라 특정 사건이 발생할 확률을 직접 입력하거나 선택할 수 있도록 한다.\n다음 질문에 대해, 해당 사건이 발생할 가능성을 0%에서 100% 사이에서 선택해 주세요.\n\"다음 선거에서 A 후보가 당선될 확률은 몇 %라고 생각하십니까?\"\n⬜ 0% ⬜ 10% ⬜ 20% ⬜ 30% ⬜ 40% ⬜ 50% ⬜ 60% ⬜ 70% ⬜ 80% ⬜ 90% ⬜ 100%\n\"내년 경제가 성장할 확률은 얼마나 된다고 생각하십니까?\"\n⬜ 0% ⬜ 10% ⬜ 20% ⬜ 30% ⬜ 40% ⬜ 50% ⬜ 60% ⬜ 70% ⬜ 80% ⬜ 90% ⬜ 100%\n\"내년 중 실업률이 증가할 가능성은 얼마나 된다고 생각하십니까?\"\n⬜ 0% ⬜ 10% ⬜ 20% ⬜ 30% ⬜ 40% ⬜ 50% ⬜ 60% ⬜ 70% ⬜ 80% ⬜ 90% ⬜ 100%\n\n\n(4) 피시바인-아즈젠 태도 모델 Theory of Reasoned Action)\n태도와 행동의 관계를 설명하는 이론적 모델로 특정 행동에 대한 개인의 태도가 행동 의도(Behavioral Intention)에 어떻게 영향을 미치는지를 분석하는 데 사용된다. 이 모델은 이성적 행동 이론의 핵심 개념을 기반으로 하며, 주로 소비자 행동, 건강 행동, 마케팅, 정책 연구 등에서 활용된다.\n피시바인-아즈젠 태도 모델의 수식\n\\[BI = (A)W_{1} + (SN)W_{2}\\]\n\nBI (Behavioral Intention): 행동 의도\nA (Attitude): 개인의 태도\nSN (Subjective Norms): 주관적 규범\nW₁, W₂: 태도와 주관적 규범의 상대적 중요도 가중치\n\n즉, 행동 의도는 개인의 태도(A)와 사회적 규범(SN)의 영향을 받아 형성되며 이 두 요소가 행동을 예측하는 중요한 변수로 작용한다.\n예시\n태도(A), 주관적 규범(SN), 행동 의도(BI)는 하나의 단순한 질문으로 완벽히 측정할 수 없는 복합적인 개념이다. 예를 들어, 태도(A)는 ”긍정적”인지 ”부정적”인지뿐만 아니라, ”유용성”, ”편리성”, ”비용” 등의 하위 요소로 구성될 수 있다. 다수의 리커트 척도(5점, 7점) 문항들로 구성하여 평균 점수를 이용한다.\n행동 의도(BI) 측정 문항\n목표: 응답자가 특정 행동(예: 전기차 구매)을 실제로 할 의향이 있는지를 평가\n”나는 앞으로 6개월 이내에 전기차를 구매할 의향이 있다.” (1 = 전혀 없음, 5 = 매우 높음\n”나는 전기차를 구매하는 것을 진지하게 고려하고 있다.” (1 = 전혀 그렇지 않다, 5 = 매우 그렇다)\n”전기차 구매를 위한 정보를 적극적으로 찾고 있다.” (1 = 전혀 그렇지 않다, 5 = 매우 그렇다)\n”내가 차량을 구매한다면, 전기차를 선택할 가능성이 높다.” (1 = 전혀 그렇지 않다, 5 = 매우 그렇다)\n태도(A) 측정 문항\n목표: 개인이 특정 행동(예: 전기차 구매)에 대해 어떤 감정을 가지고 있는지를 평가\n” 나는 전기차를 구매하는 것이 좋은 선택이라고 생각한다.” (1 = 매우 나쁜 선택, 5 = 매우 좋은 선택)\n”전기차는 환경 보호에 기여한다고 생각한다.” (1 = 전혀 그렇지 않다, 5 = 매우 그렇다)\n”전기차는 경제적으로 합리적인 선택이다.” (1 = 전혀 그렇지 않다, 5 = 매우 그렇다)\n”나는 전기차가 기존 내연기관 차량보다 성능이 더 좋다고 생각한다.” (1 = 전혀 그렇지 않다, 5 = 매우 그렇다)\n주관적 규범(SN) 측정 문항\n목표: 특정 행동(예: 전기차 구매)에 대해 주변 사람들(가족, 친구, 사회)이 어떻게 생각하는지 평가\n”내 가족은 내가 전기차를 구매하는 것을 지지할 것이다.” (1 = 전혀 지지하지 않음, 5 = 매우 지지함)\n”내 친구들은 전기차를 구매하는 것이 좋은 선택이라고 생각한다.” (1 = 전혀 그렇지 않다, 5 = 매우 그렇다)\n”주변 사람들이 나에게 전기차를 추천할 가능성이 높다.” (1 = 전혀 그렇지 않다, 5 = 매우 그렇다)\n”사회적으로 전기차를 구매하는 것이 긍정적으로 여겨진다.” (1 = 전혀 그렇지 않다, 5 = 매우 그렇다)\n가중치 추정: 관심 집단 응답자의 응답결과 데이터를 이용하여 회귀추정하여 얻는다.\n\\(W_{1} &gt; W_{2}\\) → 행동 의도에 대한 태도의 영향력이 더 큼\n\\(W_{1} &lt; W_{2}\\) → 행동 의도에 대한 사회적 규범의 영향력이 더 큼\n\\(W_{1} \\approx W_{2}\\) → 태도와 사회적 규범이 비슷한 영향을 미침\n\n\n(5) 감성 측정 척도\n감성 측정 Emotional Measuremen 척도는 응답자의 감정 상태나 특정 대상(브랜드, 제품, 광고, 서비스 등)에 대한 감성적 반응을 측정하는 방법이다. 감성은 단순한 긍정적/부정적 반응을 넘어 다양한 차원(예: 흥분, 즐거움, 불안, 신뢰, 공포 등)으로 세분화될 수 있다. 따라서, 감성 측정 척도는 응답자가 특정 경험을 통해 느끼는 감정을 정량적으로 평가할 수 있도록 설계된다.\n(1) 감성적 반응 척도 (Emotional Response Scale)\n응답자가 특정 경험—예를 들어 제품을 사용하거나, 광고를 시청하거나, 서비스를 이용한 후—그 경험을 통해 느낀 감정을 평가할 수 있도록 설계된 척도이다. 이 척도는 단순히 ’좋았다/나빴다’는 이분법적 판단을 넘어서, 경험이 유발한 감정의 종류와 강도를 수치화하여 정밀하게 측정할 수 있게 해준다.\n감성적 반응 척도는 특히 브랜드 이미지 조사나 UX/UI(User Experience/User Interface) 연구 등에서 자주 활용된다. 예를 들어 브랜드 이미지 조사의 경우, 소비자가 특정 브랜드를 접했을 때 느끼는 감정—예컨대 ‘신뢰’, ‘즐거움’, ‘불쾌감’—을 수치화함으로써, 브랜드가 어떤 정서적 인상을 남기는지를 파악할 수 있다.\n또한, UX/UI 분야에서는 사용자가 웹사이트나 앱을 사용할 때 느끼는 감정 변화를 측정하여, 인터페이스 설계가 긍정적인 사용자 경험을 유도하고 있는지 평가하는 데 활용된다. 예를 들어, 버튼의 위치나 색상, 페이지 이동 흐름이 사용자에게 혼란을 주었는지, 아니면 직관적이고 만족스러운 경험을 제공했는지 감성 반응을 통해 정량적으로 분석할 수 있다.\n이처럼 감성적 반응 척도는 소비자나 사용자의 ’감정’을 체계적으로 파악하여, 브랜드 전략, 제품 개선, 사용자 경험 디자인 등에 있어 보다 감성적이고 인간 중심적인 의사결정을 가능하게 해주는 중요한 도구이다.\n”이 브랜드를 접했을 때, 다음 감정을 얼마나 강하게 느끼셨습니까?”\n고급스럽다: 1(전혀 아님) ~ 7(매우 많이)\n세련되다: 1(전혀 아님) ~ 7(매우 많이)\n친근하다: 1(전혀 아님) ~ 7(매우 많이)\n(2) PANAS 척도 (Positive and Negative Affect Schedule)\n응답자가 특정 상황에서 느낀 긍정적 정서(Positive Affect)와 부정적 정서(Negative Affect)를 각각 독립적으로 측정할 수 있도록 설계된 심리 측정 도구이다.\n이 척도는 일반적으로 20개의 감정 항목(긍정적 감정 10개, 부정적 감정 10개)으로 구성되며, 각 항목에 대해 응답자는 자신이 느낀 감정의 빈도나 강도를 5점 또는 7점 리커트 척도를 이용해 평가하게 된다. 예를 들어 “기쁨”, “열정”, “긴장”, “불안” 등의 감정 단어에 대해 “전혀 느끼지 않았다”부터 “매우 강하게 느꼈다”까지 점수를 매기는 방식이다.\nPANAS 척도는 다음과 같은 분야에서 유용하게 활용된다.\n\n광고 및 마케팅 연구: 광고를 본 후 소비자가 경험한 긍정적 감정(예: 흥미, 기쁨)과 부정적 감정(예: 짜증, 지루함)을 측정함으로써 광고 메시지의 감정적 반응을 평가할 수 있다.\n고객 경험 분석: 서비스 이용 후 고객이 느낀 다양한 감정 상태를 분석하여, 단순한 만족도 이상으로 감정 기반의 고객 반응을 정량화할 수 있다. 예를 들어, 고객이 서비스 이후 ’짜증’을 느꼈다면 이는 단순한 불만족보다 더 즉각적인 개선이 필요한 지표로 해석될 수 있다.\n\nPANAS는 감정 상태를 분리해서 분석할 수 있기 때문에, 긍정적 감정이 높다고 해서 자동으로 부정적 감정이 낮다고 가정하지 않으며, 각각을 독립적으로 분석할 수 있다는 점에서 정서 분석의 정밀도를 높이는 데 기여한다. 이러한 특성 덕분에 심리학뿐만 아니라 마케팅, UX 디자인, 고객경험관리(CXM) 등 다양한 분야에서 활용되고 있다.\n”광고를 본 후, 다음 감정을 얼마나 강하게 느꼈습니까?”\n기쁨 😊: 1(전혀 아님) ~ 5(매우 많이)\n흥분 😃: 1(전혀 아님) ~ 5(매우 많이)\n짜증 😡: 1(전혀 아님) ~ 5(매우 많이)\n(3) SAM 척도 (Self-Assessment Manikin Scale)\n감정 측정을 위해 시각적인 아이콘(만화형 사람 그림)을 사용하는 방식으로, 언어에 대한 의존도를 최소화한 비언어적 감성 측정 도구이다. 이는 정서 반응을 세 가지 차원에서 평가한다.\n\n쾌-불쾌(valence): 얼마나 기분이 좋은지 또는 나쁜지를 나타냄\n각성(arousal): 얼마나 흥분되었는지 또는 평온한지를 나타냄\n지배감(dominance): 상황을 통제하고 있다고 느끼는 정도\n\n각 차원마다 시각적인 사람 형태의 그림이 점진적으로 변하며 표현되기 때문에, 글을 읽지 않고도 응답자가 자신의 감정을 선택할 수 있도록 설계되어 있다.\n주요 특징 및 활용\n\n언어 장벽 극복: 글을 해석할 필요가 없어 어린이, 비문해자, 다문화권 등 언어적 제약이 있는 대상에게 적합하다.\nUX/UI 디자인 평가: 사용자가 웹사이트나 애플리케이션을 사용할 때 느낀 감정 상태를 직관적으로 측정 가능하다. 예를 들어, 버튼 클릭 후 느낀 만족감이나 화면 구성에 대한 직관적 반응을 SAM 척도를 통해 정량화할 수 있다.\n제품 또는 서비스 만족도 조사: 소비자가 제품 사용 후 느낀 감정을 시각적으로 평가함으로써 감정 기반 만족도를 파악할 수 있다. 텍스트로 표현하기 어려운 감정 반응을 직관적이고 구체적으로 측정할 수 있다는 장점이 있다.\n\nSAM 척도는 특히 정서 반응을 정확하고 간편하게 수집해야 하는 경우에 매우 유용하며, 정량적 분석은 물론 감성디자인, 감정 기반 마케팅, 인터랙션 디자인 평가 등 다양한 분야에서 활용되고 있다.\n”이 제품을 사용한 후 기분은?”\n😃 😐 😢 (행복 → 슬픔)\n⚡ 🔋 💤 (흥분 → 차분)\n(4) Plutchik’s 감정 휠 (Plutchik’s Emotion Wheel)\n심리학자 로버트 플러칙(Robert Plutchik)이 제안한 감정 이론으로, 인간의 감정을 8가지(기쁨, 신뢰, 공포, 놀람, 슬픔, 혐오, 분노, 기대) 기본 감정으로 구분하고 이들 간의 관계를 시각적인 원형 구조로 표현한 모델이다.\n이 감정들은 서로 반대되는 쌍으로 구성되어 있으며, 감정 간의 혼합과 강도 변화를 통해 복합적인 감정 상태를 설명할 수 있다. 예를 들어,\n\n기쁨 + 신뢰 → 사랑\n공포 + 놀람 → 경외감(Awe)\n분노 + 혐오 → 경멸(Contempt) 등\n\n소비자 심리 연구: 광고, 브랜드, 패키징 등 소비자가 어떤 감정을 느끼는지를 감정 휠을 통해 분석할 수 있다. 브랜드가 유발하는 감정이 구매 행동에 어떤 영향을 미치는지 파악하는 데 유용하다.\n정신 건강 연구: 스트레스, 불안, 우울감 등의 감정 변화를 감정 휠 구조 내에서 시각적으로 추적할 수 있어, 심리치료, 상담, 감정일기 분석 등에서 활용된다.\n교육 및 자기이해 도구: 학습자나 환자가 자신의 감정을 더 정교하게 인식하고 표현할 수 있도록 돕는다.\n플러칙의 감정 휠은 복합적이고 동적인 감정 상태를 구조적으로 이해할 수 있게 해 주며, 감정 분석, 정서 인공지능, 감정 기반 마케팅 등의 분야에서 활용되고 있다.\n”이 브랜드를 접했을 때, 어떤 감정을 느꼈습니까?”\n➡️ 선택: 기쁨 😊 / 신뢰 🤝 / 기대감 😍 / 실망 ☹️"
  },
  {
    "objectID": "notes/intro_stat/univariate.html",
    "href": "notes/intro_stat/univariate.html",
    "title": "기초통계 3. 일변량 분석(비교분석)",
    "section": "",
    "text": "chapter 1. 비교분석(일변량분석) 기초\n\n1. 개념\n일변량분석\n데이터 분석에서 개별 확률변수를 독립적으로 분석하는 과정을 일변량 분석이라 한다. 이는 각 변수의 분포 특성을 파악하고, 중심 경향이나 산포 정도를 요약함으로써 데이터의 전반적인 구조를 이해하는 데 필수적인 출발점이 된다.\n변수의 유형에 따라 분석 방식은 달라진다. 범주형 변수는 개체를 분류하는 기능을 하며, 이 경우 분석은 주로 빈도와 상대빈도(비율)를 중심으로 이루어진다. 반면, 측정형 변수는 수치적으로 의미 있는 값을 가지므로 평균, 중앙값, 분산, 표준편차 등의 요약 통계량으로 변수의 중심과 퍼짐을 정리할 수 있다.\n모집단으로부터 추출한 확률표본 데이터는 모집단의 모든 정보를 가진 축소 데이터이므로 확률표본의 확률분포함수는 모집단 확률분포함수와 동일하다. 이를 이용하여 \\(f(x)\\)(모집단에 대한 모든 정보)와 \\(\\theta\\)(요약 정보)를 추론한다. (1) 확률분포함수 \\(f(x)\\)의 형태를 추론하는 것을 적합성 검정이라 한다. (2) 모수 \\(\\theta\\)에 대한 추론을 한다.\n\\[X \\sim f(x;\\theta)\\]\n\\[r.s. = (x_{1},x_{2},\\ldots,x_{n})\\]\n\\[x_{i} \\sim f(x_{i};\\theta)\\]\n\\[s(x_{1},x_{2},...,x_{n}) \\sim f(s)\\]\n\n\n\n\n\n비교분석\n각 확률변수는 이론적으로는 고유의 확률분포함수를 가지고 있으며, 해당 분포를 대표하는 값으로는 모비율, 모평균, 모분산과 같은 모수가 있다. 이러한 모수에 대한 추정과 비교는 단순한 요약을 넘어서 두 개 이상의 모집단 또는 조건 간의 차이를 평가하는 분석으로 이어지며, 이를 비교 분석이라 한다.\n일변량 분석은 개별 변수의 요약 통계와 분포적 특성을 이해하는 과정이며, 비교 분석은 그러한 요약값들이 서로 다른 조건이나 그룹 간에 실질적인 차이를 보이는지를 확률과 통계적 추론의 틀 안에서 검증하는 과정이다.\n\n\n\n\n\n관심 모수\n일변량 분석: 모비율 \\((\\theta = p)\\), 모평균 \\((\\theta = \\mu)\\), 모분산 \\((\\theta = \\sigma^{2})\\)\n비교분석:\n\n모비율 차이 \\((\\theta = p_{1} - p_{2})\\)\n모평균 차이 \\((\\theta = \\mu_{1} - \\mu_{2})\\)\n모분산 차이 \\((\\theta = \\sigma_{2}^{2}/\\sigma_{1}^{2})\\)\n\n\n\n2. 분석절차\n일변량 분석은 하나의 변수에 대해 그 분포 특성과 대표값을 요약하고 해석하는 분석이다. 이는 통계 분석의 가장 기본적인 단계이며, 이후 다변량 분석 또는 비교 분석의 기초가 된다.\n일변량 분석은 하나의 변수에 대한 기술적 요약뿐 아니라, 그 변수의 모집단 특성(모수)에 대해 추론적 판단을 내리기 위한 통계적 가설 을 설정하면서 시작된다. 가설 검정 중심의 일변량 분석은 단순한 분포 요약을 넘어 관측된 통계량이 우연에 의한 것인지, 실제로 의미 있는 차이를 나타내는 것인지를 판단하는 데 목적이 있다.\n모수에 대한 통계적 가설 설정\n일변량 분석은 단일 변수에 대한 자료를 기술적으로 요약하고, 그 분포 특성을 파악하는 데서 출발하지만 통계적 추론의 맥락에서는 모수에 대한 가설 설정이 분석의 첫 단계가 된다.\n통계학에서 말하는 모수란, 모집단의 특성을 수치적으로 나타내는 값으로서, 예를 들어 모평균, 모비율, 모분산 등이 이에 해당한다. 연구자는 관찰된 표본 데이터를 기반으로, 이러한 모수에 대해 일정한 주장을 하고 이를 검정하게 된다.\n【예시】 모평균에 대한 가설\n예를 들어, 어떤 교육 프로그램이 대학생의 수학 성취도에 미치는 영향을 평가하고자 할 때, 연구자는 해당 프로그램을 이수한 학생들의 수학 점수 평균이 전국 대학생 평균 점수인 70점과 유의미하게 다른지를 확인하고자 한다.\n귀무가설: \\(\\mu = 70\\) → 해당 프로그램을 이수한 학생들의 수학 점수는 전국 평균과 같다.\n대립가설:\\(\\mu \\neq 70\\) → 프로그램을 이수한 학생들의 평균 점수는 전국 평균과 다르다.\n이와 같이, 연구문제는 결국 모수에 대한 통계적 가설로 정식화되며, 그 이후의 분석 절차는 표본으로부터 계산된 통계량이 이 가설을 지지하거나 반박할 수 있는지를 판단하는 과정으로 구성된다.\n데이터 시각화\n일변량 분석에서 통계적 검정에 앞서 먼저 수행해야 할 작업은 시각화와 탐색적 분석이다. 하나의 변수에 대해 히스토그램, 박스플롯, 밀도함수 곡선 등을 통해 분포의 형태, 이상값 존재 여부, 중심과 산포의 구조 등을 직관적으로 파악할 수 있다.\n이 과정을 통해 변수의 특성을 이해하고, 이어서 설정할 가설이 타당한지, 검정에 필요한 전제 조건(예: 정규성)이 충족되는지를 평가할 수 있다. 시각화를 기반으로 연구 문제를 모수에 대한 통계적 가설로 정식화한 뒤, 이에 대한 검정통계량과 \\(p\\)값을 통해 귀무가설의 기각 여부를 판단하게 된다.\nMVUE 및 샘플링 분포 도출\n통계적 가설검정은 표본으로부터 관측된 통계량이, 귀무가설 하에서 나타날 수 있는 우연한 변동인지, 아니면 실제 모수의 차이로부터 기인한 것인지를 판단하는 데 그 목적이 있다. 이러한 판단은 근본적으로 모수에 대한 통계량의 성질과 분포를 기반으로 이루어진다.\n가설검정에서 사용하는 통계량은 해당 모수에 대한 추정량이며, 이 추정량이 가질 수 있는 여러 후보 중에서도, 최소분산 불편추정량(MVUE)이 가장 선호된다. 그 이유는 다음과 같다.\n\n불편성: 추정량의 평균이 실제 모수와 일치\n최소 분산: 동일한 정보 하에서 가장 정확하고 안정적인 추정 → 즉, MVUE는 신뢰성 있는 비교 기준을 제공하여 검정통계량의 변동성을 최소화하고, 기각 여부에 대한 결정의 오류 확률을 줄일 수 있다.\n\n가설검정은 다음과 같은 구조를 따른다.\n\\(\\text{검정통계량} = \\frac{\\text{MVUE - 귀무가설 하 모수}}{\\text{표준오차}}\\), 여기서 핵심은 표본 통계량의 분포, 즉 샘플링 분포이다. 왜냐하면,\n\n\\(p\\)값 계산은 ”이 통계량이 얼마나 극단적인가?”를 분포의 확률 기준에서 판단해야 하기 때문이다.\n기각역 또는 임계값 결정은 표본 통계량의 분포 형태를 전제로 하기 때문이다.\n\n즉, 귀무가설이 참일 때 추정량이 어떤 확률분포를 따르는지를 알아야 우리가 관측한 통계량이 우연히 나타났을 가능성을 수치적으로 판단할 수 있다.\n\n통계적 가설 검정의 수학적 정당성을 위한 필요 조건\nMVUE는 정확하고 효율적인 모수 추정의 기준선을 제공하고,\n샘플링 분포는 검정통계량의 변동성과 극단값의 확률을 판단할 기준이 된다.\n\n이 두 요소가 없다면, 가설검정은 단순한 수치 비교 이상의 의미를 갖지 못하며, \\(p\\)값이나 가설검정 결론에 대한 해석적 정당성이 크게 약화된다.\n검정통계량 및 \\(p\\)값 계산\n가설검정의 목적은, 귀무가설 하에서 관측된 표본 결과가 발생할 가능성이 얼마나 희박한지를 평가하는 것이다. 이를 위해 우리는 먼저 검정통계량을 정의하고, 그에 대한 관측값을 계산하여 기각 여부를 판단하게 된다. “교육 프로그램이 대학생의 수학 성취도 평가”에 대한 모평균 가설검정을 예시로 설명하기로 한다.\n검정통계량 정의 \\(H_{0}:\\mu = 70\\)\n\n전제: 정규분포 또는 중심극한정리 성립\n표준편차 \\(\\sigma\\)를 알고 있는 경우: \\(z = \\frac{\\overline{X} - \\mu_{0}}{\\frac{\\sigma}{\\sqrt{n}}} \\sim N(0,1)\\)\n표준편차 \\(\\sigma\\)를 모를 경우: \\(t = \\frac{\\overline{X} - \\mu_{0}}{\\frac{s}{\\sqrt{n}}} \\sim t(n - 1)\\)\n\n기각역과 유의확률 \\(p\\)값\n기각역이란, 귀무가설이 참일 때 통계량이 나타나기 어려운 극단적인 영역을 의미한다. 이 영역의 경계값을 임계값이라 하고, 이 값은 유의수준(\\(\\alpha\\))과 검정의 방향성(단측/양측)에 따라 결정된다.\n유의확률 \\(p\\)값은, 귀무가설이 참이라는 전제 하에서 관측된 검정통계량 이상으로 극단적인 값이 나타날 확률을 의미한다.\n\n\\(p &lt; \\alpha\\): 귀무가설 기각 → 결과는 통계적으로 유의하다.\n\\(p \\geq \\alpha\\): 귀무가설 기각하지 않음 → 통계적으로 유의하지 않다.\n\n결론\n통계적 가설검정 결과를 보고할 때는 단순히 기각 여부를 언급하는 것을 넘어, 주요 요약 통계량, 검정통계량, 유의확률, 그리고 필요한 경우 신뢰구간까지 함께 제시함으로써. 결과의 정량적 의미와 해석 가능성을 풍부하게 전달해야 한다. 이러한 구성은 통계 분석의 재현 가능성을 보장하고, 해석의 과학적 엄밀성을 확보하는 데 필수적이다.\n\n\n3. 예제 데이터\nseaborn 라이브러리의 titanic 데이터셋은 실제 1912년 타이타닉 호 침몰 사고의 승객 정보를 바탕으로 구성되어 있다(15개 변수, 표본크기-승객 891명)\n\n\n\n\n\n\n\n\n변수명\n설명\n데이터 타입\n\n\nsurvived\n생존 여부 (0 = 사망, 1 = 생존)\nint (범주형)\n\n\npclass\n선실 등급 (1, 2, 3등급)\nint (순서형)\n\n\nsex\n성별\ncategory\n\n\nage\n나이\nfloat\n\n\nsibsp\n동반한 형제자매/배우자 수\nint\n\n\nparch\n동반한 부모/자녀 수\nint\n\n\nfare\n운임 요금\nfloat\n\n\nembarked\n탑승 항구 (C = Cherbourg, Q = Queenstown, S = Southampton)\ncategory\n\n\nclass\n선실 등급 (문자형: First, Second, Third)\ncategory\n\n\nwho\n승객 구분 (man, woman, child)\ncategory\n\n\nadult_male\n성인 남성 여부\nbool\n\n\ndeck\n선실 위치 (A–G, 일부 결측)\ncategory\n\n\nembark_town\n탑승 도시 (Cherbourg, Queenstown, Southampton)\ncategory\n\n\nalive\n생존 여부 (yes/no)\ncategory\n\n\nalone\n혼자 탑승 여부\nbool\n\n\n\n#타이타닉 데이터 불러오기\nimport seaborn as sns\ntitanic = sns.load_dataset(“titanic\")\ntitanic.info()\n\n\n\n\n\n\n\n\nchapter 2. 두 사물이 다르다는 것을 어떻게 알 수 있는가?\n통계는 두 현상이 서로 다른지 판단할 때 유용한 도구다. 하지만 이 단순한 질문—“이 둘은 같은가, 다른가?”—의 답은 언제나 확률적인 것이다. 예를 들어 어떤 약이 효과가 있는지 알아보려 할 때, 우리는 다음과 같은 질문을 한다:\n”이 약을 복용한 환자들과 그렇지 않은 환자들 사이에 실질적인 차이가 존재하는가?”\n이는 두 그룹의 평균값이나 비율 또는 확률이 서로 통계적으로 유의미하게 다르냐는 문제로 귀결된다.\n평균의 차이가 진짜일까, 우연일까?\n가령, 한 실험에서 위약(placebo)을 투여한 집단의 평균 체중 감소량이 2파운드, 신약을 투여한 집단의 평균 감소량이 4파운드라고 하자. 이 경우 우리는 신약이 더 효과적이라고 생각할 수 있다. 하지만 그 차이가 단지 무작위 변동에 의한 것일 가능성도 있다.\n모든 측정은 일정 수준의 무작위성(randomness)을 내포하고 있다. 그렇기 때문에 단순히 평균값이 다르다는 사실만으로 두 집단이 다르다고 단정지을 수 없다.\n여기서 통계는 두 질문에 답한다:\n1. 관측된 차이가 얼마나 큰가?\n2. 그 차이가 우연히 발생했을 가능성은 얼마나 작은가?\n이 질문들에 대한 답은 신뢰 구간(confidence intervals)과 p값(p-values), 그리고 통계적 유의성(statistical significance)을 통해 평가된다.\n통계적 유의성과 실제적 유의성의 차이\n통계적으로 유의미한 차이가 있다고 해서, 그 차이가 실제로도 의미 있는 것은 아니다. 예를 들어, 두 도시의 평균 기온이 0.1도 차이 나고, 그 차이가 통계적으로 유의미하다고 하더라도, 실제 생활에서는 아무런 체감이 없을 수 있다.\n통계적 유의성은 오직 그 차이가 무작위로 발생할 가능성이 매우 낮다는 사실만을 말해줄 뿐이다.\n반면 실질적(또는 실용적) 유의성은 그 차이가 현실에서 중요하거나 영향력이 있는가를 판단한다.\n즉, ”차이가 있다”는 것과 ”중요한 차이다”는 것은 다르다.\n차이를 평가하는 기준: 효과 크기와 표준오차\n효과 크기(effect size)는 두 집단 간의 차이의 크기를 의미하며, 이를 표준오차(standard error)와 비교해 해석한다.\n\n신약 복용 그룹 평균 체중 감소: 4.0 파운드\n위약 그룹 평균 체중 감소: 2.0 파운드\n두 집단 간 차이: 2.0 파운드\n표준오차: 0.5 파운드\n\n차이(2.0)가 표준오차(0.5)의 4배이면, 이는 무작위로 발생했을 가능성이 매우 낮다고 해석할 수 있다.\n이러한 판단은 z-통계량 또는 t-통계량으로 정량화된다.\n무작위 샘플링의 중요성\n통계적 검정을 하기 위해서는, 샘플이 모집단을 대표해야 한다. 만약 샘플이 편향되어 있다면, 아무리 통계적으로 유의한 결과가 나와도 모집단에 일반화할 수 없다.\n예: 특정 체중 감량 약의 효과를 평가하는 실험에서 오직 운동선수들만을 대상으로 했다면, 그 결과를 일반 성인에게 적용하는 것은 무리다.\n통계적 검정의 전제 조건은 무작위 추출(random sampling)이다.\np-값에 대한 오해\np-값은 흔히 ”이 결과가 우연일 확률”이라고 해석되지만, 정확히는 다음과 같은 의미를 가진다:\n”귀무가설(null hypothesis)이 참일 때, 이 정도 이상의 차이가 나타날 확률”\n즉, p &lt; 0.05란,\n\n”차이가 우연히 발생했을 가능성이 5% 미만이다.”\n”따라서 우리는 귀무가설을 기각할 수 있다.” 는 의미일 뿐이다.\n\n하지만 p-값은 다음을 의미하지 않는다:\n\n”귀무가설이 5% 확률로 참이다.”\n”대립가설이 95% 확률로 참이다.”\n\n이는 매우 흔한 오해이므로 주의해야 한다.\n결론: 차이를 이해한다는 것의 의미\n두 사물, 두 그룹, 두 평균, 두 비율이 서로 다른지를 판단하려면, 우리는 그 차이가 단순한 무작위 노이즈(noise)로부터 비롯된 것인지 아닌지를 평가해야 한다. 이를 위해 통계는 다음과 같은 도구들을 제공한다:\n\n평균 및 비율\n표준편차, 표준오차\n신뢰구간 및 효과 크기\np-값과 유의수준 (보통 5%)\n\n그리고 이 모든 분석은 무작위 추출과 적절한 표본 설계라는 기반 위에서만 의미를 가진다.\n\n\nchapter 3. 모비율 추론\n\n1. 일집단 모비율\n연구문제 및 통계적 가설\n대규모 해양 재난인 타이타닉호 사고 당시, 전체 승객에 대한 사망률이 60%보다 높았다는 통계적 근거가 있는가?\n\n귀무가설: \\(H_{0}:p = 0.6( = p_{0})\\), 여기서 \\(p\\)는 모집단 사망률\n대립가설: \\(H_{1}:p &gt; 0.6( = p_{0})\\) (단측가설)\n모수: \\(\\theta = p\\)\n\n\n\n\n\n\n\n모집단 확률변수 \\(X\\)는 사망, 생존 두 개의 결과만 갖는 베르누이 시행 결과이며 모수는 사망율 \\(p\\)이다.\n데이터: 탑승인원 2,224중 신원이 확인된 891명 승객의 사망여부이다.\n\n시각화\n일변량 범주형 변수의 시각화는 빈도와 비율(확률분포함수)를 표현하는 것으로 바차트나 파이차트 도구를 이용한다. 타이타닉 승객의 표본 사망율은 61.6%이다.\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# 데이터 불러오기\ntitanic = sns.load_dataset(\"titanic\")\n\n# 생존/사망 값 개수 계산\ncounts = titanic['survived'].value_counts()\nlabels = ['Died', 'Survived']\ncolors = ['#ff9999','#66b3ff']\n\n# 파이차트 그리기\nplt.figure(figsize=(6, 6))\nplt.pie(\n    counts,\n    labels=labels,\n    autopct='%1.1f%%',\n    startangle=90,\n    colors=colors,\n    explode=(0.05, 0)  # 강조 효과\n)\nplt.title(\"Titanic Survival Distribution\")\nplt.axis('equal')  # 원형 유지\nplt.show()\n\n\n\n\n\nMVUE 및 샘플링분포\n모비율의 MVUE 추정치는 표본 비율 \\(\\widehat{p} = \\frac{\\#\\_ of\\_ death}{\\#\\_ of\\_ n} = 0.62\\)이고 샘플링분포는 중심극한정리에 의해 \\(\\widehat{p} \\sim N(p,\\frac{p(1 - p)}{n})\\)이다.\n검정통계량 및 \\(p\\)값 계산, 그리고 신뢰구간\n추정분산 \\(V(\\widehat{p}) = \\frac{p(1 - p)}{n}\\)은 이므로 표본비율의 표준편차인 표준오차 \\(s(\\widehat{p}) = \\sqrt{\\frac{p(1 - p)}{n}}\\)이다.\n\n검정통계량: \\(ts = \\frac{\\widehat{p} - p_{0}}{s(\\widehat{p}) = \\sqrt{\\frac{p_{0}(1 - p_{0})}{n}}} \\sim z\\)\n유의확률: \\(pvalue = P(z &gt; |ts|)\\)\n\n모비율 \\(100(1 - \\alpha)\\%\\)신뢰구간\n\\[\\widehat{p} \\pm z_{1 - \\alpha/2}\\sqrt{\\widehat{p}(1 - \\widehat{p})/n} = (0.584,0.648)\\]\nimport statsmodels.api as sm\nfrom statsmodels.stats.proportion import proportions_ztest\nimport seaborn as sns\n\n# 사망자 수 및 전체 탑승자 수\ncount = titanic[titanic['survived'] == 0].shape[0]\nnobs = titanic.shape[0]\np0 = 0.6  # 귀무가설: 사망률 = 0.6\n\n# 검정 수행\nstat, pval = proportions_ztest(count, nobs, value=p0, alternative='larger')\n\n# 신뢰구간 계산 (normal approximation)\nci_low, ci_upp = sm.stats.proportion_confint(count, nobs, alpha=0.05, method='normal')\n\n# 출력\nprint(f\"사망자 수: {count}\")\nprint(f\"전체 탑승자 수: {nobs}\")\nprint(f\"관측된 사망률: {count/nobs:.3f}\")\nprint(f\"검정통계량: {stat:.3f}\")\nprint(f\"p-값: {pval:.4f}\")\nprint(f\"사망률 95% 신뢰구간: ({ci_low:.3f}, {ci_upp:.3f})\")\n결론\n\n\n\n\n\n\n\n\n승객 사망률은 0.6 초과인가?\n\n\n\n\n표본비율\n검정통계량\n유의확률\n\n\n0.616\n0.992\n0.321\n\n\n95% 신뢰구간: (0.584, 0.648)\n\n\n\n유의확률이 0.321로 유의수준 0.05보다 크므로 귀무가설이 채택되어, 승객 사망률이 0.6보다 크다고 할 만한 통계적 근거는 부족하다. 따라서 관측된 사망률이 61.6%이기는 하나, 이는 표본 오차 범위 내의 우연한 차이일 수 있으며, 사망률이 정확히 60%라는 주장을 기각하기에는 충분하지 않다.\n\n\n2. 일집단 모비율-특수한 경우\n\n(1) 모비율 소표본 \\(min(np,n(1-p)) \\le 5\\)\n모비율 검정 시 검정통계량의 분포는 정규분포를 가정한다. 이는 중심극한 정리에 의한 것으로 대표본 경유에만 MVUE, 표본비율 샘플링분포는 정규분포를 따른다.\n그러나 소표본(\\(min(np,n(1 - p)) &lt; 5\\))인 경우는 이항분포를 이용하여 가설을 검정한다. 모집단의 개체가 성공(성공 확률이 \\(p\\)), 실패의 결과만 있으므로 확률표본은 베르누이 시행과 동일하다. 그러므로 표본크기 \\(n\\) 확률표본으로부터 구한 성공 개체의 수 합은 이항분포 \\(B(n,p)\\)를 따른다.\n학생 흡연 비율이 20% 미만이라고 발표했다. 맞 는지 알아보기 위하여 학생 20명을 확률 추출하여 흡연여부를 알아본 결과 3명이 흡연하고 있다고 조사 되었다. 발표가 맞는지 검정하시오.\n\n귀무가설 : \\(H_{0}:p = 0.2\\) vs. 대립가설 : \\(H_{0}:p &lt; 0.2\\)\n\\(min(20*0.2,20*0.8) = 4 &lt; 5\\)이므로 중심극한정리 적용이 불가능하다. 대신 흡연자 수는 이항분포(20, p=0.2)를 따르므로 유의확률은 다음과 같다. 귀무가설을 기각할 수 없어 학생 흡연율은 20% 미만이라 할 수 없다. \\(P(\\sum X_{i} \\leq 3|sumX_{i} \\sim B(n = 20,p = 0.2)) = 0.42\\)\n\n\n\n(2) Wilson 통계량\n표본크기 \\(n\\)에 비해 성공회수 \\(\\sum x_{i} = x\\) 가 매우 작은 경우 비율 추정치는 \\(\\widehat{p} = \\frac{x}{n}\\) 대신 \\(\\widehat{p} = \\frac{x + 2}{n + 4}\\)를 사용하고 추정방법은 대표본 동일하다.\n\n\n\n3. 독립인 두 모집단 비율 차이\n연구문제 및 통계적 가설\n타이타닉호 사고 당시, 단독 탑승 승객의 생존률이 동반자와 함께 탑승한 승객의 생존률보다 통계적으로 유의하게 15% 이상 높은지를 검정하고자 한다.\n\n\n\n\n\n\n귀무가설: \\(H_{0}:p_{1} - p_{2} = 0.15\\), 여기서 \\(p_{1}\\)는 혼자 여행하는 승객의 생존율, \\(p_{2}\\)는 동반 여행하는 승객의 생존율\n대립가설: \\(H_{1}:p_{1} - p_{2} &gt; 0.15\\) (단측가설)\n모수: \\(\\theta = p_{1} - p_{2}\\)\n\n시각화\n집단별 일변량 범주형 변수의 시각화는 빈도와 비율(확률분포함수)를 표현하는 것으로 바차트 도구를 이용한다.\n#동반여부별 생존여부 시각화\nimport matplotlib.pyplot as plt\n\n# 상대빈도 계산 (교차비율표 → 정규화)\nprop_df = (\n    titanic\n    .groupby(['alone', 'survived'])\n    .size()\n    .reset_index(name='count')\n)\n\n# 전체 대비 비율 계산 (alone 그룹 내 비율)\nprop_df['proportion'] = prop_df.groupby('alone')['count'].transform(lambda x: x / x.sum())\n\n# 시각화\nplt.figure(figsize=(6, 5))\nax = sns.barplot(data=prop_df, x='alone', y='proportion', hue='survived', palette='Set2')\n\n# 축 라벨 및 타이틀 설정\nax.set_xticklabels(['Not Alone', 'Alone'])\nplt.ylabel(\"Proportion\")\nplt.xlabel(\"Traveling Alone\")\nplt.title(\"Survival Rate by Alone Status\")\nplt.ylim(0, 1)\n\n# 비율 라벨 표시\nfor container in ax.containers:\n    ax.bar_label(container, labels=[f'{h:.1%}' for h in container.datavalues], label_type='center')\n\nplt.legend(title=\"Survived\", labels=[\"No\", \"Yes\"])\nplt.tight_layout()\nplt.show()\n\n\n\n\n\nMVUE 및 샘플링분포\n\n동반 여행하는 승객 표본비율: \\({\\widehat{p}}_{1} = 0.506\\)\n혼자 여행하는 승객 표본비율: \\({\\widehat{p}}_{2} = 0.304\\)\nMVUE: \\(\\widehat{\\theta} = {\\widehat{p}}_{1} - {\\widehat{p}}_{2} = 0.202\\)\n\n샘플링분포\n\\[{\\widehat{p}}_{1} - {\\widehat{p}}_{2} = N(p_{1} - p_{2},\\frac{p_{1}(1 - p_{1})}{n_{1}} + \\frac{p_{2}(1 - p_{2})}{n_{2}})\\]\n# alone별 survived = 1 비율 (평균값이 생존률)\nsurvival_rate_by_alone = titanic.groupby('alone')['survived'].mean()\nprint(titanic['survived'].mean(),survival_rate_by_alone)\n0.3838383838383838  alone  False 0.505650  True 0.303538\n검정통계량 및 \\(p\\)값 계산, 그리고 신뢰구간\n추정량 \\(\\widehat{\\theta} = {\\widehat{p}}_{1} - {\\widehat{p}}_{2}\\)의 표준오차는 \\(s({\\widehat{p}}_{1} - {\\widehat{p}}_{2}) = \\sqrt{\\frac{{\\widehat{p}}_{1}(1 - {\\widehat{p}}_{1})}{n_{1}} + \\frac{{\\widehat{p}}_{2}(1 - {\\widehat{p}}_{2})}{n_{2}}}\\)이다. 신뢰구간 구할 경우에는 표준오차는 위의 식을 사용해야 하나 가설검정 시에는 귀무가설이 맞다는 가정 하에 검정통계량을 계산하므로 개별 표본비율 대신 통합비율 \\({\\widehat{p}}_{0} = \\frac{n_{x} + n_{y}}{n_{1} + n_{2}}\\)을 사용하여 구해야 한다.\n\n검정통계량: \\(TS = \\frac{{\\widehat{p}}_{1} - {\\widehat{p}}_{2} - 0}{s({\\widehat{p}}_{1} - {\\widehat{p}}_{2})} \\sim z\\)\n유의확률: \\(pvalue = P(z &gt; |ts|)\\)\n\n\\(100(1-\\alpha) \\%\\) 모비율 차이(\\(p_{1} - p_{2}\\)) 신뢰구간\n\\[({\\widehat{p}}_{1} - {\\widehat{p}}_{2}) \\pm z_{1 - \\alpha/2}\\sqrt{\\frac{{\\widehat{p}}_{1}(1 - {\\widehat{p}}_{1})}{n_{1}} + \\frac{{\\widehat{p}}_{2}(1 - {\\widehat{p}}_{2})}{n_{2}}}\\]\n#모비율 차이 검정\nfrom statsmodels.stats.proportion import proportions_ztest\n\n# 집단별 생존자 수 및 전체 수\ngrouped = titanic.groupby(\"alone\")[\"survived\"].agg(['sum', 'count'])\n\nsuccess = [grouped.loc[False, 'sum'], grouped.loc[True, 'sum']]\nnobs = [grouped.loc[False, 'count'], grouped.loc[True, 'count']]\n\n# 단측 검정 : 모수차이 0.15 설정\nstat, pval = proportions_ztest(success, nobs, value=0.15, alternative='larger')\n\n# 출력\nprint(f\"검정통계량: {stat:.3f}\")\nprint(f\"p-값: {pval:.4f}\")\n결론\n\n\n\n\n\n\n\n\n\n혼자 여행 승객 생존률은 동반 여행 승객 생존률보다 15% 높은가?\n\n\n\n\n집단\n표본비율\n검정통계량\n유의확률\n\n\n동반 여행\n0.506\n1.565\n0.059\n\n\n홀로 여행\n0.304\n\n\n\n총합\n0.384\n\n\n\n유의확률이 0.059로 유의수준 0.05를 초과하므로 귀무가설을 기각할 수 없다. 동반자와 함께 여행한 승객의 생존률(50.6%)이 혼자 여행한 승객의 생존률(30.4%)보다 높기는 하지만, 그 차이가 통계적으로 15% 이상이라고 단정할 수는 없다.\n\n\n4. 짝진 집단 비율 차이 McNemar 검정\n연구문제(시나리오)\n동일한 개체에 대하여 이진형(성공, 실패) 결과의 실험(프로그램)을 서로 다른 두 기간(before - after)에 측정하여 실험(프로그램) 효과가 있는지 알아보는 방법이다.\n[사례연구] Bland (2000) 1319명 어린이에 대하여 12살에 독감에 걸릴 가능성은 나이가 14살이 되면 높아지는지, 낮아지는지 알아보기 위하여 조사한 결과이다. 즉 1,319명 어린이는 동일하며 12살에 조사하고 14살에 다시 한 번 조사한 결과이다. https://www.medcalc.org/manual/ mcnemartest2.php\n\n\n\n\n\n12세와 14세 사이에 감기에 걸릴 확률은 변하지 않는다. ⇔ 심한 감기에 걸릴 확률이 시간에 따라 달라지지 않았다.\nP(A)+P(B)(12 살에 걸린 독감 걸린 사람 비율)=P(A)+P(C)(14 살에 걸린 독감 걸린 사람 비율)은 동일하고 (2) P(C)+P(D)(12 살에 걸린 독감 안 걸린 사람 비율)=P(B)+P(D)(14 살에 걸린 독감 안 걸린 사람 비율)은 동일하다는 가정이 성립하면 \\(P(B) = P(C)\\)이성립한다. 즉 12살에 독감 감염자의 14살 미감염 비율과 14살 독감 감염자의 12살 독간 미감염 비율은 동일하다.\n통계적 가설\n\n귀무가설 : 나이에 관계없이 감기 걸릴 가능성은 동일하다. \\(P(B) = P(C)\\)\n대립가설 : 나이에 따른 독감 걸릴 가능성은 달라진다.\n\n점추정치 및 샘플링 분포: \\(TS = \\frac{(B - C)^2}{B + C}\n= \\frac{(144 - 256)^2}{144 + 256}\n= 31.36 \\sim \\chi^2(1)\\)\n유의확률: \\(p = P(\\chi_{(1)}^{2} &gt; 31.36) &lt; 0.001\\)\n결론\n귀무가설은 기각되어, 12살에 독감 걸릴 확률은 27%(=356/1319)이다. 14살에 독감 걸릴 확률은 34.7%(=468/1319)이다. 그러므로 나이가 올라가면 독감에 걸릴 가능성이 높아진다.\n\n\n\nchapter 4. 모평균 추론\n\n1. 일집단 모비율\n연구문제 및 통계적 가설\n타이타닉호 승객들의 평균 연령이 30세 미만이었는지를 알아보고자 한다.\n\n귀무가설: \\(H_{0}:\\mu = 30( = \\mu_{0})\\), 여기서 \\(\\mu\\)는 승객 평균 나이\n대립가설: \\(H_{1}:\\mu &lt; 30( = \\mu_{0})\\) (단측가설)\n\n\n\n\n\n\n\n모수: \\(\\theta = \\mu\\), 보조 ancillary 모수 \\(\\sigma^{2}\\)는 MVUE(\\(\\widehat{\\sigma^{2}} = S^{2}\\))로 대체한 후 모수 \\(\\theta\\)에 대한 추론을 한다.\n\n시각화\n\n(1) 히스토그램\n히스토그램은 연속형(측정형) 변수의 분포 특성을 시각적으로 표현하는 대표적인 그래프로, 수평축(x-축)은 데이터의 구간(계급), 수직축(y-축)은 해당 구간에 속하는 도수 또는 상대도수를 나타낸다. 이는 범주형 자료에 사용하는 막대그래프와 유사하나, 구간 간 간격이 균등하며, 데이터의 수치적 분포를 반영한다는 점에서 차이를 갖는다.\n\n\n\n\n\n데이터는 먼저 오름차순으로 정렬한 후, 최소값과 최대값을 기준으로 전체 범위를 결정한다. 그 다음, 계급의 개수(또는 bin의 수)를 정하고, 이 범위를 균등하게 나누어 계급 구간을 구성한다. 계급의 수는 일반적으로 8개에서 12개 사이가 적절하며, 표본의 크기를 \\(n\\)이라 할 때 적절한 계급 수 \\(k\\)는 다음과 같은 Sturges의 공식으로도 근사할 수 있다. \\(k = 1 + 3.322\\log_{10}(n)\\)\n각 계급 구간 내에 포함된 데이터의 수 또는 상대빈도를 계산하고, 이를 막대의 높이로 표현하면 히스토그램이 완성된다. 이때 구간 폭은 가능한 해석의 의미를 갖도록 정수 단위로 조정하는 것이 바람직하다.\n1. 분포의 모양 (Shape) - 왜도\n\n대칭 분포: 종 모양이거나 좌우 대칭 → 예: 정규분포\n우측(+) 치우침: 긴 오른쪽 꼬리, 예: 소득, 주가\n좌측(−) 치우침: 긴 왼쪽 꼬리, 예: 소득에서 기본 소득 미만 많음\n이첨성(Bimodal) 또는 다첨성(Multimodal): 두 개 이상의 중심이 있는 분포 → 이질적 모집단 가능성 예: 남녀 혼합 신장 데이터\n\n2. 중심위치 (Center)\n\n최빈값(mode) 위치에서 가장 높은 막대\n평균이나 중앙값의 대략적 위치를 가늠\n\n3. 산포도 (Spread) - 첨도\n\n분포의 넓이로 데이터의 변동성이나 표준편차의 정도를 직관적으로 판단\n고첨도 (Leptokurtic): 뾰족하고 꼬리가 두꺼움, 극단값 자주 발생 (수익률, 사고율)\n평첨도 (Platykurtic): 평평하고 꼬리가 짧음, 시험점수, 설문척도\n\n4. 이상값(Outliers)의 존재 가능성\n\n분포의 양쪽 끝에 고립된 막대가 있을 경우, 극단치 존재 가능성 있음\n상자수염 그림을 병행하여 이상값 여부를 정밀 진단할 수 있음\n\n\n\n\n\n\n상자 그림은 연속형 자료의 분포를 시각적으로 요약하는 대표적인 탐색적 자료 분석 도구로, 데이터의 중심 경향과 산포도, 분포의 비대칭성, 이상치의 존재 여부를 직관적으로 파악할 수 있다. 다음의 다섯 가지 핵심 통계량을 기반으로 구성된다.\n최소값 (minimum), 제1사분위수 (Q₁, 25th percentile), 중앙값 (median, Q₂), 제3사분위수 (Q₃, 75th percentile), 최대값 (maximum)\n\n\\(IQR = Q_{3} - Q_{1}\\): 사분위 범위(Interquartile Range)로,데이터의 중심 50%가 분포하는 구간이다.\n상자는 Q₁부터 Q₃까지를 연결하여 그리며, 이 구간은 전체 데이터의 중간 50%를 나타낸다.\n상자 내에 중앙값을 수직선으로 표시한다.\n중위값이 상자의 한쪽으로 치우쳐 있다면 비대칭 분포를 의미한다 (예: 오른쪽 치우침일 경우\\(Q_{3} - MD &gt; MD - Q_{1}\\)).\n수염(whisker): 다음의 범위 내에 있는 최소값과 최대값을 가리킨다:\n\\(\\lbrack Q_{1} - 1.5 \\times IQR,Q_{3} + 1.5 \\times IQR\\rbrack\\): 수염 밖에 존재하는 관측값은 이상치(outlier)로 간주되며, 보통 별표(*) 혹은 점(·)으로 표시한다.\n극단적인 이상치를 검출할 경우, \\(3 \\times IQR\\)을 기준으로 하기도 한다.\n\n\n\n(2) 커널추정\n확률표본 데이터를 모집단으로 유한개의 데이터 표본을 이용하여 모집단 확률분포함수를 평활법에 의해 구한다. 신호처리에서는 Parzen–Rosenblatt window 방법이라고 한다. KDE kernel density estimator는 커널 함수를 이용하여 히스토그램 폴리곤 방법을 보다 스무드한 곡선으로 연결하여 추정하는 방법이다.\n\n\n\n\n\n커널함수 \\(\\int_{- \\infty}^{\\infty}K(x)dx = 1\\)는 양의실수 적분가능한 좌우 대칭인 함수이며 Gaussian, Epanechnikov 등이 유명한 커널 함수이다. 확률분포함수 \\(f(x)\\)로 부터 확률표본 데이터 \\(x_{1},x_{2},\\cdots,x_{n}\\)가 주어진 경우 커널추정량은 \\(\\widehat{f(x)} = \\frac{1}{nh}\\overset{n}{\\sum_{i = 1}}K(\\frac{x - x_{i}}{h})\\)이다. \\(K()\\)는 커널함수이고 \\(h\\)는 bandwith 모수이다. \\(h\\)가 크면 완만한 형태가 되고 작으면 뾰족한 형태이다.\n\n최적 \\(h = (\\frac{4{\\widehat{\\sigma}}^{5}}{3n})^{\\frac{1}{5}} \\approx 1.06\\widehat{\\sigma}n^{- 0.2}\\)\n만약 좌우로 치우친 분포의 경우\\(\\widehat{\\sigma} = min(sd,IQR/1,34)\\)\n\n\n\n\n\n\n이상치 진단되면 제거하고 추정하자. 상자그림을 보면 우측 꼬리에 많은 관측치가 이상치로 진단되었다.\n#이상치 제거\n# age 변수의 이상치 기준 계산 (IQR)\nq1 = titanic['age'].quantile(0.25)\nq3 = titanic['age'].quantile(0.75)\niqr = q3 - q1\n\n# 이상치 경계\nlower_bound = q1 - 1.5 * iqr\nupper_bound = q3 + 1.5 * iqr\n\n# 이상치 제외한 데이터프레임 생성\ntitanic0 = titanic[(titanic['age'] &gt;= lower_bound) & (titanic['age'] &lt;= upper_bound)]\n\n# 결과 확인\nprint(f\"원래 데이터 수: {titanic.shape[0]}\")\nprint(f\"이상치 제거 후 데이터 수: {titanic0.shape[0]}\")\n원래 데이터 수: 891  이상치 제거 후 데이터 수: 703\nMVUE 및 샘플링분포\n모평균 \\(\\mu\\)의 MVUE 추정치는 표본평균 \\(\\overline{x} = \\sum x_{i}/n = 29.1\\)\n보조모수 \\(\\sigma^{2}\\) MVUE: \\(\\widehat{\\sigma^{2}} = s^{2} = \\frac{\\sum(x_{i} - \\overline{x})^{2}}{n - 1}\\)\n샘플링분포\n\n(대표본 \\(n &gt; 20\\)) 중심극한정리에 의해 \\(\\overline{x} \\sim N(\\mu,\\sigma^{2}/n)\\)이다.\n(소표본) 모집단 확률분포 정규분포라는 가정 하에 \\(\\frac{\\overline{x} - \\mu}{\\frac{s}{\\sqrt{n}}} \\sim t(n - 1)\\)이다.\n\n검정통계량 및 \\(p\\)값 계산, 그리고 신뢰구간\nfrom scipy.stats import ttest_1samp\nimport seaborn as sns\n\n# 이상치 제거된 titanic0 사용\nage_clean = titanic0['age'].dropna()  # 결측값 제거\n\n# 단일 평균에 대한 t-검정\nt_stat, p_val = ttest_1samp(age_clean, popmean=30)\n\n# 결과 출력\nprint(f\"표본 크기: {age_clean.shape[0]:.0f}\")\nprint(f\"표본 평균: {age_clean.mean():.1f}\")\nprint(f\"표본 표준편차: {age_clean.std():.2f}\")\nprint(f\"t-통계량: {t_stat:.3f}\")\nprint(f\"p-값: {p_val:.4f}\")\n표본 크기: 703  표본 평균: 29.1  표본 표준편차: 13.73  t-통계량: -1.792  p-값: 0.0735\n#모평균 신뢰구간\nimport scipy.stats as st\n# 'confidence' argument added to specify the confidence level\nst.t.interval(confidence=0.95, df=len(age_clean)-1, \n              loc=age_clean.mean(), scale=st.sem(age_clean))\n(np.float64(28.06), np.float64(30.09))\n\nMVUE 표본평균: \\(\\widehat{\\theta} = \\overline{x} = 29.1\\)\n\\(\\overline{x}\\)의 표준오차: \\(se(\\overline{x}) = \\frac{s}{\\sqrt{n}} = \\frac{13.73}{\\sqrt{793}}\\)\n(대표본)검정통계량: \\(ts = \\frac{\\overline{x} - \\mu_{0}}{se(\\overline{x})} = \\frac{29.1 - 30}{\\frac{13.73}{\\sqrt{703}}} = - 1.79\\)\n유의확률: \\(p = P(z &gt; | - 1.79|) = 0.0735\\)\n모평균 [\\(100(1 - \\alpha)\\%\\)신뢰구간]\n\n\\[(\\overline{x} - t_{1 - \\alpha/2;n - 1}\\frac{s}{\\sqrt{n}},\\overline{x} + t_{1 - \\alpha/2;n - 1}\\frac{s}{\\sqrt{n}})\\]\n결론\n\n\n\n\n\n\n\n\n\n타이타닉 승객 평균 나이는 30세 미만인가?\n\n\n\n\n표본평균\n표준편차\n검정통계량\n유의확률\n\n\n29.1\n13.73\n-1.792\n0.074\n\n\n숭객 평균 95% 신뢰구간: (28.06, 30.09)\n\n\n\n타이타닉 승객의 평균 나이가 30세 미만인지를 검정한 결과, 표본 평균은 29.1세, 표준편차는 13.73으로 나타났으며, 단측 t-검정의 검정통계량은 -1.792, 유의확률은 0.074였다. 일반적으로 사용하는 유의수준 0.05를 기준으로 할 때, 유의확률이 이를 초과하므로 귀무가설을 기각할 수 없으며, 따라서 타이타닉 승객의 평균 나이가 30세 미만이라고 통계적으로 유의하게 말하기는 어렵다.\n\n\n\n2. 독립인 두 모집단 평균 차이\n\n\n\n\n\n연구문제 및 통계적 가설\n타이타닉호 승객들의 성별에 따른 나이의 차이가 있는지 알아보고자 한다.\n\n귀무가설: \\(H_{0}:\\mu_{1} - \\mu_{2} = 0\\), 여기서 \\(\\mu_{1}\\)은 남자 승객 평균 나이, \\(\\mu_{2}\\)은 여자 승객 평균 나이는 같다.\n대립가설: \\(H_{1}:\\mu_{1} - \\mu_{2} \\neq 0\\) (양측가설) 승객 남녀 평균 나이 차이는 유의하다.\n모수: \\(\\theta = \\mu_{1} - \\mu_{2}\\)\n\n시각화\n집단변수(성별)별 관심변수(나이)에 대한 (히스토그램, 상자그림, KDE)로 데이터를 시각화 한다.\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# 데이터 불러오기\ntitanic = sns.load_dataset(\"titanic\")\ntitanic = titanic[['sex', 'age']].dropna()\n\n# 그래프 그리기\nfig, axs = plt.subplots(2, 1, figsize=(10, 8), sharex=True)\n\n# 히스토그램\nsns.histplot(data=titanic, x=\"age\", hue=\"sex\", bins=30, ax=axs[0],\n             element=\"step\", stat=\"density\", common_norm=False, alpha=0.4)\n\n# KDE 추가\nsns.kdeplot(data=titanic, x=\"age\", hue=\"sex\", ax=axs[0], common_norm=False)\n\naxs[0].set_title(\"age distribution by gender (histogram + KDE)\")\naxs[0].set_ylabel(\"density\")\n\n# 상자그림\nsns.boxplot(data=titanic, x=\"age\", y=\"sex\", ax=axs[1])\naxs[1].set_title(\"age distribution by gender (box plot)\")\naxs[1].set_xlabel(\"age\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\nMVUE 및 샘플링분포\n\n모평균 \\(\\mu_{1}\\)의 MVUE 추정치: 표본 평균 \\(\\overline{x} = \\frac{\\sum x_{i}}{n_{1}} = 27.9\\)\n모평균 \\(\\mu_{2}\\)의 MVUE 추정치: 표본 평균 \\(\\overline{y} = \\frac{\\sum y_{i}}{n_{2}} = 30.7\\)\n모수의 MVUE: \\(\\widehat{\\theta} = \\overline{x} - \\overline{y} = - 2.8\\)\n보조 모수 \\(\\sigma_{1}^{2},\\sigma_{1}^{2}\\)\n모집단 분산 추정량 \\(\\widehat{\\sigma_{1}^{2}} = \\frac{(\\sum x_{i} - \\overline{x})^{2}}{n_{1} - 1} = s_{1}^{2}\\), \\(\\widehat{\\sigma_{2}} = \\frac{(\\sum y_{i} - \\overline{y})^{2}}{n_{2} - 1} = s_{2}^{2}\\)\n추정 분산 : \\(\\widehat{V(\\overline{x} - \\overline{y})} = \\frac{\\sigma_{1}^{2}}{n_{1}} + \\frac{\\sigma_{2}^{2}}{n_{2}}\\)\n등분산 가정 시(통합분산): \\(s_{p}^{2} = \\frac{(n_{1} - 1)s_{1}^{2} + (n_{2} - 1)s_{2}^{2}}{n_{1} + n_{2} - 2}\\)\n\n모평균 검정 시 모분산 동일한지 검정해야 하는 이유\n\n\n\n\n\n두 독립된 모집단의 평균이 동일하더라도, 분산이 서로 다를 경우 분산이 큰 모집단(예: 모집단 2)에서는 확률적으로 극단값, 즉 왼쪽 또는 오른쪽 꼬리 부분에서 더 많은 표본이 추출될 가능성이 있다. 이로 인해 표본 평균이 왜곡되어(극단값 쪽으로 치우쳐) 실제 모집단 평균을 정확히 반영하지 못할 수 있으며, 검정 통계량 계산에 사용되는 분산 또한 과대 추정될 위험이 있다. 따라서 이러한 분산 차이를 고려한 통계적 방법을 사용하는 것이 중요하다.\n등분산성(homoscedasticity): Levene’s test\n\\[H_{0}:\\sigma_{1}^{2} = \\sigma_{2}^{2}\\text{vs.}H_{1}:\\sigma_{1}^{2} \\neq \\sigma_{2}^{2}\\]\n검정통계량: \\(ts = \\frac{s_{1}^{2}}{s_{2}^{2}} \\sim F(df_{1} = n_{1} - 1,df_{2} = n_{2} - 1)\\)\nfrom scipy.stats import levene\n\n# 타이타닉 데이터에서 성별별 나이\ndf = titanic[['sex', 'age']].dropna()\nmale_age = df[df['sex'] == 'male']['age']\nfemale_age = df[df['sex'] == 'female']['age']\n\n# 등분산 검정\nstat, p = levene(male_age, female_age)\nprint(f\"Levene 검정 통계량: {stat:.3f}, p-값: {p:.4f}\")\nLevene 검정 통계량: 0.001, p-값: 0.9712\n등분산에 대한 귀무가설을 기각할 수 없으므로, 두 집단은 등분산성을 만족한다고 볼 수 있다.\n샘플링분포(대표본)\n\n이분산: \\(\\overline{x} - \\overline{y} \\sim N(\\mu_{1} - \\mu_{2},\\frac{s_{1}^{2}}{n_{1}} + \\frac{s_{2}^{2}}{n_{2}})\\)\n등분산: \\(\\overline{x} - \\overline{y} \\sim N(\\mu_{1} - \\mu_{2},s_{p}^{2}(\\frac{1}{n_{1}} + \\frac{1}{n_{2}}))\\)\n\n샘플링분포(소표본) 정규분포 가정\n\n이분산: \\(t = \\frac{{\\overline{X}}_{1} - {\\overline{X}}_{2}}{\\sqrt{\\frac{s_{1}^{2}}{n_{1}} + \\frac{s_{2}^{2}}{n_{2}}}} \\sim t(df^{*})\\) , Welch–Satterthwaite\\(df^{*} = \\frac{\\left( \\frac{s_{1}^{2}}{n_{1}} + \\frac{s_{2}^{2}}{n_{2}} \\right)^{2}}{\\frac{(s_{1}^{2}/n_{1})^{2}}{n_{1} - 1} + \\frac{(s_{2}^{2}/n_{2})^{2}}{n_{2} - 1}}\\)\n등분산: \\(\\frac{{\\overline{X}}_{1} - {\\overline{X}}_{2}}{s_{p} \\cdot \\sqrt{\\frac{1}{n_{1}} + \\frac{1}{n_{2}}}} \\sim t(df = n_{1} + n_{2} - 2)\\)\n\n검정통계량 및 \\(p\\)값 계산, 그리고 신뢰구간\n등분산 시\n\n검정통계량: \\(ts = {\\overline{X}}_{1} - {\\overline{X}}_{2}/s_{p} \\cdot \\sqrt{\\frac{1}{n_{1}} + \\frac{1}{n_{2}}}\\)\n유의확률: \\(p = P(t_{(n_{1} + n_{2} - 2)} &gt; |ts|)\\)\n\n이분산 시\n\n검정통계량: \\(ts = {\\overline{X}}_{1} - {\\overline{X}}_{2}/\\sqrt{\\frac{s_{1}^{2}}{n_{1}} + \\frac{s_{2}^{2}}{n_{2}}}\\)\n유의확률: \\(p = P(t_{(df^{*})} &gt; |ts|)\\)\n\n\\[(\\overline{x} - \\overline{y}) \\pm t(\\alpha/2,n + m - 2)\\sqrt{\\frac{s_{x}^{2}}{n} + \\frac{s_{y}^{2}}{m}}\\]\nimport seaborn as sns\nimport pandas as pd\nfrom scipy.stats import ttest_ind\n\n# 데이터 불러오기\ntitanic = sns.load_dataset(\"titanic\")\ndf = titanic[['sex', 'age']].dropna()\n\n# 성별에 따른 나이의 평균 및 표준편차\nsummary = df.groupby('sex')['age'].agg(['mean', 'std']).rename(columns={'mean': '평균 나이', 'std': '표준편차'})\n\n# 소수점 2자리로 출력\nsummary = summary.round(2)\nprint(summary)\n\n#승객 나이 평균, 표준편차\nprint(df['age'].mean(),df['age'].std())\n\n# 성별 그룹 분리\nmale_age = df[df['sex'] == 'male']['age']\nfemale_age = df[df['sex'] == 'female']['age']\n\n# 독립표본 t-검정 (등분산 가정 X)\nt_stat, p_value = ttest_ind(male_age, female_age, equal_var=False)\n\nprint(f\"t-통계량: {t_stat:.3f}\")\nprint(f\"p-값: {p_value:.4f}\")\n\n\n평균 나이 표준편차  female 27.92 14.11  male 30.73 14.68  29.69911764705882 14.526497332334044  t-통계량: 2.526  p-값: 0.0118\n\n\n결론\n\n\n\n\n\n\n\n\n\n\n여성 승객과 남성 승객 평균 나이는 차이가 있나?\n\n\n\n\n집단\n평균\n표준편차\n검정통계량\n유의확률\n\n\n여성\n27.9\n14.1\n2.53\n0.012\n\n\n남성\n30.7\n14.7\n\n\n\n총합\n29.7\n14.5\n\n\n\n귀무가설이 기각되어, 여성 승객의 평균 나이 27.9세와 남성 승객의 평균 나이 30.7세 사이에는 통계적으로 유의한 차이가 있는 것으로 나타났다.\n남자 승객 나이 이상치 제거 후 분석\n\n\n\n\n\n\n\n\n\n\n여성 승객과 남성 승객 평균 나이는 차이가 있나?\n\n\n\n\n집단\n평균\n표준편차\n검정통계량\n유의확률\n\n\n여성\n27.9\n14.1\n1.977\n0.049\n\n\n남성\n30.1\n13.8\n\n\n\n총합\n29.3\n13.96\n\n\n\nimport seaborn as sns\nimport pandas as pd\nfrom scipy.stats import ttest_ind\n# 1. 데이터 불러오기 및 결측 제거\ntitanic = sns.load_dataset(\"titanic\")\ndf = titanic[['sex', 'age']].dropna()\n# 2. 남성 나이 이상치 제거 (IQR 방식)\nmale = df[df['sex'] == 'male']['age']\nQ1 = male.quantile(0.25)\nQ3 = male.quantile(0.75)\nIQR = Q3 - Q1\nlower = Q1 - 1.5 * IQR\nupper = Q3 + 1.5 * IQR\nmale_clean = male[(male &gt;= lower) & (male &lt;= upper)]\n# 3. 여성 나이\nfemale = df[df['sex'] == 'female']['age']\n# 4. 통합 데이터 (여성 + 이상치 제거된 남성)\ncombined = pd.concat([female, male_clean], axis=0)\n# 5. 평균 및 표준편차 요약표\nsummary = pd.DataFrame({'평균 나이': [female.mean(),male_clean.mean(),combined.mean()],\n    '표준편차': [female.std(),male_clean.std(),combined.std()],\n    '표본 수': [len(female),len(male_clean),len(combined)]\n}, index=['female', 'male(이상치제외)', '통합']).round(2)\n\nprint(\"📊 성별 및 통합 평균 나이와 표준편차:\")\nprint(summary)\n\n# 6. 독립표본 t-검정 (등분산 가정 X)\nt_stat, p_val = ttest_ind(male_clean, female, equal_var=False)\n\nprint(\"\\n🧪 평균 차이 검정 (Welch's t-test):\")\nprint(f\"t-통계량: {t_stat:.3f}\")\nprint(f\"p-값: {p_val:.4f}\")\n📊 성별 및 통합 평균 나이와 표준편차:  평균 나이 표준편차 표본 수  female 27.92 14.11 261  male(이상치제외) 30.07 13.82 446  통합 29.28 13.96 707  🧪 평균 차이 검정 (Welch’s t-test):  t-통계량: 1.977  p-값: 0.0486\n\n\n3. 짝진 두 모집단 평균 차이\n개념\n짝진 집단의 평균 차이 검정은 동일하거나 연관된 실험 단위로부터 두 번 측정된 자료(예: 처리 전·후 측정값)에 대해, 두 처리 간 평균 차이에 통계적으로 유의한 차이가 있는지를 검정하는 절차이다. 이는 일반적인 독립표본 간 평균 비교와 달리, 측정값 간 짝이 지어져 있다는 점에서 통계적 종속성이 존재하며, 이로 인해 보다 정밀한 비교가 가능해진다.\n\n\n\n\n\n짝진 집단 평균 차이 검정은 동일한 실험 단위를 두 번 측정하거나, 서로 밀접한 관련이 있는 두 개체 간 비교 상황에서 주로 사용된다. 대표적인 예로는 동일한 피험자에게 어떤 처치를 가하기 전과 후에 측정한 결과를 비교하는 경우가 있으며, 이는 약물 복용 전후의 혈압 변화와 같은 사례로 설명할 수 있다.\n또한, 한 피험자의 좌우 혹은 상하 신체 부위처럼 구조적으로 연결된 부위 간 비교에서도 짝진 구조가 성립한다. 예를 들어, 같은 사람의 왼팔과 오른팔 근력을 비교하거나 좌·우 시력의 차이를 분석할 때가 이에 해당한다.\n짝지어진 두 표본 간의 평균 차이를 비교하는 것은, 그 차이값들의 평균이 0인지 검정하는 것과 완전히 동일하다.\n짝진 표본 평균 차이 검정은 본질적으로 단일 집단 평균이 0인지 검정하는 것과 같은 구조를 가진다. 예를 들어, 같은 환자의 치료 전과 후 혈압을 비교하거나, 유전적으로 일치하는 쌍둥이의 특정 특성을 비교하는 경우가 이에 해당한다.각 짝\\((x_{i},y_{i})\\)마다 두 값의 차이를 계산하면, 새로운 변수인 ”차이값\\(d_{i} = (x_{i} - y_{i})\\)“의 집합이 형성된다.\n이제 분석의 초점은 각 쌍의 차이값이 평균적으로 0인지 여부를 확인하는 데 있다. 즉, 원래의 두 변수를 비교하는 문제가 아니라, 차이값의 평균이 0인지에 대한 단일 집단 평균 검정(one-sample t-test) 문제가 되는 것이다. 이렇게 보면, 짝진 평균 차이 검정은 단지 두 집단 평균의 차이를 보는 것이 아니라, ’차이’라는 하나의 변수에 대한 평균 검정 문제로 변환된다는 점에서, 단일 집단의 평균이 어떤 값(보통 0)과 다른지를 검정하는 문제와 구조적으로 동일하다고 할 수 있다.\n\n\n\nchapter 5. 모분산 추론\n\n1. 일집단 모분산\n연구문제 및 통계적 가설\n분산은 통계학에서 관측값이 평균으로부터 얼마나 퍼져 있는지를 나타내는 지표로, 매우 기본적이면서도 핵심적인 개념이다. 이 분산이라는 개념은 분야에 따라 서로 다른 이름과 의미로 해석된다. 특히 금융 분야와 품질 관리 분야에서는 전혀 다른 철학과 시각에서 분산을 바라본다.\n투자에서의 수익률은 불확실하며, 이 불확실성의 크기를 정량화한 것이 바로 변동성(volatility)이다. 통계적으로 변동성은 수익률의 분산 또는 표준편차로 측정된다. 수익률의 분산이 클수록, 즉 수익이 평균에서 크게 벗어날 가능성이 클수록 투자자는 더 많은 위험을 감수하게 된다. 따라서 투자 분야에서는 분산이 클수록 리스크가 크다, 다시 말해 예측이 어렵고 손실 가능성도 크다고 해석한다. 이 경우, 분산은 부정적인 신호이며, 분산이 작을수록 더 안정적인 투자 자산으로 간주된다.\n반면, 제조나 공정 품질 관리 분야에서는 일관성과 정밀성이 품질의 핵심이다. 여기서도 분산은 중심값(목표 사양값)으로부터의 퍼짐 정도를 나타내며, 작은 분산은 제품이 일관되게 생산되고 있다는 신호로 받아들여진다. 따라서 품질 분야에서는 분산이 클수록 공정이 불안정하거나 품질이 나쁘다고 판단하며, “품질은 분산의 역수”라는 표현이 널리 사용된다. 즉, 분산이 작을수록 품질이 높다고 보는 것이다.\n\n귀무가설: \\(H_{0}:\\sigma^{2} = \\sigma_{0}^{2}\\)\n대립가설: \\(H_{1}:\\sigma^{2} \\neq \\sigma_{0}^{2}\\)\n\n시각화\n모분산(또는 표준편차)에 대한 검정은 데이터의 퍼짐 정도, 즉 흩어짐의 크기를 평가하는 데 목적이 있다. 하지만 분산이라는 수치는 단순히 하나의 숫자이기 때문에, 그 분산이 실제로 데이터에 어떻게 드러나 있는지를 직관적으로 이해하려면 시각화가 반드시 필요하다.\n이때 사용할 수 있는 시각화 도구는 히스토그램과 상자그림이다. 이 두 도구는 모평균 분석에서도 사용되는 시각화 방법이지만, 데이터의 분산이나 변동성 판단에도 매우 효과적인 도구다.\n히스토그램과 분산\n히스토그램은 데이터의 전체 분포 형태를 보여준다. 평균은 히스토그램의 중심이 어디에 있는지를 말해주지만, 분산은 그 히스토그램이 얼마나 좌우로 넓게 퍼져 있는지를 반영한다.\n\n히스토그램이 넓고 평평하게 퍼져 있다면 → 분산이 크다.\n히스토그램이 뾰족하고 좁게 몰려 있다면 → 분산이 작다.\n\n상자그림과 분산\n상자그림은 데이터의 중심, 범위, 사분위수, 이상치 등을 요약해서 보여주는 도구로, 특히 데이터의 퍼짐 정도를 시각적으로 파악하기에 매우 유용하다.\n\n사분위범위(제3사분위수 - 제1사분위수)가 넓을수록 → 분산이 클 가능성이 크다\n수염의 길이, 이상치의 분포도 분산의 크기와 관련된 정보를 제공한다\n\nMVUE 및 샘플링분포\n\nMVUE : \\(\\widehat{\\sigma^{2}} = s^{2} = \\frac{\\sum(x_{i} - \\overline{x})^{2}}{n - 1}\\)\n샘플링분포: \\((n - 1)\\frac{s^{2}}{\\sigma^{2}} \\sim \\chi^{2}(n - 1)\\), 모집단 정규분포 가정이 필요함\n\n검정통계량 및 \\(p\\)값 계산\n\n검정통계량: \\(ts = (n - 1)\\frac{s^{2}}{\\sigma_{0}^{2}}\\)\n유의확률: \\(p = P(\\chi_{(df = n - 1)}^{2} &gt; ts)\\)\n\n사례\n한 자산운용사는 단기 투자용 펀드의 월간 수익률이 안정적인 수준을 유지하고 있는지 점검하고자 한다. 이 펀드는 ”월간 수익률의 표준편차가 4% 이하여야 한다”는 내부 리스크 관리 기준을 갖고 있다.\n펀드운용팀은 최근 24개월 동안의 수익률 데이터(\\(n = 24,s^{2} = {0.05}^{2}\\))를 수집한 뒤, 실제 수익률 변동성이 이 기준보다 통계적으로 유의하게 크지 않은지 검정하고자 한다.\n\n귀무가설: \\(H_{0}:\\sigma^{2} = \\sigma_{0}^{2} = {0.04}^{2}\\)\n대립가설: \\(H_{1}:\\sigma^{2} \\neq \\sigma_{0}^{2} = {0.04}^{2}\\)\n검정통계량: \\(ts = (n - 1)\\frac{s^{2}}{\\sigma_{0}^{2}} = (24 - 1)\\frac{{0.05}^{2}}{{0.04}^{2}} \\approx 35.9\\)\n유의확률: \\(p = P(\\chi_{(df = n - 1)}^{2} &gt; 35.9) &lt; 0.001\\)\n\n결론: 귀무가설이 기각되어 펀드의 수익률 표준편차가 4%라는 내부 기준을 초과하는 것으로 나타나며, 이는 투자자 보호 및 리스크 관리 차원에서 제도적 개입이 필요함을 시사한다.\n\n\n2. 독립인 두 모집단 분산 차이\n개념\n서로 다른 두 개의 자산이 있을 때, 단순히 평균 수익률만 비교하는 것은 투자 전략 수립에 불충분할 수 있다. 특히, 두 자산의 수익률이 독립적으로 관측된 경우, 이들 사이의 분산 차이가 유의미한지를 검정함으로써, 어떤 자산이 더 불안정하고 위험한가를 평가할 수 있다.\n\n귀무가설: \\(H_{0}:\\sigma_{1}^{2} = \\sigma_{2}^{2}\\)\n대립가설: \\(H_{1}:\\sigma_{1}^{2} \\neq \\sigma_{2}^{2}\\)\n모수: \\(\\theta = \\frac{\\sigma_{1}^{2}}{\\sigma_{2}^{2}}\\)\n\nMVUE 및 샘플링분포\n\nMVUE : \\(\\widehat{\\theta} = \\frac{s_{1}^{2}}{s_{2}^{2}}\\)\n샘플링분포: \\(\\frac{s_{1}^{2}}{s_{2}^{2}} \\sim F(df_{1} = n_{1} - 1,df_{2} = n_{2} - 1)\\), 두 모집단 각각 정규분포 가정이 필요함\n\n검정통계량 및 \\(p\\)값 계산\n\n검정통계량: \\(ts = \\frac{max(s_{1}^{2},s_{2}^{2})}{min(s_{1}^{2},s_{2}^{2})}\\)\n유의확률: \\(p = P(F &gt; ts)\\)\n\n사례\n한 투자자는 기술주 중심의 자산 A와 에너지주 중심의 자산 B를 포트폴리오에 포함시키는 것을 고려하고 있다. 그는 두 자산의 월간 수익률을 지난 36개월 동안 수집하였으며, 각각의 수익률 평균은 유사하다고 판단하고 있다. 하지만, 수익률의 평균이 같더라도, 변동성(위험)의 차이는 투자 전략에 큰 영향을 미칠 수 있으므로, 두 자산의 수익률 분산(변동성)이 통계적으로 같은지 여부를 판단하고자 한다.\n가설 설정\n\n귀무가설: 자산 A와 B의 수익률 분산은 같다. \\(H_{0}:\\sigma_{A}^{2} = \\sigma_{B}^{2}\\)\n대립가설: 자산 A와 B의 수익률 분산은 다르다. \\(H_{0}:\\sigma_{A}^{2} \\neq \\sigma_{B}^{2}\\)\n\n자산 A: 표본 표준편차 = 0.058 → 분산 = 0.003364\n자산 B: 표본 표준편차 = 0.039 → 분산 = 0.001521\n\\[F = \\frac{0.003364}{0.001521} \\approx 2.211\\]\\[\\sim F_{(36 - 1),(26 - 1)}\\]\n유의확률 0.031이므로 귀무가설을 기각되어 두 자산의 투자 변동성은 통계적으로 유의하게 다르다고 판단한다."
  },
  {
    "objectID": "notes/intro_stat/crosstab.html",
    "href": "notes/intro_stat/crosstab.html",
    "title": "기초통계 4. 교차표 검정",
    "section": "",
    "text": "chapter 1. 교차표 개념\n\n1. 교차표란?\n\n(1) 개념\n변수 간의 관계를 분석할 때는 먼저 예측변수(X)와 목표변수(Y)가 어떤 속성을 가지는지 확인해야 한다. 변수 속성은 크게 범주형(categorical)과 측정형(numerical)으로 나눌 수 있으며, 두 변수의 조합에 따라 적절한 분석 방법이 달라진다.\nX와 Y가 모두 범주형 변수\n각 범주 조합별로 관측 빈도를 정리한 교차표를 작성한 뒤 카이제곱 검정과 같은 방법으로 두 변수 간의 관련성을 검토한다. 교차분석은 명목척도 변수 간의 관계를 파악하는 대표적인 접근이다. 다만 기대빈도가 지나치게 작은 셀이 많으면 피셔의 정확 검정과 같은 보정된 방법이 필요하다.\nX가 범주형이고 Y가 측정형인 경우\n각 범주 수준별로 Y의 평균을 비교하는 방식이 사용된다. 이는 분산분석(ANOVA)이나 t-검정을 통해 수행되며, 집단 간 평균 차이가 통계적으로 유의한지를 판단하는 데 적합하다. 예를 들어 교육 수준별 월평균 소득 차이를 분석하는 경우가 이에 해당한다. 분석 과정에서는 집단 간 분산이 동질하다는 가정을 점검해야 하며, 이를 위배할 경우 Welch ANOVA와 같은 대안 기법을 적용할 수 있다.\n반대로 X가 측정형이고 Y가 범주형\n선형회귀와 같은 전통적 연속형 분석은 적합하지 않다. 대신 범주형 반응변수를 예측할 수 있는 로지스틱 회귀분석이나 판별분석을 활용한다. 이러한 기법은 이항 또는 다항 범주 문제에서 널리 쓰이며, 예를 들어 시험 점수로 합격 여부를 예측하는 경우가 이에 해당한다.\nX와 Y가 모두 측정형 변수\n두 변수 간의 연속적인 변화 관계를 파악하는 상관분석이나 회귀분석을 사용한다. 상관분석은 두 변수의 선형 관계의 강도와 방향을 수치로 제시하고, 회귀분석은 한 변수를 다른 변수로부터 예측할 수 있는 수학적 모형을 제공한다. 다만 상관이 높더라도 반드시 인과관계를 의미하지 않으며, 회귀분석에서는 잔차의 정규성, 등분산성, 독립성 등을 반드시 검토해야 한다.\n\n\n\n\n\n\n\n\nY\nX\n범주형\n측정형\n\n\n범주형\n교차분석\n분산분석\n\n\n측정형\n로지스틱 회귀분석\n상관분석/회귀분석\n\n\n\n\n\n(2) 이차원 교차표 맛보기\n하나의 범주형 자료에 정리 방법으로 사용되는 것이 빈도표(혹은 다양한 차트)를 작성하는 것이다. 예를 들어 대학생 120에 대한 정치성향에 대한 조사 결과 다음 표를 얻었다.\n\n\n\n\n\n\n\n\n\n지지정당\n보수\n진보\n중도\n\n\n빈도(비율)\n40(33.3%)\n30(25%)\n50(41.7%)\n\n\n\n동일 학생 120명들에 대해 “AI 규제 법안” 지지여부를 물어 아래 결과를 얻었다.\n\n\n\n\n\n\n\n\nAI 규제 법안\n지지\n반대\n\n\n빈도(비율)\n80(66.7%)\n40(33.3%)\n\n\n\n두 범주형 변수간의 연관성을 알아보기 위해 한 범주형 변수에 대한 빈도표는 열로, 다른 범주형 변수에 대한 빈도표는 행으로 하여 교차표를 작성하게 되는데 이를 이차원 분할표이라 한다. 일반적으로 영향을 미친다고 생각되는 변수(~따라서, 예측변수 \\(X\\))를 행으로, 영향을 받는다고 생각되는 것을 변수(~차이가 있다, 목표변수 \\(Y\\))를 열로 하여 교차표를 작성하면 된다.\n위의 예제에서 정치성향에 따른 AI 규제 법안 지지 여부 차이가 있는지 알아보기 위하여 분할표를 작성하여 보자. 위의 두 표만으로는 분할표를 작성할 수 없다. 조사할 때 학생들의 (정치성향, K-방역 지지여부)를 조사하여 분할표를 작성해야 한다.\n\n\n\n\n\n\n\n\n\nAI 규제 법안\n정치성향\n지지\n반대\n합계\n\n\n보수\n30(75%)\n10(25%)\n40\n\n\n진보\n10(33.3%)\n20(66.7%)\n30\n\n\n중도\n40(80%)\n10(20%)\n50\n\n\n합계\n80\n40\n120\n\n\n\n괄호 안에 표시된 비율은 행 비율로 정치성향별 AI 규제 법안 지지여부의 차이를 알 수 있다. 보수와 진보는 AI 규제 법안지지도가 높고 진보는 반대 비율이 높음을 알 수 있다.\n\n\n\n2. 2x2 분할표\n\n(1) 모비율 차이 검정\n두 개의 범주형 변수가 각각 2개의 범주만을 가지는 경우, 이들의 관계는 2×2 교차표를 통해 요약할 수 있다. 이러한 형태의 자료에서는 전통적인 분할표 검정(카이제곱 독립성 검정)뿐만 아니라, 두 집단 간 비율 차이에 대한 검정으로도 분석을 대체할 수 있다.\n특히 변수의 수준이 각각 성공과 실패로 이분되는 경우, 이를 확률적으로 모델링할 수 있다. 어떤 사건이 발생할 확률을 \\(p\\)라고 하면, 그 사건이 발생하지 않을 확률은 \\(1 - p\\)이다. 이를 바탕으로 다음과 같이 2×2 교차표를 표현할 수 있다.\n\n\\(X = \\text{집단 1}\\): 성공 확률 =\\(\\pi_{1}\\), 실패 확률 =\\(1 - \\pi_{1}\\)\n\\(Y = \\text{집단 2}\\): 성공 확률 =\\(\\pi_{2}\\), 실패 확률 =\\(1 - \\pi_{2}\\)\n\n이러한 구조를 통해 두 집단 간 성공 확률의 차이 검정을 수행할 수 있다. 이 검정은 2×2 교차표의 독립성 검정과 본질적으로 유사한 통계적 근거를 가지며, 데이터의 해석을 보다 직관적으로 도와준다.\n\n\n\n\n\n\n\n\nY\nX\n범주1\n범주2\n\n\n범주1\n\\[\\pi_{1}\\]\n\\[1 - \\pi_{1}\\]\n\n\n범주2\n\\[\\pi_{2}\\]\n\\[1 - \\pi_{2}\\]\n\n\n\n\\(2 \\times 2\\) 분할표에서는 “두 범주형 변수가 서로 독립이다” &lt;=&gt; \\(\\pi_{1} = \\pi_{2}\\)와 동일하다. 방사능 물질에 노출여부에 따른 건강의 차이가 있는지 알아보기 위하여 주민 1,000명을 임의추출하여 방사능 물질에 노출된 400, 그렇지 않은 600명을 대상으로 건강 여부를 조사한 자료이다. [위키피디아 예제]\n\n\n\n\n\n\n\n\n질병여부\n노출여부\n질병\n건강\n\n\n노출\n20\n380\n\n\n미노출\n6\n594\n\n\n\n\n방사능 노출 집단 질병 발생률 추정치: \\({\\widehat{\\pi}}_{1} = p_{1} = \\frac{20}{400} = 0.05\\)\n방사능 미노출 집단 질병 발생률 추정치: \\({\\widehat{\\pi}}_{2} = p_{2} = \\frac{6}{600} = 0.01\\)\n\n\n\n(2) 비율 차이 검정 순서\n\n귀무가설: \\(\\pi_{1} = \\pi_{2}\\) 방사능 노출집단과 미노출 집단 질병 발생율은 동일하다.\n대립가설: \\(\\pi_{1} \\neq \\pi_{2}\\) 방사능 노출집단과 미노출 집단 질병 발생율은 다르다.\n검정통계량 : \\(TS = \\frac{{\\widehat{p}}_{1} - {\\widehat{p}}_{2} - 0}{s({\\widehat{p}}_{1} - {\\widehat{p}}_{2})} \\sim z\\),\n\n\\[s({\\widehat{p}}_{1} - {\\widehat{p}}_{2}) = \\sqrt{\\frac{p_{0}(1 - p_{0})}{n_{1}} + \\frac{p_{0}(1 - p_{0})}{n_{2}}}\\]\n\n통합 비율(pooled proportion) : \\(p_{0} = \\frac{x + y}{n_{1} + n_{2}} = 26/1000 = 0.026\\)\n\n#독립인 두 모집단 비율 차이 검정\nfrom statsmodels.stats.proportion import proportions_ztest\nimport numpy as np\ncount=[20,6]\nnobs=[400,600]\nnp.array(count)/np.array(nobs),proportions_ztest(count,nobs)\n(array([0.05, 0.01]), (3.894, 9.85912e-05))\n검정통계량은 3.89이며, 이에 대응하는 유의확률(p-value)은 0.001 미만으로 나타났다. 따라서 유의수준 0.05에서 귀무가설은 기각된다. 분석 결과, 방사능 노출 집단의 질병 발생률은 5%로, 미노출 집단의 발생률인 1%에 비해 통계적으로 유의미하게 높았다. 이는 방사능에 노출된 경우 질병에 걸릴 위험이 유의하게 증가함을 시사한다.\n\n\n(3) 상대위험도 relative risk와 승산비 odds ratio\n역학 연구에서 질병의 발생과 특정 위험요인 간의 관련성을 평가하기 위해 사용되는 지표로는 상대위험도와 승산비가 있다. 이 중 상대위험도는 코호트 연구와 같이 연구자가 시간의 흐름에 따라 질병의 발생을 추적할 수 있는 전향적 연구에서 사용되는 지표이다. 이는 위험요인에 노출된 집단과 그렇지 않은 집단 각각에 대해 질병이 발생한 비율을 비교할 수 있을 때 정의되며, 실제로 발생률을 비교하는 구조가 요구된다.\n반면, 사례-대조군 연구는 일반적으로 후향적 연구로 분류되며, 이미 질병이 발생한 사례군과 질병이 발생하지 않은 대조군을 사후적으로 모집한 후, 과거의 위험요인 노출 여부를 조사하는 방식이다. 이 연구 설계에서는 실험군과 대조군의 비율이 연구자가 임의로 설정한 표본 구성에 따라 정해지기 때문에, 각 집단의 실제 발생률을 직접 추정할 수 없다. 따라서 상대위험도는 정의될 수 없으며, 대신 승산비를 통해 위험요인과 질병 간의 연관성을 추정한다.\n예를 들어, 암 발생과 흡연 간의 관련성을 사례-대조군 연구를 통해 분석하고자 할 때, 이미 암이 발생한 집단과 암이 발생하지 않은 집단을 모집하여 이들의 과거 흡연 여부를 조사하게 된다. 그러나 이러한 연구에서는 흡연자와 비흡연자의 전체 집단 규모를 명확히 알 수 없으므로, 질병 발생률의 직접 비교가 불가능하다. 이와 같은 상황에서 유일하게 적용 가능한 측도는 흡연자의 질병 발생 승산 대비 비흡연자의 질병 발생 승산, 즉 승산비이다.\n상대위험도 정의\n두 집단 간 비율의 차이가 동일하더라도, 그 해석은 비율의 절대적 크기에 따라 달라질 수 있다. 일반적으로 비율이 0.5에 가까운 경우보다는 0 또는 1에 가까운 극단적인 경우에서 작은 차이도 더 큰 의미를 갖는다. 예를 들어, 두 집단 간 비율 차이가 0.0077이라고 할 때, (0.0171, 0.0094)라는 조합은 매우 낮은 발생률 간의 차이이며, 상대적으로 의미 있는 차이일 수 있다. 반면, (0.5000, 0.5077)은 전체 발생률이 높아 두 비율 간 차이의 상대적 중요성이 크지 않다.\n이처럼 비율의 절대 차이가 아닌, 비율 간의 상대적 크기를 비교하는 개념이 바로 상대위험도이다.\n\\(\\text{Relative Risk (RR)} = \\frac{P_{1}}{P_{2}}\\), 여기서\n\n\\(P_{1}\\)은 위험요인에 노출된 집단에서의 사건 발생률,\n\\(P_{2}\\)는 위험요인에 노출되지 않은 집단에서의 사건 발생률이다.\n\n상대위험도는 다음과 같이 해석할 수 있다:\n\n\\(RR = 1\\): 두 집단 간 사건 발생률이 동일함을 의미한다.\n\\(RR &gt; 1\\): 노출 집단에서 사건 발생 위험이 더 큼.\n\\(RR &lt; 1\\): 노출 집단에서 사건 발생 위험이 더 낮음(보호 효과 가능).\n\n방사능 노출여부와 질병 발병 여부 실험\n상대 위험도 추정치 : \\(\\frac{{\\widehat{\\pi}}_{1}}{{\\widehat{\\pi}}_{2}} = \\frac{0.05}{0.01} = 5\\), 방사능 노출집단의 질병 발생율은 미노출 집단보다 5배 높다.\n오즈\n오즈란 어떤 사건이 성공할 확률을 실패할 확률로 나눈 비율로 정의된다. 즉, 성공 확률을 \\(\\pi\\)라고 할 때, 오즈는 다음과 같이 정의된다.\n\\[\\text{Odds} = \\frac{\\pi}{1 - \\pi}\\]\n이 개념은 스포츠 경기(예: 축구, 농구 등)에서의 배당률 산정 등에서 자주 활용되며, 성공과 실패 간의 상대적 가능성을 정량적으로 비교하는 지표로 사용된다. 예를 들어, 한국이 폴란드와의 축구 경기에서 이길 확률이 \\(\\pi = 0.1\\)이라면, 한국의 오즈는 \\(\\frac{0.1}{1 - 0.1} = \\frac{0.1}{0.9} = \\frac{1}{9}\\)이다. 한국이 한 번 이기기 위해서는 평균적으로 9번 질 것이라는 의미로 해석할 수 있다. 반면, 폴란드가 이길 확률은 0.9이므로, 그 오즈는 \\(\\frac{0.1}{1 - 0.1} = \\frac{0.1}{0.9} \\approx \\frac{1}{9}\\)이다. 즉, 폴란드는 한국보다 9배 더 자주 이길 것으로 기대된다.\n한편, 스포츠 베팅에서 말하는 배당률은 도박사가 설정하는 금전적 보상 비율이며, 보통 오즈를 기반으로 하지만 여기에 도박사 마진이 포함된다. 만약 오즈 그대로 1:9를 배당률로 설정한다면, 한국에 1달러를 걸면 9달러를 돌려받게 된다. 그러나 실제로는 도박사의 이윤을 위해 더 낮은 배당률이 제공된다. 예를 들어, 한국 승리 시 배당을 7.5배로 책정하는 방식이다.\n오즈비의 정의와 해석\n오즈비는 두 집단 간 사건 발생의 상대적인 가능성을 비교하는 지표로, 각 집단의 오즈를 비율로 나타낸 값이다. 특정 집단의 사건 발생 확률을 \\(\\pi_{1}\\), 비교 집단의 사건 발생 확률을 \\(\\pi_{2}\\)라고 할 때, 오즈비는 다음과 같이 정의된다.\n\n\\(\\text{OR} = \\frac{\\frac{\\pi_{1}}{1 - \\pi_{1}}}{\\frac{\\pi_{2}}{1 - \\pi_{2}}} = \\frac{\\pi_{1}(1 - \\pi_{2})}{\\pi_{2}(1 - \\pi_{1})}\\)\n\\(\\text{OR} = 1\\): 두 집단의 오즈가 동일함을 의미하며, 두 변수 간 독립성을 시사한다.\n\\(\\text{OR} &gt; 1\\): 분자 집단(비교 대상 집단)에 비해 분모 집단보다 사건 발생의 가능성이 높다. 즉, 해당 노출이 사건(또는 질병) 발생 위험을 증가시키는 경향이 있다.\n\\(\\text{OR} &lt; 1\\): 분자 집단의 사건 발생 가능성이 더 낮으며, 해당 노출이 사건 발생을 줄이는 보호 효과(protective effect)를 가질 수 있음을 의미한다.\n\n오즈비는 두 범주형 변수 간 연관성을 수치화하는 데 널리 사용되며, 특히 사례–대조군 연구(case-control study)나 후향적 연구(retrospective study)에서 유용하다. 한 가지 중요한 성질은, 2×2 분할표에서 행과 열을 서로 바꾸더라도 오즈비 값은 변하지 않는다는 점이다. 이는 오즈비가 변수 간의 비대칭적 관계를 측정하는 것이 아니라, 쌍방향적인 연관성의 정도를 나타내는 지표임을 보여준다..\n건강집단과 질병집단 오즈비\n\\(oddsratio = \\frac{(20/6)}{(380/594)} = 5.2\\), 분석 결과, 방사능 노출은 질병 발생과 유의한 양의 연관성을 보였으며, 노출군의 질병 오즈는 비노출군에 비해 약 5.2배 높았다. 여기서 오즈(odds)란, 특정 집단에서 질병이 발생할 가능성을 해당 집단에서 질병이 발생하지 않을 가능성과 비교한 비율을 의미한다.\n따라서 오즈비 5.2는, 방사능에 노출된 사람의 경우 질병에 걸릴 가능성이 걸리지 않을 가능성 대비 비노출군보다 약 5.2배 더 크다는 것을 뜻한다. 다시 말해, 방사능 노출은 질병 위험을 크게 증가시키는 요인임을 시사한다.\n오즈비 추론\n두 반응 변수가 서로 독립인지 (연관성 검정) 어떻게 검정할 수 있을까? 오즈비(\\(OR\\))의 값은 0과 \\(\\infty\\)을 가지고 독립인 경우는 1이다. 그러므로 좌우 비대칭 형태의 분포를 가지므로 \\(ln(OR)\\)생각해보자. 두 변수가 독립이면 \\(ln(1) = 0\\)이고 (한 개념에서) 좌우 대칭의 형태를 갖는다. (예: \\(ln(4) = 1.39,ln(1/4) = - 1.39\\))\n표본의 크기가 커지면 \\(\\widehat{ln(OR)} \\sim N(ln(OR),\\frac{1}{n_{11}} + \\frac{1}{n_{12}} + \\frac{1}{n_{21}} + \\frac{1}{n_{22}})\\)이므로 신뢰구간은 \\(\\widehat{ln(OR)} \\pm z_{1 - \\alpha/2}\\sqrt{\\frac{1}{n_{11}} + \\frac{1}{n_{12}} + \\frac{1}{n_{21}} + \\frac{1}{n_{22}}}\\)이다.\n#오즈비 신뢰구간\nimport scipy.stats as st\nhat_or=(20/6)/(380/594)\nse_or=(1/20+1/6+1/380+1/594)**0.5\nz=st.norm.ppf(0.975,0,1)\nhat_or-z*se_or,hat_or+z*se_or\n(4.289172810142199, 6.131879821436749)\n방사능 노출은 질병 발생과 유의한 양의 연관성이 있으며, 노출군의 질병 오즈는 비노출군에 비해 약 5.2배 유의적으로 높다.\n\n\n\n\nchapter 2. 교차표 검정\n\n1. 개념\n두 개의 범주형 변수를 각각 X와 Y로 표시하고 각각 R, C 수준을 갖고 있다고 하자. X를 행으로 Y를 열로 하여 분할표를 만들면 \\(R \\times C\\)개의 결합 조건이 존재한다. 이를 \\(R \\times C\\) 교차표라 한다.\n\n\n\n\n\n\n\n\n\n\n\nY\nX\n범주1\n범주2\n…\n범주C\n합계\n\n\n범주1\n\\[\\pi_{11}\\]\n\\[\\pi_{12}\\]\n…\n\\[\\pi_{1C}\\]\n\\[\\pi_{1 +}\\]\n\n\n범주2\n\\[\\pi_{21}\\]\n\\[\\pi_{22}\\]\n…\n\\[\\pi_{2C}\\]\n\\[\\pi_{2 +}\\]\n\n\n…\n\n\n…\n\n\n\n\n범주R\n\\[\\pi_{R1}\\]\n\\[\\pi_{R2}\\]\n…\n\\[\\pi_{RC}\\]\n\\[\\pi_{R +}\\]\n\n\n합계\n\\[\\pi_{+ 1}\\]\n\\[\\pi_{+ 2}\\]\n…\n\\[\\pi_{+ C}\\]\n\\[\\pi_{+ +}\\]\n\n\n\n\\(\\pi_{ij}\\) : \\(P(X = i,Y = j)\\) X는 범주 \\(i\\), Y는 범주 \\(j\\)에 속할 확률변수 \\((X,Y)\\) 결합확률분포함수\n\\(\\pi_{i +}\\) : \\(P(X = i)\\) 확률변수 \\(X\\)의 주변확률분포함수\n\\(\\pi_{+ j}\\) : \\(P(Y = ij)\\) 확률변수 \\(Y\\)의 주변확률분포함수\n\\[\\pi_{+ +} = 1\\]\n\\(O_{ij}\\): 셀 \\((i,j)\\) 관측빈도\n\\(O_{i +}\\): 행 \\(i\\) 주변 합 관측빈도, \\(O_{j +}\\): 열 \\(j\\) 주변 합 관측빈도\n\n\n2. 독립성 검정\n범주형 자료의 관계를 분석할 때, 두 범주형 변수 간의 연관성이 존재하는지를 평가하기 위한 대표적인 방법이 독립성 검정이다. 독립성 검정의 목적은, 두 범주형 변수 간의 분포가 서로 독립적인지를 검토하는 데 있다. 즉, 한 변수의 수준이 다른 변수의 분포에 영향을 주는지를 판단한다.\n통계적 가설\n\n귀무가설: 두 변수는 서로독립이다. \\(\\pi_{ij} = \\pi_{i +}\\pi_{+ j}forall(i,j)\\)\n대랍가설: 두 변수는 서로 독립이 아니다 (즉, 관련이 있다).\n\n독립이라는 것은, 한 변수의 특정 범주에 속하는 비율이 다른 변수의 범주에 관계없이 일정하다는 의미다. 이를 검정하기 위해, 각 셀의 관측도수와 귀무가설 하에서 기대되는 도수를 비교한다. 기대도수는 귀무가설이 옳다는 가정 하에서 계산되는 셀의 빈도이다.\n검정통계량\n\\(ts = \\sum_{i,j}\\frac{(O_{ij} - E_{ij})^{2}}{E_{ij}} \\sim \\chi^{2}((R - 1) \\times (C - 1))\\), 여기서 \\(E_{ij} = \\frac{O_{i +}O_{+ j}}{O_{+ +}}\\)이다.\n사례분석\n성별(남, 여)에 따른 선호하는 커피 종류(아메리카노, 라떼, 에스프레소) 간의 관계를 분석하고자 한다.\n\n\n\n\n\n\n\n\n\n\n\n아메리카노\n라떼\n에스프레소\n합계\n\n\n남성\n30\n25\n15\n70\n\n\n여성\n20\n30\n30\n80\n\n\n합계\n50\n55\n45\n150\n\n\n\n귀무가설: 성별과 커피 선호는 독립이다 (즉, 성별에 따라 커피 선호가 달라지지 않는다)\nimport pandas as pd\nfrom scipy.stats import chi2_contingency\n# 교차표 데이터\ndata = [[30, 25, 15],   # 남성\n        [20, 30, 30]]   # 여성\ncolumns = ['Americano', 'Latte', 'Espresso']\nindex = ['Male', 'Female']\n\ndf = pd.DataFrame(data, index=index, columns=columns)\n\n# 카이제곱 독립성 검정\nchi2, p, dof, expected = chi2_contingency(df)\n\n# 결과 출력\nprint(\"✅ 카이제곱 통계량:\", round(chi2, 3))\nprint(\"✅ p-value:\", round(p, 4))\n\n# 행 퍼센트 계산\nrow_percent = df.div(df.sum(axis=1), axis=0) * 100\nrow_percent_rounded = row_percent.round(2)\nprint(\"\\n📊 행 퍼센트 (%):\")\nprint(row_percent_rounded)\n✅ 카이제곱 통계량: 6.818  ✅ p-value: 0.0331  📊 행 퍼센트 (%):  Americano Latte Espresso  Male 42.86 35.71 21.43  Female 25.00 37.50 37.50\n유의확률(0.0247)이 0.05보다 작으므로 귀무가설은 기각되어 성별과 커피 선호 간에 통계적으로 유의한 연관성이 존재함을 의미한다. 남성은 아메리카노(42.86%)를 가장 많이 선호하는 반면, 여성은 에스프레소(37.5%)와 라떼(37.5%)에 대한 선호가 상대적으로 높다.\n\n\n3. 동질성 검정\n동질성 검정은 두 개 이상의 모집단으로부터 각각 표본을 추출하였을 때, 이 모집단들이 하나의 공통된 분포(또는 비율 구조)를 가지고 있는지를 검정하는 절차이다. 즉, 여러 집단이 같은 비율 분포를 공유하고 있는지, 다시 말해 질적 특성의 분포가 동일한지를 평가하는 검정이다.\n1900 Karl Pearson에 제안한 방법으로 multinomial(다항) distribution의 확률이 귀무가설에서 설정한 값과 동일한지를 검정한다. 즉 확률변수 \\(X\\)의 주변확률분포함수가 모두 동일한지 검정한다.\n\n귀무가설 : \\(\\pi_{ij} = \\pi_{kj}\\), 모든 \\(j = 1,2,...,C\\) 그리고 \\(i \\neq k\\)\n대립가설 : 귀무가설은 사실이 아니다.\n\nPearson Chi-square Statistic \\(\\chi ^2\\) -검정 통계량\n\\[ts = \\sum_{i,j}\\frac{(O_{ij} - E_{ij})^{2}}{E_{ij}} \\sim \\chi^{2}((R - 1) \\times (C - 1))\\]\n사례분석\n한 음료회사가 지역별(서울, 부산) 소비자들을 대상으로, 선호하는 음료 종류(탄산음료, 주스, 생수)에 대한 조사를 실시하였다. 각 지역에서 100명씩 표본을 추출하여 선호도를 조사한 결과는 다음과 같다.\n\n\n\n\n\n\n\n\n\n\n지역\n탄산음료\n주스\n생수\n합계\n\n\n서울\n40\n35\n25\n100\n\n\n부산\n30\n25\n45\n100\n\n\n합계\n70\n60\n70\n200\n\n\n\n귀무가설: 두 지역(서울, 부산)의 음료 선호 분포는 동일하다 (즉, 모집단의 분포가 동질적이다)\n# 독립성 검정 코드와 동일함\n✅ 카이제곱 통계량: 8.81  ✅ p-value: 0.0122  📊 행 퍼센트 (%):  Soda Juice Water  Seoul 40.0 35.0 25.0  Busan 30.0 25.0 45.0\n유의확률이 0.012이므로, 귀무가설은 기각되어 서울과 부산의 음료 선호 분포는 동일하지 않으며, 지역 간 분포의 차이가 통계적으로 유의미하다고 볼 수 있다. 서울은 탄산음료와 주스 선호도가 높고, 부산은 생수 선호 비율이 상대적으로 높다.\n\n\n4. Fisher’s 정확검정 exact test\nFisher’s exact test는 1934년, 피셔가 유전자 분리비 연구 및 실험설계의 정교화를 위해 고안한 검정이다. 이 검정은 당시의 계산 자원 한계에도 불구하고, ”작은 표본에서의 정확한 결론”을 도출할 수 있는 기법으로 주목받았다. 특히, 정확한 확률을 기반으로 한 논리적 검정 방식을 도입한 점에서 이후의 이항 검정이나 로지스틱 회귀 등의 기반이 되었다. 일반적인 \\(R \\times C\\)교차표에서도 이론적으로 수행 가능합니다. 하지만 실무에서는 계산량이 급격히 증가하므로 2×2 표 이외에는 거의 사용되지 않습니다\nFisher가 이 검정을 소개하면서 사용한 일화로 유명한 것이 바로 ”홍차 마시는 여인” 실험이다. 한 여성이 ”우유를 먼저 넣은 차”와 ”차를 먼저 넣은 우유”를 맛만 보고 구분할 수 있다고 주장하자, Fisher는 8잔 중 4잔씩 두 조건을 섞어 무작위로 제공하고, 이 여성이 구분할 확률을 순열 조합으로 계산하여 검정했다. 이 실험이 바로 Fisher의 정확 검정의 실제 적용 예로 인용된다.\n2X2 교차표\n\n\n\n\n\n\n\n\n\n\n성공\n실패\n합계\n\n\nA 그룹\na\nb\na+b\n\n\nB 그룹\nc\nd\nc+d\n\n\n합계\na+c\nb+d\nn\n\n\n\n검정통계량\n주어진 행과 열의 합이 고정된 상황에서 셀 a에 특정한 값이 나올 확률은 다음과 같이 초기하분포를 따른다. 이러한 확률은 각 가능한 \\(a\\) 값에 대해 계산되며, 관측값과 같거나 더 극단적인 경우의 확률을 모두 합산하여 유의확률을 산출한다.\n\\[P(a) = \\frac{\\binom{a + b}{a} \\cdot \\binom{c + d}{c}}{\\binom{n}{a + c}}\\]\n피셔 사례분석\n\n귀무가설: 여성의 선택은 무작위이며, 실제 구분 능력이 없다\n대립가설: 여성은 차/우유 순서를 식별할 수 있다\n\n\n\n\n\n\n\n\n\nFisher 사례\n성공\n실패\n합계\n\n\n차를 먼저라 판단\n3\n1\n4\n\n\n우유를 먼저라 판단\n1\n3\n4\n\n\n합계\n4\n4\n8\n\n\n\n유의확률, \\(P(a \\geq 3|a \\sim HG(N = 8,K = 4,n = 4))\\)0.2429이므로 귀무가설을 기각할 수 없으므로 이 결과만으로는 여성이 정확하게 구분한다고 보기는 어렵다.하지만 만약 4잔 모두 정답(4 vs. 0)이었다면, p-value는 약 0.014로, 유의미한 능력이 있다고 결론내릴 수 있습니다.\n\nfrom scipy.stats import fisher_exact\n\n# 2x2 교차표 (차례대로: 정답, 오답)\ntable = [[3, 1],  # 여성 선택 = 차 먼저 (맞춤 3, 틀림 1)\n         [1, 3]]  # 여성 선택 = 우유 먼저 (틀림 1, 맞춤 3)\n\n# 양측 검정\noddsratio, p_value = fisher_exact(table, alternative='two-sided')\n\nprint(\"오즈비:\", round(oddsratio, 2))\nprint(\"p-value:\", round(p_value, 4)/2)\n오즈비: 9.0  p-value: 0.2429\n여성은 차와 우유 중 어느 것을 먼저 넣었는지 구별할 능력은 없지만(귀무가설 채택), 오즈비가 9라는 것은 이는 그녀가 실제로 차와 우유의 순서를 일정 부분 구분할 수 있었을 가능성을 시사하는 정량적 지표이다. 여성이 차를 먼저 넣었다고 판단한 잔에서 실제로 차를 먼저 넣었을 오즈가, 우유를 먼저 넣었다고 판단한 잔에서 실제로 우유를 먼저 넣었을 오즈보다 9배 더 높다는 것을 의미한다."
  },
  {
    "objectID": "notes/intro_stat/index.html",
    "href": "notes/intro_stat/index.html",
    "title": "기초통계",
    "section": "",
    "text": "기초통계는 데이터를 요약·시각화·해석하는 통계 분석의 출발점이다.\n이 섹션에서는 기술통계, 분포의 특성, 변수 간 관계를 중심으로\n이후의 추론통계와 모형 분석을 이해하기 위한 기초 개념을 다룬다.\n왼쪽 메뉴에서 주제를 선택하시오."
  },
  {
    "objectID": "notes/intro_stat/correlation.html",
    "href": "notes/intro_stat/correlation.html",
    "title": "기초통계 7. 상관분석",
    "section": "",
    "text": "chapter 1. 상관계수 기초\n\n1. 상관계수 개념\n상관계수는 두 개의 측정형 변수(순서형 변수 포함) 간 직선적 관계의 강도와 방향을 수치적으로 나타내는 척도이다. 이 값은 –1에서 1 사이의 범위를 가지며, 1에 가까울수록 두 변수 사이에 강한 양의 선형 관계가 존재함을 의미한다. 반대로 –1에 가까운 값은 강한 음의 선형 관계를 나타내며, 0에 가까울수록 선형 관계가 거의 없다는 것을 뜻한다.\n상관계수는 공분산과 유사하게 두 변수 간의 공변동을 바탕으로 계산되지만, 공분산과 달리 단위를 가지지 않는 무차원 지표이다. 이는 변수들의 크기나 단위에 영향을 받지 않기 때문에 서로 다른 척도의 변수들 간에도 관계의 강도를 비교하는 데 유리하다.\n두 변수 간에 직선적 관계가 유의하다는 것은 한 변수가 증가할 때 다른 변수도 일정한 비율로 증가하거나 감소한다는 의미이며, 이는 두 변수가 동일한 개체의 특성을 일정 부분 공유하고 있다는 점을 시사한다. 다시 말해, 두 변수는 대상이 가진 정보 중 일부를 중복하여 설명하고 있으며, 이로 인해 하나의 변수만으로도 다른 변수에 대한 예측 가능성이 존재하게 된다. 상관계수는 이러한 변수 간의 정보 중첩도를 간단하고 직관적인 방식으로 요약해주는 지표로서, 탐색적 자료 분석이나 모형 구축의 기초 단계에서 유용하게 활용된다.\n다음 그림은 두 변수 간의 상관계수를 시각적으로 나타낸 예이다.\n\n\n\n\n\n(a)는 상관계수가 양수인 경우로, X가 증가함에 따라 Y도 함께 증가하는 양의 선형 관계를 보인다. 점들이 제1사분면과 제3사분면에 주로 분포하며, 상관계수 r은 0보다 크다.\n(b)는 상관계수가 음수인 경우로, X가 증가하면 Y가 감소하는 음의 선형 관계를 보인다. 점들이 좌상단에서 우하단으로 뚜렷하게 분포하며, r은 0보다 작다.\n(c)는 상관계수가 0에 가까운 경우로, X와 Y 사이에 뚜렷한 선형 관계가 존재하지 않는다. 점들이 원형에 가깝게 고르게 흩어져 있어, 선형 경향이 없다.\n(d) 역시 상관계수가 0에 가까운 경우이지만, 곡선 형태의 관계가 존재한다. X와 Y 사이에 비선형적 패턴이 있으나, 직선 관계를 전제로 한 상관계수 값은 거의 0에 가깝다.\n\n\n2. 인과관계와 상관관계\n상관관계와 인과관계는 두 변수 간의 관련성을 설명할 때 자주 사용되지만, 본질적으로 서로 다른 개념이다. 상관관계는 두 변수의 값이 함께 변하는 경향성을 수치적으로 나타내며, 이러한 관계는 양의 방향이거나 음의 방향일 수 있다. 그러나 상관관계가 존재한다고 해서, 반드시 한 변수가 다른 변수의 변화에 영향을 주는 원인이라고 단정할 수는 없다.\n두 변수 간의 상관계수가 높게 나타나는 경우에도 그 관계는 단순한 우연일 수 있으며, 때로는 제3의 숨은 요인(잠재변수)이 두 변수 모두에 영향을 주어 상관관계를 만들어낼 수도 있다. 예를 들어, 아이스크림 판매량과 익사 사고 건수 간에는 여름철에 모두 증가하는 경향이 있어 양의 상관관계가 나타날 수 있으나, 이 둘 사이에 인과적 관계가 있는 것은 아니다. 여기서 실제 영향 요인은 계절, 즉 기온이라는 제3의 변수이다.\n따라서 상관관계는 변수들 간의 관련성을 탐색하거나 예비적 가설을 세우는 데 유용한 도구이지만, 인과관계를 입증하기 위한 충분조건은 아니다. 변수 간의 인과성을 확인하기 위해서는 보다 정교한 연구 설계, 예를 들어 실험 연구나 종단적 자료 분석, 또는 통제 변수와 도구 변수 등을 활용한 통계적 모형이 필요하다. 결론적으로, 상관관계를 인과관계로 잘못 해석하는 오류를 방지하기 위해서는 분석 목적과 방법론에 대한 신중한 접근이 요구된다.\n상관관계(Correlation) \\(X \\leftrightarrow Y\\)\n상관관계는 두 변수 간의 관련성을 나타냅니다. 어떤 변수의 값이 변할 때 다른 변수의 값도 함께 변하는 정도를 나타낸다.\n상관관계는 피어슨 상관계수, 스피어만 상관계수 등과 같은 통계적 방법을 사용하여 측정된다.\n상관관계는 두 변수 간의 연관성을 보여줄 뿐, 한 변수가 다른 변수를 원인으로 하여 변화시키는지에 대한 정보를 제공하지 않는다.\n인과관계(Causation) \\(X \\rightarrow Y\\)\n인과관계는 한 변수가 다른 변수에 영향을 주는 관계를 나타내며 한 변수의 변화가 다른 변수의 변화를 일으키는 관계를 설명한다.\n인과관계를 확인하기 위해서는 실험적인 접근이 필요하다. 예를 들어, 실험 그룹과 대조 그룹을 설정하여 한 변수를 조작하고 그 결과를 다른 변수에 대비하여 관찰한다. 인과관계를 확인하기 위해서는 상관관계 이외의 추가적인 통계적 또는 실험적인 증거가 필요하다.\n인과관계와 상관관계 예제\n여름철에 아이스크림의 판매량이 증가하고 생수 판매량도 함께 증가한다. 이 경우 아이스크림 판매량과 마실 물의 양 사이에는 양의 상관관계가 있을 것으로 예상되지만 둘 간에는 인과관계가 존재한다고 단정할 수 없다.\n다른 예를 들어보자. A도시에서 고학력자 비율과 맥주 소비량의 상관계수 높다고 하여 고학력자일수록 맥주를 많이 마신다는 인과 관계 설명은 적절하지 않다.\n\n\n3. 산점도\n두 개의 측정형 변수 데이터 \\((x_{i},y_{i})\\)를 2차원 좌표평면에 나타내면, 두 변수 간의 함수적 관계를 시각적으로 파악할 수 있다. X축에는 결정 요인으로서 예측변수, 독립변수, 또는 설명변수라 불리는 변수를 배치한다. Y축에는 결과를 나타내는 목표변수, 종속변수, 또는 반응변수를 배치한다. 이러한 산점도는 두 변수 간의 인과관계를 직관적으로 관찰할 수 있는 시각적 도구로 활용된다.\n\n\n\n\n\n\n(1) 진단내용: 함수관계\n산점도는 두 변수 간의 함수 관계를 파악하는 데 사용된다. 일반적으로 해석이 용이하고 상관계수의 기준이 되는 직선적 관계의 정도를 확인하는 데 활용된다. 이러한 함수 관계의 해석은 수집된 데이터의 범위 내에서만 타당하다. 관측 범위를 벗어난 구간에서는 두 변수 간의 관계가 기존 데이터에서 나타난 패턴과 다를 수 있으므로, 범위를 넘어선 해석에는 주의가 필요하다.\n\n\n(2) 진단내용 이상치\n이상치의 정의와 특성\n이상치(outlier)란 데이터 집합에서 다른 관측치들과 비교하여 현저히 동떨어진 값을 뜻한다. 다시 말해, 관측된 데이터 분포의 범위를 크게 벗어나 극도로 작거나 큰 값이 이상치이다 . 이러한 값은 보통 데이터 입력 실수, 측정 오류, 또는 실제로 드물게 발생하는 예외적 상황이나 외부 요인에 의해 생길 수 있다 . 이상치는 그 특이성 때문에 분석가의 주의를 끌며, 데이터의 전반적 패턴과 다르기 때문에 특별한 관리가 필요하다. 실제로 이상치는 데이터 분석이나 모델링 결과에 큰 영향을 미칠 수 있으므로 적절한 처리 없이는 의사결정에 오류를 초래할 수 있다 . 특히 평균, 표준편차와 같은 통계량이나 상관계수 등 대부분의 모수적 통계 측도들은 이상치에 민감하여, 몇 개의 이상치만으로도 통계 결과가 크게 왜곡될 수 있다 . 따라서 통계 분석에서는 이상치를 반드시 확인하고 진단하는 절차가 요구된다.\n두 변수 간 직선 관계에서 이상치의 중요성\n두 변수 사이의 선형 관계(linear relationship)를 분석할 때 이상치는 특히 중요한 역할을 한다. 산점도 상에서 대부분의 점들이 어느 직선 부근에 모여있더라도, 하나의 극단적인 이상치가 있으면 회귀선의 기울기와 절편, 상관계수 등 추정치가 크게 변할 수 있다. 예를 들어, 아래 산점도는 한 이상치 집단(빨간색 동그라미 친 부분)의 존재가 상관계수에 미치는 영향을 보여준다. 이 경우 Pearson의 피어슨 상관계수는 0.67에 불과하지만, 이상치의 순위를 무시하는 Spearman의 스피어만 순위상관은 0.84로 유지된다.\n이는 극단적인 관측점들이 들어옴으로써 두 변수 간 선형적 상관이 약화된 반면, 순서 관계에 기반한 상관은 비교적 덜 영향을 받는다는 것을 시사한다. 일반적으로 이상치가 직선 관계에서 벗어난 위치에 있으면 데이터 전체의 상관관계를 희석시키거나 왜곡시켜 결정계수(R²)와 같은 적합도 지표를 떨어뜨린다 . 실제 한 사례에서는, 이상치가 없을 때 R²가 0.94로 높았지만 이상치 하나를 포함시키자 R²가 0.55로 크게 감소하였다 . 이처럼 이상치는 선형 회귀식의 적합성을 감소시키고, 회귀 계수의 통계적 유의성마저 흔들 수 있다.\n한편, 이상치의 영향은 항상 적합도를 낮추는 방향으로만 작용하지 않을 수도 있다. 자료 끝부분의 극단적인 한 점이 다른 점들과 동일한 직선 패턴 상에 놓여 있지만 X값 범위가 매우 크다면, 그 점은 회귀 직선을 강하게 끌어당겨 기울기를 크게 변화시킬 수 있다. 이러한 점을 보통 영향점(influential point)이라고 부르며, 영향점이 존재하면 회귀 직선의 기울기가 크게 변하고 상관계수가 오히려 증가하거나 감소하는 등 모델에 과도한 영향을 미친다 . 예컨대 한 연구에서 X축의 오른쪽 끝에 위치한 단 하나의 영향점 때문에 회귀선의 기울기가 -2.5에서 -1.6으로 변하고, 결정계수도 0.46에서 0.52로 증가한 경우가 보고된 바 있다 . 따라서 이상치는 데이터 패턴에 어긋나는 경우 모델 적합성을 떨어뜨리지만, 데이터 분포의 가장자리에서 지렛대(leverage) 역할을 하는 점은 모델의 추세 자체를 크게 바꾸기도 한다. 더욱이 드물지만, 데이터 대부분은 관계가 없는데 특이점 하나 때문에 거짓된 상관관계가 발생하는 경우도 있다. 즉 이상치 하나가 존재할 때에만 두 변수 간에 상관이 생기고, 그 점을 제외하면 전혀 관계가 없는 상황이 있을 수 있다 . 이러한 경우 회귀모형의 추정된 관계는 실제 현상을 반영했다기보다 이상치 한 개에 의해 인위적으로 생성된 것이므로, 해석에 각별한 주의가 필요하다. 정리하면, 이상치는 두 변수 간 직선적 연관성 분석에서 관계의 강도를 약화시키거나 (혹은 때로는 과대평가하는 방향으로) 왜곡할 수 있기 때문에, 선형 모델링에서는 이러한 점들을 간과해서는 안 된다.\n이상치의 시각적 진단과 통계적 기준\n시각적 진단은 이상치를 탐색하는 첫 단계로서, 산점도(scatter plot)는 두 변수 관계에서 이상치를 찾아내는 유용한 도구이다 . 산점도를 그려보면 대다수의 점들은 어떤 군집 또는 패턴을 이루는데, 이상치는 이 군집에서 현저히 동떨어져 있는 점으로 나타난다. 예를 들어, 한 변수의 값이 주어졌을 때 다른 변수의 예측되는 범위에서 크게 벗어난 위치에 존재하는 점은 이상치로 의심할 수 있다 . 회귀선을 그렸다면, 그 선으로부터 수직 방향으로 멀리 떨어진 점들이 바로 반응변수 방향의 이상치를 의미한다. 이러한 점들은 잔차(residual)가 매우 큰 관측치들로, 회귀모형이 잘 설명하지 못한 값들이다 . 마찬가지로, 산점도에서 수평 방향으로 한쪽에 치우쳐 다른 점들과 X값 범위가 동떨어진 관측치는 설명변수 쪽 이상치로 볼 수 있다. 일반적으로 2차원 산점도에서 이상치는 육안으로 어느 정도 발견이 가능하지만, 주관적 판단에 의존하므로 통계적 기준과 함께 활용하는 것이 좋다.\n통계적 기준으로는 보통 잔차의 크기를 표준화하여 이상치를 판별한다. 표준화 잔차(standardized residual)란 잔차 $e_i$를 그 표준오차로 나누어 무단위화한 값으로, 잔차의 z-점수에 해당하는 지표이다 . 표준화 잔차의 절댓값이 지나치게 크면 해당 관측치는 회귀 직선으로부터 과도하게 벗어나 있음을 의미한다. 관행적으로 $|r_i| &gt; 2$이면 이상치 가능성을, $|r_i| &gt; 3$이면 유력한 이상치로 간주한다 . 즉 잔차가 평균으로부터 3표준편차보다 더 떨어져 있으면 데이터 분포의 극단에 위치한 것이므로 이상치일 확률이 높다고 보는 것이다 . 이 기준은 정규분포 가정하에 약 99%의 관측치는 ±3 범위 내에 존재한다는 원리에 따른 경험법칙이다. 보다 엄밀한 회귀진단에서는 스튜던트화 잔차(studentized residual)와 같은 지표를 사용하여 이상치를 판정하고, 본페로니 보정까지 적용해 유의확률 관점에서 이상치 여부를 테스트하기도 한다 . 예를 들어 외부 스튜던트화 잔차의 검정에서 유의확률이 0.05 미만인 점을 이상치로 판정하는 식이다 . 이렇듯 통계적 기준은 시각적 판단을 보조하여 이상치를 객관적으로 식별하는 역할을 한다. 다만, 표준화 잔차나 스튜던트화 잔차가 임계값을 넘더라도 그것만으로 바로 해당 점이 잘못된 데이터라고 단정할 수는 없으므로, 추가적인 검토가 필요하다.\n예측 모형에서 이상치의 영향\n이상치는 예측 모형(predictive model)의 성능과 해석에 큰 영향을 주므로, 종종 ”양날의 검”으로 비유된다 . 한편으로 이상치는 회귀 계수 추정과 같은 모형 해석을 교란시켜 모형의 정확도를 떨어뜨리고 신뢰성을 훼손할 수 있다. 다른 한편으로는, 그 비정상적인 관측치 자체가 중요한 단서나 정보를 제공하기도 한다 . 예를 들어, 생산 공정 데이터에서 발생한 이상치는 시스템의 고장이나 이상 신호를 나타낼 수 있고, 사회 과학 데이터에서 발견된 이례적인 패턴은 새로운 인과 관계의 발견으로 이어질 수 있다 . 이러한 긍정적 측면에도 불구하고, 일반적으로 이상치가 포함되면 모형의 적합도가 저하되고 예측력이 약화되는 부정적 영향이 두드러진다.\n통계학적으로 볼 때, 최소제곱 회귀와 같은 기법은 모든 관측치를 대상으로 오차 제곱합을 최소화하므로 한 개의 극단적 오차가 전체 모형을 좌우할 수 있다. 이는 곧 이상치 하나가 잔차 제곱합에 크게 기여하여 회귀선의 기울기나 절편을 왜곡시키고, 결과적으로 대부분의 정상적인 관측치에 대한 예측 정확도를 떨어뜨릴 수 있다는 뜻이다. 아울러 이상치는 회귀모형의 잔차 정규성, 등분산성 등의 가정을 위반하게 만들어 모형의 신뢰 구간이나 검정 결과에 문제를 일으킬 수 있다 . 예컨데 20대 소비생활 데이터에 수집된 표본 중 한 명이 재벌가 자녀처럼 극도로 지출이 큰 경우를 생각해보자. 이 한 명은 다른 999명의 일반적인 생활비 수준과 동떨어져 있기 때문에, 회귀 분석을 통해 20대 평균 생활비를 추정할 때 그 추정치를 한층 높이고 오차를 크게 만들 것이다 . 이러한 이상치는 모형을 학습시킬 때 포함할지 여부를 고민해야 할 만큼 예측 결과에 영향을 미칠 수 있다 . 따라서 이상치가 존재하면 모형의 성능 지표(예: $R^2$, RMSE 등)가 나빠지거나, 회귀 계수의 통계적 유의성이 떨어지며, 예측에 대한 불확실성이 증가하는 경향이 있다. 이상치로 인해 학습된 모델은 새로운 데이터에 대해 과소적합 혹은 과대적합되는 양상을 보일 수도 있다 (이상치를 맞추려고 복잡해지거나, 반대로 이상치를 무시하면서 일반 패턴을 놓칠 수 있다). 요컨대, 이상치는 예측 모형의 정확성과 견고성(robustness)을 해칠 수 있으므로 모형 개발 단계에서 면밀한 진단과 대처가 필요하다.\n이상치 처리: 고전적 접근과 정보적 가치\n과거 고전적 통계학에서는 이상치를 발견하면 데이터의 오류나 이분산의 원인으로 간주하여 제거하는 것이 일반적인 처리 방식이었다 . 만약 해당 관측값이 명백히 잘못 기록된 값이거나 물리적으로 불가능한 값이라면 제거는 당연한 조치다. (예를 들어 어느 데이터셋에서 성인 여성의 몸무게가 19파운드로 입력되어 있다면 이는 입력 오류이므로 그 자료점을 분석에서 제외해야 한다 .) 또한 이상치가 뚜렷하게 모형의 결과를 왜곡하고 있다면, 분석가는 그 데이터를 제거한 후 모형을 재적합하여 결과가 개선되는지 확인해볼 수 있다 . 실제로 회귀진단에서는 잔차와 지렛값을 함께 고려한 쿡의 거리(Cook’s distance) 등의 지표를 활용하여, 영향력이 큰 이상치를 식별하고 이러한 점들을 제외한 모형의 적합도를 비교하는 것이 하나의 절차로 확립되어 있다 . 만약 이상치 제거 후에 결정계수 등이 크게 향상되고 모형 가정도 만족된다면, 해당 관측치를 이유있는 제거(justified exclusion)로 문서화한 뒤 최종 모델에서 제외하기도 한다 . 다만 통계 분석 보고에서는 임의로 데이터를 삭제했다는 오해를 피하기 위해, 어떤 점을 이상치로 판단하여 제외했는지 그리고 그 효과가 어땠는지 언급하는 것이 권장된다 . 고전적 접근의 이 같은 제거 방법은 모형의 안정성과 가정 충족을 높이는 장점이 있지만, 동시에 잠재적으로 유용한 정보까지 잃어버릴 위험도 수반한다 . 특히 표본 크기가 크지 않은 연구에서는 몇 개 안 되는 이상치를 삭제함으로써 정보 손실이나 표본 편향이 발생할 수 있으므로 신중한 판단이 필요하다 .\n현대의 대용량 데이터 분석이나 머신러닝 관점에서는 이상치를 단순히 제거 대상이 아닌 학습과 발견의 대상으로 바라보는 경향이 강해지고 있다. 무엇보다도 이상치 중 상당수는 잘못된 값이 아니라 실제로 일어난 의미 있는 사건의 지표일 수 있다 . 따라서 ”단지 이상치라는 이유만으로 데이터를 버려서는 안 된다. 어떤 이상치는 온전히 실제 관측값이며, 때로는 가장 흥미로운 정보를 담고 있는 경우도 있다”는 지적이 설득력을 얻고 있다 . 예컨데 금융 사기 탐지에서는 정상 거래들과 달리 극단적인 패턴을 보이는 이상치가 사기의 징후이며, 제조 품질 데이터에서는 이상치가 장비 이상이나 새로운 결함을 시사할 수 있다. 이렇듯 분야에 따라 이상치는 도리어 분석자가 찾아내고자 하는 핵심 목표가 되기도 한다. 실제로 이상치 탐지(outlier detection 또는 이상감지 anomaly detection)는 데이터 세트에서 기대되는 패턴과 다른 특이한 데이터 포인트를 찾아내는 독자적인 분석 분야로 발전해왔다 . 이는 곧, 이상치를 **노이즈(noise)**로 간주하여 제거만 할 것이 아니라 **잠재적 시그널(signal)**로 해석하여 추가 조사나 모형 개선에 활용해야 한다는 현대적 관점이다. 예를 들어, 이상치가 기존 모형에 포함되지 않은 어떤 숨은 요인의 영향으로 발생한 것이라면, 이를 제거하는 대신 그 요인을 설명 변수로 모형에 추가하여 모형 구조를 확장해볼 수 있다 . 또는 이상치의 영향력을 줄이는 로버스트 회귀나 변환 방법(예: 로그 변환) 등을 통해 모든 데이터를 포괄하면서도 이상치에 둔감한 모델을 구축할 수도 있다 .\n대용량 데이터의 맥락에서는, 수백만 건의 데이터 중 몇 개의 이상치가 포함되더라도 전체 통계량에 미치는 비중은 상대적으로 작아질 수 있다. 따라서 이전보다 이상치로 인한 왜곡이 덜 심각할 수도 있지만, 동시에 방대한 데이터에서는 극단치 자체가 더 많이 발생할 확률이 높기에 복잡한 이상치 패턴이 나타날 가능성도 있다. 대규모 데이터 분석에서 중요한 것은, 이상치를 판별하여 배제하는 것과 탐지하여 활용하는 것 사이에서 문제의 목적에 맞게 균형을 잡는 일이다. 이상치가 단순한 입력 오류로 확인된다면 이는 제거하거나 보정해야 한다. 그러나 이상치가 도메인 상 의미를 지닐 가능성이 있다면, 해당 관측치를 별도로 분석하거나 도메인 전문가와 협의하여 추가적인 인사이트를 얻는 방향으로 활용하는 것이 바람직하다. 결국 이상치 처리에 정해진 정답은 없으며, 맥락에 따른 판단이 요구된다. 통계적 신뢰성을 위해 이상치를 과감히 제외해야 할 때도 있지만, 새로운 정보를 얻기 위해 이상치를 탐구해야 할 때도 있다 . 오늘날의 데이터 과학자는 이 두 측면을 모두 고려하여 이상치를 다룸으로써, 보다 견고하면서도 정보를 놓치지 않는 분석을 수행해야 할 것이다.\n\n\n(3) 진단내용 영향치\n영향치(influential observation)란 X축, 즉 예측변수의 값이 데이터 전체 분포의 범위를 벗어난 관측치를 말한다. 이러한 영향치는 회귀 분석에서 추정된 회귀식과 적합도 지표에 직접적인 영향을 미칠 수 있다. 영향치는 크게 두 가지 유형으로 구분된다. 첫째, 순수 영향치(pure leverage point)는 추정된 회귀 직선 위에 위치하여 기울기에는 영향을 주지 않지만, 예측변수의 범위를 넓힘으로써 결정계수를 인위적으로 높인다. 이로 인해 예측변수가 반응변수를 설명하는 능력이 실제보다 과대평가될 수 있다. 둘째, 이상 영향치(outlying leverage point)는 회귀 직선에서 크게 벗어난 위치에 있는 관측치로, 이상치(outlier)의 성격을 지닌다. 이러한 점은 회귀계수 추정과 예측 성능 모두에 왜곡을 일으킬 가능성이 크다.\n영향치의 존재 여부는 Hat 통계량(Hat matrix diagonal), 즉 지렛값(leverage)을 통해 진단한다. 지렛값은 각 관측치의 예측변수 값이 전체 데이터 중심에서 얼마나 떨어져 있는지를 나타내며, 값이 클수록 영향치일 가능성이 높다.\n영향치가 발견되면, 해당 점의 주변에서 추가 데이터를 수집하여 분석의 안정성을 높이는 것이 바람직하다. 만약 해당 관측치의 값이 실제로 발생할 가능성이 없거나 측정 오류로 확인된다면 분석에서 제외할 수 있다. 그러나 현실적으로 발생 가능한 값이라면, 그 관측치가 분석 대상 현상을 설명하는 데 어떤 의미를 갖는지 면밀히 검토한 뒤 처리 방안을 결정해야 한다.\n\n\n\n\nchapter 2. 상관계수 계산\n상관계수는 두 변수 간의 직선 관계를 측정하는데 사용되는 통계적 지표이다. 두 변수 간의 선형 관계성의 방향과 강도를 나타낸다.\n\n1. 상관계수 종류\nKarl Peason 공식\n모집단 상관계수 : \\(\\rho = \\frac{COV(X,Y)}{\\sqrt{V(X)\\sqrt{V(Y)}}}\\)\n데이터(표본 상관계수) 계산식 : \\(r = \\frac{\\sum_{i}^{n}(x_{i} - \\overline{x})(y_{i} - \\overline{y})}{\\sqrt{\\sum_{i}^{n}(x_{i} - \\overline{x})^{2}}\\sqrt{\\sum_{i}^{n}(y_{i} - \\overline{y})^{2}}}\\)\n분모는 확률변수의 표준편차이므로 상관계수의 부호를 결정하는 것은 분자항의 공분산이다.\nSpearman 순위 상관계수\n【방법1】\n\\[ r_s = Corr(R_{X_i}, R_{Y_i})\n\\quad \\text{where } R_{X_i} \\text{는 } X_i \\text{의 크기 순위이며, }\nR_{Y_i} \\text{는 } Y_i \\text{의 크기 순위이다.} \\]\n【방법2】\n\\[ r_s = 1 - \\frac{6 \\sum d_i^2}{n(n^2 - 1)},\n\\quad d_i = R_{X_i} - R_{Y_i} \\]\n대부분의 데이터 범위 밖에 있는 관측치(타원형 내 관측치)는 상관계수 값을 높이는 역할을 하므로 상관계수를 계산하기 전에 반드시 산점도를 그려 데이터의 범위를 많이 벗어난 관측치가 있는지 확인하여 상관분석의 활용도를 높일 필요가 있음.\n【두 상관계수의 관계】\n아래 그림은 동일한 자료에 대해 Spearman 상관계수와 Pearson 상관계수를 비교한 것이다. Pearson 상관계수는 두 변수 간의 선형 관계 강도를 나타내며, 값이 0.67로 비교적 강한 양의 상관관계를 보인다. 반면 Spearman 상관계수는 변수의 순위에 기반하여 단조 관계의 강도를 측정하며, 값이 0.84로 Pearson 계수보다 높다.\n\n\n\n\n\n이러한 차이는 오른쪽 상단에 위치한 일부 이상치 집단의 영향 때문이다. Pearson 상관계수는 값의 크기 자체를 사용하므로 이상치에 민감하게 반응하여 계수 값이 낮아진다. 반면 Spearman 상관계수는 순위 정보만을 사용하므로 이상치가 순서를 크게 변경하지 않는 한 영향을 적게 받는다.\n따라서 이 자료는 단조 관계가 매우 강하지만, 이상치로 인해 선형 관계가 상대적으로 약화된 경우에 해당한다. 이상치가 존재하는 경우 값의 크기에 의해 상관관계 척도를 계산하는 피어슨 상관계수 값이 작아진다. 그러므로 피어슨 상관관계를 활용하는 경우 이상치 진단 후 이상치를 제거한 후 상관계수를 구하는 것을 권한다.\nKendall \\(\\tau\\) 상관계수\n\\[ \\tau = \\frac{\\# of\\ concordant\\ pairs - \\# of\\ disconcordant\\ pairs}{n(n-1)/2} \\]\n\\[ \\text{concordant} =\n\\begin{cases}\n(x_i &gt; x_j,\\; y_i &gt; y_j) \\quad \\text{이거나} \\\\\n(x_i &lt; x_j,\\; y_i &lt; y_j)\n\\end{cases} \\]\n\\[ \\text{즉, 두 관측치 } (x_i, y_i), (x_j, y_j) \\text{가 위 조건을 만족하면 concordant 쌍이라 한다.} \\]\n\n\n2. (피어슨) 상관계수 유의성 검정\n스피어만 상관계수, 켄달 \\(\\tau\\)의 샘플링 분포를 알려지지 않아 비모수적 방법으로 검정하게 된다.\n\n귀무가설 : 두 변수의 직선 상관관계는 유의하지 않다. &lt;=&gt; 서로 독립이다. \\(H_{0}:\\rho = 0\\)\n대립가설 : 두 변수의 직선 상관관계는 유의하다. \\(H_{0}:\\rho \\neq 0\\)\n\n데이터 검증\n데이터는 이변량 정규분포에 근사해야 한다. 단 n&gt;20 인 대표본에서는 문제 없으므로 대용량 데이터 상관계수 구할 때는 문제가 없다.\n이상치가 상관계수에 영향을 주므로 산점도를 그려 데이터 범위(X-) 밖의 관측치 존재 여부를 체크해야 하나 대용량 데이터에서는 큰 의미가 없다.\n검정통계량\n표본 크기가 n, 표본 (피어슨) 상관계수가 r인 경우 검정통계량은 다음과 같고 검정통계량의 샘플링 분포는 t-분포를 따른다.\n\\[ts = \\frac{r - \\rho = 0}{\\sqrt{\\frac{1 - r^{2}}{n - 2}}} \\sim t(n - 2)\\]\n결론\n유의확률 \\(P(t(n - 2) \\geq |ts|) = p_{value}\\)이 유의수준보다 작다면 귀무가설을 기각하여 상관관계의 유의하다고 결론내리고 표본상관계수의 부호를 이용하여 해석한다.\n귀무가설이 기각, 표본상관계수 부호 + =&gt; 두 변수는 양의 상관관계가 있고 한 변수의 값이 증가(감소)하면 다른 변수의 값도 증가(감소)한다.\n귀무가설이 기각, 표본상관계수 부호 - =&gt; 두 변수는 음의 상관관계가 있고 한 변수의 값이 증가(감소)하면 다른 변수의 값도 감소(증가)한다.\n\n\n3. (피어슨) 상관계수 해석\n\n\n\n\n\n\n\n\n\n\n출처 : 위키피디아\n회귀모형의 회귀계수와 부호는 동일하나 예측모형의 인과 관계 해석은 적절하지 않다.\n상관관계 높다는 것은 상관계수의 절대값이 1에 가깝다는 것을 의미하며 대부분의 관측점들이 직선 상에 있다는 것을 의미한다.\n상관계수가 1에 가까우면 양의 선형 상관 관계가 존재한다. 한 변수의 값이 증가(감소)하면 다른 변수 값도 증가(감소)한다. –1에 가까우면 음의 선형 상관 관계가 존재한다. 한 변수의 값이 증가(감소)하면 다른 변수 값은 감소(증가)한다.\n상관계수가 0이 의미하는 것은 직선 관계가 존재하지 않는다는 것이지 함수 관계가 존재하지 않는 것은 아니다. 위의 그림에서 볼 수 있듯이 2차 함수나 원 함수의 상관계수도 0이다.\n두 변수의 상관 관계가 높다는 것은 두 변수가 동일한(comparable) 개념을 측정한다는 의미도 담고 있다(두 변수가 유사함). 그러므로 변수를 축약하거나 개체를 분류하는데 사용되는 다변량 분석에서는 공분산(혹은 상관계수) 개념이 사용된다.\n\n\n\n\n\n\n\n상관계수 해석 (Rule of Thumb)\n\n\n\n\n\\[|r| \\approx 0\\]\n상관관계 존재하지 않음\n\n\n\\[0 &lt; |r| &lt; 0.2\\]\n매우 약한 상관관계 존재함\n\n\n\\[0.2 \\leq |r| &lt; 0.4\\]\n약한 상관관계 존재함\n\n\n\\[0.4 \\leq |r| &lt; 0.6\\]\n상관관계 존재함\n\n\n\\[0.6 \\leq |r| &lt; 0.8\\]\n강한 상관관계 존재함\n\n\n\\[0.8 &lt; |r| \\approx 1\\]\n매우 강한 상관관계 존재함\n\n\n\\[|r| = 1\\]\n완벽한 상관관계 존재함\n\n\n\n\n\n4. 상관계수 관련 코맨트\n\\(H_{0}:\\rho = \\rho_{0}\\) 검정\n상관관계 유의성(상관 관계 존재 여부, \\(H_{0}:\\rho = 0\\))을 검정하는 것이 아니라 임의의 상관계수와 동일한지 검정한다. (예제) 미국의 경우 부자 키의 상관계수는 0.65이다. 한국의 경우 미국과 부자의 키의 상관계수가 같다고 할 수 있나? 귀무가설 : \\(H_{0}:\\rho = 0.65\\)\n검정통계량 : \\(ts = \\frac{\\frac{1}{2}ln\\frac{1 + r}{1 - r} - \\frac{1}{2}ln\\frac{1 + \\rho}{1 - \\rho}}{\\frac{1}{\\sqrt{n - 3}}} \\sim N(0,1)\\)\n서로 독립인 2집단 상관계수 차이 \\(H_{0}:\\rho_{x} = \\rho_{y}\\) 검정\n\\(z(x) = \\frac{1}{2}ln\\frac{1 + r_{x}}{1 - r_{x}}\\), \\(z(y) = \\frac{1}{2}ln\\frac{1 + r_{y}}{1 - r_{y}}\\)\n검정통계량 : \\(ts = \\frac{z(x) - z(y)}{\\sqrt{1/\\sqrt{n_{x} - 1} + 1/\\sqrt{n_{y} - 1}}} \\sim N(0,1)\\)\n회귀계수와 관계 \\(Y = \\alpha + \\beta X + e\\)\n독립변수 X가 Y에 선형적 영향을 미치는지 검정 &lt;=&gt; 기울기 \\(H_{0}:\\beta = 0\\)(영향을 미치는 않음) 유의성 검정 &lt;=&gt; 상관계수의 유의성 검정(\\(H_{0}:\\rho = 0\\))과 동일하다.\n\\(\\widehat{\\beta} = \\sqrt{\\frac{S_{XY}}{S_{XX}}}r\\), \\(S_{XX} = \\sum(X_{i} - \\overline{X})^{2}\\), \\(S_{XY} = \\sum(X_{i} - \\overline{X})(Y_{i} - \\overline{Y})\\)\n상관계수 부호와 회귀계수 부호는 동일하다.\n두 통계량 모두 \\(t(n - 2)\\) 분포를 갖는 검정통계량으로 검정한다. 그러므로 두 통계량의 유의확률은 동일하다.\n단순 회귀모형에서 결정계수 Determination Coefficient(\\(R^{2} = \\frac{SST}{SSR} = \\frac{\\sum(y_{i} - \\overline{y})^{2}}{\\sum({\\widehat{y}}_{i} - \\overline{y})^{2}}\\))의 제곱근은 상관계수이다. \\(r = \\pm \\sqrt{R^{2}}\\)\n\n\n5. 상관계수 활용\n변수의 유사성 척도\n상관계수는 두 변수 간의 직선적 관계의 강도와 방향을 나타내는 통계적 척도이며, 이를 통해 변수들 간의 유사성을 평가할 수 있다. 상관계수가 1 또는 –1에 가까울수록 두 변수는 매우 강한 선형 관계를 가지며, 이는 곧 두 변수가 공통적으로 설명하는 정보가 많음을 의미한다. 다시 말해, 상관관계가 높다는 것은 두 변수 간의 정보 중복도가 크다는 뜻이며, 동일한 대상을 설명하기 위해 두 변수 모두를 사용할 필요가 없을 수 있음을 시사한다.\n예를 들어, 키와 몸무게는 일반적으로 양의 상관관계를 보인다. 키가 큰 사람일수록 몸무게도 무거운 경향이 있으므로, 이 두 변수는 상당한 수준의 정보를 공유하고 있다. 따라서 신체 특성을 설명하거나 분류하는 데 있어 경우에 따라 두 변수 중 하나만 사용해도 충분할 수 있다.\n또 다른 예로, 기성복 하의를 구매하는 상황을 들 수 있다. 하의의 착용 적합성을 판단할 때 길이(허리에서 무릎까지, 무릎에서 발목까지)와 넓이(허리둘레, 허벅지둘레, 발목둘레) 등 다양한 측정 변수를 고려할 수 있으나, 이들 사이에는 높은 상관관계가 존재할 수 있다. 예컨대 기장 관련 변수들은 서로 밀접하게 연관되어 있으며, 넓이 관련 변수들은 허리둘레와 강한 상관관계를 가질 수 있다. 이 경우 각 범주에서 하나의 대표 변수만 측정하더라도 나머지 변수들을 상당 부분 예측하거나 대체할 수 있다.\n이와 같이 상관계수는 변수 선택이나 차원 축소 과정에서 유용하게 활용된다. 상관관계가 높은 변수들을 모두 사용할 경우 분석 모델에 불필요한 중복이 생기고, 다중공선성 문제를 유발할 수 있다. 따라서 상관계수를 이용하여 변수 간 유사성을 평가한 뒤, 정보의 손실을 최소화하면서 일부 변수만을 선택하여 분석의 효율성과 해석력을 높이는 것이 바람직하다.\n결론적으로, 상관계수는 두 변수 간 관계를 파악하는 것을 넘어, 변수 간 정보 중복을 진단하고 데이터 구조를 간결하게 정리하는 데 중요한 도구로 작용한다. 이는 통계 분석뿐 아니라 기계학습, 추천 시스템, 설문지 문항 축소 등 다양한 응용 분야에서 핵심적인 개념으로 활용된다.\n회귀분석 : 종속변수 설명력 높은 변수 사전 선택\n회귀분석은 종속변수와 하나 이상의 설명변수 간의 관계를 분석하여, 종속변수를 예측하거나 설명하는 데 목적이 있다. 이때 설명변수의 수가 많고, 데이터의 관측치 수가 상대적으로 적은 경우, 회귀모형은 과적합(overfitting) 문제에 직면할 수 있다. 과적합이란 모형이 학습 데이터에는 지나치게 잘 맞지만, 새로운 데이터에 대해서는 일반화 능력이 떨어지는 현상을 말한다. 따라서 설명변수의 수를 적절히 제한하여 모형의 복잡도를 줄이는 것이 바람직하다.\n특히 고차원 데이터나 빅데이터 환경에서는 변수 선택의 중요성이 더욱 강조된다. 이와 같은 상황에서는 종속변수와 각 설명변수 간의 상관계수를 우선적으로 계산하고, 일정 기준 이상의 상관관계를 보이는 변수들만을 선택하여 예측모형을 구성하는 접근이 널리 사용된다. 예를 들어, 상관계수가 일정 수준(예: 절댓값 기준 0.3 이상)인 변수만을 선택함으로써, 종속변수와의 연관성이 낮은 설명변수를 사전에 제거하고 모형의 안정성과 해석력을 동시에 확보할 수 있다.\n이러한 변수 축소 방식은 계산 효율성은 물론, 예측 정확도 향상과 해석 용이성 측면에서도 유용하다. 다만, 상관계수는 두 변수 간의 단순한 선형 관계만을 반영하므로, 변수 간 상호작용이나 비선형적 관계를 고려하지 못한다는 한계도 존재한다. 그럼에도 불구하고, 회귀분석의 초기 단계에서 설명변수의 수를 줄이기 위한 기준으로 상관계수는 실용적이고 직관적인 도구로 널리 활용되고 있다.\n회귀분석 : 다중공선성 문제 해결\n회귀모형 \\(y = X\\beta + e\\)의 \\(\\beta\\)의 OLS 추정치는 \\(\\beta = (X'X)^{- 1}X'y\\)이다. 설명변수(\\(X\\)) 간의 상관관계가 높으면 \\(det(X'X) \\simeq 0\\)이므로 \\(X'X^{- 1}\\)의 값이 매우 커진다.\n선형 회귀분석에서 가장 널리 사용되는 추정 방법은 최소제곱법(Ordinary Least Squares, OLS)이다. OLS는 잔차 제곱합을 최소화하는 회귀계수의 추정치를 제공하며, 일정한 가정이 충족될 경우 일치성과 불편성을 갖는 효율적인 추정량으로 평가된다.\n그러나 설명변수들 간에 높은 상관관계가 존재하는 경우, 즉 다중공선성(multicollinearity)이 나타나면 이러한 추정의 안정성이 심각하게 훼손된다. 설명변수들 간의 상관관계가 높을수록 설계행렬의 자기상관 구조가 강해지며, 이로 인해 (X’X)^{-1} 행렬의 값이 커지게 된다. 이는 OLS 추정량의 분산, 즉 추정값의 변동성이 커짐을 의미하며, 결과적으로 회귀계수의 추정치가 민감하게 변동하거나 심지어 부호가 바뀌는 등 해석상 혼란을 초래할 수 있다.\n이러한 문제는 특히 설명변수 간의 중복 정보가 많고, 각 변수의 독립적 기여도가 명확하지 않은 상황에서 두드러진다. 모형의 설명력은 높게 나타날 수 있으나, 개별 회귀계수의 통계적 유의성이 낮아지고 신뢰구간이 넓어지는 현상이 관찰된다. 이는 예측의 정확도뿐 아니라 회귀모형 해석의 타당성에도 악영향을 미친다.\n다중공선성 문제를 해결하기 위한 대표적인 방법은 다음과 같다. 첫째, 종속변수와의 상관관계가 더 높은 설명변수만을 선택하고, 상관관계가 높은 다른 설명변수는 제거하는 방식이다. 이를 통해 중복되는 정보를 제거하고 회귀계수의 안정성을 확보할 수 있다. 둘째, 주성분 분석(Principal Component Analysis, PCA)을 이용하여 원래의 상관된 변수들을 공분산 구조에 따라 변환하고, 서로 직교(상관계수 0)인 새로운 주성분을 생성하여 이들을 설명변수로 활용하는 방법이 있다. 이러한 방식은 정보의 손실을 최소화하면서 다중공선성을 효과적으로 제거할 수 있다는 점에서 이론적, 실무적으로 모두 유용하다.\n따라서 회귀모형을 설정할 때는 설명변수 간의 상관구조를 면밀히 진단하고, 다중공선성으로 인한 추정 불안정성을 줄이기 위한 변수 선택 및 차원 축소 기법을 적극적으로 고려할 필요가 있다.\n\n\n\nchapter 3. 다변량 상관계수\n\n1. 행렬 연산\n다변량 데이터 행렬: 행의 차수 n, 열(변수)의 개수 p인 데이터 행렬 \\(X_{n \\times p} = \\begin{pmatrix}\nx_{11} & x_{12} & \\ldots & x_{1p} \\\\\nx_{21} & x_{22} & \\ldots & x_{2p} \\\\\n\\ldots & & & \\\\\nx_{n1} & x_{n2} & \\ldots & x_{np}\n\\end{pmatrix}\\)\n확률변수 열벡터: \\(\\underset{¯}{x} = \\left( \\begin{array}{r}\nx_{1} \\\\\nx_{2} \\\\\n\\ldots \\\\\nx_{p}\n\\end{array} \\right)\\)\n평균벡터\n\nONE 벡터 : \\({\\underset{¯}{1}}_{n} = \\left\\lbrack \\begin{array}{r}\n1 \\\\\n1 \\\\\n\\ldots \\\\\n1\n\\end{array} \\right\\rbrack\\) 모든 원소가 1인 벡터\n평균벡터 : \\(E(\\underset{¯}{x})_{1 \\times p} = {\\underset{¯}{1}}^{T}X\\)\n공분산 행렬 \\(\\Sigma_{p \\times p}\\)\n중심화 행렬 : \\(C_{n \\times p} = X_{n \\times p} - {\\underset{¯}{1}}_{n \\times 1}E(X)\\)\n공분산행렬 : \\(\\Sigma_{p \\times p} = (C^{T}C)/(n - 1)\\)\n상관계수 행렬 \\[ R_{p \\times p} = D^{-1} \\Sigma_{p \\times p} D^{-1},\n\\quad D = \\mathrm{diag}(\\sigma_1, \\sigma_2, \\dots, \\sigma_p) \\]\n\n\n\n2. 사례\nSEABORN Iris data\n3개 품종 분꽃 :  (Iris setosa, virginica and versicolor)\n4개 측정형 : 꽃받침 조각(petal) 길이, 넓이 - 꽃잎(sepal) 길이, 넓이\n\n\n\n\n\n#분꽃 데이터 불러오기\nimport pandas as pd\nimport seaborn as sns\ndf = sns.load_dataset('iris')\n#산점도 그리기\nimport seaborn as sns\nax=sns.pairplot(df,kind=\"reg\",hue='species',corner=True,diag_kind=\"hist\")\n\n#피어슨 상관계수\ndf.iloc[:,0:4].corr(method='pearson') #spearman, kendall\n\n#유의성 검정\nimport scipy.stats as st\nst.pearsonr(df['petal_length'],df['petal_width'])\n(꽃받침 길이, 꽃받침 넓이, 꽃잎 길이)-(꽃잎 넓이) 음의 상관계수 꽃받침 길이, 꽃받침 넓이 상관계수는 0.96이고 유의확률은 &lt;0.001이다.\n\n\n\n\n\n\n\n\n\n\n#함수 이용 평균, 공분산, 상관계수\ndf.iloc[:,0:4].mean(),df.iloc[:,0:4].cov(),df.iloc[:,0:4].corr()\n(sepal_length 5.843333, sepal_width 3.057333, petal_length 3.758000, petal_width 1.199333)  sepal_length sepal_width petal_length petal_width  sepal_length 0.685694 -0.042434 1.274315 0.516271  sepal_width -0.042434 0.189979 -0.329656 -0.121639  petal_length 1.274315 -0.329656 3.116278 1.295609  petal_width 0.516271 -0.121639 1.295609 0.581006,  sepal_length sepal_width petal_length petal_width  sepal_length 1.000000 -0.117570 0.871754 0.817941  sepal_width -0.117570 1.000000 -0.428440 -0.366126  petal_length 0.871754 -0.428440 1.000000 0.962865  petal_width 0.817941 -0.366126 0.962865 1.000000)\n#평균벡터\nn=df_mat.shape[0]\none=np.array(np.ones(n))\ndf_mean=np.dot(one,df_mat)/n\ndf_mean\narray([5.84333333, 3.05733333, 3.758 , 1.19933333])\n#공분산 행렬\nC=df_mat-df_mean\ndf_cov=np.dot(C.T,C)/(n-1)\ndf_cov\narray([[ 0.68569351, -0.042434 , 1.27431544, 0.51627069],  [-0.042434 , 0.18997942, -0.32965638, -0.12163937],  [ 1.27431544, -0.32965638, 3.11627785, 1.2956094 ],  [ 0.51627069, -0.12163937, 1.2956094 , 0.58100626]])\n#상관계수 행렬\nimport numpy.linalg as la\nD=np.diag(np.sqrt(np.diag(df_cov)))\nD_inv=la.inv(D)\nD_inv@df_cov@D_inv\narray([[ 1. , -0.11756978, 0.87175378, 0.81794113],  [-0.11756978, 1. , -0.4284401 , -0.36612593],  [ 0.87175378, -0.4284401 , 1. , 0.96286543],  [ 0.81794113, -0.36612593, 0.96286543, 1. ]])"
  },
  {
    "objectID": "notes/intro_stat/data.html",
    "href": "notes/intro_stat/data.html",
    "title": "기초통계 2. 데이터 개념",
    "section": "",
    "text": "chapter 1. 데이터란?\n\n1. 데이터 철학\n과학은 단순한 이론의 축적이 아니라, 경험과 관찰, 모델과 검증, 이론과 현실 사이의 끊임없는 상호작용을 통해 발전해왔다. 아인슈타인의 상대성 이론이나 케플러의 행성 궤도 법칙처럼 이론적 통찰과 천문학적 관측을 바탕으로 탄생한 위대한 발견들은 역사에 길이 남지만, 과학의 진보 대부분은 반복적인 실험과 자료 분석을 통해 이루어진다.\n예를 들어, 벼 품종 개량은 수많은 세대에 걸친 교배 실험과 생육 결과 분석을 통해 이루어졌다. 신약 개발은 초기 후보 물질의 효과를 확인하기 위해 수천 건의 실험 설계와 통계 검증을 거쳐 승인된다. 화학 공정 개선 역시 다양한 조건에서 반복되는 실험과 그 결과에 대한 정량적 분석을 통해 점진적으로 최적화된다.\n이러한 흐름은 통계학이 과학의 핵심적인 실천 도구임을 명확히 보여준다.\n통계학자는 과학의 번역자이자 검증자이다.\n통계 전문가는 자연과학자, 공학자, 사회과학자가 제안한 이론이나 아이디어를 통계적 언어로 변환하는 역할을 수행한다.\n먼저, 제시된 이론이나 현상에 대한 가설을 통계적으로 정의한다. 예를 들어, ”A 약물이 B 약물보다 혈압을 더 많이 낮춘다”는 주장을 귀무가설과 대립가설로 구체화한다.\n다음으로, 이 가설을 검정하기 위해 데이터를 수집하거나 실험을 설계한다. 이후 분석 결과를 바탕으로 가설의 유의성을 판단하며, 이러한 과정을 확증적 데이터 분석이라 한다.\n반대로, 명시적 이론이 없는 상태에서 데이터를 먼저 탐색하여 새로운 규칙이나 관계를 발견하는 방식도 있다. 이를 탐색적 데이터 분석이라 하며, 특히 빅데이터 분석과 데이터 마이닝 분야에서 활발히 활용된다.\n탐색과 확증의 순환: 이론 발전의 실제 구조\n과학 이론이 견고해지기 위해서는 단순히 데이터를 수집하거나 수식을 세우는 것만으로는 부족하다. 탐색과 확증이 상호 순환하며 이론을 수정·보완하는 과정이 필수적이다. 중요한 점은 이론, 데이터, 모형이 각각 독립적으로 존재하는 것이 아니라 유기적으로 순환하며 서로 영향을 주고받는다는 철학적 관점이다. 이를 위해서는 다음 세 가지 요소가 반드시 함께 고려되어야 한다.\n1. 도메인 지식: 의학, 농업, 교육 등 해당 분야에 대한 배경 지식 없이는 데이터 해석이나 의미 있는 가설 설정이 어렵다.\n2. 통계 모형과 자료: 자료 수집과 해석을 위해 적절한 통계 모형을 설정해야 하며, 수집된 자료는 해당 모형을 충실히 반영하도록 설계되어야 한다.\n3. 이론–모형–데이터의 순환 구조: 이론에서 모형을 도출하고, 데이터를 통해 이를 검정한 뒤, 결과를 반영해 이론을 수정하는 순환이 반복되어야 한다. 이 순환을 이해하고 실천하는 능력은 데이터 과학자와 통계학자의 핵심 역량이다.\n데이터는 도구이자 통찰의 창구\n통계학은 단순한 ”숫자를 다루는 기술”이 아니라, 현상에 내재한 구조를 탐색하고 이를 이론으로 일반화하는 사고의 방식이다. 통계 전문가는 이론을 수학적으로 표현하고 검정할 수 있도록 연결하는 다리 역할을 하며, 데이터는 그 다리를 통해 현실 세계와 학문을 이어주는 통찰의 통로가 된다.\n\n\n2. 모형과 데이터의 순환 사이클\n과학적 연구는 흔히 ’이론을 먼저 제안하고, 이를 검증하기 위해 데이터를 수집하는 과정’으로 이해된다. 그러나 실제 연구 현장에서는 데이터가 먼저 수집되고, 그 안에서 의미 있는 패턴이나 관계를 발견한 뒤, 이를 바탕으로 새로운 이론이나 모형이 제시되는 경우가 훨씬 더 많다.\n\n\n\n\n\n이러한 흐름은 데이터 기반 탐색적 분석의 발달과 함께 그 중요성이 더욱 커지고 있다. 과학적 이론은 종종 자료의 구조나 경향을 탐색하는 과정에서 도출되며, 이렇게 제안된 이론이나 통계적 모형은 다시 확증적 분석을 통해 유의성과 적합성을 검증받는다.\n따라서 통계에서의 모형과 데이터는 일방향적인 직선 구조가 아니라, 탐색과 확증이 반복되는 순환 구조를 형성한다.\n모형과 데이터의 순환 흐름\n1. 데이터 수집: 실험, 관찰, 센서 등 다양한 방법으로 자료를 수집한다.\n2. 탐색적 분석(EDA): 그래프나 요약 통계를 활용하여 패턴, 변수 간 관계, 이상값 등을 파악하고, 이를 바탕으로 잠정적인 모형 또는 이론의 단서를 도출한다.\n3. 모형 수립: 탐색 과정에서 발견된 패턴을 설명할 수 있는 통계적 또는 수학적 모형을 설정한다.\n4. 모형 검증(CDA): 설정된 모형이 데이터에 얼마나 적합한지를 통계적으로 검증한다. 유의성 검정과 적합도 판단을 수행한다.\n5. 이론 수정 또는 확장: 검증 결과를 토대로 이론을 강화하거나 수정한다. 이후 수정된 이론에 따라 새로운 데이터 수집과 분석이 반복된다.\n이러한 순환 구조를 통해 과학은 진보하며, 통계학은 그 중심에서 핵심적인 역할을 수행한다.\n통계 모형은 현실의 ’요약’일 뿐\n통계학에서 말하는 모형은 현실의 ’진실’ 자체가 아니라, 현실을 설명하려는 수학적 요약 또는 근사에 불과하다. 모형은 대상 현상을 완전하게 표현하지 못하며, 항상 일정한 오차(error)를 포함한다. 예를 들어, 단순 회귀모형은 다음과 같이 표현된다:\n\\(Y_{i} = \\beta_{0} + \\beta_{1}X_{i} + \\varepsilon_{i}\\) 여기서,\n\\(\\beta_{0},\\beta_{1}\\)은 모형의 계수로서, 변수 간의 관계를 요약한다.\n\\(\\varepsilon_{i}\\)는 오차항으로, 모형이 설명하지 못한 부분을 나타낸다. 이 오차항은 \\(\\varepsilon_{i} \\sim N(0,\\sigma^{2})\\)을 가정된다:\n즉, 회귀모형은 현실 세계의 완벽한 진리를 설명하는 것이 아니라, 주어진 데이터를 가장 잘 설명하는 근사적 구조를 제시하는 도구일 뿐이다. 이 점을 이해하는 것은 통계학적 사고의 핵심이다.\n통계 분석은 순환하는 탐색과 검증의 과정이다.\n통계 분석은 데이터 → 이론 → 검증 → 데이터로 이어지는 반복적 사이클 속에서 이루어진다. 이 순환은 과학적 진리 탐구의 핵심 구조이며, 통계학자는 이 흐름을 수학적·계량적 방법으로 구체화하고 안내하는 역할을 수행한다.\n따라서 통계학을 학습하는 학생은 단순한 계산 능력에 머무르지 않고, 데이터와 모형 사이의 철학적 관계와 순환적 구조를 이해하는 태도를 갖추어야 한다.\n\n\n3. 데이터 정의\n데이터는 통계학을 포함한 수많은 학문과 산업 분야에서 핵심 자원으로 활용된다. 사전적 정의에 따르면, 데이터란 ”추론, 정보 획득, 계산에 사용되는 실제 조사되거나 측정된 값”을 의미한다(Webster Dictionary). 통계학의 관점에서 데이터는 단순한 숫자의 나열이 아니라, 정보를 담고 있는 숫자의 체계적인 집합이다.\n데이터란 무엇인가?\n통계학에서 데이터는 분석의 대상이 되는 개체로부터 변수를 측정하거나 관측하여 얻은 값들의 집합이다. 이러한 데이터는 수치일 수도 있고 문자일 수도 있으며, 표나 행렬과 같이 구조화되어야 분석이 가능하다. 즉, 데이터는 수학적으로 표현 가능한 형태로 정리되어야 하며, 이를 통해 다양한 통계적 추론이 가능해진다.\n통계학에서의 데이터 구조\n통계학에서 분석하는 데이터는 보통 행과 열로 구성된 2차원 테이블(또는 행렬) 형태를 가진다.\n\n행: 개별 관측 단위, 즉 개체의 관측값\n열: 각 개체에 대해 측정된 변수\n\n\\[\\begin{bmatrix}\nx_{11} & x_{12} & \\cdots & x_{1p} \\\\\nx_{21} & x_{22} & \\cdots & x_{2p} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nx_{n1} & x_{n2} & \\cdots & x_{np}\n\\end{bmatrix}\\]\n\n\\(x_{ij}\\)는 \\(i\\)번째 개체의 \\(j\\)번째 변수에 대한 관측값이다.\n총 \\(n\\)개의 개체, \\(p\\)개의 변수로 구성된 데이터를 \\(n \\times p\\) 행렬이라 부른다.\n\n데이터는 통계학의 출발점\n구조화된 데이터를 기반으로 통계학은 기술, 추론, 예측을 수행한다. 정확한 분석을 위해서는 단순히 수치를 수집하는 것에 그치지 않고, 어떤 개체로부터 어떤 변수를 어떤 방식으로 측정했는지를 명확히 정의하는 것이 무엇보다 중요하다.\n\n\n4. 데이터 종류\n데이터는 관측 대상의 특성을 표현한 정보이며, 그 특성의 종류에 따라 구분 기준과 분석 방법이 달라진다. 통계학에서는 이러한 구분을 변수의 성격에 따라 나누며, 사회과학에서는 이를 측정수준이라고 부르기도 한다. 데이터의 성격을 명확히 분류하는 것은 통계 분석의 첫걸음이다. 변수의 유형에 따라 적절한 요약 방법(예: 평균, 중앙값, 빈도 등)과 분석 기법(예: t-검정, 카이제곱 검정, 회귀분석 등)이 달라지기 때문이다. 따라서 통계학을 공부하는 학생은 단순히 수치를 다루는 것뿐만 아니라, 해당 수치가 어떤 성격의 변수에서 나온 것인지를 판단하는 눈을 길러야 한다.\n\n(1) 질적변수와 양적변수\n질적 변수 qualitative variable\n질적 변수는 개체를 분류하거나 구분하기 위해 측정된 속성으로, 그 값은 숫자가 아니라 범주 형태로 주어진다. 예를 들어, 성별, 결혼 여부, 직업, 거주 지역과 같은 변수들은 수치 자체가 크기나 양을 의미하지 않으며, 단순히 어떤 집단에 속하는지를 나타낸다. 질적 변수는 다시 명목척도(nominal scale)와 서열척도(ordinal scale)로 구분된다.\n명목척도는 각 개체를 단순히 구분하거나 식별하는 목적으로 사용된다. 예를 들어 성별(남/여), 혈액형(A/B/O/AB), 거주지(서울/부산) 등이 이에 해당한다. 이 변수들은 계산이 불가능하며, 오직 같은지 다른지만 비교할 수 있다.\n서열척도는 명목척도와 달리 범주 간에 일정한 순서가 존재한다. 예를 들어 학점(A, B, C…), 소득 수준(상/중/하), 만족도(매우 만족 ~ 매우 불만족)와 같이 정해진 순서에 따라 분류된 변수들이다. 다만, 각 단계 간의 간격이 일정하다는 보장은 없기 때문에, 평균이나 표준편차와 같은 계산에는 제한이 따른다.\n양적 변수 quantitative variable\n양적 변수는 개체의 특성을 수치로 직접 측정한 변수로, 계산이 가능한 데이터를 제공한다. 키, 몸무게, 시험 점수, 소득, 교통량, 연령 등이 대표적인 예이다. 양적 변수는 데이터를 수량적으로 다룰 수 있기 때문에, 다양한 통계 분석에 활용된다. 양적 변수는 측정 수준에 따라 등간척도(interval scale)와 비율척도(ratio scale)로 구분된다.\n등간척도는 변수의 값 사이 간격이 동일하다는 특징이 있지만, 절대적인 0의 의미가 없다. 대표적인 예로는 섭씨 또는 화씨 온도, 지능지수(IQ), 리커트 척도(예: 1~5점 척도)가 있다. 이 경우 덧셈과 뺄셈은 가능하지만, 배율의 비교는 의미가 없다. 예를 들어, 20도는 10도의 두 배라고 말할 수 없다.\n비율척도는 등간척도의 성질을 모두 가지면서도, 0이 절대적 기준으로서 의미를 갖는다. 0은 ’존재하지 않음’을 나타내며, 모든 사칙연산(+, –, ×, ÷)이 가능하다. 예를 들어 키 0cm, 나이 0세, 수입 0원은 모두 해당 특성이 존재하지 않음을 의미하며, 10kg은 5kg의 두 배로 해석될 수 있다.\n\n\n\n\n\n\n\n\n\n\n\n구간\n비율\n순서\n명목\n\n\n빈도표\nX\nX\nX\nX\n\n\n순서 있음\n\nX\nX\nX\n\n\n최빈값\nX\nX\nX\nX\n\n\n평균\nX\nX\n\n\n\n\n중위수\nX\nX\nX\n\n\n\n+, - 가능\nX\nX\n\n\n\n\n곱셈, 나누셈\n\nX\n\n\n\n\n0의 개념, 배율\n\nX\n\n\n\n\n\n\n\n(2) 시간에 따른 데이터의 구분\n통계학에서 데이터를 분류할 때 중요한 기준 중 하나는 시간의 흐름이 반영되었는가이다. 같은 변수라도 시간에 따라 수집 방식이 달라지면, 분석 방법도 달라지게 된다. 시간을 기준으로 데이터는 크게 횡단 자료와 종단 자료로 나눌 수 있다.\n횡단 자료 Cross-sectional Data\n횡단 자료는 일정한 시점에서 여러 개체에 대해 수집한 데이터를 의미한다. 즉, 시간은 고정되어 있고, 관심의 대상은 동일한 시점에서의 개체 간 차이이다. 예를 들어, 2024년 현재 H대학교 재학생 500명의 키와 몸무게를 조사한 경우, 이는 하나의 시점에서 여러 사람의 특성을 수집한 전형적인 횡단 자료이다.\n또한 한 해 동안 전국 시·도별 평균 월급, 병원별 환자 수, 기업별 연간 매출액 등을 조사한 자료도 모두 횡단 자료에 속한다. 횡단 자료에서는 시간 흐름을 고려하지 않기 때문에, 분석에서는 주로 개체 간 비교가 중심이 된다. 예를 들어, 학년별 평균 점수를 비교하거나, 지역별 소득 격차를 분석할 때 주로 사용된다.\n종단 자료 Time Series Data\n반면 종단 자료는 동일한 개체나 현상에 대해 시간의 흐름에 따라 연속적으로 수집한 데이터이다. 이 경우 시간은 분석에서 매우 중요한 변수가 되며, 이전 시점의 정보가 이후 결과에 영향을 줄 수 있는 구조를 갖는다. 예를 들어, 2010년부터 2024년까지 H대학교 총 입학자 수의 연도별 변화를 기록한 데이터는 종단 자료이다. 또한 국가의 월별 실업률, 일별 환율이나 주가, 연도별 수출입 규모처럼 시간 순서에 따라 관측된 경제 지표도 모두 종단 자료에 해당한다.\n종단 자료에서는 추세(trend), 계절성(seasonality), 주기성(cycle), 예측 가능성 등 시간 구조의 특성이 중요하며, 이를 다루기 위한 시계열 분석 기법이 별도로 발달되어 있다.\n\n\n(3) 인과관계와 변수의 역할\n통계 분석에서 가장 궁극적인 질문 중 하나는 ”무엇이 무엇에 영향을 미치는가?“, 다시 말해 인과관계에 대한 것이다. 단순히 두 변수 간에 상관관계가 있다고 해서, 한 변수가 다른 변수의 원인이라고 단정할 수는 없다. 통계학은 이처럼 인과적 주장을 숫자와 모형을 통해 검증할 수 있는 과학적 체계로 정립하려는 학문이다.\n인과관계는 통계분석이 아닌 모형 설정에서 시작된다.\n인과 관계는 통계적 분석에 의해 ’발견’되는 것이 아니라, 이론이나 경험에 기반해 ’가정’되고, 그 가설이 통계적으로 ’검증’되는 것이다. 즉, 어떤 변수(\\(X\\))가 다른 변수(\\(Y\\))에 영향을 미친다는 인과적 주장 자체는 통계분석의 결과가 아니라, 연구자의 이론적 배경, 선행 연구, 또는 실험적 설계에 근거한 전제로 설정된다. 이러한 전제는 통계 모형을 구성할 때 반드시 명확히 표현되어야 하며, 그에 따라 추정과 검정이 이루어진다.\n예를 들어, ”공부 시간이 많을수록 시험 점수가 높아진다”는 가설은 이론적으로 합리적일 수 있지만, 그 자체는 데이터가 말해주는 것이 아니다. 이 가설이 옳은지를 통계적 분석을 통해 검증하는 것이 통계학의 역할이다.\n변수의 역할: 원인과 결과\n인과관계를 분석하기 위해서는 변수들 사이의 역할을 명확히 구분해야 한다. 일반적으로 인과관계는 다음과 같은 두 변수로 구성된다:\n\n독립변수 (X): 원인으로 작용하는 변수이다. ’독립변수’ 외에도 요인(factor), 처리변수(treatment), 예측변수(predictor), 설명변수(explanatory variable), 내생변수(endogenous variable) 등의 용어로 불리기도 한다.\n종속변수 (Y): 결과로 나타나는 변수이며, 독립변수의 변화에 영향을 받는다. 통계학에서는 반응변수(response variable), 목표변수(target variable), 결과변수(outcome), 외생변수(exogenous variable) 등의 명칭으로도 사용된다.\n\n예를 들어, 실험에서 비료의 종류가 벼 수확량에 미치는 영향을 분석하고자 할 때, 비료는 독립변수, 수확량은 종속변수가 된다.\n인과관계와 통계 분석의 관계\n통계적 모형, 예를 들어 회귀모형이나 분산분석 모형은 이러한 변수 간의 인과 구조를 수학적으로 표현한 도구이다. 그러나 통계적 유의성은 인과성 그 자체를 보장하지 않는다. 모형의 타당성은 분석 이전의 이론적 설계에 달려 있으며, 분석 결과는 그 이론을 지지하거나 반박할 수 있는 하나의 증거일 뿐이다. 따라서 인과관계를 제대로 이해하고 설명하기 위해서는 다음 세 가지가 모두 필요하다:\n\n타당한 이론적 가설 설정\n적절한 통계적 분석 모형 선택\n해석의 주의와 제한점 인식\n\n결론적으로, 통계 분석은 인과관계를 ”발견”하기 위한 도구가 아니라, 이미 제시된 인과 가설이 데이터에 의해 지지되는지를 평가하는 수단이다. 따라서 인과 추론은 통계학의 계산 능력뿐 아니라, 이론적 판단력과 연구 설계의 엄밀성을 요구하는 통합적 작업이라 할 수 있다.\n\n\n(4) 데이터의 표현 형식과 분석 가능성\n현대의 통계학과 데이터 과학은 단순한 숫자 자료뿐 아니라 다양한 형태의 비정형 데이터를 포함하여 다룰 수 있는 범위를 넓혀가고 있다. 데이터는 수집되는 방식이나 저장 형식에 따라 서로 다른 포맷으로 존재하며, 이를 어떻게 해석하고 처리할 수 있는지에 따라 분석 방법이 달라진다.통계학에서는 데이터를 수치화하여 분석하는 것이 일반적이지만, 문자, 음성, 영상 등 비정형 데이터도 일정한 전처리를 거쳐 수치 형태로 전환 가능하다.\n\n\n\n\n\n숫자(numeric) 형태의 데이터\n통계학에서 가장 기본이 되는 데이터 형식은 숫자이다. 이는 연속형 또는 이산형 수치로 존재하며, 대부분의 통계적 기법은 이러한 수치 데이터를 대상으로 설계되어 있다. 그러나 모든 숫자가 수학적으로 해석 가능한 것은 아니며, 숫자로 표현된 범주형(명목형) 데이터는 주의가 필요하다. 예를 들어, 성별을 남성(1), 여성(2)으로 부호화한다고 해도, ’2는 1보다 크다’는 수학적 해석은 적용되지 않는다. 이 경우, 숫자는 단지 범주를 구분하는 용도로 사용되며, 통계 소프트웨어에서는 class 변수나 factor 변수로 인식하여 범주형 처리한다.\n문자(text) 데이터\n문자 데이터는 자연어로 기록된 정보로, 전통적인 통계 분석보다는 텍스트 마이닝이나 자연어 처리(NLP) 기법을 통해 분석된다. 설문 응답의 자유서술형 항목, 뉴스 기사, SNS 댓글, 논문 초록, 이메일 내용 등 다양한 형태의 텍스트가 이에 해당한다. 문자 데이터는 전처리 과정을 거쳐 수치형 데이터로 변환된다. 예를 들어 단어의 출현 빈도를 기반으로 문서를 수치화하거나, 특정 키워드를 시각화한 워드 클라우드 형태로 분석 결과를 표현하기도 한다.\n음성(audio) 데이터\n음성은 인간의 대화를 포함한 소리의 파형으로, 본래는 오디오 파일 형식(mp3, wav 등)으로 저장된다. 하지만 이러한 파일도 디지털 신호처리 기술을 통해 시간 단위의 수치 데이터로 변환되며, 이를 통해 음성 분석, 감정 분석, 화자 인식 등 다양한 분석이 가능해진다.\n예를 들어, 음성 파일에서 추출된 주파수, 진폭, 속도, 높낮이 등의 정보는 숫자 벡터로 표현되며, 기계학습이나 통계적 분류 모형에 입력값으로 활용될 수 있다. 반대로, 인공지능 기반 음성합성(TTS)에서는 숫자 데이터를 다시 음성으로 변환하기도 한다.\n\n\n\n\n\n데이터 형식 간의 경계와 확장성\n현대 통계학과 데이터 과학에서는 숫자, 문자, 음성, 영상, 센서 데이터 등 다양한 데이터 유형이 서로 융합되어 분석된다. 기존에는 통계학이 숫자 중심의 학문으로 이해되었지만, 이제는 다양한 데이터 포맷을 수치화하고 해석하는 기술이 통계학자의 중요한 역량이 되고 있다.\n데이터가 어떤 형식으로 존재하든지 간에, 중요한 것은 분석 가능성이며, 이를 위해서는 데이터의 본질과 의미를 이해한 뒤, 적절한 방법으로 구조화하거나 수치화하는 것이 필수적이다.\n\n\n\n\nchapter 2. 데이터와 확률변수\n\n1. 확률이란 무엇인가?\n확률(probability)은 어떤 사건이 발생할 가능성을 의미한다. 일상 대화에서 사람들은 확실한 표현을 선호한다. 예를 들어, ”오늘 비가 올 거야” 또는 ”정오에 만나자”와 같이 말한다. 그러나 현실에서 확실한 일은 거의 없으며, 우리의 삶은 대부분 확률적(probablistic) 요소로 구성되어 있다.\n하늘이 아무리 어둡더라도 비가 올 것을 100% 확신할 수는 없다. 비가 올 가능성이 높더라도, 오지 않을 가능성은 작지만 존재한다. 마찬가지로 누군가와 정오에 만나기로 약속했더라도, 타이어 펑크, 교통 체증, 갑작스러운 업무 지시와 같은 변수로 인해 제시간에 도착하지 못할 확률이 있다.\n우리는 대부분의 일을 확률적인 현상으로 인식하지 않고 확정된 사실처럼 표현한다. 이는 확률을 다루는 일이 불편하고 복잡하게 느껴지기 때문이다. 예를 들어 ”내일까지 우유가 더 필요할 것 같아”라고 말하는 것이 ”내일까지 우유가 하나 더 필요할 확률이 95%야”라고 말하는 것보다 훨씬 간단하다. 이러한 이유로 사람들은 확률에 대한 직관이 부족하며, 실제보다 확률을 과소평가하거나 잘못 이해하는 경우가 많다.\n관찰 편향이 확률 추정의 오류를 유발한다.\n관찰 편향(observation bias)이란, 어떤 사건을 직접 겪거나 목격한 경험이 해당 사건의 실제 발생 빈도를 왜곡해 인식하게 만드는 현상이다. 이 경우 사람들은 실제보다 더 자주 일어나는 것으로 착각하게 된다.\n대표적인 사례가 항공기 사고에 대한 두려움이다. 많은 이들이 비행기를 탈 때 ”착륙하면 연락해”라는 당부를 받지만, 통계적으로는 비행 중보다 공항까지 이동하는 자동차 운전이 훨씬 위험하다. 2013년 미국의 경우 자동차 사고 사망자는 약 3만4천 명, 상업용 항공기 사고 사망자는 5명에 불과했다. 이동 거리를 고려하지 않더라도, 자동차 이용이 항공기 이용보다 수천 배 더 위험했다.\n물론 위험 수준은 이동 거리와 연관된다. 자동차는 주행 거리가 길어질수록 위험이 커지고, 항공기는 주로 이륙과 착륙 구간에 위험이 집중된다. 하지만 하루 30마일을 운전하는 사람은 매일 500마일을 비행하는 사람보다 여전히 몇 배 높은 위험에 노출된다.\n총기에 대한 공포 역시 비슷하다. 미국에서는 ’공격용 소총’에 의한 사망보다 맨주먹이나 발길질로 인한 사망이 두 배 많다. 그럼에도 불구하고 총기에 대한 위험 인식이 더 큰 이유는, 희귀하지만 충격적인 사건이 뉴스에서 크게 다뤄지기 때문이다.\n언론 보도의 특성상, 일상적으로 발생하는 사건(예: 교통사고)은 잘 다뤄지지 않지만, 드문 사건(예: 항공기 추락)은 대규모 뉴스로 전파된다. 결과적으로 사람들은 뉴스에 자주 등장하는 사건을 실제보다 흔한 일로 착각하게 된다.\n반복적 노출은 간접적인 관찰 편향을 초래한다.\n관찰 편향의 한 형태는 동일한 주장이나 정보를 반복적으로 접하면서 형성된다. 예를 들어, ”어린이들은 스마트폰 때문에 공부에 집중하지 못한다”는 말을 방송, 기사, 주변 대화에서 여러 번 듣다 보면, 실제 통계 자료를 확인하지 않았더라도 그 주장이 사실이라고 믿게 될 수 있다.\n이처럼 반복적 노출은 사건의 실제 빈도나 영향력을 과대평가하게 만들며, 경험적 검증 없이도 확신을 형성하게 하는 간접적 관찰 편향을 유발한다.\n아무리 드문 사건이라도 시도 횟수가 충분하면 결국 발생한다.\n지구와 소행성 충돌 가능성에 적용해 보자. 1년 안에 지구에 위험한 소행성이 충돌할 확률은 0.0003%로, 단기간에는 사실상 발생하지 않는 사건이다. 그러나 이 확률이 1만 년 동안 매년 반복되면 누적 확률은 약 3%로 증가한다. 10만 년이면 약 30%, 100만 년이면 약 96%에 이른다.\n결국, 발생 확률이 아무리 작아도 시도 횟수나 시간이 충분히 길면 언젠가는 일어나는 것이 확률의 본질이다.\n\n\n2. 확률변수와 데이터\n통계학은 데이터를 수집하고 요약하는 기술에서 나아가, 그 데이터가 관측되기 전부터 어떤 값이 나올 가능성이 얼마나 되는지를 미리 수학적으로 설명하려는 이론적 틀, 즉 확률 이론을 바탕으로 한다. 이러한 확률 이론의 핵심 개념이 바로 확률변수이며, 통계학에서 다루는 데이터는 사실상 이 확률변수의 실현값이다.\n확률변수란 무엇인가?\n우리가 통계 분석을 통해 다루는 데이터는 단순한 숫자의 나열이 아니라, 불확실한 현상의 결과로서 관측된 값들이다. 이러한 불확실한 수치 결과를 수학적으로 다루기 위한 도구가 바로 확률변수이다. 확률변수(random variable)는 어떤 실험이나 조사에서 관측 가능한 결과를 숫자로 대응시키는 함수이다.\n예를 들어, 동전을 던져 앞(Head)이 나오면 1, 뒤(Tail)이 나오면 0으로 대응시키는 것이 확률변수의 가장 단순한 예이다. 즉, 확률변수는 모든 가능한 관측 결과를 수치로 표현함으로써, 이후의 통계적 계산과 해석이 가능하게 한다.\n데이터와 확률변수의 연결\n통계학에서는 특정 현상에 대해 자료를 수집하고 분석한다. 이때 수집되는 데이터는 확률변수가 실제로 실현된 결과이며, 관측값 하나하나는 확률변수가 취할 수 있는 값 중 하나다. 예를 들어 H대학교 재학생 200명의 키 데이터를 수집하였다면, 이는 ’재학생의 키’라는 확률변수가 현실에서 특정 학생에게서 나타난 값(실현값)의 집합이다.\n정리하면, 데이터는 확률변수의 표본 실현값, 즉 관측된 값들이며,확률변수는 그 데이터를 산출한 수학적 메커니즘이다.\n확률분포와 확률밀도함수\n확률변수는 단지 수치 값을 가질 수 있다는 사실뿐만 아니라, 어떤 값이 얼마나 자주, 또는 얼마나 자주 발생할 가능성이 있는가에 대한 정보도 함께 가진다. 이러한 정보를 정리한 것이 바로 확률분포이다. 확률분포는 확률변수가 가질 수 있는 값과, 그 값이 발생할 가능성(확률) 사이의 관계를 표현한 함수이다. 확률분포는 이산형 확률변수의 경우 확률질량함수로, 연속형 확률변수의 경우 확률밀도함수로 정의된다.\n확률밀도함수(PDF)의 의미\n연속형 확률변수에 대해, 확률밀도함수 \\(f(x)\\)는 다음과 같은 의미를 가진다:\n\\(f(x)\\) 자체는 특정 값 \\(x\\)의 확률이 아니라, 해당 점에서의 상대적인 가능성을 나타내는 밀도이다.\n실제 확률은 구간 단위로 계산되며, 예를 들어 \\(a \\leq X \\leq b\\)의 확률은 \\(P(a \\leq X \\leq b) = \\int_{a}^{b}f(x)dx\\) 로 주어진다.\nPDF는 데이터가 가진 모든 정보를 담고 있다.\n확률밀도함수는 단순히 가능성만 나타내는 것이 아니라, 데이터가 가질 수 있는 전체적인 구조적 특성을 요약한다. 예를 들어,\n\\(f(x)\\)의 정점은 가장 자주 관측되는 값(최빈값, mode)을 의미\n곡선의 넓이는 특정 범위 내에서 값이 나타날 가능성(확률)을 보여준다.\n분포의 비대칭성(skewness)이나 퍼짐(spread) 등도 함수 형태를 통해 직관적으로 이해할 수 있다.\n또한 확률밀도함수는 데이터가 어떤 값을 중심으로 분포하는지(중심위치), 어떤 구간에서 데이터가 주로 나타나는지(분포 범위), 극단적 값은 얼마나 발생하는지(꼬리의 두께) 등에 대한 정보를 모두 포함한다.\n데이터의 수학적 해석을 위한 핵심 개념\n데이터 분석은 본질적으로 확률변수의 실현값을 관찰하고, 이를 바탕으로 확률분포를 추정하거나 검정하는 과정이다. 따라서 확률변수와 확률밀도함수의 개념을 이해하는 것은 단순한 수학적 이론을 넘어,현실 세계의 불확실성을 수치적으로 해석하고 설명하는 데 있어 핵심적인 출발점이다. 데이터는 단순한 숫자의 나열이 아니라, 확률적 구조를 가진 함수적 대상이다. 따라서 통계 분석은 이러한 구조를 바탕으로 이루어지며, 데이터에 기반한 분석의 흐름을 이해하는 것은 통계학의 핵심이다.\n\n\n\nchapter 3. 데이터와 요약통계\n일변량 분석은 하나의 변수에 대한 자료 분포를 이해하고 요약하는 데 목적을 둔다. 이때 가장 기본적이고 핵심적인 분석 도구는 숫자 요약이며, 이는 주어진 데이터가 어떠한 중심 경향, 산포, 그리고 분포 형태를 갖고 있는지를 수치적으로 요약하여 설명한다.\n\n1. 범주형 데이터\n범주형 변수는 관측값이 수치가 아닌 범주 또는 집단 수준에서 구분되는 변수를 의미한다. 예를 들어, 성별(남/여), 지역(서울/부산/광주), 고객 등급(일반/우수/VIP) 등이 이에 해당한다. 이러한 변수에 대한 일변량 분석은 수치적 요약보다는 빈도와 비율을 통한 분포의 구조 파악에 초점을 둔다.\n숫자요약: 빈도분석\n범주형 변수에서 가장 기본적인 분석은 각 범주에 속하는 관측값의 수를 세는 것이다. 이를 빈도라고 하며, 전체 관측 수에 대한 상대빈도(비율)도 함께 제시한다. 범주형 데이터의 상대빈도는 (실증적) 확률밀도함수이다.\n시각화 도구\n데이터의 시각화는 비전문가도 자료 구조를 직관적으로 이해하는 데 매우 유용하다. 범주형 변수의 분포를 시각적으로 표현하기 위해 자주 사용되는 도구는 다음과 같다.\n막대그래프(Bar Chart): 각 범주에 해당하는 빈도 또는 비율을 막대의 높이로 나타냄\n원그래프(Pie Chart): 전체에서 각 범주가 차지하는 비율을 원의 부채꼴 면적으로 표현 (하지만 해석상 막대그래프보다 덜 직관적일 수 있음)\nimport seaborn as sns\nimport pandas as pd\nimport matplotlib.pyplot as plt\n# 데이터 불러오기\ntitanic = sns.load_dataset(\"titanic\")\n# 2. embarked 변수의 빈도 계산\nfreq_table = titanic['embarked'].value_counts(dropna=False)\nrel_freq_table = titanic['embarked'].value_counts(normalize=True, dropna=False)\n# 3. 빈도표와 상대빈도표 결합\nembarked_summary = pd.DataFrame({\n    '빈도': freq_table,\n    '상대빈도': rel_freq_table.round(4)\n})\nprint(embarked_summary)\n# 막대그래프 그리기\nplt.figure(figsize=(6, 4))\nax = rel_freq_table.plot(kind='bar', color='lightgreen')\nplt.title(\"Embarked relative freq.\")\nplt.xlabel(\"Embarked\")\nplt.ylabel(\"Relative frequency\")\nplt.ylim(0, 1)\nplt.xticks(rotation=0)\nplt.grid(axis='y', linestyle='--', alpha=0.7)\n# 막대 위에 상대빈도 값 표시\nfor i, v in enumerate(rel_freq_table):\n    ax.text(i, v + 0.02, f\"{v:.4f}\", ha='center', va='bottom', fontsize=10)\nplt.tight_layout()\nplt.show()\nembarked 빈도 상대빈도  S 644 0.7228  C 168 0.1886  Q 77 0.0864  NaN 2 0.0022\n\n\n\n\n\n\n\n2. 측정형 데이터\n측정형 자료는 수치적 값을 가지며, 관측값 간의 간격이나 비율이 의미 있는 변수로 구성된다. 이러한 자료에 대한 일변량 분석은 한 변수의 분포를 중심, 산포, 형태 등 다양한 측면에서 요약하고 해석하는 데 목적이 있다. 주요 요약 지표로는 중심 경향, 산포도, 분포 형태, 그리고 상대적 산포(변동계수) 등이 있다.\n중심경향 척도\n중심 경향은 데이터가 어디에 몰려 있는가를 나타낸다. 주요 대표값으로는 평균, 중앙값, 최빈값이 있다.\n\n산술평균 mean: \\(\\overline{x} = \\frac{1}{n}\\overset{n}{\\sum_{i = 1}}x_{i}\\), 산술평균은 전체 자료의 평균적인 크기를 나타내는 중심 척도로, 계산이 간단하고 널리 사용되지만 이상값에 민감하다는 단점을 가진다.\n중앙값 median: \\(MD = x_{((n + 1)/2)}\\) 중앙값은 데이터를 크기순으로 정렬했을 때 정중앙에 위치한 값으로, 이상값이나 극단값의 영향을 거의 받지 않는 대표적인 위치 척도이다.\n최빈값 mode: 가장 자주 나타나는 값으로, 특히 이산형 자료에서 중심 경향을 설명하는 데 유용하다.\n\n산술평균 대안 중심경향 척도\n산술평균은 자료의 중심 경향을 나타내는 가장 일반적인 지표이지만, 극단값(이상값)에 민감하다는 단점이 있다. 이에 따라 통계 분석에서는 특정 상황에 맞는 대안적 평균값을 사용하여 평균의 왜곡을 줄이고, 자료 특성에 맞는 해석을 가능하게 한다. 대표적으로 기하평균, 윈저화 평균, 절사 평균이 있다.\n기하평균 (Geometric Mean): 기하평균은 모든 값의 곱을 표본크기 \\(n\\)-제곱근 한 값으로 정의된다. 주로 양의 실수 값에서만 정의되며, 특히 비율 자료나 성장률 분석에 적합하다. \\(\\text{기하평균} = \\left( \\prod_{i = 1}^{n}x_{i} \\right)^{\\frac{1}{n}}\\), \\(\\log(\\text{기하평균}) = \\frac{1}{n}\\overset{n}{\\sum_{i = 1}}\\log x_{i}\\)\n\n비율 또는 성장률 데이터 (예: 투자 수익률, 물가 상승률 등)\n데이터가 지수적 또는 곱셈 구조를 가질 때 적절\n극단적으로 큰 값에 덜 민감함\n예: 연 5%, 10%, -2% 수익률 → 산술평균은 왜곡될 수 있음\n\n조화평균 (Harmonic Mean): 조화평균은 각 관측값의 역수의 평균의 역수로 정의된다. 주로 속도, 밀도, 단위당 가격처럼 ”정해진 양에 걸리는 시간” 또는 ”단위당 값”이 중요할 때 사용된다.\n\\[\\text{조화평균} = \\frac{n}{\\sum_{i = 1}^{n}\\frac{1}{x_{i}}},x_{i} &gt; 0\\]\n\n가장 작은 값에 민감하다 (작은 값이 전체 평균을 크게 끌어내림)\n산술평균보다 항상 작거나 같으며, 비율 자료에 적합\n관측값 \\(x_{i}\\) 중 0이 포함되면 정의되지 않음\n사용 사례: 평균 속도 계산: 왕복 60km/h ↔︎ 40km/h → 평균속도는 산술평균(50km/h)이 아니라 조화평균(48km/h)\n효율성 평가, 투자 수익률에서 단위당 성과비교 시 적합\n\n절사 평균 (Trimmed Mean): 자료의 상하위 일부를 제거하고, 나머지 값으로 평균을 구하는 방법이다. 보통 상하위 \\(k\\%\\)의 값을 제거하며, 중심부의 데이터만으로 평균을 산출한다.\n\n이상값에 민감한 평균의 보완\n산술평균과 중앙값의 절충적인 대표값\n소득, 주가, 측정 오차 포함 데이터 등에서 유용\n예: 5% 절사평균(5% trimmed mean)\n\n윈저화 평균 (Winsorized Mean): 절사 평균과 달리, 극단값을 제거하지 않고 상하위 극단값을 중앙값에 가까운 값으로 치환한 후 평균을 계산한다.\n\n이상값의 영향 완화 + 데이터 유지\n데이터 손실 없이 안정된 평균을 추정\nrobust 통계학에서 빈번히 사용\n\n중앙절사평균 (Midmean 또는 Interquartile Mean): 전체 데이터 중 중간 50% 구간(제1사분위수 Q₁ ~ 제3사분위수 Q₃)에 해당하는 값들의 평균을 의미한다. 이는 절사평균의 일종이지만, 중앙 부분만 반영한다는 점에서 특수한 형태이다.\n\n이상값이나 극단값 제거 효과가 있음\n평균보다 robust(강건)하며, 중앙값보다 더 많은 정보를 반영\n정규분포에 가까운 자료에서는 산술평균과 유사\n사용 사례: 시험점수 평가, 수익률 분석, 스포츠 채점 등 이상값 영향을 줄이려는 경우, 데이터의 핵심 분포구간(IQR)을 중심으로 대표값 추정\n\n가중평균 (Weighted Mean): 가중평균은 각 데이터에 중요도 또는 빈도에 해당하는 가중치를 부여하여 계산하는 평균이다. 모든 데이터가 동일한 영향력을 가진다는 가정이 부적절할 때 사용된다.\n\\[\\overline{x}w = \\frac{\\sum{i = 1}^{n}w_{i}x_{i}}{\\sum_{i = 1}^{n}w_{i}},w_{i} &gt; 0\\]\n\n\\(w_{i}\\)는 가중치이며, 가중치가 클수록 해당 x_i의 영향력도 커짐\n산술평균은 \\(w_{i} = 1\\)일 때의 특수한 경우\n표본 크기, 신뢰도, 빈도수 등에 따라 가중치를 조정할 수 있음\n사용 사례: 성적 계산 (중간고사 40%, 기말고사 60%), 복합지수 계산 (소비자물가지수, 교육지수 등), 층화 표본조사에서 표본 비율 보정\n\n퀀타일 평균 (Quantile Mean 또는 Truncated Mean): 특정 분위수 범위(예: 하위 10%~상위 90%)에 속하는 관측값만을 사용하여 평균을 계산하는 방식이다. 이는 극단값을 제거하고 특정 분위수 구간 내의 평균적인 경향을 요약하는데 효과적이다.\n\n절사평균보다 유연한 구조 (구간을 조절 가능)\n중앙절사평균과 유사하지만 범위를 사용자가 조절\n분위수 구간 이외의 관측값은 무시되므로 분포 외곽값의 영향 최소화\n사용 사례: 소득분포 분석 시 중간층 평균 소득 평가, 신뢰도 낮은 측정값 제거 후 대표값 산출, Robust 회귀나 경제통계의 분포 요약\n\n흩어짐 척도\n산포는 데이터가 평균을 중심으로 얼마나 퍼져 있는지를 나타낸다\n분산 variance: \\(s^{2} = \\frac{1}{n - 1}\\overset{n}{\\sum_{i = 1}}(x_{i} - \\overline{x})^{2}\\)\n표준편차 standard deviation: \\(s = \\sqrt{s^{2}}\\), 분산의 제곱근으로, 원래 데이터와 동일한 단위로 표현된다.\n범위 range: \\(\\text{Range} = \\max(x) - \\min(x)\\)\n사분위 범위 interquartile range:\\(\\text{IQR} = Q_{3} - Q_{1}\\), 중간 50% 데이터의 범위이며, 이상값에 덜 민감하다.\n상대적 산포\n변동계수(CV)는 표준편차를 평균으로 나눈 비율로, 자료의 상대적 산포 크기를 나타낸다. 서로 단위가 다르거나, 평균 크기가 현저히 다른 두 변수의 산포 비교에 유용하다.\n\\(\\text{CV} = \\frac{s}{\\overline{x}} \\times 100\\%\\), 여기서 \\(\\overline{x}\\)는 평균, \\(s\\)는 표준편차이다.\n변동계수는 단위가 없는 지표로서, 변동의 상대적 크기를 비교하거나 데이터 안정성 판단에 사용된다. 예를 들어, 두 장비의 출력이 각각 \\(50W( \\pm 2W)\\)와 \\(100W( \\pm 5W)\\)일 때, 표준편차는 둘째 장비가 크지만 CV는 둘째가 더 안정적일 수 있다.\n분포의 형태\n분포의 형태는 자료가 좌우로 대칭적인지, 중심에 집중되어 있는지를 나타낸다.\n왜도 skewness:\\(\\text{Skewness} = \\frac{1}{n}\\overset{n}{\\sum_{i = 1}}\\left( \\frac{x_{i} - \\overline{x}}{s} \\right)^{3}\\), 값이 0에 가까우면 대칭 분포, 양수면 오른쪽 꼬리, 음수면 왼쪽 꼬리가 긴 분포이다.\n첨도 kurtosis: \\(\\text{Kurtosis} = \\frac{1}{n}\\overset{n}{\\sum_{i = 1}}\\left( \\frac{x_{i} - \\overline{x}}{s} \\right)^{4}\\), 정규분포의 첨도는 3이며, 이보다 크면 뾰족, 작으면 평평한 분포이다.\n시각화 도구\n\n히스토그램 histogram: 연속 데이터를 구간별로 나누어 빈도를 막대그래프로 표현\n상자그림 boxplot: 중앙값, 사분위수, 이상값을 함께 보여주는 요약 도표\n커널함수 Kernel Density Estimation: 히스토그램처럼 구간을 나누지 않고, 각 데이터 점마다 커널(kernel) 함수를 얹어서 전체를 합쳐 확률밀도 함수를 근사\n\nimport seaborn as sns\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom scipy.stats import trim_mean\nfrom scipy.stats.mstats import winsorize\nimport numpy as np\n# 데이터 불러오기 및 결측치 제거\ntitanic = sns.load_dataset(\"titanic\")\nage = titanic['age'].dropna()\n# 시각화: 히스토그램, 상자그림, KDE\nfig, axs = plt.subplots(3, 1, figsize=(8, 10))\n# 1. 히스토그램\naxs[0].hist(age, bins=20, color='skyblue', edgecolor='black')\naxs[0].set_title('Histogram of Age')\naxs[0].set_xlabel('Age')\naxs[0].set_ylabel('Frequency')\n# 2. 상자그림\naxs[1].boxplot(age, vert=False)\naxs[1].set_title('Boxplot of Age')\naxs[1].set_xlabel('Age')\n# 3. KDE (커널 밀도 추정)\nsns.kdeplot(age, ax=axs[2], fill=True, color='green')\naxs[2].set_title('Kernel Density Estimate of Age')\naxs[2].set_xlabel('Age')\nplt.tight_layout()\nplt.show()\n# 통계량 계산\nmean_age = age.mean()\nstd_age = age.std()\ncv_age = std_age / mean_age * 100\ntrimmed_mean_5 = trim_mean(age, proportiontocut=0.05)\nwinsorized_age = winsorize(age, limits=0.05)\nwinsorized_mean_5 = winsorized_age.mean()\n# 출력\nprint(f\"산술평균: {mean_age:.2f}\")\nprint(f\"표본 표준편차: {std_age:.2f}\")\nprint(f\"변동계수: {cv_age:.2f}%\")\nprint(f\"5% 절사평균: {trimmed_mean_5:.2f}\")\nprint(f\"5% 윈저화 평균: {winsorized_mean_5:.2f}\")\n산술평균: 29.70  표본 표준편차: 14.53  변동계수: 48.91%  5% 절사평균: 29.38  5% 윈저화 평균: 29.44"
  },
  {
    "objectID": "consult.html",
    "href": "consult.html",
    "title": "통계상담",
    "section": "",
    "text": "📋 통계상담 안내\n데이터 분석, 통계 해석, 설문 설계 등 상담이 필요하신 분은 아래 폼을 제출해 주세요.\n👉 상담 신청하기\n\n온라인/비대면 상담 가능합니다."
  },
  {
    "objectID": "cardnews/news002.html",
    "href": "cardnews/news002.html",
    "title": "출산율 감소",
    "section": "",
    "text": "👶 출산율 감소, 바닥을 뚫다\n\n\n\n출산\n\n\n\n2023년 합계출산율: 0.72명\n전 세계 최저\n지방소멸 → 학교 폐교 → 일자리 축소의 악순환\n\n\n데이터가 보여주는 출산의 현실"
  },
  {
    "objectID": "cardnews/news004.html",
    "href": "cardnews/news004.html",
    "title": "전공별 취업률 격차",
    "section": "",
    "text": "title: “전공별 취업률 격차” format: html page-layout: full —"
  },
  {
    "objectID": "cardnews/news004.html#왜-이런-차이가-날까",
    "href": "cardnews/news004.html#왜-이런-차이가-날까",
    "title": "전공별 취업률 격차",
    "section": "💡 왜 이런 차이가 날까?",
    "text": "💡 왜 이런 차이가 날까?\n\n산업 수요와의 불균형\n졸업 후 진로 다양성, 인프라 차이\n지역 대학일수록 격차 더 큼"
  },
  {
    "objectID": "cardnews/news004.html#졸업생-1인의-목소리",
    "href": "cardnews/news004.html#졸업생-1인의-목소리",
    "title": "전공별 취업률 격차",
    "section": "💬 졸업생 1인의 목소리",
    "text": "💬 졸업생 1인의 목소리\n\n“취업률 통계는 높지만,\n실제로는 계약직, 인턴이 대부분이에요.”\n(사회계열 졸업생 인터뷰 중)"
  },
  {
    "objectID": "cardnews/news004.html#통계의-해석은-숫자-너머",
    "href": "cardnews/news004.html#통계의-해석은-숫자-너머",
    "title": "전공별 취업률 격차",
    "section": "📚 통계의 해석은 숫자 너머",
    "text": "📚 통계의 해석은 숫자 너머\n통계는 단순 수치보다 맥락과 경험을 함께 살필 때 의미를 갖습니다.\n\n전공 선택이 삶 전체에 어떤 영향을 주는가,\n그 통계로 함께 이야기해야 합니다."
  }
]