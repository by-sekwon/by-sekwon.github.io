<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.32">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>회귀분석 2. 변수선택 – 세상의 모든 통계 이야기</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-37eea08aefeeee20ff55810ff984fec1.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-c1fac2584b48ed01fb6e278e36375074.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="floating nav-fixed quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">세상의 모든 통계 이야기</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">기초수학</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-">    
        <li>
    <a class="dropdown-item" href="../../notes/math/function.html">
 <span class="dropdown-text">함수</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../notes/math/function_slides.html">
 <span class="dropdown-text">함수(슬라이드)</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../notes/math/derivate_integral.html">
 <span class="dropdown-text">미분적분</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../notes/math/vector.html">
 <span class="dropdown-text">벡터</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../notes/math/matrix.html">
 <span class="dropdown-text">행렬</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu--1" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">수리통계</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu--1">    
        <li>
    <a class="dropdown-item" href="../../notes/math_stat/probability.html">
 <span class="dropdown-text">확률</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../notes/math_stat/random_variable.html">
 <span class="dropdown-text">확률변수</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../notes/math_stat/famous_distribution.html">
 <span class="dropdown-text">유명한분포</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../notes/math_stat/multi_variate.html">
 <span class="dropdown-text">다변량확률변수</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../notes/math_stat/random_sample.html">
 <span class="dropdown-text">확률표본_난수생성</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../notes/math_stat/estimation.html">
 <span class="dropdown-text">추정</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../notes/math_stat/hypothesis_testing.html">
 <span class="dropdown-text">가설검정_신뢰구간</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu--2" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">조사방법론</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu--2">    
        <li>
    <a class="dropdown-item" href="../../notes/survey/survey_intro.html">
 <span class="dropdown-text">조사방법 기초</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../notes/survey/sample_design.html">
 <span class="dropdown-text">표본설계</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../notes/survey/questionnaire.html">
 <span class="dropdown-text">설문지</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../notes/survey/nonresponse.html">
 <span class="dropdown-text">무응답 대체</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../notes/survey/data_process.html">
 <span class="dropdown-text">데이터 처리</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../notes/survey/survey_scale.html">
 <span class="dropdown-text">조사지 척도</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../notes/survey/delphi_ahp_conjoint.html">
 <span class="dropdown-text">델파이_AHP_컨조인트</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../notes/survey/psm.html">
 <span class="dropdown-text">PSM 성향점수매칭</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu--3" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">일변량분석</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu--3">    
        <li>
    <a class="dropdown-item" href="../../notes/intro_stat/concept_of_stat.html">
 <span class="dropdown-text">일변량분석 개념</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../notes/intro_stat/data.html">
 <span class="dropdown-text">데이터와 통계</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../notes/intro_stat/univariate.html">
 <span class="dropdown-text">일변량분석</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../notes/intro_stat/crosstab.html">
 <span class="dropdown-text">교차표분석</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../notes/intro_stat/goodness_of_fits.html">
 <span class="dropdown-text">적합성검정</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../notes/intro_stat/normality.html">
 <span class="dropdown-text">정규변환</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../notes/intro_stat/correlation.html">
 <span class="dropdown-text">상관분석</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../notes/intro_stat/anova.html">
 <span class="dropdown-text">실험설계 분산분석</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../notes/intro_stat/time_series.html">
 <span class="dropdown-text">시계열분석</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu--4" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">회귀분석</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu--4">    
        <li>
    <a class="dropdown-item" href="../../notes/linear_model/lm_concept.html">
 <span class="dropdown-text">개념&amp;추정</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../notes/linear_model/lm_selection.html">
 <span class="dropdown-text">변수선택</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../notes/linear_model/lm_multicolin.html">
 <span class="dropdown-text">다중공선성</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../notes/linear_model/lm_diagnosis.html">
 <span class="dropdown-text">회귀진단</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../notes/linear_model/lm_logistic.html">
 <span class="dropdown-text">로지스틱회귀</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item">
    <a class="nav-link" href="../../cardnews/index.html"> 
<span class="menu-text">카드뉴스</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../consult.html"> 
<span class="menu-text">통계상담</span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu--5" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">📡스트리밍</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu--5">    
        <li>
    <a class="dropdown-item" href="https://by-sekwonappio-esqshnv7wueapp4da6jrizn.streamlit.app" target="_blank">
 <span class="dropdown-text">실시간주가[5대종목]</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://by-sekwonappio-k5e5n7wasvj3kveyqbwmgc.streamlit.app/" target="_blank">
 <span class="dropdown-text">대전유성구 일기예보</span></a>
  </li>  
    </ul>
  </li>
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">목차</h2>
   
  <ul>
  <li><a href="#chapter-1.-변수선택-개념" id="toc-chapter-1.-변수선택-개념" class="nav-link active" data-scroll-target="#chapter-1.-변수선택-개념"><span style="color:green">chapter 1. 변수선택 개념</span></a></li>
  <li><a href="#chapter-2.-변수선택-고전적-방법" id="toc-chapter-2.-변수선택-고전적-방법" class="nav-link" data-scroll-target="#chapter-2.-변수선택-고전적-방법"><span style="color:green">chapter 2. 변수선택 고전적 방법</span></a>
  <ul>
  <li><a href="#모형-설명력-척도" id="toc-모형-설명력-척도" class="nav-link" data-scroll-target="#모형-설명력-척도"><span style="color:blue"> 1. 모형 설명력 척도</span></a></li>
  <li><a href="#최종-회귀모형-선택-방법-모형비교" id="toc-최종-회귀모형-선택-방법-모형비교" class="nav-link" data-scroll-target="#최종-회귀모형-선택-방법-모형비교"><span style="color:blue"> 2. 최종 회귀모형 선택 방법 : 모형비교</span></a></li>
  <li><a href="#고전적-변수선택-방법" id="toc-고전적-변수선택-방법" class="nav-link" data-scroll-target="#고전적-변수선택-방법"><span style="color:blue"> 3. 고전적 변수선택 방법</span></a></li>
  </ul></li>
  <li><a href="#chapter-3.-변수선택-빅데이터-방법" id="toc-chapter-3.-변수선택-빅데이터-방법" class="nav-link" data-scroll-target="#chapter-3.-변수선택-빅데이터-방법"><span style="color:green">chapter 3. 변수선택 빅데이터 방법 </span></a>
  <ul>
  <li><a href="#과적합과-과소적합" id="toc-과적합과-과소적합" class="nav-link" data-scroll-target="#과적합과-과소적합"><span style="color:blue"> 1. 과적합과 과소적합</span></a></li>
  <li><a href="#l1-and-l2-regularization" id="toc-l1-and-l2-regularization" class="nav-link" data-scroll-target="#l1-and-l2-regularization"><span style="color:blue"> 2. L1 and L2 Regularization</span></a></li>
  <li><a href="#features-enginerring" id="toc-features-enginerring" class="nav-link" data-scroll-target="#features-enginerring"><span style="color:blue"> 3. Features Enginerring</span></a></li>
  <li><a href="#데이터-분할-data-split" id="toc-데이터-분할-data-split" class="nav-link" data-scroll-target="#데이터-분할-data-split"><span style="color:blue"> 4. 데이터 분할 Data Split</span></a></li>
  <li><a href="#사례분석" id="toc-사례분석" class="nav-link" data-scroll-target="#사례분석"><span style="color:blue"> 5. 사례분석</span></a></li>
  </ul></li>
  </ul>
</nav>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar zindex-bottom">
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">회귀분석 2. 변수선택</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p><br></p>
<section id="chapter-1.-변수선택-개념" class="level3">
<h3 class="anchored" data-anchor-id="chapter-1.-변수선택-개념"><span style="color:green">chapter 1. 변수선택 개념</span></h3>
<p>회귀분석에서 첫 단계는 어떤 예측변수를 모형에 포함할 것인가를 결정하는 것이다. 이는 단순히 통계적 절차로만 이루어지는 것이 아니라, 연구자의 이론적 근거와 사전 지식에 바탕을 두고 목표변수 Y에 영향을 줄 것으로 예상되는 변수를 선택하는 과정이다.</p>
<p>이렇게 선정한 예측변수를 이용해 선형 회귀모형을 설정하고, 데이터를 수집하여 회귀계수를 최소자승법(OLS) 으로 추정한다. 추정된 모형이 실제로 목표변수의 변동을 설명할 수 있는지를 판단하기 위해서는 검정 절차가 필요하다.</p>
<p>먼저, 분산분석(ANOVA)의 F-검정을 통해 전체 회귀모형의 유의성을 검정한다.</p>
<ul>
<li><p>귀무가설: 모든 회귀계수는 0이다. <span class="math inline">\(\beta_{1} = \beta_{2} = \cdots = \beta_{k} = 0\)</span></p></li>
<li><p>대립가설: 적어도 하나의 회귀계수는 0이 아니다.</p></li>
</ul>
<p>만약 F-검정에서 귀무가설이 기각되지 않으면, 선택한 예측변수들이 목표변수를 설명하지 못한다는 의미가 된다. 반대로 귀무가설이 기각되면, 모형 전체가 유의하다고 판단할 수 있으며, 이때 개별 예측변수가 유의한지도 살펴보아야 한다.</p>
<p>개별 예측변수의 효과는 t-검정을 통해 확인한다.</p>
<ul>
<li>귀무가설: <span class="math inline">\(\beta_{j} = 0\)</span>, 대립가설: <span class="math inline">\(\beta_{j} \neq 0\)</span></li>
</ul>
<p>t-검정에서 유의하다면 해당 예측변수는 목표변수에 선형적으로 의미 있는 영향을 준다고 해석할 수 있다.</p>
<p><strong>오컴의 면도날(Occam<span dir="rtl">’</span>s Razor)</strong></p>
<p>회귀분석에서 모형을 설정할 때, 가능한 한 단순하면서도 충분히 설명력이 있는 모형을 추구하는 것이 원칙이다. 이러한 사고방식은 철학적 개념인 오컴의 면도날(Occam<span dir="rtl">’</span>s Razor) 과 연결된다. 오컴의 면도날은 “모든 것이 동일하다면 가장 간단한 설명이 최선이다”라는 원리를 말하며, 통계학에서는 이를 경제성의 원리(principle of economy) 또는 절약성의 원리(principle of parsimony) 라고 부른다.</p>
<p>회귀분석에 적용하면, 동일한 설명력을 가진 두 개의 모형이 있다면, 예측변수가 더 적은 모형을 선택하는 것이 바람직하다는 의미가 된다. 즉, 불필요하게 많은 변수를 넣는 것은 해석을 복잡하게 만들고 과적합(overfitting)의 위험까지 초래하기 때문에, 가능한 한 간결한 모형을 유지하는 것이 좋다.</p>
<p>실제 변수 선택 과정에서는 다음과 같은 절차가 자주 사용된다.</p>
<p>1. 먼저 모든 후보 예측변수를 포함한 완전모형(full model) 을 적합한다.</p>
<p>2. 그 후 유의하지 않은 변수를 하나씩 제거해 나간다.</p>
<p>3. 제거할 변수는 가장 유의하지 않은 변수, 즉 t-검정 통계량 절댓값이 가장 작은 변수(동일하게는 p-value가 가장 큰 변수)를 우선적으로 제외한다.</p>
<p>4. 변수를 제거할 때마다 다시 모형을 적합하고, 모든 남아 있는 변수가 유의해질 때까지 이 과정을 반복한다.</p>
<p><strong>다중공선성 분석과 순서 문제</strong></p>
<p>대부분의 교과서에서는 다중공선성 진단을 먼저 설명하고 있으나 어느 단계를 먼저해도 동일 결과를 얻는 경우가 대부분이므로 간편 작업(변수선택 과정을 먼저 거치면 다중공선성 진단을 하면 진단 변수의 수 줄어듬)을 위하여 변수선택을 먼저하는 것이 적절하다.</p>
<p><strong>변수선택 반드시 필요한가?</strong></p>
<p>회귀분석 과정에서 변수 선택(variable selection) 과 다중공선성(multicollinearity) 진단은 모두 필수적인 절차이다. 전통적인 교과서에서는 보통 다중공선성 진단을 먼저 소개하고, 그 후 변수 선택 방법을 설명하는 경우가 많다. 하지만 실제 분석 실무에서는 이 순서를 반드시 고정할 필요는 없다.</p>
<p>왜냐하면, 변수 선택과 다중공선성 진단의 순서를 어떻게 두더라도 최종적으로 동일한 결과를 얻는 경우가 대부분이기 때문이다. 다만, 효율성을 고려한다면 순서를 바꾸는 것이 더 합리적일 수 있다.</p>
<p>특히, 모든 후보 변수를 포함한 상태에서 다중공선성을 먼저 진단하면 불필요한 변수까지 함께 고려해야 하므로 계산과 해석이 번거로워진다. 반대로, 먼저 변수 선택 과정을 거쳐 유의하지 않은 예측변수를 제거하고 난 뒤 다중공선성을 진단하면, 분석해야 할 변수가 줄어들어 작업이 간편해진다.</p>
<p>따라서 실무적으로는 변수 선택 → 다중공선성 진단 순서로 접근하는 것이 적절하다. 이는 분석의 효율성을 높이고, 해석해야 할 변수 집합을 줄여주는 장점이 있다.</p>
<p>교과서에서 <span dir="rtl">”</span>다중공선성 진단을 먼저”라고 가르치는 이유도 나름 있습니다. 만약 강한 공선성이 있으면, t-검정 값이 작게 나와 유의하지 않은 것처럼 보일 수 있습니다. 즉, 공선성 때문에 유효한 변수가 잘려 나갈 위험이 있다는 것이죠. =&gt; 이를 방지하기 위하여 목표변수와 상관계수가 일정 값 이상인 예측변수는 변수선택 과정에서 우선 선택한다.</p>
</section>
<section id="chapter-2.-변수선택-고전적-방법" class="level3">
<h3 class="anchored" data-anchor-id="chapter-2.-변수선택-고전적-방법"><span style="color:green">chapter 2. 변수선택 고전적 방법</span></h3>
<section id="모형-설명력-척도" class="level4">
<h4 class="anchored" data-anchor-id="모형-설명력-척도"><span style="color:blue"> 1. 모형 설명력 척도</span></h4>
<p><strong>결정계수 Determination Coefficient <span class="math inline">\(R_{p}^{2} = \frac{SSR_{p}}{SST_{p}}\)</span></strong></p>
<p>회귀분석에서 결정계수는 모형 내의 예측변수들이 목표변수의 총변동(즉, 목표변수 관측값들의 변동)을 얼마나 잘 설명하는지를 나타내는 수치이므로, 변수 선택의 중요한 지표가 된다. 결정계수는 총변동 중 회귀모형이 설명하는 부분의 비율로 정의되며, 단순회귀의 경우에는 상관계수의 제곱과 동일하다.</p>
<p>결정계수의 크기가 70%라면 설정된 예측변수의 설명력이 충분하다고 볼 수 있고, 80% 이상이면 매우 충분하다고 평가할 수 있다. 90% 이상이면 거의 완전한 설명력을 가진 것으로, 현재 모형에 포함된 예측변수만으로도 목표변수의 변동을 매우 잘 설명할 수 있음을 의미한다. 다만 이 해석은 학문 분야에 따라 상대적인 차이가 있음을 유념해야 한다. 예를 들어, 물리학·공학 분야에서는 자연 현상이 비교적 규칙적이므로 결정계수가 0.9 이상인 경우가 흔하지만, 사회과학·의학 연구에서는 인간 행위나 복잡한 환경 요인으로 인해 <span class="math inline">\(R^{2}\)</span>가 0.3~0.5 정도만 되어도 상당히 높은 설명력으로 간주된다. 따라서 결정계수 크기의 <span dir="rtl">”</span>충분하다”는 판단 기준은 절대적인 것이 아니라 연구 맥락에 따라 달라진다.</p>
<p>결정계수는 예측변수의 개수가 같은 경우, 어떤 변수 집합이 더 큰 설명력을 가지는지를 비교하는 데 유용하다. 그러나 결정계수 자체에는 검정 통계량이 존재하지 않는다는 한계가 있다. 따라서 모형이 유의한지를 확인하기 위해서는 분산분석(ANOVA)을 통한 F-검정을 별도로 실시해야 한다.</p>
<p>또한 예측변수의 개수가 증가할수록 결정계수는 항상 커지므로, 단순 결정계수를 변수 개수가 다른 모형을 비교하는 데 사용하는 것은 적절하지 않다. 불필요한 변수를 계속 추가해도 <span class="math inline">\(R^{2}\)</span> 값은 인위적으로 증가할 수 있기 때문이다. 이러한 한계를 보완하기 위해서는 수정결정계수를 사용하는 것이 바람직하다. 수정결정계수는 표본 크기와 변수 개수를 함께 고려하여, 유효한 변수가 추가될 때만 값이 증가하며, 불필요한 변수가 들어가면 오히려 감소할 수 있다. 따라서 변수 개수가 다른 모형을 비교할 때는 반드시 수정결정계수를 기준으로 판단해야 한다.</p>
<p>더 나아가, 실제 분석에서는 결정계수만을 의존하는 것도 위험하다. <span class="math inline">\(R^{2}\)</span>는 훈련 데이터에서의 설명력을 반영할 뿐, 새로운 데이터에 대한 예측 성능을 보장하지 않는다. 변수를 많이 넣으면 설명력은 높아 보이지만 실제로는 과적합(overfitting)이 발생하여 예측력이 떨어질 수 있다. 따라서 교차검증(cross-validation)을 통한 예측력 평가나, AIC·BIC 같은 정보 기준을 함께 사용하는 것이 바람직하다.</p>
<p><strong>수정결정계수 adjusted <span class="math inline">\(R_{adj}^{2} = 1 - \frac{(1 - R^{2})/(n - 1)}{(n - p - 1)}\)</span></strong></p>
<p>수정결정계수는 결정계수의 문제점, 즉 유의하지 않은 예측변수가 삽입되어도 <span class="math inline">\(R^{2}\)</span> 값이 항상 증가한다는 한계를 보완하기 위해 고안된 척도이다. 수정결정계수는 예측변수의 수가 늘어날 때 감소할 수도 있기 때문에, 불필요한 변수를 넣었을 때 자동적으로 설명력이 과대평가되는 문제를 줄여 준다.</p>
<p>그러나 이 지표 역시 완벽하지 않다. 예측변수의 개수가 늘어나면 SSE(오차제곱합)는 줄어들지만 동시에 오차 자유도는 감소하므로, 이 두 효과가 상쇄되어 값이 크게 달라지지 않는다. 따라서 수정결정계수를 회귀모형의 절대적인 설명력 지표로 보기는 어렵다. 또한 수정결정계수 자체에는 통계적 유의성을 검정할 수 있는 별도의 검정통계량이 존재하지 않는다. 이 때문에 모형의 유의성은 여전히 분산분석의 F-검정이나 개별 계수의 t-검정을 통해 확인해야 한다.</p>
<p>수정결정계수의 가장 큰 활용 가치는, 예측변수 집합과 개수가 서로 다른 회귀모형을 비교할 때이다. 단순 결정계수는 변수 개수가 많아질수록 무조건 커지므로 모형 비교에 적절하지 않지만, 수정결정계수는 불필요한 변수가 포함되면 오히려 감소할 수 있어 모형 간의 상대적 비교에 유리하다. 따라서 분석 목적이 목표변수의 예측력을 높이는 최적 모형을 찾는 것이라면, 수정결정계수를 기준으로 모형을 선택하는 방법을 사용할 수 있다.</p>
<p><strong>Mallow <span class="math inline">\(C_{p} = \frac{SSE_{p}}{{\widehat{\sigma}}^{2}} - (n - 2p)\)</span></strong></p>
<p>Mallow <span class="math inline">\(C_{p}\)</span> 값은 회귀모형 선택 기준 가운데 하나로, 예측변수 개수(절편 포함 <span class="math inline">\(p + 1\)</span>)와 <span class="math inline">\(C_{p}\)</span> 값이 근사할 경우 좋은 회귀모형으로 판단한다. 이때 중요한 점은, 이 방법 역시 개별 예측변수의 유의성을 직접 검정하는 절차는 아니라는 것이다.</p>
<p>이론적 근거는 다음과 같다. Mallow <span class="math inline">\(C_{p}\)</span>는 모형의 추정치와 실제 모형이 주는 기대값 사이의 평균제곱오차(MSE)를 근사하는 척도로, 불편성과 효율성을 동시에 고려한 기준이다. 즉, 만약 모형이 적합하다면 <span class="math inline">\(C_{p}\)</span> 값은 대략적으로 예측변수 개수(<span class="math inline">\(p + 1\)</span>)와 유사해져야 한다. 따라서 C_p \approx p 인 경우, 해당 모형은 설명력이 충분하면서도 불필요한 변수를 포함하지 않는 간명한 모형이라고 해석할 수 있다.</p>
<ul>
<li><p><span class="math inline">\(C_{p} \approx p + 1\)</span>: 적절한 모형 → 편향(bias)과 분산(variance)의 균형이 잘 맞음.</p></li>
<li><p><span class="math inline">\(C_{p} \gg p + 1\)</span>: 불필요한 변수가 많아 오차항이 커진 모형.</p></li>
<li><p><span class="math inline">\(C_{p} &lt; p + 1\)</span>: 과소적합 가능성(중요 변수를 놓쳤을 수 있음).</p></li>
</ul>
<p>여전히 개별 계수의 유의성은 따로 검정하지 않으며, 표본 수가 작거나 예측변수 간 다중공선성이 강하면 안정성이 떨어진다.</p>
<p><strong>예측잔차자승합(Prediction REsidual Sum of Squares) <span class="math inline">\(PRESS = \sum(y_{i} - \widehat{y_{(i)}})^{2}\)</span></strong></p>
<p><span class="math inline">\(\widehat{y_{(i)}}\)</span> : <span class="math inline">\(i\)</span>-번째 관측치를 제외한 후 모형을 추정한 후 구한 목표변수 <span class="math inline">\(y_{i}\)</span>의 적합값, i-번째 관측치 제외 잔차 <span class="math inline">\((y_i - \hat y_{(i)})\)</span>, 각 관측치에 대해 <span dir="rtl">”</span>나를 빼고 학습했을 때 나를 얼마나 잘 맞추는가?“를 재는 LOOCV(Leave-One-Out Cross-Validation) 오차의 합으로 값이 작을수록 일반화 예측력이 좋다는 뜻이다.</p>
<p><strong>AIC, SBC 모두 작을수록 적합도가 높다.</strong></p>
<p>AIC(Akaike Information Criteria)</p>
<p><span class="math inline">\(AIC = 2p - 2ln(\widehat{L})\)</span>, <span class="math inline">\(\widehat{L}\)</span> : 회귀모델의 최대우도함수</p>
<p>BIC(Bayesian information criterion)</p>
<p><span class="math display">\[BIC = p*ln(n) - 2ln(\widehat{L})\]</span></p>
<p><strong>Comment</strong></p>
<p>(수정)결정계수, Mallow <span class="math inline">\(C_{p}\)</span>, PRESS 통계량은 모두 <span dir="rtl">”</span>어떤 변수 집합이 좋은 모형인가”를 평가하는 지표일 뿐, 개별 변수의 유의성을 직접 검정하는 절차는 아니다. 따라서 이 방법들로 선택된 모형 내에서도 각 예측변수가 통계적으로 유의한지는 별도의 t-검정이나 F-검정을 통해 확인해야 한다.</p>
<p>일반적으로는 다음과 같은 절차가 활용된다. 먼저 수정결정계수와 Mallow <span class="math inline">\(C_{p}\)</span> 기준에 따라 설명력이 충분하다고 판단되는 변수 집합을 몇 개 선택한다. 그 다음 각 후보 집합에 대해 PRESS 값을 계산하여 실제 예측력이 가장 좋은 모형을 최종 선택하는 방식이다.</p>
<p>AIC, BIC, PRESS와 같은 지표는 서로 전혀 다른 변수 집합을 비교할 때 특히 유용하다. 즉, 이미 선택된 변수들이 모두 유의하다고 가정한 뒤, <span dir="rtl">”</span>어떤 조합이 가장 적합한가”를 평가하는 데 사용된다. 이들은 적합도의 정도를 비교하는 수치로 널리 쓰이며, 특히 AIC와 BIC는 빅데이터 분석에서 예측모형을 비교·선택할 때 표준적으로 사용된다.</p>
<ul>
<li><p>수정결정계수: 변수 개수가 다른 모형 비교 시 필수적이지만, 유의성 검정은 제공하지 않는다.</p></li>
<li><p>Mallow <span class="math inline">\(C_{p}\)</span>: <span class="math inline">\(C_{p} \approx p + 1\)</span>일 때 좋은 모형으로 간주되며, 잔차분산과 변수 개수 균형을 본다.</p></li>
<li><p>PRESS: LOOCV(Leave-One-Out Cross Validation) 기반 예측력 지표로, 값이 작을수록 일반화 성능이 좋다.</p></li>
<li><p>AIC/BIC: 정보 기준으로, 모형의 적합도와 복잡도를 동시에 고려한다. BIC는 변수 수에 더 큰 페널티를 주므로, 빅데이터 환경에서 과적합 방지에 특히 효과적이다.</p></li>
<li><p>해석 관점: 이 지표들은 모두 <span dir="rtl">”</span>모형 수준의 비교”를 위한 것이지, <span dir="rtl">”</span>개별 변수의 유의성 판단”을 대체하지 못한다.</p></li>
</ul>
<p><strong>부분 결정계수 Coefficient of Partial Determination</strong></p>
<p>부분 결정계수는 <span dir="rtl">”</span>기존 모형에 새로운 변수를 넣었을 때 설명력이 얼마나 더 증가했는가”를 나타낸다. 예를 들어 3개의 예측변수가 존재하는 <span class="math inline">\(X_{1},X_{2},X_{3}\)</span> 모형을 가정하자. <span class="math inline">\(X_{1}\)</span>에 의해 설명되고 남은 예측변수 변동을 두 예측변수(<span class="math inline">\(X_{2},X_{3}\)</span>)가 설명하는 변동을 다음과 같이 정의하고 이를 부분 결정계수라 한다.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/부분결정계수.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:80.0%"></p>
</figure>
</div>
<ul>
<li><p>분자: <span class="math inline">\(X_{2},X_{3}\)</span>가 <span class="math inline">\(X_{1}\)</span>이 설명하지 못한 부분에서 추가적으로 설명한 변동</p></li>
<li><p>분모: <span class="math inline">\(X_{1}\)</span>만 썼을 때 남아 있던 오차 변동</p></li>
<li><p>값이 0에 가까우면, 새로운 변수(<span class="math inline">\(X_{2},X_{3}\)</span>)는 기존 변수(<span class="math inline">\(X_{1}\)</span>)가 설명하지 못한 변동을 거의 설명하지 못한다.</p></li>
<li><p>값이 1에 가까우면, 새로운 변수가 기존 모형에 비해 상당한 설명력을 추가했음을 의미한다.</p></li>
</ul>
<p>부분결정계수의 활용을 정리하면 다음과 같다.</p>
<p>1. 변수 선택: 단계적 회귀(stepwise regression)에서 새로운 변수를 추가할지 말지 판단하는 기준. 예: <span class="math inline">\(X_{1}\)</span>이 이미 들어간 상태에서 <span class="math inline">\(X_{2},X_{3}\)</span>를 추가했을 때 부분 <span class="math inline">\(R^{2}\)</span>가 크면 포함할 가치가 있음.</p>
<p>2. 공헌도 평가: 여러 변수들이 동시에 들어가 있을 때, 특정 변수 집합이 추가적으로 기여하는 설명력 파악. 예: <span dir="rtl">”</span>인구학적 변수(연령·성별) 통제 후, 사회경제적 변수(소득·학력)가 얼마만큼 설명력을 추가하는가?”</p>
<p>3. 다중공선성 관련 해석: 어떤 변수가 기존 변수들과 강하게 상관되어 있으면, 부분 결정계수가 낮게 나와 실제로는 중요하지만 모형에서 기여가 작아 보일 수 있음. 따라서 부분 R^2 해석 시 공선성 여부를 함께 고려해야 한다.</p>
</section>
<section id="최종-회귀모형-선택-방법-모형비교" class="level4">
<h4 class="anchored" data-anchor-id="최종-회귀모형-선택-방법-모형비교"><span style="color:blue"> 2. 최종 회귀모형 선택 방법 : 모형비교</span></h4>
<p>서로 예측변수가 고려된 모형 중 최적 모형을 선택하고자 할 때는 MAE(Mean Absolute Error&nbsp;오차평균절대), MSE(Mean Sqaured Error 오차평균자승합), RMSE(Root Mean Sqaured Error 자승합 제곱근) 나 수정결정계수 값을 비교한다.</p>
<p>표본데이터 크기 : <span class="math inline">\(n\)</span>이고 OLS 목표변수 적합치(추정치) : <span class="math inline">\({\widehat{y}}_{i}\)</span>인 경우</p>
<ul>
<li><p><span class="math inline">\(MAE = \frac{1}{n}\sum|Y_{i} - {\widehat{Y}}_{i}|\)</span></p></li>
<li><p><span class="math inline">\(MSE = \frac{1}{n}\sum(Y_{i} - {\widehat{Y}}_{i})^{2}\)</span>, <span class="math inline">\(RMSE = \sqrt{MSE}\)</span></p></li>
</ul>
<p>새로운 자료 (표본크기 <span class="math inline">\(n^{*}\)</span>) 수집하고 모형의 예측력을 다음 방법에 의해 계산하여 각 모형을 비교한다.</p>
<p><strong>평균자승합예측오차 Mean Square Prediction Error</strong></p>
<p><span class="math inline">\(MSPE = \frac{1}{n^{*}}\sum^{n^{*}}(y_{i} - {\widehat{y}}_{i})^{2}\)</span>, 새로운 자료로 회귀 모형을 추정하였을 때 이전 자료에서 추정된 회귀 모형과 유사하면 추정된 회귀 모형은 좋다고 판단할 수 있다. 새로운 자료 수집이 불가능한 경우에는 데이터를 splitting하여 모형추정 데이터와 예측 데이터로 나누어 분석한다.</p>
<p>수정결정계수나 PRESS, AIC, SBC에 의한 최적 회귀모형 선택은 예측변수가 서로 다른 그룹을 비교할 때 사용되는 통계량이다.</p>
</section>
<section id="고전적-변수선택-방법" class="level4">
<h4 class="anchored" data-anchor-id="고전적-변수선택-방법"><span style="color:blue"> 3. 고전적 변수선택 방법</span></h4>
<section id="후진제거-backward" class="level5">
<h5 class="anchored" data-anchor-id="후진제거-backward">(1) 후진제거 Backward</h5>
<p>후진제거법은 완전모형에서 시작하여, 가장 유의하지 않은 변수를 하나씩 제거하면서 최종적으로 모든 변수가 유의한 상태의 모형을 선택하는 방법이다. 직관적이고 계산이 간편하다는 장점이 있지만, 다중공선성이나 p-value 의존성으로 인해 변수 선택이 불안정해질 수 있으므로 다른 모형 선택 지표와 함께 활용하는 것이 바람직하다</p>
<p>1. 우선 고려된 모든 예측변수를 모형에 포함하여 회귀모형을 적합한다.</p>
<p>2. 추정된 회귀계수들에 대해 유의성 검정을 실시하고, 이 중 가장 유의하지 않은 변수, 즉 유의확률(p-value)이 가장 크거나 F값이 가장 작은 변수를 선택하여 제외한다.</p>
<p>3. 변수를 제거한 새로운 모형을 다시 적합한 뒤, 동일한 절차를 반복한다.</p>
<p>4. 최종적으로 모형에 남은 모든 예측변수가 통계적으로 유의할 때 과정이 종료된다.</p>
<p><strong>장점</strong></p>
<ul>
<li><p>모든 변수를 고려한 상태에서 시작하기 때문에 중요한 변수를 놓칠 위험이 상대적으로 적다.</p></li>
<li><p>계산 절차가 비교적 단순하고 직관적이다.</p></li>
</ul>
<p><strong>단점</strong></p>
<ul>
<li><p>표본 크기가 작거나 예측변수 간 다중공선성이 심할 경우, 변수 제거 과정이 불안정할 수 있다.</p></li>
<li><p>p-value 기준으로만 판단하기 때문에, 실제로는 설명력이 있는 변수가 제외될 가능성이 있다.</p></li>
<li><p>변수 제거 순서에 따라 결과 모형이 달라질 수 있다.</p></li>
</ul>
<p>후진제거법은 보통 <span dir="rtl">”</span>단계적 회귀(stepwise regression)“의 한 형태로 사용되며, 수정결정계수나 AIC/BIC, PRESS와 같은 모형 선택 기준을 함께 고려하면 더 안정적인 모형 선택이 가능하다.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># ==============================================</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Boston Housing: 후진제거 변수선택</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="co"># ==============================================</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> statsmodels.api <span class="im">as</span> sm</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> fetch_openml</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="co"># --- 0) 데이터 로드 ---</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>boston <span class="op">=</span> fetch_openml(name<span class="op">=</span><span class="st">"boston"</span>, version<span class="op">=</span><span class="dv">1</span>, as_frame<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> boston.frame.copy()</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="co"># 종속변수 / 설명변수</span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> pd.to_numeric(df[<span class="st">"MEDV"</span>], errors<span class="op">=</span><span class="st">"coerce"</span>)</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> df.drop(columns<span class="op">=</span>[<span class="st">"MEDV"</span>]).copy()</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a><span class="co"># 숫자형으로 변환</span></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> c <span class="kw">in</span> X.columns:</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> X[c].dtype.name <span class="kw">in</span> [<span class="st">"object"</span>, <span class="st">"category"</span>]:</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>        X[c] <span class="op">=</span> X[c].astype(<span class="bu">str</span>).<span class="bu">str</span>.strip()</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>        X[c] <span class="op">=</span> pd.to_numeric(X[c], errors<span class="op">=</span><span class="st">"coerce"</span>)</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>        X[c] <span class="op">=</span> X[c].astype(<span class="bu">float</span>)</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a><span class="co"># 결측 제거</span></span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>mask <span class="op">=</span> y.notna() <span class="op">&amp;</span> X.notna().<span class="bu">all</span>(axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> y[mask].reset_index(drop<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> X.loc[mask].reset_index(drop<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a><span class="co"># --- 1) 후진제거 함수 ---</span></span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> backward_elimination(X, y, alpha<span class="op">=</span><span class="fl">0.05</span>, verbose<span class="op">=</span><span class="va">True</span>):</span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a>    selected <span class="op">=</span> <span class="bu">list</span>(X.columns)   <span class="co"># 모든 변수 포함</span></span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>    changed <span class="op">=</span> <span class="va">True</span></span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a>    <span class="cf">while</span> changed <span class="kw">and</span> <span class="bu">len</span>(selected) <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a>        changed <span class="op">=</span> <span class="va">False</span></span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a>        X_try <span class="op">=</span> sm.add_constant(X[selected], has_constant<span class="op">=</span><span class="st">'add'</span>)</span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a>        model <span class="op">=</span> sm.OLS(y, X_try).fit()</span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 절편 제외 p-value 확인</span></span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a>        pvals <span class="op">=</span> model.pvalues.drop(<span class="st">"const"</span>)</span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a>        worst_feature <span class="op">=</span> pvals.idxmax()</span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a>        worst_pval <span class="op">=</span> pvals.<span class="bu">max</span>()</span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> worst_pval <span class="op">&gt;</span> alpha:</span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a>            selected.remove(worst_feature)</span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a>            changed <span class="op">=</span> <span class="va">True</span></span>
<span id="cb1-49"><a href="#cb1-49" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> verbose:</span>
<span id="cb1-50"><a href="#cb1-50" aria-hidden="true" tabindex="-1"></a>                <span class="bu">print</span>(<span class="ss">f"[REMOVE] </span><span class="sc">{</span>worst_feature<span class="sc">}</span><span class="ss"> (p=</span><span class="sc">{</span>worst_pval<span class="sc">:.4g}</span><span class="ss">)"</span>)</span>
<span id="cb1-51"><a href="#cb1-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-52"><a href="#cb1-52" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> selected</span>
<span id="cb1-53"><a href="#cb1-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-54"><a href="#cb1-54" aria-hidden="true" tabindex="-1"></a><span class="co"># --- 2) 실행 ---</span></span>
<span id="cb1-55"><a href="#cb1-55" aria-hidden="true" tabindex="-1"></a>selected_features <span class="op">=</span> backward_elimination(X, y, alpha<span class="op">=</span><span class="fl">0.05</span>, verbose<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb1-56"><a href="#cb1-56" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">최종 선택 변수:"</span>, selected_features)</span>
<span id="cb1-57"><a href="#cb1-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-58"><a href="#cb1-58" aria-hidden="true" tabindex="-1"></a><span class="co"># --- 3) 최종 모형 적합 ---</span></span>
<span id="cb1-59"><a href="#cb1-59" aria-hidden="true" tabindex="-1"></a>X_final <span class="op">=</span> sm.add_constant(X[selected_features], has_constant<span class="op">=</span><span class="st">'add'</span>)</span>
<span id="cb1-60"><a href="#cb1-60" aria-hidden="true" tabindex="-1"></a>final_model <span class="op">=</span> sm.OLS(y, X_final).fit()</span>
<span id="cb1-61"><a href="#cb1-61" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(final_model.summary())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>[REMOVE] AGE (p=0.9582) <br> [REMOVE] INDUS (p=0.738)</p>
</section>
<section id="전진삽입-forward" class="level5">
<h5 class="anchored" data-anchor-id="전진삽입-forward">(2) 전진삽입 Forward</h5>
<p>전진삽입법은 후진제거법과 반대로, 아무 변수도 포함하지 않은 상태에서 시작하여, 하나씩 변수를 추가하면서 최종 모형을 완성하는 방식이다.</p>
<p>1. 고려된 예측변수들 중에서 목표변수 변동을 가장 많이 설명하는 변수, 즉 결정계수 증가량(\Delta R^2)이 가장 크거나 유의확률(p-value)이 가장 작은 변수를 찾는다. 그 설명력이 통계적으로 유의하면 이 변수를 첫 번째 예측변수로 선택한다.</p>
<p>2. 첫 번째 변수가 선택된 이후에는, 이미 선택된 변수들이 설명하고 남은 목표변수의 변동(잔차 부분)을 기준으로 한다. 남은 후보 변수들 중에서 이 잔차 변동을 가장 많이 설명하는 변수(부분 F-통계량이 가장 큰 변수)를 찾고, 그 설명력이 유의하다면 두 번째 변수로 선택한다.</p>
<p>3. 이 과정을 반복하여, 더 이상 추가할 만한 유의한 변수가 없을 때 멈춘다. 이때 남은 후보 변수들은 기존 변수들과의 상관관계 때문에 추가 설명력이 통계적으로 유의하지 않다고 판단된 경우이다.</p>
<p><strong>장점</strong></p>
<ul>
<li><p>모형에 변수가 점점 추가되는 과정이 직관적이며, 분석자가 단계별 변화를 쉽게 이해할 수 있다.</p></li>
<li><p>불필요한 변수가 처음부터 들어오지 않으므로, 단순한 모형을 우선적으로 고려할 수 있다.</p></li>
</ul>
<p><strong>단점</strong></p>
<ul>
<li><p>초기에 잘못 선택된 변수가 있으면, 이후 절차에 큰 영향을 미쳐 전체 모형이 왜곡될 수 있다.</p></li>
<li><p>일단 포함된 변수는 나중에 제거되지 않기 때문에, 후진제거법보다 유연성이 떨어진다.</p></li>
<li><p>다중공선성이 강한 경우에는 유의성 판단이 불안정할 수 있다.</p></li>
</ul>
<p>전진삽입법은 후진제거법과 함께 단계적 회귀(stepwise regression) 의 기본 틀을 이룬다. 단순히 p-value만 기준으로 삼기보다는 AIC, BIC, PRESS, 교차검증(CV)과 같은 다른 모형 선택 기준을 병행하면 안정적인 결과를 얻을 수 있다.</p>
<p>전진삽입법은 공변량이 없는 상태에서 시작하여, 설명력이 가장 큰 변수를 하나씩 추가해 나가되, 각 단계에서 통계적으로 유의한 경우에만 포함시키는 방법이다. 단순하고 직관적이라는 장점이 있지만, 초기 선택이 잘못되면 최종 모형이 왜곡될 수 있으며, 다중공선성 문제에 취약하므로 다른 기준과 병행하는 것이 바람직하다.”</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co"># ==============================================</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Boston Housing: 전진삽입 변수선택</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="co"># ==============================================</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> statsmodels.api <span class="im">as</span> sm</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> fetch_openml</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a><span class="co"># --- 0) 데이터 로드 ---</span></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>boston <span class="op">=</span> fetch_openml(name<span class="op">=</span><span class="st">"boston"</span>, version<span class="op">=</span><span class="dv">1</span>, as_frame<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> boston.frame.copy()</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a><span class="co"># --- 1) 숫자형으로 강제 변환 + 결측 제거 ---</span></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a><span class="co"># 종속변수</span></span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> pd.to_numeric(df[<span class="st">"MEDV"</span>], errors<span class="op">=</span><span class="st">"coerce"</span>)</span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a><span class="co"># 설명변수: MEDV 제외</span></span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> df.drop(columns<span class="op">=</span>[<span class="st">"MEDV"</span>]).copy()</span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a><span class="co"># 모든 열 숫자형으로 변환 (object/category -&gt; numeric)</span></span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> c <span class="kw">in</span> X.columns:</span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> X[c].dtype.name <span class="kw">in</span> [<span class="st">"object"</span>, <span class="st">"category"</span>]:</span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 공백/기호가 있을 수 있으므로 문자열이면 strip</span></span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a>        X[c] <span class="op">=</span> X[c].astype(<span class="bu">str</span>).<span class="bu">str</span>.strip()</span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a>        X[c] <span class="op">=</span> pd.to_numeric(X[c], errors<span class="op">=</span><span class="st">"coerce"</span>)</span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a>        X[c] <span class="op">=</span> X[c].astype(<span class="bu">float</span>)</span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true" tabindex="-1"></a><span class="co"># y와 X 모두 결측 없는 행만 사용</span></span>
<span id="cb2-30"><a href="#cb2-30" aria-hidden="true" tabindex="-1"></a>mask <span class="op">=</span> y.notna() <span class="op">&amp;</span> X.notna().<span class="bu">all</span>(axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb2-31"><a href="#cb2-31" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> y[mask]</span>
<span id="cb2-32"><a href="#cb2-32" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> X.loc[mask].reset_index(drop<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb2-33"><a href="#cb2-33" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> y.reset_index(drop<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb2-34"><a href="#cb2-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-35"><a href="#cb2-35" aria-hidden="true" tabindex="-1"></a><span class="co"># --- 2) 전진삽입 함수 (p-value 기준, 방어 로직 포함) ---</span></span>
<span id="cb2-36"><a href="#cb2-36" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> forward_selection(X, y, alpha<span class="op">=</span><span class="fl">0.05</span>, verbose<span class="op">=</span><span class="va">True</span>):</span>
<span id="cb2-37"><a href="#cb2-37" aria-hidden="true" tabindex="-1"></a>    selected <span class="op">=</span> []</span>
<span id="cb2-38"><a href="#cb2-38" aria-hidden="true" tabindex="-1"></a>    remaining <span class="op">=</span> <span class="bu">list</span>(X.columns)</span>
<span id="cb2-39"><a href="#cb2-39" aria-hidden="true" tabindex="-1"></a>    best_changed <span class="op">=</span> <span class="va">True</span></span>
<span id="cb2-40"><a href="#cb2-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-41"><a href="#cb2-41" aria-hidden="true" tabindex="-1"></a>    <span class="cf">while</span> best_changed <span class="kw">and</span> <span class="bu">len</span>(remaining) <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb2-42"><a href="#cb2-42" aria-hidden="true" tabindex="-1"></a>        best_changed <span class="op">=</span> <span class="va">False</span></span>
<span id="cb2-43"><a href="#cb2-43" aria-hidden="true" tabindex="-1"></a>        pvals <span class="op">=</span> []</span>
<span id="cb2-44"><a href="#cb2-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-45"><a href="#cb2-45" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> feat <span class="kw">in</span> remaining:</span>
<span id="cb2-46"><a href="#cb2-46" aria-hidden="true" tabindex="-1"></a>            <span class="cf">try</span>:</span>
<span id="cb2-47"><a href="#cb2-47" aria-hidden="true" tabindex="-1"></a>                X_try <span class="op">=</span> sm.add_constant(X[selected <span class="op">+</span> [feat]], has_constant<span class="op">=</span><span class="st">'add'</span>)</span>
<span id="cb2-48"><a href="#cb2-48" aria-hidden="true" tabindex="-1"></a>                model <span class="op">=</span> sm.OLS(y, X_try).fit()</span>
<span id="cb2-49"><a href="#cb2-49" aria-hidden="true" tabindex="-1"></a>                <span class="co"># 후보 feat의 p-value만 본다</span></span>
<span id="cb2-50"><a href="#cb2-50" aria-hidden="true" tabindex="-1"></a>                pv <span class="op">=</span> model.pvalues.get(feat, np.nan)</span>
<span id="cb2-51"><a href="#cb2-51" aria-hidden="true" tabindex="-1"></a>                pvals.append((feat, pv))</span>
<span id="cb2-52"><a href="#cb2-52" aria-hidden="true" tabindex="-1"></a>            <span class="cf">except</span> <span class="pp">Exception</span> <span class="im">as</span> e:</span>
<span id="cb2-53"><a href="#cb2-53" aria-hidden="true" tabindex="-1"></a>                <span class="co"># 특이행렬/수치 문제는 건너뛴다</span></span>
<span id="cb2-54"><a href="#cb2-54" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> verbose:</span>
<span id="cb2-55"><a href="#cb2-55" aria-hidden="true" tabindex="-1"></a>                    <span class="bu">print</span>(<span class="ss">f"[skip] </span><span class="sc">{</span>feat<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>e<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb2-56"><a href="#cb2-56" aria-hidden="true" tabindex="-1"></a>                <span class="cf">continue</span></span>
<span id="cb2-57"><a href="#cb2-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-58"><a href="#cb2-58" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="kw">not</span> pvals:</span>
<span id="cb2-59"><a href="#cb2-59" aria-hidden="true" tabindex="-1"></a>            <span class="cf">break</span></span>
<span id="cb2-60"><a href="#cb2-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-61"><a href="#cb2-61" aria-hidden="true" tabindex="-1"></a>        <span class="co"># p-value 최소 찾기</span></span>
<span id="cb2-62"><a href="#cb2-62" aria-hidden="true" tabindex="-1"></a>        feat_best, pv_best <span class="op">=</span> <span class="bu">sorted</span>(pvals, key<span class="op">=</span><span class="kw">lambda</span> x: (np.isnan(x[<span class="dv">1</span>]), x[<span class="dv">1</span>]))[<span class="dv">0</span>]</span>
<span id="cb2-63"><a href="#cb2-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-64"><a href="#cb2-64" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> (<span class="kw">not</span> np.isnan(pv_best)) <span class="kw">and</span> (pv_best <span class="op">&lt;</span> alpha):</span>
<span id="cb2-65"><a href="#cb2-65" aria-hidden="true" tabindex="-1"></a>            selected.append(feat_best)</span>
<span id="cb2-66"><a href="#cb2-66" aria-hidden="true" tabindex="-1"></a>            remaining.remove(feat_best)</span>
<span id="cb2-67"><a href="#cb2-67" aria-hidden="true" tabindex="-1"></a>            best_changed <span class="op">=</span> <span class="va">True</span></span>
<span id="cb2-68"><a href="#cb2-68" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> verbose:</span>
<span id="cb2-69"><a href="#cb2-69" aria-hidden="true" tabindex="-1"></a>                <span class="bu">print</span>(<span class="ss">f"[ADD] </span><span class="sc">{</span>feat_best<span class="sc">}</span><span class="ss"> (p=</span><span class="sc">{</span>pv_best<span class="sc">:.4g}</span><span class="ss">)"</span>)</span>
<span id="cb2-70"><a href="#cb2-70" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb2-71"><a href="#cb2-71" aria-hidden="true" tabindex="-1"></a>            <span class="co"># 더 이상 유의한 변수 없음</span></span>
<span id="cb2-72"><a href="#cb2-72" aria-hidden="true" tabindex="-1"></a>            <span class="cf">break</span></span>
<span id="cb2-73"><a href="#cb2-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-74"><a href="#cb2-74" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> selected</span>
<span id="cb2-75"><a href="#cb2-75" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-76"><a href="#cb2-76" aria-hidden="true" tabindex="-1"></a><span class="co"># --- 3) 실행 ---</span></span>
<span id="cb2-77"><a href="#cb2-77" aria-hidden="true" tabindex="-1"></a>selected_features <span class="op">=</span> forward_selection(X, y, alpha<span class="op">=</span><span class="fl">0.05</span>, verbose<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb2-78"><a href="#cb2-78" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">최종 선택 변수:"</span>, selected_features)</span>
<span id="cb2-79"><a href="#cb2-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-80"><a href="#cb2-80" aria-hidden="true" tabindex="-1"></a><span class="co"># --- 4) 최종 모형 적합 ---</span></span>
<span id="cb2-81"><a href="#cb2-81" aria-hidden="true" tabindex="-1"></a>X_final <span class="op">=</span> sm.add_constant(X[selected_features], has_constant<span class="op">=</span><span class="st">'add'</span>)</span>
<span id="cb2-82"><a href="#cb2-82" aria-hidden="true" tabindex="-1"></a>final_model <span class="op">=</span> sm.OLS(y, X_final).fit()</span>
<span id="cb2-83"><a href="#cb2-83" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(final_model.summary())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>[ADD] LSTAT (p=5.081e-88) [ADD] RM (p=3.472e-27) [ADD] PTRATIO (p=1.645e-14) [ADD] DIS (p=1.668e-05) [ADD] NOX (p=5.488e-08) [ADD] CHAS (p=0.0002655) [ADD] B (p=0.0007719) [ADD] ZN (p=0.004652) [ADD] CRIM (p=0.04457) [ADD] RAD (p=0.001692) [ADD] TAX (p=0.0005214) <br> 최종 선택 변수: [‘LSTAT’, ‘RM’, ‘PTRATIO’, ‘DIS’, ‘NOX’, ‘CHAS’, ‘B’, ‘ZN’, ‘CRIM’, ‘RAD’, ‘TAX’]</p>
</section>
<section id="단계삽입-stepwise" class="level5">
<h5 class="anchored" data-anchor-id="단계삽입-stepwise">(3) 단계삽입 stepwise</h5>
<p>단계삽입 방법은 기본적으로 전진삽입법과 유사하다. 그러나 중요한 차이는, 이미 선택된 변수들도 새롭게 들어온 변수를 기준으로 다시 유의성 검정을 받는다는 점이다. 따라서 불필요해진 변수가 있으면 모형에서 제거될 수 있다.</p>
<p>1. 첫 단계: 고려된 예측변수들 중에서 목표변수 변동을 가장 많이 설명하는 변수, 즉 유의확률(p-value)이 가장 작은 변수를 선택한다. 설명력이 유의하면 이 변수를 첫 번째로 포함한다. → 전진삽입법과 동일하다.</p>
<p>2. 두 번째 단계: 첫 번째 변수가 설명한 부분을 제외하고 남은 목표변수의 변동을 기준으로, 남은 변수들 중 가장 설명력이 큰 변수를 선택한다. 설명력이 유의하다면 두 번째 변수로 포함한다. → 역시 전진삽입법과 동일하다.</p>
<p>3. 선택된 변수 검정: 새로운 변수가 추가되면, 이미 모형에 포함된 기존 변수들이 여전히 유의한지를 다시 검정한다. 만약 기존 변수 중 하나가 더 이상 유의하지 않다면, 그 변수를 제거한다. 즉, 선택과 제거가 반복적으로 일어나는 것이 stepwise의 핵심이다.</p>
<p>4. 종료 조건: 새로운 변수가 더 이상 추가될 수 없고, 기존 변수들이 모두 유의성을 유지할 때 절차가 멈춘다.</p>
<p><strong>장점</strong></p>
<ul>
<li><p>불필요한 변수를 자연스럽게 걸러낼 수 있고, 전진/후진 방식의 단점을 보완한다.</p></li>
<li><p>자동화된 변수 선택 절차로 계산이 간편하다.</p></li>
</ul>
<p><strong>단점</strong></p>
<ul>
<li><p>표본이 작거나 다중공선성이 심한 경우, 변수의 포함 여부가 불안정하다.</p></li>
<li><p>모형 선택 기준이 순전히 통계적(p-value, F값)에만 의존하기 때문에, 이론적 타당성을 무시할 위험이 있다.</p></li>
<li><p>데이터가 조금만 달라져도 선택된 변수가 크게 달라질 수 있다.</p></li>
</ul>
<p>stepwise 결과를 그대로 <span dir="rtl">”</span>최종 모형”으로 사용하는 것은 위험하다. 연구에서는 이론적 근거를 기반으로 주요 변수를 미리 지정하고, stepwise는 보조적으로 활용하는 것이 바람직하다. PRESS, AIC, BIC, 교차검증(CV) 같은 다른 모형 비교 지표와 함께 사용하는 것이 좋다.</p>
<p><span dir="rtl">”</span>단계삽입(stepwise) 회귀는 전진삽입법과 유사하지만, 새로운 변수가 들어올 때마다 기존 변수를 다시 검정하여 유의하지 않은 경우 제거한다는 점에서 더 유연한 방법이다. 선택과 제거가 반복되면서 최적 모형을 찾아가는 과정이지만, 표본 특성이나 다중공선성에 민감하므로 반드시 이론적 근거 및 다른 모형 선택 지표와 병행하는 것이 필요하다.”</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># ==============================================</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Boston Housing: 단계삽입 변수선택</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="co"># ==============================================</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> statsmodels.api <span class="im">as</span> sm</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> fetch_openml</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a><span class="co"># --- 0) 데이터 로드 ---</span></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>boston <span class="op">=</span> fetch_openml(name<span class="op">=</span><span class="st">"boston"</span>, version<span class="op">=</span><span class="dv">1</span>, as_frame<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> boston.frame.copy()</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a><span class="co"># 종속변수 / 설명변수</span></span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> pd.to_numeric(df[<span class="st">"MEDV"</span>], errors<span class="op">=</span><span class="st">"coerce"</span>)</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> df.drop(columns<span class="op">=</span>[<span class="st">"MEDV"</span>]).copy()</span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a><span class="co"># 숫자형 변환</span></span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> c <span class="kw">in</span> X.columns:</span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> X[c].dtype.name <span class="kw">in</span> [<span class="st">"object"</span>, <span class="st">"category"</span>]:</span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a>        X[c] <span class="op">=</span> X[c].astype(<span class="bu">str</span>).<span class="bu">str</span>.strip()</span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a>        X[c] <span class="op">=</span> pd.to_numeric(X[c], errors<span class="op">=</span><span class="st">"coerce"</span>)</span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a>        X[c] <span class="op">=</span> X[c].astype(<span class="bu">float</span>)</span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-26"><a href="#cb3-26" aria-hidden="true" tabindex="-1"></a><span class="co"># 결측 제거</span></span>
<span id="cb3-27"><a href="#cb3-27" aria-hidden="true" tabindex="-1"></a>mask <span class="op">=</span> y.notna() <span class="op">&amp;</span> X.notna().<span class="bu">all</span>(axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb3-28"><a href="#cb3-28" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> y[mask].reset_index(drop<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb3-29"><a href="#cb3-29" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> X.loc[mask].reset_index(drop<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb3-30"><a href="#cb3-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-31"><a href="#cb3-31" aria-hidden="true" tabindex="-1"></a><span class="co"># --- 1) 단계적 회귀 함수 ---</span></span>
<span id="cb3-32"><a href="#cb3-32" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> stepwise_selection(X, y, alpha_in<span class="op">=</span><span class="fl">0.05</span>, alpha_out<span class="op">=</span><span class="fl">0.10</span>, verbose<span class="op">=</span><span class="va">True</span>):</span>
<span id="cb3-33"><a href="#cb3-33" aria-hidden="true" tabindex="-1"></a>    <span class="co">""" 단계적 선택: forward + backward """</span></span>
<span id="cb3-34"><a href="#cb3-34" aria-hidden="true" tabindex="-1"></a>    included <span class="op">=</span> []</span>
<span id="cb3-35"><a href="#cb3-35" aria-hidden="true" tabindex="-1"></a>    <span class="cf">while</span> <span class="va">True</span>:</span>
<span id="cb3-36"><a href="#cb3-36" aria-hidden="true" tabindex="-1"></a>        changed <span class="op">=</span> <span class="va">False</span></span>
<span id="cb3-37"><a href="#cb3-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-38"><a href="#cb3-38" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Forward Step</span></span>
<span id="cb3-39"><a href="#cb3-39" aria-hidden="true" tabindex="-1"></a>        excluded <span class="op">=</span> <span class="bu">list</span>(<span class="bu">set</span>(X.columns) <span class="op">-</span> <span class="bu">set</span>(included))</span>
<span id="cb3-40"><a href="#cb3-40" aria-hidden="true" tabindex="-1"></a>        new_pvals <span class="op">=</span> pd.Series(index<span class="op">=</span>excluded, dtype<span class="op">=</span><span class="bu">float</span>)</span>
<span id="cb3-41"><a href="#cb3-41" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> new_col <span class="kw">in</span> excluded:</span>
<span id="cb3-42"><a href="#cb3-42" aria-hidden="true" tabindex="-1"></a>            model <span class="op">=</span> sm.OLS(y, sm.add_constant(X[included <span class="op">+</span> [new_col]], has_constant<span class="op">=</span><span class="st">'add'</span>)).fit()</span>
<span id="cb3-43"><a href="#cb3-43" aria-hidden="true" tabindex="-1"></a>            new_pvals[new_col] <span class="op">=</span> model.pvalues[new_col]</span>
<span id="cb3-44"><a href="#cb3-44" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="kw">not</span> new_pvals.empty:</span>
<span id="cb3-45"><a href="#cb3-45" aria-hidden="true" tabindex="-1"></a>            best_pval <span class="op">=</span> new_pvals.<span class="bu">min</span>()</span>
<span id="cb3-46"><a href="#cb3-46" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> best_pval <span class="op">&lt;</span> alpha_in:</span>
<span id="cb3-47"><a href="#cb3-47" aria-hidden="true" tabindex="-1"></a>                best_feature <span class="op">=</span> new_pvals.idxmin()</span>
<span id="cb3-48"><a href="#cb3-48" aria-hidden="true" tabindex="-1"></a>                included.append(best_feature)</span>
<span id="cb3-49"><a href="#cb3-49" aria-hidden="true" tabindex="-1"></a>                changed <span class="op">=</span> <span class="va">True</span></span>
<span id="cb3-50"><a href="#cb3-50" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> verbose:</span>
<span id="cb3-51"><a href="#cb3-51" aria-hidden="true" tabindex="-1"></a>                    <span class="bu">print</span>(<span class="ss">f"[ADD] </span><span class="sc">{</span>best_feature<span class="sc">}</span><span class="ss"> (p=</span><span class="sc">{</span>best_pval<span class="sc">:.4g}</span><span class="ss">)"</span>)</span>
<span id="cb3-52"><a href="#cb3-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-53"><a href="#cb3-53" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Backward Step</span></span>
<span id="cb3-54"><a href="#cb3-54" aria-hidden="true" tabindex="-1"></a>        model <span class="op">=</span> sm.OLS(y, sm.add_constant(X[included], has_constant<span class="op">=</span><span class="st">'add'</span>)).fit()</span>
<span id="cb3-55"><a href="#cb3-55" aria-hidden="true" tabindex="-1"></a>        pvals <span class="op">=</span> model.pvalues.drop(<span class="st">"const"</span>)</span>
<span id="cb3-56"><a href="#cb3-56" aria-hidden="true" tabindex="-1"></a>        worst_pval <span class="op">=</span> pvals.<span class="bu">max</span>()</span>
<span id="cb3-57"><a href="#cb3-57" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> worst_pval <span class="op">&gt;</span> alpha_out:</span>
<span id="cb3-58"><a href="#cb3-58" aria-hidden="true" tabindex="-1"></a>            worst_feature <span class="op">=</span> pvals.idxmax()</span>
<span id="cb3-59"><a href="#cb3-59" aria-hidden="true" tabindex="-1"></a>            included.remove(worst_feature)</span>
<span id="cb3-60"><a href="#cb3-60" aria-hidden="true" tabindex="-1"></a>            changed <span class="op">=</span> <span class="va">True</span></span>
<span id="cb3-61"><a href="#cb3-61" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> verbose:</span>
<span id="cb3-62"><a href="#cb3-62" aria-hidden="true" tabindex="-1"></a>                <span class="bu">print</span>(<span class="ss">f"[REMOVE] </span><span class="sc">{</span>worst_feature<span class="sc">}</span><span class="ss"> (p=</span><span class="sc">{</span>worst_pval<span class="sc">:.4g}</span><span class="ss">)"</span>)</span>
<span id="cb3-63"><a href="#cb3-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-64"><a href="#cb3-64" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="kw">not</span> changed:</span>
<span id="cb3-65"><a href="#cb3-65" aria-hidden="true" tabindex="-1"></a>            <span class="cf">break</span></span>
<span id="cb3-66"><a href="#cb3-66" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> included</span>
<span id="cb3-67"><a href="#cb3-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-68"><a href="#cb3-68" aria-hidden="true" tabindex="-1"></a><span class="co"># --- 2) 실행 ---</span></span>
<span id="cb3-69"><a href="#cb3-69" aria-hidden="true" tabindex="-1"></a>selected_features <span class="op">=</span> stepwise_selection(X, y, alpha_in<span class="op">=</span><span class="fl">0.05</span>, alpha_out<span class="op">=</span><span class="fl">0.10</span>, verbose<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb3-70"><a href="#cb3-70" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">최종 선택 변수:"</span>, selected_features)</span>
<span id="cb3-71"><a href="#cb3-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-72"><a href="#cb3-72" aria-hidden="true" tabindex="-1"></a><span class="co"># --- 3) 최종 모형 적합 ---</span></span>
<span id="cb3-73"><a href="#cb3-73" aria-hidden="true" tabindex="-1"></a>X_final <span class="op">=</span> sm.add_constant(X[selected_features], has_constant<span class="op">=</span><span class="st">'add'</span>)</span>
<span id="cb3-74"><a href="#cb3-74" aria-hidden="true" tabindex="-1"></a>final_model <span class="op">=</span> sm.OLS(y, X_final).fit()</span>
<span id="cb3-75"><a href="#cb3-75" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(final_model.summary())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>[ADD] LSTAT (p=5.081e-88) [ADD] RM (p=3.472e-27) [ADD] PTRATIO (p=1.645e-14) [ADD] DIS (p=1.668e-05) [ADD] NOX (p=5.488e-08) [ADD] CHAS (p=0.0002655) [ADD] B (p=0.0007719) [ADD] ZN (p=0.004652) [ADD] CRIM (p=0.04457) [ADD] RAD (p=0.001692) [ADD] TAX (p=0.0005214) <br> 최종 선택 변수: [‘LSTAT’, ‘RM’, ‘PTRATIO’, ‘DIS’, ‘NOX’, ‘CHAS’, ‘B’, ‘ZN’, ‘CRIM’, ‘RAD’, ‘TAX’]</p>
</section>
<section id="최적모형-adjusted-r²-기준" class="level5">
<h5 class="anchored" data-anchor-id="최적모형-adjusted-r²-기준">(4) 최적모형 (Adjusted R² 기준)</h5>
<div class="sourceCode" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># ==============================================</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Boston Housing: 최적모형: 수정 결정계수 기준</span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="co"># ==============================================</span></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> itertools</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> statsmodels.api <span class="im">as</span> sm</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> fetch_openml</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. 데이터 불러오기</span></span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>boston <span class="op">=</span> fetch_openml(name<span class="op">=</span><span class="st">"boston"</span>, version<span class="op">=</span><span class="dv">1</span>, as_frame<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> boston.frame.copy()</span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a><span class="co"># 종속변수 / 설명변수</span></span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> pd.to_numeric(df[<span class="st">"MEDV"</span>], errors<span class="op">=</span><span class="st">"coerce"</span>)</span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> df.drop(columns<span class="op">=</span>[<span class="st">"MEDV"</span>]).copy()</span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a><span class="co"># 숫자형 변환</span></span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> c <span class="kw">in</span> X.columns:</span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> X[c].dtype.name <span class="kw">in</span> [<span class="st">"object"</span>, <span class="st">"category"</span>]:</span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a>        X[c] <span class="op">=</span> X[c].astype(<span class="bu">str</span>).<span class="bu">str</span>.strip()</span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a>        X[c] <span class="op">=</span> pd.to_numeric(X[c], errors<span class="op">=</span><span class="st">"coerce"</span>)</span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a>        X[c] <span class="op">=</span> X[c].astype(<span class="bu">float</span>)</span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a><span class="co"># 결측 제거</span></span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a>mask <span class="op">=</span> y.notna() <span class="op">&amp;</span> X.notna().<span class="bu">all</span>(axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> y[mask].reset_index(drop<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb4-30"><a href="#cb4-30" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> X.loc[mask].reset_index(drop<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb4-31"><a href="#cb4-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-32"><a href="#cb4-32" aria-hidden="true" tabindex="-1"></a><span class="co"># 2. Best Subset 함수</span></span>
<span id="cb4-33"><a href="#cb4-33" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> best_subset(X, y, max_features<span class="op">=</span><span class="dv">5</span>, criterion<span class="op">=</span><span class="st">"adj_r2"</span>):</span>
<span id="cb4-34"><a href="#cb4-34" aria-hidden="true" tabindex="-1"></a>    results <span class="op">=</span> []</span>
<span id="cb4-35"><a href="#cb4-35" aria-hidden="true" tabindex="-1"></a>    n <span class="op">=</span> <span class="bu">len</span>(y)</span>
<span id="cb4-36"><a href="#cb4-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-37"><a href="#cb4-37" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, max_features<span class="op">+</span><span class="dv">1</span>):  <span class="co"># 변수 개수 1개부터 max_features까지</span></span>
<span id="cb4-38"><a href="#cb4-38" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> combo <span class="kw">in</span> itertools.combinations(X.columns, k):</span>
<span id="cb4-39"><a href="#cb4-39" aria-hidden="true" tabindex="-1"></a>            X_combo <span class="op">=</span> sm.add_constant(X[<span class="bu">list</span>(combo)], has_constant<span class="op">=</span><span class="st">"add"</span>)</span>
<span id="cb4-40"><a href="#cb4-40" aria-hidden="true" tabindex="-1"></a>            model <span class="op">=</span> sm.OLS(y, X_combo).fit()</span>
<span id="cb4-41"><a href="#cb4-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-42"><a href="#cb4-42" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> criterion <span class="op">==</span> <span class="st">"adj_r2"</span>:</span>
<span id="cb4-43"><a href="#cb4-43" aria-hidden="true" tabindex="-1"></a>                score <span class="op">=</span> model.rsquared_adj</span>
<span id="cb4-44"><a href="#cb4-44" aria-hidden="true" tabindex="-1"></a>            <span class="cf">elif</span> criterion <span class="op">==</span> <span class="st">"aic"</span>:</span>
<span id="cb4-45"><a href="#cb4-45" aria-hidden="true" tabindex="-1"></a>                score <span class="op">=</span> <span class="op">-</span>model.aic  <span class="co"># 최소화 기준 → 부호 바꿔서 최대화</span></span>
<span id="cb4-46"><a href="#cb4-46" aria-hidden="true" tabindex="-1"></a>            <span class="cf">elif</span> criterion <span class="op">==</span> <span class="st">"bic"</span>:</span>
<span id="cb4-47"><a href="#cb4-47" aria-hidden="true" tabindex="-1"></a>                score <span class="op">=</span> <span class="op">-</span>model.bic</span>
<span id="cb4-48"><a href="#cb4-48" aria-hidden="true" tabindex="-1"></a>            <span class="cf">else</span>:</span>
<span id="cb4-49"><a href="#cb4-49" aria-hidden="true" tabindex="-1"></a>                <span class="cf">raise</span> <span class="pp">ValueError</span>(<span class="st">"criterion은 'adj_r2', 'aic', 'bic' 중 선택"</span>)</span>
<span id="cb4-50"><a href="#cb4-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-51"><a href="#cb4-51" aria-hidden="true" tabindex="-1"></a>            results.append({</span>
<span id="cb4-52"><a href="#cb4-52" aria-hidden="true" tabindex="-1"></a>                <span class="st">"num_features"</span>: k,</span>
<span id="cb4-53"><a href="#cb4-53" aria-hidden="true" tabindex="-1"></a>                <span class="st">"features"</span>: combo,</span>
<span id="cb4-54"><a href="#cb4-54" aria-hidden="true" tabindex="-1"></a>                <span class="st">"criterion"</span>: score</span>
<span id="cb4-55"><a href="#cb4-55" aria-hidden="true" tabindex="-1"></a>            })</span>
<span id="cb4-56"><a href="#cb4-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-57"><a href="#cb4-57" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 최적 모형 선택</span></span>
<span id="cb4-58"><a href="#cb4-58" aria-hidden="true" tabindex="-1"></a>    best_model <span class="op">=</span> <span class="bu">max</span>(results, key<span class="op">=</span><span class="kw">lambda</span> x: x[<span class="st">"criterion"</span>])</span>
<span id="cb4-59"><a href="#cb4-59" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> pd.DataFrame(results), best_model</span>
<span id="cb4-60"><a href="#cb4-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-61"><a href="#cb4-61" aria-hidden="true" tabindex="-1"></a><span class="co"># 3. 실행 (예: 최대 10개 변수까지 탐색, Adjusted R² 기준)</span></span>
<span id="cb4-62"><a href="#cb4-62" aria-hidden="true" tabindex="-1"></a>all_results, best <span class="op">=</span> best_subset(X, y, max_features<span class="op">=</span><span class="dv">7</span>, criterion<span class="op">=</span><span class="st">"adj_r2"</span>)</span>
<span id="cb4-63"><a href="#cb4-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-64"><a href="#cb4-64" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Best Model (Adjusted R² 기준):"</span>)</span>
<span id="cb4-65"><a href="#cb4-65" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"변수:"</span>, best[<span class="st">"features"</span>])</span>
<span id="cb4-66"><a href="#cb4-66" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Adj R²:"</span>, best[<span class="st">"criterion"</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Best Model (Adjusted R² 기준): 변수: (‘CHAS’, ‘NOX’, ‘RM’, ‘DIS’, ‘PTRATIO’, ‘B’, ‘LSTAT’) <br> Adj R²: 0.7182560407158507</p>
</section>
</section>
</section>
<section id="chapter-3.-변수선택-빅데이터-방법" class="level3">
<h3 class="anchored" data-anchor-id="chapter-3.-변수선택-빅데이터-방법"><span style="color:green">chapter 3. 변수선택 빅데이터 방법 </span></h3>
<section id="과적합과-과소적합" class="level4">
<h4 class="anchored" data-anchor-id="과적합과-과소적합"><span style="color:blue"> 1. 과적합과 과소적합</span></h4>
<section id="과적합-과소적합-개념" class="level5">
<h5 class="anchored" data-anchor-id="과적합-과소적합-개념">(1) 과적합, 과소적합 개념</h5>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/과적합개념.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:80.0%"></p>
</figure>
</div>
<p><strong>과적합 overfitting</strong></p>
<p>과적합은 우리가 추정한 모형이<span dir="rtl">”</span>너무 잘 훈련”되었고 현재는 훈련 데이터 세트를 가장 잘 설명하고 있다는 것을 의미한다. 위의 오른쪽 두 추정 모형을 보면 과적합으로 인하여 데이터에 범용(향후 검증 데이터에 적용 가능한지? 혹은 일반 상황에도 적용 가능한지?) 적합한 모형인지 이 훈련 데이터에만 적합한지 알 수 없다는 사실이다.</p>
<p>관측치 수에 비해 너무 많은 특징(변수)가 존재하는 경우 발생하며 추정 모형은 훈련 데이터에서 매우 정확하지만 훈련되지 않은 또는 새로운 데이터에서는 정확하지 않을 수 있다.</p>
<p>이 추정모형의 결과를 일반화하고 다른 데이터, 즉 궁극적으로 수행하려는 작업에 대해 추론 할 수 없음을 의미한다. 기본적으로 과적합이 발생하면 모델은 데이터의 변수 간 실제 관계 대신 학습 데이터의 "노이즈"를 학습하거나 설명하게 되므로 이 노이즈가 포함되어 있지 않은 (검증)데이터 세트에는 적용할 수 없게 된다.</p>
<p><strong>과소적합 underfitting</strong></p>
<p>과적합과 대조적으로, 모델이 과소적합되면 모델이 훈련 데이터에 적합하지 않으므로 데이터의 추세를 놓치게 되며 또한 추정 모형을 새 데이터로 일반화 할 수 없음을 의미한다.</p>
<p>이는 매우 간단한 모형 (충분한 예측 변수 / 독립 변수가 아님)의 결과로 선형이 아닌 데이터에 선형 모델 (예 : 선형 회귀)을 적용 할 때도 발생할 수 있다. 이 모델은 예측 능력이 좋지 않을 것이며 훈련 데이터에서 다른 데이터로 일반화 할 수 없음을 알 수 있다.</p>
</section>
<section id="과적합과-변수선택" class="level5">
<h5 class="anchored" data-anchor-id="과적합과-변수선택">(2) 과적합과 변수선택</h5>
<p>과적합(overfitting)이란 모형이 훈련 데이터의 노이즈까지 학습하여, 새로운 데이터 예측력이 떨어지는 현상으로 주된 원인 중 하나가 불필요한 변수가 너무 많이 포함된 경우이다. 특히 빅데이터(p ≫ n)에서는, 변수가 많아질수록 OLS 회귀는 데이터를 지나치게 잘 맞추고, 일반화는 망가진다.</p>
<p>변수 선택(variable selection)의 목적은 필요한 설명변수만 남기고, 불필요한 변수는 제거하여 모형을 단순화하고, 과적합을 줄이는 것이다.즉, 변수 선택은 편향–분산 균형에서 분산을 줄이는 전략입니다.</p>
<p><strong>편향–분산 관점</strong></p>
<p>변수를 많이 넣으면 → 분산(variance) ↑, 과적합 ↑.</p>
<p>변수를 줄이면 → 편향(bias) ↑, 그러나 분산 ↓, 일반화 성능 ↑.</p>
<p>따라서 변수 선택은 <span dir="rtl">”</span>조금의 편향을 감수하고, 분산을 크게 줄여 과적합을 방지”하는 절차입니다.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/과적합개념2.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:30.0%"></p>
</figure>
</div>
</section>
</section>
<section id="l1-and-l2-regularization" class="level4">
<h4 class="anchored" data-anchor-id="l1-and-l2-regularization"><span style="color:blue"> 2. L1 and L2 Regularization</span></h4>
<p>과적합 문제를 해결하기 위한 전처리 작업에 해당된다.</p>
<ul>
<li><p>Lasso(L1): 일부 계수를 0으로 만들어 변수 제거 → 불필요한 변수 자동 차단 → 과적합 방지.</p></li>
<li><p>Ridge(L2): 계수를 수축해 노이즈에 덜 민감하게 함 → 다중공선성 완화.</p></li>
<li><p>Elastic Net: L1+L2 혼합으로 변수 선택 + 안정성.</p></li>
<li><p>트리 기반(Random Forest, XGBoost): 변수 중요도 기반으로 핵심 변수만 반영.</p></li>
<li><p>PCA/PLS: 차원 축소로 고차원 문제 해결.</p></li>
</ul>
<p><strong>Norm <span class="math inline">\(||X_{p}|| = (\sum|x_{i}|^{p})^{\frac{1}{p}}\)</span></strong></p>
<p>벡터의 크기 혹은 두 벡터의 거리를 측정하는 함수이다. p=놈의 차수를 의미하며 p=1이면 <span class="math inline">\(L_{1}\)</span> 놈, p=2이면 <span class="math inline">\(L_{2}\)</span>놈이라 한다.</p>
<p>【<span class="math inline">\(L_{1}\)</span>놈】 맨하턴(Manhattan) 거리이다. 좌표축을 따라 이동하는 거리로 <span dir="rtl">”</span>맨해튼 거리”라고 불리는 이유는 뉴욕 맨해튼처럼 격자형 도로만 따라 움직여야 할 때의 거리와 같기 때문이다. 회귀에서 L1 규제(Lasso) 는 이 L1 놈을 이용해 계수들의 크기를 제약. → 많은 계수가 정확히 0이 되어 변수 선택 효과 발생.</p>
<p>【<span class="math inline">\(L_{2}\)</span> 놈】 유클리디안(Eucleadian) 거리이다. 우리가 익숙한 <span dir="rtl">”</span>직선 거리”, 즉 피타고라스의 정리로 계산되는 거리이다. 회귀에서 L2 규제(Ridge) 는 이 L2 놈을 이용해 계수들을 제약하여, 계수를 0에 가깝게 줄이되, 정확히 0으로 만들지는 않는다.</p>
<p><strong>L1 vs L2 규제와의 연결</strong></p>
<p>L1 (맨해튼) → 축을 따라 sharp한 모서리가 있기 때문에 계수가 정확히 0으로 떨어질 가능성이 높음. → 변수 선택 효과.</p>
<p>L2 (유클리디안) → 원 모양 제약이라 계수를 부드럽게 shrink. → 안정성은 높지만, 변수 선택은 없다.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/LASSO개념.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:60.0%"></p>
</figure>
</div>
<p><strong>L1 Regularization: Lasso Regression&nbsp;(Least Absolute Shrinkage and Selection Operator)</strong>&nbsp;</p>
<p>손실함수 <span class="math inline">\(OLS(L(y_i, \hat y_i))+\lambda \sum |\beta_j |)\)</span></p>
<ul>
<li><p><span class="math inline">\(\lambda=0\)</span> 이면 OLS 추정과 동일하고 <span class="math inline">\(\lambda\)</span> 가 커지면 회귀계수 영향이 작아져 과소적합이 된다.</p></li>
<li><p>목표변수에 영향도가 적은 예측변수의 회귀계수를 크기를 최소화 하고 영향도가 큰 예측변수의 회귀계수 크기를 상대적으로 크게 하여 주요 예측변수 <span dir="rtl">’</span>feature selection<span dir="rtl">’</span>에 사용된다.</p></li>
</ul>
<p><strong>L2 Regularization: Ridge Regression 능형 회귀분석</strong></p>
<p>손실함수 <span class="math inline">\(OLS(L(y_i, \hat y_i))+\lambda \sum \beta_j^2 )\)</span></p>
<ul>
<li><p><span class="math inline">\(\lambda =0\)</span> 이면 OLS 추정과 동일하고 <span class="math inline">\(\lambda\)</span> 가 커지면 불편성이 발생하고 MSE를 최소화 한다.</p></li>
<li><p>다중공선성 문제를 해결하는 정규화 방법이다.</p></li>
</ul>
</section>
<section id="features-enginerring" class="level4">
<h4 class="anchored" data-anchor-id="features-enginerring"><span style="color:blue"> 3. Features Enginerring</span></h4>
<p><strong>Feature 선택</strong></p>
<p>Feature 순위 또는 Feature 중요도 부여하는 과정으로 중요 변수(주요 신호)의 부분집합을 선택하거나, 불필요한(중요하지 않은) 변수들을 제거하여 변수의 차원(개수)을 줄이는데 그 목적이 있다. 분류모델 중 Decision Tree 같은 경우는 트리의 상단에 있을수록 중요도가 높으므로 이를 반영하여 특징 별로 중요도를 매길 수 있다. 회귀모델의 경우에는 유의 변수선책 알고리즘을 통해 변수 선택이 가능하다.</p>
<p>Feature 선택기법에는 상관분석과 모델기반 방법인 Lasso Regression, Recursive Feature Elimination, Tree-based Model, (Logistics) Discriminant Analysis 등이 있다</p>
<p><strong>Feature 추출</strong></p>
<p>원데이터 변수들의 구조적 관계를 활용하여 새로운 변수를 생성하거나(고차원의 공간을 저차원의 새로운 공간으로 차원축소) 변수들 간의 상관계수를 변수 유사성 척도로 하여 Feature를 탐색한다. PCA, SVD 차원축소 방법도 여기에 해당하며, 상관관계 유의한 변수군을 만드는 Canonical Correlation Analysis(CCA) 등이 Feature 추출 과정의 방법론이다.</p>
</section>
<section id="데이터-분할-data-split" class="level4">
<h4 class="anchored" data-anchor-id="데이터-분할-data-split"><span style="color:blue"> 4. 데이터 분할 Data Split</span></h4>
<p><strong>train and test data</strong></p>
<p>대용량 빅데이터의 경우에는 데이터 세트를 2분화 하여 모형을 추정하는 데이터(훈련 데이터 train dataset), 추정 모형을 이용하여 예측 정확도를 판단하는데(평가) 활용되는 데이터(검증 test dataset)로 활용한다. 일반적으로 8:2 혹은 7:3으로 나눈다. 이는 예측모형의 과적합과 underfitting 문제를 해결하기 위하여 머신러닝 기법에서 제안된 방법이다. 이방법은 전혀 새롭지 않은 개념이다. 이전 데이터마이닝에서 데이터를 3단계, train, validation, test 데이터로 나눈 것과 유사하다.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/데이터분할.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:60.0%"></p>
</figure>
</div>
<p><strong>cross validation 필요 이유</strong></p>
<p>훈련 데이터와 검증 데이터의 스플릿이 무작위가 아닌 경우 문제가 발생한다. 훈련 데이터가 특정 변인에 의해 왜곡되어 있다면 과적합 문제가 발생하다. 예를 들어 우연히 소득수준이 높은 집단이 훈련 데이터에 많이 포함되어 있다면? 한 번의 데이터 스플릿에 의한 분석은 왜곡된 정보를 준다.</p>
<p><strong>K-Folds Cross Validation</strong>에서 훈련 데이터를 k 개의 다른 부분 집합으로 나눈다. k-1 부분 집합을 사용하여 데이터를 훈련시키고 마지막 부분 집합을 테스트 데이터로 하여 검증하게 된다. 이런 과정을 k번 거쳐 계산된 정확도 평균을 훈련 데이터 정확도로 하게 되며 그런 다음 검증 데이터 세트에 대해 검증한다.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/크로스벨리데이션.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:70.0%"></p>
</figure>
</div>
</section>
<section id="사례분석" class="level4">
<h4 class="anchored" data-anchor-id="사례분석"><span style="color:blue"> 5. 사례분석</span></h4>
<section id="univariate-selection-전진삽입방법과-동일" class="level5">
<h5 class="anchored" data-anchor-id="univariate-selection-전진삽입방법과-동일">(1) Univariate Selection : 전진삽입방법과 동일</h5>
<div class="sourceCode" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co"># ==============================================</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Boston Housing: Univariate Selection</span></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="co"># ==============================================</span></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> statsmodels.api <span class="im">as</span> sm</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> fetch_openml</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. 데이터 불러오기 (OpenML에서 Boston Housing)</span></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>boston <span class="op">=</span> fetch_openml(name<span class="op">=</span><span class="st">"boston"</span>, version<span class="op">=</span><span class="dv">1</span>, as_frame<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> boston.frame</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a><span class="co">#Univariate Selection :  전진삽입방법과 동일</span></span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.feature_selection <span class="im">import</span> SelectKBest</span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.feature_selection <span class="im">import</span> f_regression</span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a>y<span class="op">=</span>df[<span class="st">'MEDV'</span>]</span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a>X<span class="op">=</span>df.iloc[:,<span class="dv">0</span>:<span class="dv">13</span>]</span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a>bestfeatures <span class="op">=</span> SelectKBest(score_func<span class="op">=</span>f_regression, k<span class="op">=</span><span class="dv">10</span>) <span class="co">#default k=10</span></span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a>fit<span class="op">=</span>bestfeatures.fit(X,y)</span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a>dfpvalues <span class="op">=</span> pd.DataFrame(fit.pvalues_)</span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a>dfscores <span class="op">=</span> pd.DataFrame(fit.scores_)</span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a>dfcolumns <span class="op">=</span> pd.DataFrame(X.columns)</span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a><span class="co">#concat two dataframes for better visualization</span></span>
<span id="cb5-25"><a href="#cb5-25" aria-hidden="true" tabindex="-1"></a>featureScores <span class="op">=</span> pd.concat([dfcolumns,dfscores,dfpvalues],axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb5-26"><a href="#cb5-26" aria-hidden="true" tabindex="-1"></a>featureScores.columns <span class="op">=</span> [<span class="st">'features'</span>,<span class="st">'score'</span>,<span class="st">'pvalues'</span>]  <span class="co">#naming the dataframe columns</span></span>
<span id="cb5-27"><a href="#cb5-27" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(featureScores.nlargest(<span class="dv">7</span>,<span class="st">'score'</span>))  <span class="co">#print best features</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>features score pvalues <br> 12 LSTAT 601.617871 5.081103e-88 <br> 5 RM 471.846740 2.487229e-74 <br> 10 PTRATIO 175.105543 1.609509e-34 <br> 2 INDUS 153.954883 4.900260e-31 <br> 9 TAX 141.761357 5.637734e-29 <br> 4 NOX 112.591480 7.065042e-24 <br> 0 CRIM 89.486115 1.173987e-19</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co">#유의수준 5% 유의한 변수 선택</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>select_vars<span class="op">=</span>featureScores.loc[featureScores[<span class="st">'pvalues'</span>]<span class="op">&lt;</span><span class="fl">0.05</span>,:][<span class="st">'features'</span>]</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(select_vars)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>0 CRIM 1 ZN 2 INDUS 3 CHAS 4 NOX 5 RM 6 AGE <br> 7 DIS 8 RAD 9 TAX 10 PTRATIO. 11 B 12 LSTAT</p>
</section>
<section id="재귀적-변수제거-recursive-feature-elimination-rfe" class="level5">
<h5 class="anchored" data-anchor-id="재귀적-변수제거-recursive-feature-elimination-rfe">(2) 재귀적 변수제거 Recursive Feature Elimination (RFE)</h5>
<p>RFE는 모형 적합 → 중요도 평가 → 가장 덜 중요한 변수 제거 → 반복이라는 순환 절차를 통해 최적 변수 집합을 찾는 방법이다. 이는 변수 선택을 자동화할 수 있는 강력한 도구지만, 계산량이 많고 변수 간 상관관계에 민감하므로 이론적 근거와 병행해 사용하는 것이 바람직하다.</p>
<p>재귀적 변수 제거(RFE)는 많은 예측변수 중에서 중요하지 않은 변수를 단계적으로 제거하면서 최적의 변수 집합을 찾는 방법이다. 이름 그대로 <span dir="rtl">”</span>재귀적으로(feature by feature)” 변수를 줄여 나간다는 점이 특징이다.</p>
<p>1. 우선 모든 예측변수를 포함하여 모형을 적합시킨다. 이때 사용할 수 있는 모형은 선형 회귀, 로지스틱 회귀, 서포트 벡터 머신(SVM), 트리 기반 모형 등 예측력이 있는 알고리즘이면 된다.</p>
<p>2. 적합된 모형에서 각 변수의 중요도(feature importance 또는 회귀계수 크기)를 평가한다.</p>
<p>3. 가장 중요도가 낮은 변수를 제거한 후, 남은 변수들로 다시 모형을 적합한다.</p>
<p>4. 동일한 과정을 반복하면서 변수를 하나씩 줄여 나가고, 각 단계별로 모형의 성능(예: 교차검증 정확도, RMSE, AUC 등)을 기록한다.</p>
<p>5. 성능이 가장 우수한 시점에서 남아 있는 변수 집합을 최종 선택한다.</p>
<p><strong>장점</strong></p>
<ul>
<li><p>변수 선택 과정을 체계적이고 자동화할 수 있다.</p></li>
<li><p>단순히 p-value 기준으로 변수 제거하는 방법보다, 예측 성능을 직접 기준으로 삼기 때문에 실무 데이터 분석에서 신뢰성이 높다.</p></li>
<li><p>특정 변수들의 상호작용이나 중요도의 상대적 크기를 평가할 수 있다.</p></li>
</ul>
<p><strong>단점</strong></p>
<ul>
<li><p>변수 개수가 많으면 모든 반복 단계에서 모형을 다시 학습해야 하므로 계산량이 매우 크다.</p></li>
<li><p>변수 간 다중공선성이 심할 경우, 중요도 평가가 왜곡될 수 있다.</p></li>
<li><p><span dir="rtl">”</span>어떤 변수가 반드시 포함되어야 한다”는 이론적 근거를 반영하지 못하고, 순전히 데이터 기반으로 선택한다는 한계가 있다.</p></li>
</ul>
<div class="sourceCode" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="co"># ==============================================</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Boston Housing: RFE 방법</span></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="co"># ==============================================</span></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. 데이터 불러오기 (OpenML에서 Boston Housing)</span></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>boston <span class="op">=</span> fetch_openml(name<span class="op">=</span><span class="st">"boston"</span>, version<span class="op">=</span><span class="dv">1</span>, as_frame<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> boston.frame</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a><span class="co">#Recursive Feature Elimination : 후진 제거방법과 동일</span></span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>y<span class="op">=</span>df[<span class="st">'MEDV'</span>]</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>X<span class="op">=</span>df.iloc[:,<span class="dv">0</span>:<span class="dv">13</span>]</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.feature_selection <span class="im">import</span> RFE</span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.svm <span class="im">import</span> SVR</span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a>estimator <span class="op">=</span> SVR(kernel<span class="op">=</span><span class="st">"linear"</span>)</span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>selector <span class="op">=</span> RFE(estimator,n_features_to_select<span class="op">=</span><span class="dv">7</span>) <span class="co">#최적 7개 변수만 선택</span></span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a>selector <span class="op">=</span> selector.fit(X, y)</span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(X.columns,<span class="st">'</span><span class="ch">\n</span><span class="st">'</span>, selector.support_,<span class="st">'</span><span class="ch">\n</span><span class="st">'</span>,selector.ranking_)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Index(['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', ’TAX', 'PTRATIO', 'B', 'LSTAT'],</p>
<p>[ True False False True True True False True False False True False True]</p>
<p>[1 3 2 1 1 1 4 1 6 7 1 5 1]</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="co">#유의한 예측변수</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>var_select<span class="op">=</span>pd.DataFrame(np.transpose([np.array(X.columns),selector.ranking_]))</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>var_select.columns<span class="op">=</span>[<span class="st">'var_name'</span>,<span class="st">'rank'</span>]</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(var_select.loc[var_select[<span class="st">'rank'</span>]<span class="op">==</span><span class="dv">1</span>,<span class="st">"var_name"</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>0 CRIM 3 CHAS 4 NOX 5 RM 7 DIS 10 PTRATIO. 12 LSTAT</p>
</section>
<section id="lasso-방법" class="level5">
<h5 class="anchored" data-anchor-id="lasso-방법">(3) LASSO 방법</h5>
<p>데이터 값을 축소(shrinkage) - 중심점 평균으로 중심화 하여(L1 정규화) 목표변수에 영향을 많이 미치는(목표변수 변동을 보다 잘 설명하는) 예측변수를 선택하거나 다중공선성 문제 진단하는데도 사용된다.</p>
<p>손실함수 <span class="math inline">\(OLS(L(y_i, \hat y_i))+\lambda \sum |\beta_j |)\)</span> 을 최소화 하는 것은 $ | _j | s $ 제약 조건 하에서 OLS의 SSE를 최소화 하는 추정치를 구하는 것과 동일하다.</p>
<p><strong><span class="math inline">\(\lambda\)</span> 결정</strong></p>
<ul>
<li><p><span class="math inline">\(\lambda = 0\)</span> : OLS 추정과 동일, 모든 예측변수가 추정된다.</p></li>
<li><p><span class="math inline">\(\lambda = \infty\)</span> : 모든 예측변수의 회귀계수가 0이 되어 모든 예측변수 제거된다.</p></li>
<li><p><span class="math inline">\(\lambda\)</span> 가 증가하면 추정 편이는 증가하고 제거되는 예측변수의 개수는 많아진다.</p></li>
<li><p>최적 람다는 AIC, BIC를 최소화 하는 람다를 구한다.</p></li>
</ul>
<div class="sourceCode" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="co"># ==============================================</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Boston Housing: LASSO 방법</span></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="co"># ==============================================</span></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn <span class="im">import</span> linear_model</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> StandardScaler</span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. 데이터 불러오기 (OpenML에서 Boston Housing)</span></span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>boston <span class="op">=</span> fetch_openml(name<span class="op">=</span><span class="st">"boston"</span>, version<span class="op">=</span><span class="dv">1</span>, as_frame<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> boston.frame</span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a>y<span class="op">=</span>df[<span class="st">'MEDV'</span>]</span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a>X<span class="op">=</span>df.iloc[:,<span class="dv">0</span>:<span class="dv">13</span>]</span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Standardize the features</span></span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a>scaler <span class="op">=</span> StandardScaler()</span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a>X_scaled <span class="op">=</span> scaler.fit_transform(X)</span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Apply Lasso regression</span></span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true" tabindex="-1"></a>lassoReg <span class="op">=</span> linear_model.Lasso(alpha<span class="op">=</span><span class="fl">0.1</span>, fit_intercept<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb9-20"><a href="#cb9-20" aria-hidden="true" tabindex="-1"></a>fit <span class="op">=</span> lassoReg.fit(X_scaled, y)</span>
<span id="cb9-21"><a href="#cb9-21" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Reg. Coefficients'</span>)</span>
<span id="cb9-22"><a href="#cb9-22" aria-hidden="true" tabindex="-1"></a>plt.plot(fit.coef_)</span>
<span id="cb9-23"><a href="#cb9-23" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/LASSO결과.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:60.0%"></p>
</figure>
</div>
<div class="sourceCode" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="co">#모든 예측변수 출력</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>var_nm<span class="op">=</span>pd.Series(X.columns,name<span class="op">=</span><span class="st">'Var'</span>)</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>coeff<span class="op">=</span>pd.Series(fit.coef_,name<span class="op">=</span><span class="st">'Coeff'</span>)</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>LASSO<span class="op">=</span>pd.concat([var_nm,coeff],axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(LASSO[<span class="bu">abs</span>(LASSO[<span class="st">'Coeff'</span>])<span class="op">&gt;</span><span class="fl">0.8</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Var Coeff <br> 4 NOX -1.574193 <br> 5 RM 2.826269 <br> 7 DIS -2.422079 <br> 8 RAD 1.195937 <br> 9 TAX -0.846468 <br> 10 PTRATIO -1.922493 <br> 12 LSTAT -3.726184</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="co"># ==============================================</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Boston Housing: LASSO 방법 최종 회귀모형 </span></span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a><span class="co"># ==============================================</span></span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> statsmodels.api <span class="im">as</span> sm</span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a><span class="co"># 1) LASSO에서 선택된 변수명 목록 만들기 (계수 절댓값 &gt; 1인 변수 6개)</span></span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a><span class="co">#    LASSO는 변수명 'Var', 계수 'Coeff' 컬럼을 가진 DataFrame이라고 가정</span></span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a>selected_vars <span class="op">=</span> LASSO.loc[LASSO[<span class="st">'Coeff'</span>].<span class="bu">abs</span>() <span class="op">&gt;</span> <span class="dv">1</span>, <span class="st">'Var'</span>].tolist()</span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a><span class="co"># 2) 종속/설명변수 준비 (반드시 DataFrame df에서 꺼냄)</span></span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> pd.to_numeric(df[<span class="st">'MEDV'</span>], errors<span class="op">=</span><span class="st">'coerce'</span>)</span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a><span class="co"># 선택된 변수들만 추출 (숫자형 강제 변환은 상황에 따라 추가)</span></span>
<span id="cb11-16"><a href="#cb11-16" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> df[selected_vars].copy()</span>
<span id="cb11-17"><a href="#cb11-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-18"><a href="#cb11-18" aria-hidden="true" tabindex="-1"></a><span class="co"># 필요시 숫자형 보정 (object/category 방지)</span></span>
<span id="cb11-19"><a href="#cb11-19" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> c <span class="kw">in</span> X.columns:</span>
<span id="cb11-20"><a href="#cb11-20" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> X[c].dtype.name <span class="kw">in</span> [<span class="st">'object'</span>, <span class="st">'category'</span>]:</span>
<span id="cb11-21"><a href="#cb11-21" aria-hidden="true" tabindex="-1"></a>        X[c] <span class="op">=</span> pd.to_numeric(X[c], errors<span class="op">=</span><span class="st">'coerce'</span>)</span>
<span id="cb11-22"><a href="#cb11-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-23"><a href="#cb11-23" aria-hidden="true" tabindex="-1"></a><span class="co"># y, X 결측 행 제거(동일 행 유지)</span></span>
<span id="cb11-24"><a href="#cb11-24" aria-hidden="true" tabindex="-1"></a>mask <span class="op">=</span> y.notna() <span class="op">&amp;</span> X.notna().<span class="bu">all</span>(axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb11-25"><a href="#cb11-25" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> y[mask]</span>
<span id="cb11-26"><a href="#cb11-26" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> X.loc[mask]</span>
<span id="cb11-27"><a href="#cb11-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-28"><a href="#cb11-28" aria-hidden="true" tabindex="-1"></a><span class="co"># 3) 절편 추가 후 OLS 적합</span></span>
<span id="cb11-29"><a href="#cb11-29" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> sm.add_constant(X, has_constant<span class="op">=</span><span class="st">'add'</span>)</span>
<span id="cb11-30"><a href="#cb11-30" aria-hidden="true" tabindex="-1"></a>fit <span class="op">=</span> sm.OLS(y, X).fit()</span>
<span id="cb11-31"><a href="#cb11-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-32"><a href="#cb11-32" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(selected_vars)      <span class="co"># 선택된 변수명 확인</span></span>
<span id="cb11-33"><a href="#cb11-33" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(fit.summary())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>[‘NOX’, ‘RM’, ‘DIS’, ‘RAD’, ‘PTRATIO’, ‘LSTAT’]</p>
<p>결정계수는 70.9%로 양호하며, 결과에서도 다중공선성 경고는 나타나지 않았다. 그러나 예측변수 RAD의 상관계수 부호와 회귀계수 부호가 일치하지 않아, 다중공선성 문제가 존재하는 것으로 판단된다(이에 대한 자세한 논의는 다음 장에서 다룬다).</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/LASSO보스톤예제.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:80.0%"></p>
</figure>
</div>
</section>
<section id="트리-기반-모형-tree-based-model" class="level5">
<h5 class="anchored" data-anchor-id="트리-기반-모형-tree-based-model">(4) 트리 기반 모형 (Tree-based Model)</h5>
<p>트리 기반 모형은 데이터를 조건에 따라 여러 번 분할하여 나무(tree) 구조 형태로 의사결정을 만들어내는 예측 기법이다. 각 분할은 특정 변수의 값을 기준으로 이루어지며, 최종적으로 분할이 끝난 말단 노드(leaf node)는 예측값이나 분류 집단을 의미한다. 즉, <span dir="rtl">”</span>조건 → 결과” 의 규칙들을 계층적으로 정리한 방법이 트리 기반 모형이다.</p>
<p>트리 모형은 크게 두 가지 목적으로 사용된다.</p>
<ul>
<li><p>회귀 트리(Regression Tree): 목표변수가 연속형일 때 사용. 각 노드에서 목표변수의 평균값 등을 예측값으로 사용한다.</p></li>
<li><p>분류 트리(Classification Tree): 목표변수가 범주형일 때 사용. 각 노드에서 다수 클래스 또는 확률을 기준으로 분류한다.</p></li>
</ul>
<p><strong>트리 모형의 분할 원리</strong></p>
<ul>
<li><p>트리는 데이터를 잘게 나누면서 <span dir="rtl">”</span>비슷한 값끼리 모으는” 것을 목표로 한다.</p></li>
<li><p>분류 문제에서는 지니지수(Gini Index), 엔트로피(Entropy) 같은 불순도(impurity) 기준을 사용한다.</p></li>
<li><p>회귀 문제에서는 잔차제곱합(SSE)을 최소화하는 방향으로 분할한다.</p></li>
<li><p>이렇게 반복적으로 최적의 분할을 찾아 내려가면서 트리 모형이 완성된다.</p></li>
</ul>
<p><strong>트리 기반 주요 방법: 1. 의사결정나무(Decision Tree)</strong></p>
<ul>
<li><p>하나의 트리를 완성하여 예측에 활용한다.</p></li>
<li><p>해석이 직관적이고 <span dir="rtl">”</span>규칙(rule)“을 제시할 수 있다는 장점이 있다.</p></li>
<li><p>그러나 트리를 깊게 성장시키면 과적합(overfitting)이 발생하기 쉽다.</p></li>
</ul>
<p>【변수선택】 변수를 하나씩 분할 기준으로 사용 → 분할에 자주 등장하는 변수가 중요한 변수이다. 각 노드에서 <span dir="rtl">”</span>얼마나 불순도를 줄였는가(분산 감소, 지니 감소, 정보이득)“를 통해 변수의 기여도를 평가한다.</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="co"># ==============================================</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Boston Housing - 의사결정나무 (회귀 / 분류)</span></span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a><span class="co"># ==============================================</span></span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> fetch_openml</span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> mean_squared_error, r2_score, accuracy_score, roc_auc_score</span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.tree <span class="im">import</span> DecisionTreeRegressor, DecisionTreeClassifier, plot_tree, export_text</span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> StandardScaler</span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a><span class="co"># -----------------------------</span></span>
<span id="cb12-15"><a href="#cb12-15" aria-hidden="true" tabindex="-1"></a><span class="co"># 0) 데이터 로드 &amp; 전처리</span></span>
<span id="cb12-16"><a href="#cb12-16" aria-hidden="true" tabindex="-1"></a><span class="co"># -----------------------------</span></span>
<span id="cb12-17"><a href="#cb12-17" aria-hidden="true" tabindex="-1"></a>boston <span class="op">=</span> fetch_openml(name<span class="op">=</span><span class="st">"boston"</span>, version<span class="op">=</span><span class="dv">1</span>, as_frame<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb12-18"><a href="#cb12-18" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> boston.frame.copy()</span>
<span id="cb12-19"><a href="#cb12-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-20"><a href="#cb12-20" aria-hidden="true" tabindex="-1"></a><span class="co"># 숫자형 강제 변환 및 결측 제거</span></span>
<span id="cb12-21"><a href="#cb12-21" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> c <span class="kw">in</span> df.columns:</span>
<span id="cb12-22"><a href="#cb12-22" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> df[c].dtype.name <span class="kw">in</span> [<span class="st">"object"</span>, <span class="st">"category"</span>]:</span>
<span id="cb12-23"><a href="#cb12-23" aria-hidden="true" tabindex="-1"></a>        df[c] <span class="op">=</span> pd.to_numeric(df[c].astype(<span class="bu">str</span>).<span class="bu">str</span>.strip(), errors<span class="op">=</span><span class="st">"coerce"</span>)</span>
<span id="cb12-24"><a href="#cb12-24" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> df.dropna().reset_index(drop<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb12-25"><a href="#cb12-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-26"><a href="#cb12-26" aria-hidden="true" tabindex="-1"></a><span class="co"># -----------------------------</span></span>
<span id="cb12-27"><a href="#cb12-27" aria-hidden="true" tabindex="-1"></a><span class="co"># 1) 회귀 트리 (y = MEDV)</span></span>
<span id="cb12-28"><a href="#cb12-28" aria-hidden="true" tabindex="-1"></a><span class="co"># -----------------------------</span></span>
<span id="cb12-29"><a href="#cb12-29" aria-hidden="true" tabindex="-1"></a>y_reg <span class="op">=</span> df[<span class="st">"MEDV"</span>].astype(<span class="bu">float</span>)</span>
<span id="cb12-30"><a href="#cb12-30" aria-hidden="true" tabindex="-1"></a>X_reg <span class="op">=</span> df.drop(columns<span class="op">=</span>[<span class="st">"MEDV"</span>]).astype(<span class="bu">float</span>)</span>
<span id="cb12-31"><a href="#cb12-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-32"><a href="#cb12-32" aria-hidden="true" tabindex="-1"></a>Xtr, Xte, ytr, yte <span class="op">=</span> train_test_split(X_reg, y_reg, test_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb12-33"><a href="#cb12-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-34"><a href="#cb12-34" aria-hidden="true" tabindex="-1"></a><span class="co"># 기본 트리</span></span>
<span id="cb12-35"><a href="#cb12-35" aria-hidden="true" tabindex="-1"></a>dt_reg <span class="op">=</span> DecisionTreeRegressor(random_state<span class="op">=</span><span class="dv">42</span>)  <span class="co"># 필요시 max_depth, min_samples_leaf 조정</span></span>
<span id="cb12-36"><a href="#cb12-36" aria-hidden="true" tabindex="-1"></a>dt_reg.fit(Xtr, ytr)</span>
<span id="cb12-37"><a href="#cb12-37" aria-hidden="true" tabindex="-1"></a>pred <span class="op">=</span> dt_reg.predict(Xte)</span>
<span id="cb12-38"><a href="#cb12-38" aria-hidden="true" tabindex="-1"></a>rmse <span class="op">=</span> np.sqrt(mean_squared_error(yte, pred))</span>
<span id="cb12-39"><a href="#cb12-39" aria-hidden="true" tabindex="-1"></a>r2 <span class="op">=</span> r2_score(yte, pred)</span>
<span id="cb12-40"><a href="#cb12-40" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"=== 회귀 트리 ==="</span>)</span>
<span id="cb12-41"><a href="#cb12-41" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"RMSE=</span><span class="sc">{</span>rmse<span class="sc">:.3f}</span><span class="ss">  R^2=</span><span class="sc">{</span>r2<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb12-42"><a href="#cb12-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-43"><a href="#cb12-43" aria-hidden="true" tabindex="-1"></a><span class="co"># 변수 중요도</span></span>
<span id="cb12-44"><a href="#cb12-44" aria-hidden="true" tabindex="-1"></a>imp_reg <span class="op">=</span> pd.Series(dt_reg.feature_importances_, index<span class="op">=</span>X_reg.columns).sort_values(ascending<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb12-45"><a href="#cb12-45" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">[회귀 트리 변수 중요도 Top 7]"</span>)</span>
<span id="cb12-46"><a href="#cb12-46" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(imp_reg.head(<span class="dv">7</span>))</span>
<span id="cb12-47"><a href="#cb12-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-48"><a href="#cb12-48" aria-hidden="true" tabindex="-1"></a><span class="co"># 가지치기(비용-복잡도 프루닝)</span></span>
<span id="cb12-49"><a href="#cb12-49" aria-hidden="true" tabindex="-1"></a>path <span class="op">=</span> dt_reg.cost_complexity_pruning_path(Xtr, ytr)</span>
<span id="cb12-50"><a href="#cb12-50" aria-hidden="true" tabindex="-1"></a>ccp_alphas <span class="op">=</span> path.ccp_alphas</span>
<span id="cb12-51"><a href="#cb12-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-52"><a href="#cb12-52" aria-hidden="true" tabindex="-1"></a>best_score <span class="op">=</span> <span class="op">-</span>np.inf</span>
<span id="cb12-53"><a href="#cb12-53" aria-hidden="true" tabindex="-1"></a>best_alpha <span class="op">=</span> <span class="va">None</span></span>
<span id="cb12-54"><a href="#cb12-54" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> alpha <span class="kw">in</span> ccp_alphas:</span>
<span id="cb12-55"><a href="#cb12-55" aria-hidden="true" tabindex="-1"></a>    m <span class="op">=</span> DecisionTreeRegressor(random_state<span class="op">=</span><span class="dv">42</span>, ccp_alpha<span class="op">=</span>alpha)</span>
<span id="cb12-56"><a href="#cb12-56" aria-hidden="true" tabindex="-1"></a>    m.fit(Xtr, ytr)</span>
<span id="cb12-57"><a href="#cb12-57" aria-hidden="true" tabindex="-1"></a>    s <span class="op">=</span> r2_score(yte, m.predict(Xte))</span>
<span id="cb12-58"><a href="#cb12-58" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> s <span class="op">&gt;</span> best_score:</span>
<span id="cb12-59"><a href="#cb12-59" aria-hidden="true" tabindex="-1"></a>        best_score, best_alpha <span class="op">=</span> s, alpha</span>
<span id="cb12-60"><a href="#cb12-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-61"><a href="#cb12-61" aria-hidden="true" tabindex="-1"></a>dt_reg_pruned <span class="op">=</span> DecisionTreeRegressor(random_state<span class="op">=</span><span class="dv">42</span>, ccp_alpha<span class="op">=</span>best_alpha)</span>
<span id="cb12-62"><a href="#cb12-62" aria-hidden="true" tabindex="-1"></a>dt_reg_pruned.fit(Xtr, ytr)</span>
<span id="cb12-63"><a href="#cb12-63" aria-hidden="true" tabindex="-1"></a>pred_p <span class="op">=</span> dt_reg_pruned.predict(Xte)</span>
<span id="cb12-64"><a href="#cb12-64" aria-hidden="true" tabindex="-1"></a>rmse_p <span class="op">=</span> np.sqrt(mean_squared_error(yte, pred_p))</span>
<span id="cb12-65"><a href="#cb12-65" aria-hidden="true" tabindex="-1"></a>r2_p <span class="op">=</span> r2_score(yte, pred_p)</span>
<span id="cb12-66"><a href="#cb12-66" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">[Pruning] best ccp_alpha=</span><span class="sc">{</span>best_alpha<span class="sc">:.4g}</span><span class="ss">  →  RMSE=</span><span class="sc">{</span>rmse_p<span class="sc">:.3f}</span><span class="ss">  R^2=</span><span class="sc">{</span>r2_p<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb12-67"><a href="#cb12-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-68"><a href="#cb12-68" aria-hidden="true" tabindex="-1"></a><span class="co"># 트리 시각화 (회귀)</span></span>
<span id="cb12-69"><a href="#cb12-69" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">6</span>))</span>
<span id="cb12-70"><a href="#cb12-70" aria-hidden="true" tabindex="-1"></a>plot_tree(dt_reg_pruned, feature_names<span class="op">=</span>X_reg.columns, filled<span class="op">=</span><span class="va">True</span>, max_depth<span class="op">=</span><span class="dv">3</span>, fontsize<span class="op">=</span><span class="dv">8</span>)</span>
<span id="cb12-71"><a href="#cb12-71" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Decision Tree Regressor (pruned, depth≤3 view)"</span>)</span>
<span id="cb12-72"><a href="#cb12-72" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb12-73"><a href="#cb12-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-74"><a href="#cb12-74" aria-hidden="true" tabindex="-1"></a><span class="co"># 텍스트 규칙</span></span>
<span id="cb12-75"><a href="#cb12-75" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">[회귀 트리 규칙 일부]"</span>)</span>
<span id="cb12-76"><a href="#cb12-76" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(export_text(dt_reg_pruned, feature_names<span class="op">=</span><span class="bu">list</span>(X_reg.columns), max_depth<span class="op">=</span><span class="dv">3</span>))</span>
<span id="cb12-77"><a href="#cb12-77" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-78"><a href="#cb12-78" aria-hidden="true" tabindex="-1"></a><span class="co"># -----------------------------</span></span>
<span id="cb12-79"><a href="#cb12-79" aria-hidden="true" tabindex="-1"></a><span class="co"># 2) 분류 트리 (MEDV 고가/저가)</span></span>
<span id="cb12-80"><a href="#cb12-80" aria-hidden="true" tabindex="-1"></a><span class="co"># -----------------------------</span></span>
<span id="cb12-81"><a href="#cb12-81" aria-hidden="true" tabindex="-1"></a>median_price <span class="op">=</span> df[<span class="st">"MEDV"</span>].median()</span>
<span id="cb12-82"><a href="#cb12-82" aria-hidden="true" tabindex="-1"></a>y_clf <span class="op">=</span> (df[<span class="st">"MEDV"</span>] <span class="op">&gt;=</span> median_price).astype(<span class="bu">int</span>)</span>
<span id="cb12-83"><a href="#cb12-83" aria-hidden="true" tabindex="-1"></a>X_clf <span class="op">=</span> df.drop(columns<span class="op">=</span>[<span class="st">"MEDV"</span>]).astype(<span class="bu">float</span>)</span>
<span id="cb12-84"><a href="#cb12-84" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-85"><a href="#cb12-85" aria-hidden="true" tabindex="-1"></a>Xtr, Xte, ytr, yte <span class="op">=</span> train_test_split(X_clf, y_clf, test_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">42</span>, stratify<span class="op">=</span>y_clf)</span>
<span id="cb12-86"><a href="#cb12-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-87"><a href="#cb12-87" aria-hidden="true" tabindex="-1"></a>dt_clf <span class="op">=</span> DecisionTreeClassifier(random_state<span class="op">=</span><span class="dv">42</span>)  <span class="co"># 필요시 max_depth 등 조정</span></span>
<span id="cb12-88"><a href="#cb12-88" aria-hidden="true" tabindex="-1"></a>dt_clf.fit(Xtr, ytr)</span>
<span id="cb12-89"><a href="#cb12-89" aria-hidden="true" tabindex="-1"></a>proba <span class="op">=</span> dt_clf.predict_proba(Xte)[:, <span class="dv">1</span>]</span>
<span id="cb12-90"><a href="#cb12-90" aria-hidden="true" tabindex="-1"></a>predc <span class="op">=</span> (proba <span class="op">&gt;=</span> <span class="fl">0.5</span>).astype(<span class="bu">int</span>)</span>
<span id="cb12-91"><a href="#cb12-91" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-92"><a href="#cb12-92" aria-hidden="true" tabindex="-1"></a>acc <span class="op">=</span> accuracy_score(yte, predc)</span>
<span id="cb12-93"><a href="#cb12-93" aria-hidden="true" tabindex="-1"></a><span class="cf">try</span>:</span>
<span id="cb12-94"><a href="#cb12-94" aria-hidden="true" tabindex="-1"></a>    auc <span class="op">=</span> roc_auc_score(yte, proba)</span>
<span id="cb12-95"><a href="#cb12-95" aria-hidden="true" tabindex="-1"></a><span class="cf">except</span> <span class="pp">Exception</span>:</span>
<span id="cb12-96"><a href="#cb12-96" aria-hidden="true" tabindex="-1"></a>    auc <span class="op">=</span> np.nan</span>
<span id="cb12-97"><a href="#cb12-97" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-98"><a href="#cb12-98" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">=== 분류 트리 ==="</span>)</span>
<span id="cb12-99"><a href="#cb12-99" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"ACC=</span><span class="sc">{</span>acc<span class="sc">:.3f}</span><span class="ss">  AUC=</span><span class="sc">{</span>auc<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb12-100"><a href="#cb12-100" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-101"><a href="#cb12-101" aria-hidden="true" tabindex="-1"></a><span class="co"># 변수 중요도</span></span>
<span id="cb12-102"><a href="#cb12-102" aria-hidden="true" tabindex="-1"></a>imp_clf <span class="op">=</span> pd.Series(dt_clf.feature_importances_, index<span class="op">=</span>X_clf.columns).sort_values(ascending<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb12-103"><a href="#cb12-103" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">[분류 트리 변수 중요도 Top 7]"</span>)</span>
<span id="cb12-104"><a href="#cb12-104" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(imp_clf.head(<span class="dv">7</span>))</span>
<span id="cb12-105"><a href="#cb12-105" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-106"><a href="#cb12-106" aria-hidden="true" tabindex="-1"></a><span class="co"># 가지치기(분류)</span></span>
<span id="cb12-107"><a href="#cb12-107" aria-hidden="true" tabindex="-1"></a>path_c <span class="op">=</span> dt_clf.cost_complexity_pruning_path(Xtr, ytr)</span>
<span id="cb12-108"><a href="#cb12-108" aria-hidden="true" tabindex="-1"></a>best_alpha_c, best_acc <span class="op">=</span> <span class="va">None</span>, <span class="op">-</span>np.inf</span>
<span id="cb12-109"><a href="#cb12-109" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> alpha <span class="kw">in</span> path_c.ccp_alphas:</span>
<span id="cb12-110"><a href="#cb12-110" aria-hidden="true" tabindex="-1"></a>    m <span class="op">=</span> DecisionTreeClassifier(random_state<span class="op">=</span><span class="dv">42</span>, ccp_alpha<span class="op">=</span>alpha)</span>
<span id="cb12-111"><a href="#cb12-111" aria-hidden="true" tabindex="-1"></a>    m.fit(Xtr, ytr)</span>
<span id="cb12-112"><a href="#cb12-112" aria-hidden="true" tabindex="-1"></a>    acc_tmp <span class="op">=</span> accuracy_score(yte, m.predict(Xte))</span>
<span id="cb12-113"><a href="#cb12-113" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> acc_tmp <span class="op">&gt;</span> best_acc:</span>
<span id="cb12-114"><a href="#cb12-114" aria-hidden="true" tabindex="-1"></a>        best_acc, best_alpha_c <span class="op">=</span> acc_tmp, alpha</span>
<span id="cb12-115"><a href="#cb12-115" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-116"><a href="#cb12-116" aria-hidden="true" tabindex="-1"></a>dt_clf_pruned <span class="op">=</span> DecisionTreeClassifier(random_state<span class="op">=</span><span class="dv">42</span>, ccp_alpha<span class="op">=</span>best_alpha_c)</span>
<span id="cb12-117"><a href="#cb12-117" aria-hidden="true" tabindex="-1"></a>dt_clf_pruned.fit(Xtr, ytr)</span>
<span id="cb12-118"><a href="#cb12-118" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-119"><a href="#cb12-119" aria-hidden="true" tabindex="-1"></a><span class="co"># 시각화 (분류)</span></span>
<span id="cb12-120"><a href="#cb12-120" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">6</span>))</span>
<span id="cb12-121"><a href="#cb12-121" aria-hidden="true" tabindex="-1"></a>plot_tree(dt_clf_pruned, feature_names<span class="op">=</span>X_clf.columns, class_names<span class="op">=</span>[<span class="st">"low"</span>,<span class="st">"high"</span>],</span>
<span id="cb12-122"><a href="#cb12-122" aria-hidden="true" tabindex="-1"></a>          filled<span class="op">=</span><span class="va">True</span>, max_depth<span class="op">=</span><span class="dv">3</span>, fontsize<span class="op">=</span><span class="dv">8</span>)</span>
<span id="cb12-123"><a href="#cb12-123" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Decision Tree Classifier (pruned, depth≤3 view)"</span>)</span>
<span id="cb12-124"><a href="#cb12-124" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb12-125"><a href="#cb12-125" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-126"><a href="#cb12-126" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">[분류 트리 규칙 일부]"</span>)</span>
<span id="cb12-127"><a href="#cb12-127" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(export_text(dt_clf_pruned, feature_names<span class="op">=</span><span class="bu">list</span>(X_clf.columns), max_depth<span class="op">=</span><span class="dv">3</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>[회귀 트리 변수 중요도 Top 7] <br> RM 0.600326 <br> LSTAT 0.193328 <br> DIS 0.070688 <br> CRIM 0.051296 <br> NOX 0.027148 <br> AGE 0.013617 <br> TAX 0.012464</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/회귀트리분류.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:80.0%"></p>
</figure>
</div>
<p>=== 분류 트리 ===</p>
<p>ACC=0.765 AUC=0.765 <br> [분류 트리 변수 중요도 Top 7] <br> LSTAT 0.550320 <br> RM 0.130962 <br> CRIM 0.096375 <br> AGE 0.073649 <br> B 0.052958 <br> DIS 0.035318 <br> PTRATIO 0.022226</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/분류트리분류.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:80.0%"></p>
</figure>
</div>
<p><strong>트리 기반 주요 방법: 2. 랜덤 포레스트(Random Forest)</strong></p>
<p>랜덤 포레스트는 여러 개의 결정나무(weak learners)를 무작위성(randomness)과 배깅(bagging)으로 결합해 강한 예측기(strong learner) 를 만드는 앙상블 기법이다. 각 트리는 서로 다른 부트스트랩 표본(행 무작위 추출)과 분할 시 무작위로 선택된 일부 특성(열 무작위 추출)만을 사용하여 학습한다. 이렇게 다양성을 확보한 다수의 트리를 생성하고, 회귀는 평균, 분류는 다수결로 최종 예측을 만든다. 개별 트리는 편향이 크지만(underfit), 포레스트의 평균화가 분산을 크게 줄여 과적합을 억제한다.</p>
<p>여러 개의 트리를 만들고, 그 결과를 평균(회귀) 또는 다수결(분류) 방식으로 결합한다. 부트스트랩 표본(bagging)과 무작위 변수 선택(random feature selection)을 결합하여 과적합을 크게 줄인다. 각 변수의 상대적 중요도(feature importance)를 계산할 수 있어 해석에도 도움을 준다.</p>
<p>【변수선택】 여러 개의 트리에서 평균적으로 분할에 가장 많이 기여한 변수 → 중요 변수로 간주한다. 이를 통해 변수 중요도(feature importance) 값을 계산할 수 있다.</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="co"># ==============================================</span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Boston Housing: Random Forest 중요도 비교</span></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a><span class="co"># ==============================================</span></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> fetch_openml</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.ensemble <span class="im">import</span> RandomForestRegressor</span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> mean_squared_error, r2_score</span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.inspection <span class="im">import</span> permutation_importance</span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a><span class="co"># 1) 데이터 로드 &amp; 전처리</span></span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a>boston <span class="op">=</span> fetch_openml(name<span class="op">=</span><span class="st">"boston"</span>, version<span class="op">=</span><span class="dv">1</span>, as_frame<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> boston.frame.copy()</span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> c <span class="kw">in</span> df.columns:</span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> df[c].dtype.name <span class="kw">in</span> [<span class="st">"object"</span>, <span class="st">"category"</span>]:</span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"></a>        df[c] <span class="op">=</span> pd.to_numeric(df[c].astype(<span class="bu">str</span>).<span class="bu">str</span>.strip(), errors<span class="op">=</span><span class="st">"coerce"</span>)</span>
<span id="cb13-18"><a href="#cb13-18" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> df.dropna().reset_index(drop<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb13-19"><a href="#cb13-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-20"><a href="#cb13-20" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> df.drop(columns<span class="op">=</span>[<span class="st">"MEDV"</span>]).astype(<span class="bu">float</span>)</span>
<span id="cb13-21"><a href="#cb13-21" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> df[<span class="st">"MEDV"</span>].astype(<span class="bu">float</span>)</span>
<span id="cb13-22"><a href="#cb13-22" aria-hidden="true" tabindex="-1"></a>Xtr, Xte, ytr, yte <span class="op">=</span> train_test_split(X, y, test_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb13-23"><a href="#cb13-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-24"><a href="#cb13-24" aria-hidden="true" tabindex="-1"></a><span class="co"># 2) 랜덤포레스트 학습</span></span>
<span id="cb13-25"><a href="#cb13-25" aria-hidden="true" tabindex="-1"></a>rf <span class="op">=</span> RandomForestRegressor(</span>
<span id="cb13-26"><a href="#cb13-26" aria-hidden="true" tabindex="-1"></a>    n_estimators<span class="op">=</span><span class="dv">500</span>, random_state<span class="op">=</span><span class="dv">42</span>, n_jobs<span class="op">=-</span><span class="dv">1</span>,</span>
<span id="cb13-27"><a href="#cb13-27" aria-hidden="true" tabindex="-1"></a>    max_features<span class="op">=</span><span class="st">"sqrt"</span>, min_samples_leaf<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb13-28"><a href="#cb13-28" aria-hidden="true" tabindex="-1"></a>    oob_score<span class="op">=</span><span class="va">True</span>, bootstrap<span class="op">=</span><span class="va">True</span></span>
<span id="cb13-29"><a href="#cb13-29" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb13-30"><a href="#cb13-30" aria-hidden="true" tabindex="-1"></a>rf.fit(Xtr, ytr)</span>
<span id="cb13-31"><a href="#cb13-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-32"><a href="#cb13-32" aria-hidden="true" tabindex="-1"></a><span class="co"># 3) 성능 평가</span></span>
<span id="cb13-33"><a href="#cb13-33" aria-hidden="true" tabindex="-1"></a>pred <span class="op">=</span> rf.predict(Xte)</span>
<span id="cb13-34"><a href="#cb13-34" aria-hidden="true" tabindex="-1"></a>rmse <span class="op">=</span> np.sqrt(mean_squared_error(yte, pred))</span>
<span id="cb13-35"><a href="#cb13-35" aria-hidden="true" tabindex="-1"></a>r2   <span class="op">=</span> r2_score(yte, pred)</span>
<span id="cb13-36"><a href="#cb13-36" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"RMSE=</span><span class="sc">{</span>rmse<span class="sc">:.3f}</span><span class="ss">  R^2=</span><span class="sc">{</span>r2<span class="sc">:.3f}</span><span class="ss">  OOB R^2=</span><span class="sc">{</span>rf<span class="sc">.</span>oob_score_<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb13-37"><a href="#cb13-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-38"><a href="#cb13-38" aria-hidden="true" tabindex="-1"></a><span class="co"># 4) 기본 중요도</span></span>
<span id="cb13-39"><a href="#cb13-39" aria-hidden="true" tabindex="-1"></a>imp <span class="op">=</span> pd.Series(rf.feature_importances_, index<span class="op">=</span>X.columns).sort_values(ascending<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb13-40"><a href="#cb13-40" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">[Feature Importance Top 7]"</span>)</span>
<span id="cb13-41"><a href="#cb13-41" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(imp.head(<span class="dv">7</span>))</span>
<span id="cb13-42"><a href="#cb13-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-43"><a href="#cb13-43" aria-hidden="true" tabindex="-1"></a><span class="co"># 5) Permutation Importance</span></span>
<span id="cb13-44"><a href="#cb13-44" aria-hidden="true" tabindex="-1"></a>pi <span class="op">=</span> permutation_importance(rf, Xte, yte, n_repeats<span class="op">=</span><span class="dv">10</span>, random_state<span class="op">=</span><span class="dv">42</span>, n_jobs<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb13-45"><a href="#cb13-45" aria-hidden="true" tabindex="-1"></a>perm_imp <span class="op">=</span> pd.Series(pi.importances_mean, index<span class="op">=</span>X.columns).sort_values(ascending<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb13-46"><a href="#cb13-46" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">[Permutation Importance Top 7]"</span>)</span>
<span id="cb13-47"><a href="#cb13-47" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(perm_imp.head(<span class="dv">7</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><em>랜덤 포레스트 중요도 개념</em></p>
<p>기본 중요도(feature_importances_) → 트리 분할에서 자주 쓰이고 불순도를 많이 줄인 변수에 점수가 집중된다.</p>
<p>Permutation Importance → 변수를 섞었을 때 예측 성능이 얼마나 떨어지느냐를 직접 측정하므로, 실제 예측 기여도를 더 잘 반영한다.</p>
<p>따라서 두 방법을 나란히 보면 상위 주요 변수(LSTAT, RM 등)는 공통적으로 높게 나오고, 일부 변수는 기본 중요도에서는 높지만 Permutation에서는 낮을 수 있다. (편향 차이 때문)</p>
<p>RMSE=3.255 R^2=0.856 OOB R^2=0.852</p>
<p>[Feature Importance Top ] <br> RM 0.296902 <br> LSTAT 0.240556 <br> NOX 0.075812 <br> PTRATIO 0.069155 <br> INDUS 0.068880 <br> CRIM 0.068249 <br> DIS 0.054796</p>
<p>[Permutation Importance Top 7]</p>
<p>LSTAT 0.313767 <br> RM 0.246843 <br> NOX 0.064840 <br> PTRATIO 0.046634 <br> DIS 0.045469 <br> CRIM 0.031338 <br> TAX 0.020284</p>
<p><strong>트리 기반 주요 방법: 3. 부스팅(Boosting, Gradient Boosting, XGBoost 등)</strong></p>
<p>약한 모형(얕은 트리)을 순차적으로 연결하면서 이전 단계의 오차를 보완해 나가는 방법.</p>
<p>높은 예측력을 가지지만, 매개변수 조정이 복잡하고 계산량이 많다는 단점이 있다.</p>
<p>【변수선택】 순차적 학습 과정에서 반복적으로 선택되는 변수일수록 중요한 변수이다.</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="co"># ==============================================</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Boston Housing: Gradient Boosting 기반 변수선택 (회귀)</span></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a><span class="co">#  - 방식 A: SelectFromModel (importance threshold)</span></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a><span class="co">#  - 방식 B: Top-k 중요변수 교차검증 선택</span></span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a><span class="co"># ==============================================</span></span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> fetch_openml</span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split, KFold, cross_val_score</span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> mean_squared_error, r2_score</span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.ensemble <span class="im">import</span> GradientBoostingRegressor</span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.feature_selection <span class="im">import</span> SelectFromModel</span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a><span class="co"># ---------------------------</span></span>
<span id="cb14-17"><a href="#cb14-17" aria-hidden="true" tabindex="-1"></a><span class="co"># 0) 데이터 로드 &amp; 전처리</span></span>
<span id="cb14-18"><a href="#cb14-18" aria-hidden="true" tabindex="-1"></a><span class="co"># ---------------------------</span></span>
<span id="cb14-19"><a href="#cb14-19" aria-hidden="true" tabindex="-1"></a>boston <span class="op">=</span> fetch_openml(name<span class="op">=</span><span class="st">"boston"</span>, version<span class="op">=</span><span class="dv">1</span>, as_frame<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb14-20"><a href="#cb14-20" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> boston.frame.copy()</span>
<span id="cb14-21"><a href="#cb14-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-22"><a href="#cb14-22" aria-hidden="true" tabindex="-1"></a><span class="co"># 전 컬럼 숫자화 (object/category 방지)</span></span>
<span id="cb14-23"><a href="#cb14-23" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> c <span class="kw">in</span> df.columns:</span>
<span id="cb14-24"><a href="#cb14-24" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> df[c].dtype.name <span class="kw">in</span> [<span class="st">"object"</span>, <span class="st">"category"</span>]:</span>
<span id="cb14-25"><a href="#cb14-25" aria-hidden="true" tabindex="-1"></a>        df[c] <span class="op">=</span> pd.to_numeric(df[c].astype(<span class="bu">str</span>).<span class="bu">str</span>.strip(), errors<span class="op">=</span><span class="st">"coerce"</span>)</span>
<span id="cb14-26"><a href="#cb14-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-27"><a href="#cb14-27" aria-hidden="true" tabindex="-1"></a><span class="co"># 결측 제거</span></span>
<span id="cb14-28"><a href="#cb14-28" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> df.dropna().reset_index(drop<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb14-29"><a href="#cb14-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-30"><a href="#cb14-30" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> df[<span class="st">"MEDV"</span>].astype(<span class="bu">float</span>)</span>
<span id="cb14-31"><a href="#cb14-31" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> df.drop(columns<span class="op">=</span>[<span class="st">"MEDV"</span>]).astype(<span class="bu">float</span>)</span>
<span id="cb14-32"><a href="#cb14-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-33"><a href="#cb14-33" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(</span>
<span id="cb14-34"><a href="#cb14-34" aria-hidden="true" tabindex="-1"></a>    X, y, test_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">42</span></span>
<span id="cb14-35"><a href="#cb14-35" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb14-36"><a href="#cb14-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-37"><a href="#cb14-37" aria-hidden="true" tabindex="-1"></a><span class="co"># ---------------------------</span></span>
<span id="cb14-38"><a href="#cb14-38" aria-hidden="true" tabindex="-1"></a><span class="co"># 1) 기준 성능 (모든 변수)</span></span>
<span id="cb14-39"><a href="#cb14-39" aria-hidden="true" tabindex="-1"></a><span class="co"># ---------------------------</span></span>
<span id="cb14-40"><a href="#cb14-40" aria-hidden="true" tabindex="-1"></a>base_gbr <span class="op">=</span> GradientBoostingRegressor(</span>
<span id="cb14-41"><a href="#cb14-41" aria-hidden="true" tabindex="-1"></a>    random_state<span class="op">=</span><span class="dv">42</span>,</span>
<span id="cb14-42"><a href="#cb14-42" aria-hidden="true" tabindex="-1"></a>    n_estimators<span class="op">=</span><span class="dv">500</span>,      <span class="co"># 기본 강화 단계 수</span></span>
<span id="cb14-43"><a href="#cb14-43" aria-hidden="true" tabindex="-1"></a>    learning_rate<span class="op">=</span><span class="fl">0.05</span>,   <span class="co"># 학습률(작게 → 일반화 안정)</span></span>
<span id="cb14-44"><a href="#cb14-44" aria-hidden="true" tabindex="-1"></a>    max_depth<span class="op">=</span><span class="dv">3</span>,          <span class="co"># 개별 트리 깊이</span></span>
<span id="cb14-45"><a href="#cb14-45" aria-hidden="true" tabindex="-1"></a>    subsample<span class="op">=</span><span class="fl">1.0</span>         <span class="co"># &lt; 1.0 이면 확률적 부스팅(과적합 완화)</span></span>
<span id="cb14-46"><a href="#cb14-46" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb14-47"><a href="#cb14-47" aria-hidden="true" tabindex="-1"></a>base_gbr.fit(X_train, y_train)</span>
<span id="cb14-48"><a href="#cb14-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-49"><a href="#cb14-49" aria-hidden="true" tabindex="-1"></a>pred_base <span class="op">=</span> base_gbr.predict(X_test)</span>
<span id="cb14-50"><a href="#cb14-50" aria-hidden="true" tabindex="-1"></a>rmse_base <span class="op">=</span> np.sqrt(mean_squared_error(y_test, pred_base))</span>
<span id="cb14-51"><a href="#cb14-51" aria-hidden="true" tabindex="-1"></a>r2_base   <span class="op">=</span> r2_score(y_test, pred_base)</span>
<span id="cb14-52"><a href="#cb14-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-53"><a href="#cb14-53" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"=== 기준(모든 변수, GradientBoosting) ==="</span>)</span>
<span id="cb14-54"><a href="#cb14-54" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"RMSE: </span><span class="sc">{</span>rmse_base<span class="sc">:.3f}</span><span class="ss">   R^2: </span><span class="sc">{</span>r2_base<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb14-55"><a href="#cb14-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-56"><a href="#cb14-56" aria-hidden="true" tabindex="-1"></a><span class="co"># ---------------------------</span></span>
<span id="cb14-57"><a href="#cb14-57" aria-hidden="true" tabindex="-1"></a><span class="co"># 2) 변수 중요도 표 보기</span></span>
<span id="cb14-58"><a href="#cb14-58" aria-hidden="true" tabindex="-1"></a><span class="co"># ---------------------------</span></span>
<span id="cb14-59"><a href="#cb14-59" aria-hidden="true" tabindex="-1"></a>imp <span class="op">=</span> pd.Series(base_gbr.feature_importances_, index<span class="op">=</span>X.columns).sort_values(ascending<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb14-60"><a href="#cb14-60" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">[변수 중요도 Top 7]"</span>)</span>
<span id="cb14-61"><a href="#cb14-61" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(imp.head(<span class="dv">7</span>))</span>
<span id="cb14-62"><a href="#cb14-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-63"><a href="#cb14-63" aria-hidden="true" tabindex="-1"></a><span class="co"># ==============================================</span></span>
<span id="cb14-64"><a href="#cb14-64" aria-hidden="true" tabindex="-1"></a><span class="co"># 방식 A) SelectFromModel: 중요도 임계값으로 자동 선택</span></span>
<span id="cb14-65"><a href="#cb14-65" aria-hidden="true" tabindex="-1"></a><span class="co"># ==============================================</span></span>
<span id="cb14-66"><a href="#cb14-66" aria-hidden="true" tabindex="-1"></a>sfm <span class="op">=</span> SelectFromModel(</span>
<span id="cb14-67"><a href="#cb14-67" aria-hidden="true" tabindex="-1"></a>    estimator<span class="op">=</span>GradientBoostingRegressor(</span>
<span id="cb14-68"><a href="#cb14-68" aria-hidden="true" tabindex="-1"></a>        random_state<span class="op">=</span><span class="dv">42</span>, n_estimators<span class="op">=</span><span class="dv">500</span>, learning_rate<span class="op">=</span><span class="fl">0.05</span>, max_depth<span class="op">=</span><span class="dv">3</span></span>
<span id="cb14-69"><a href="#cb14-69" aria-hidden="true" tabindex="-1"></a>    ),</span>
<span id="cb14-70"><a href="#cb14-70" aria-hidden="true" tabindex="-1"></a>    threshold<span class="op">=</span><span class="st">"median"</span>  <span class="co"># 또는 "mean", 혹은 0.01 같은 절대값</span></span>
<span id="cb14-71"><a href="#cb14-71" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb14-72"><a href="#cb14-72" aria-hidden="true" tabindex="-1"></a>sfm.fit(X_train, y_train)</span>
<span id="cb14-73"><a href="#cb14-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-74"><a href="#cb14-74" aria-hidden="true" tabindex="-1"></a>mask <span class="op">=</span> sfm.get_support()</span>
<span id="cb14-75"><a href="#cb14-75" aria-hidden="true" tabindex="-1"></a>selected_A <span class="op">=</span> X.columns[mask].tolist()</span>
<span id="cb14-76"><a href="#cb14-76" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-77"><a href="#cb14-77" aria-hidden="true" tabindex="-1"></a>Xtr_A <span class="op">=</span> sfm.transform(X_train)</span>
<span id="cb14-78"><a href="#cb14-78" aria-hidden="true" tabindex="-1"></a>Xte_A <span class="op">=</span> sfm.transform(X_test)</span>
<span id="cb14-79"><a href="#cb14-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-80"><a href="#cb14-80" aria-hidden="true" tabindex="-1"></a>gbr_A <span class="op">=</span> GradientBoostingRegressor(</span>
<span id="cb14-81"><a href="#cb14-81" aria-hidden="true" tabindex="-1"></a>    random_state<span class="op">=</span><span class="dv">42</span>, n_estimators<span class="op">=</span><span class="dv">500</span>, learning_rate<span class="op">=</span><span class="fl">0.05</span>, max_depth<span class="op">=</span><span class="dv">3</span></span>
<span id="cb14-82"><a href="#cb14-82" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb14-83"><a href="#cb14-83" aria-hidden="true" tabindex="-1"></a>gbr_A.fit(Xtr_A, y_train)</span>
<span id="cb14-84"><a href="#cb14-84" aria-hidden="true" tabindex="-1"></a>pred_A <span class="op">=</span> gbr_A.predict(Xte_A)</span>
<span id="cb14-85"><a href="#cb14-85" aria-hidden="true" tabindex="-1"></a>rmse_A <span class="op">=</span> np.sqrt(mean_squared_error(y_test, pred_A))</span>
<span id="cb14-86"><a href="#cb14-86" aria-hidden="true" tabindex="-1"></a>r2_A   <span class="op">=</span> r2_score(y_test, pred_A)</span>
<span id="cb14-87"><a href="#cb14-87" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-88"><a href="#cb14-88" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">=== 방식 A: SelectFromModel (GBR) ==="</span>)</span>
<span id="cb14-89"><a href="#cb14-89" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"선택 변수(</span><span class="sc">{</span><span class="bu">len</span>(selected_A)<span class="sc">}</span><span class="ss">개): </span><span class="sc">{</span>selected_A<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb14-90"><a href="#cb14-90" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"RMSE: </span><span class="sc">{</span>rmse_A<span class="sc">:.3f}</span><span class="ss">   R^2: </span><span class="sc">{</span>r2_A<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb14-91"><a href="#cb14-91" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-92"><a href="#cb14-92" aria-hidden="true" tabindex="-1"></a><span class="co"># ==============================================</span></span>
<span id="cb14-93"><a href="#cb14-93" aria-hidden="true" tabindex="-1"></a><span class="co"># 방식 B) Top-k: 중요도 상위 k개를 CV로 최적 k 선택</span></span>
<span id="cb14-94"><a href="#cb14-94" aria-hidden="true" tabindex="-1"></a><span class="co"># ==============================================</span></span>
<span id="cb14-95"><a href="#cb14-95" aria-hidden="true" tabindex="-1"></a>ranked_features <span class="op">=</span> imp.index.tolist()</span>
<span id="cb14-96"><a href="#cb14-96" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-97"><a href="#cb14-97" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> cv_rmse_for_k(k):</span>
<span id="cb14-98"><a href="#cb14-98" aria-hidden="true" tabindex="-1"></a>    feats <span class="op">=</span> ranked_features[:k]</span>
<span id="cb14-99"><a href="#cb14-99" aria-hidden="true" tabindex="-1"></a>    gbr <span class="op">=</span> GradientBoostingRegressor(</span>
<span id="cb14-100"><a href="#cb14-100" aria-hidden="true" tabindex="-1"></a>        random_state<span class="op">=</span><span class="dv">42</span>, n_estimators<span class="op">=</span><span class="dv">400</span>, learning_rate<span class="op">=</span><span class="fl">0.06</span>, max_depth<span class="op">=</span><span class="dv">3</span></span>
<span id="cb14-101"><a href="#cb14-101" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb14-102"><a href="#cb14-102" aria-hidden="true" tabindex="-1"></a>    cv <span class="op">=</span> KFold(n_splits<span class="op">=</span><span class="dv">5</span>, shuffle<span class="op">=</span><span class="va">True</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb14-103"><a href="#cb14-103" aria-hidden="true" tabindex="-1"></a>    neg_mse <span class="op">=</span> cross_val_score(</span>
<span id="cb14-104"><a href="#cb14-104" aria-hidden="true" tabindex="-1"></a>        gbr, X_train[feats], y_train,</span>
<span id="cb14-105"><a href="#cb14-105" aria-hidden="true" tabindex="-1"></a>        scoring<span class="op">=</span><span class="st">"neg_mean_squared_error"</span>,</span>
<span id="cb14-106"><a href="#cb14-106" aria-hidden="true" tabindex="-1"></a>        cv<span class="op">=</span>cv, n_jobs<span class="op">=-</span><span class="dv">1</span></span>
<span id="cb14-107"><a href="#cb14-107" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb14-108"><a href="#cb14-108" aria-hidden="true" tabindex="-1"></a>    rmse <span class="op">=</span> np.sqrt(<span class="op">-</span>neg_mse.mean())</span>
<span id="cb14-109"><a href="#cb14-109" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> rmse, feats</span>
<span id="cb14-110"><a href="#cb14-110" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-111"><a href="#cb14-111" aria-hidden="true" tabindex="-1"></a>results <span class="op">=</span> []</span>
<span id="cb14-112"><a href="#cb14-112" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, X_train.shape[<span class="dv">1</span>] <span class="op">+</span> <span class="dv">1</span>):</span>
<span id="cb14-113"><a href="#cb14-113" aria-hidden="true" tabindex="-1"></a>    rmse_k, feats_k <span class="op">=</span> cv_rmse_for_k(k)</span>
<span id="cb14-114"><a href="#cb14-114" aria-hidden="true" tabindex="-1"></a>    results.append((k, rmse_k, feats_k))</span>
<span id="cb14-115"><a href="#cb14-115" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-116"><a href="#cb14-116" aria-hidden="true" tabindex="-1"></a>best_k, best_cv_rmse, best_feats <span class="op">=</span> <span class="bu">min</span>(results, key<span class="op">=</span><span class="kw">lambda</span> x: x[<span class="dv">1</span>])</span>
<span id="cb14-117"><a href="#cb14-117" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-118"><a href="#cb14-118" aria-hidden="true" tabindex="-1"></a>gbr_B <span class="op">=</span> GradientBoostingRegressor(</span>
<span id="cb14-119"><a href="#cb14-119" aria-hidden="true" tabindex="-1"></a>    random_state<span class="op">=</span><span class="dv">42</span>, n_estimators<span class="op">=</span><span class="dv">500</span>, learning_rate<span class="op">=</span><span class="fl">0.05</span>, max_depth<span class="op">=</span><span class="dv">3</span></span>
<span id="cb14-120"><a href="#cb14-120" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb14-121"><a href="#cb14-121" aria-hidden="true" tabindex="-1"></a>gbr_B.fit(X_train[best_feats], y_train)</span>
<span id="cb14-122"><a href="#cb14-122" aria-hidden="true" tabindex="-1"></a>pred_B <span class="op">=</span> gbr_B.predict(X_test[best_feats])</span>
<span id="cb14-123"><a href="#cb14-123" aria-hidden="true" tabindex="-1"></a>rmse_B <span class="op">=</span> np.sqrt(mean_squared_error(y_test, pred_B))</span>
<span id="cb14-124"><a href="#cb14-124" aria-hidden="true" tabindex="-1"></a>r2_B   <span class="op">=</span> r2_score(y_test, pred_B)</span>
<span id="cb14-125"><a href="#cb14-125" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-126"><a href="#cb14-126" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">=== 방식 B: Top-k (CV로 k 선택, GBR) ==="</span>)</span>
<span id="cb14-127"><a href="#cb14-127" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"최적 k: </span><span class="sc">{</span>best_k<span class="sc">}</span><span class="ss">  | CV RMSE: </span><span class="sc">{</span>best_cv_rmse<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb14-128"><a href="#cb14-128" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"선택 변수(</span><span class="sc">{</span><span class="bu">len</span>(best_feats)<span class="sc">}</span><span class="ss">개): </span><span class="sc">{</span>best_feats<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb14-129"><a href="#cb14-129" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Test RMSE: </span><span class="sc">{</span>rmse_B<span class="sc">:.3f}</span><span class="ss">   R^2: </span><span class="sc">{</span>r2_B<span class="sc">:.3f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>=== 기준(모든 변수, GradientBoosting) ===</p>
<p>RMSE: 2.515 R^2: 0.914</p>
<p>[변수 중요도 Top 7] <br> RM 0.406142 <br> LSTAT 0.374462 <br> DIS 0.072480 <br> NOX 0.034583 <br> PTRATIO 0.031870 <br> CRIM 0.027173 <br> AGE 0.021013</p>
<p>=== 방식 A: SelectFromModel (GBR) ===</p>
<p>선택 변수(7개): ['CRIM', 'NOX', 'RM', 'AGE', 'DIS', 'PTRATIO', 'LSTAT']</p>
<p>RMSE: 2.474 R^2: 0.917</p>
<p>=== 방식 B: Top-k (CV로 k 선택, GBR) ===</p>
<p>최적 k: 11 | CV RMSE: 3.476</p>
<p>선택 변수(11개): ['RM', 'LSTAT', 'DIS', 'NOX', 'PTRATIO', 'CRIM', 'AGE', 'B', 'TAX', 'RAD', 'INDUS']</p>
<p>Test RMSE: 2.648 R^2: 0.904</p>
</section>
<section id="최종-임시-모형" class="level5">
<h5 class="anchored" data-anchor-id="최종-임시-모형">(5) 최종 임시 모형</h5>
<p>다중 공선성 진단 및 최종 회귀진단 전에 잠정적으로 유의한 예측변수들을 선택할 필요가 있다. 유의한 혹은 주요한 변수를 선택하는 방법은 기준은 다음과 같다.</p>
<ul>
<li><p>예측변수의 목표변수의 변동을 가장 잘 설명하는가? &lt;=&gt; 결정계수 (SSR Best Model)</p></li>
<li><p>예측변수는 유의한가? 개별 유의성 (상관계수, Univariate Selection), 공동 유의성(후진제거), REF 방법</p></li>
<li><p>영향력 높은 변수 선택 LASSO)</p></li>
</ul>
<p>Best subset(수정된 <span class="math inline">\(R^{2}\)</span>) : ’CHAS', 'NOX', 'RM', 'DIS', 'PTRATIO', 'B', ’LSTAT'</p>
<p>REF: CRIM CHAS NOX RM DIS PTRATIO. LSTAT</p>
<p>LASSO: NOX RM DIS RAD TAX PTRATIO LSTAT</p>
<p>회귀트리: RM LSTAT DIS CRIM NOX AGE TAX</p>
<p>분류트리: LSTAT RM CRIM AGE B DIS PTRATIO</p>
<p>Feature Importance Top ] RM LSTAT NOX PTRATIO. INDUS CRIM DIS</p>
<p>[Permutation Importance Top 7] LSTAT RM NOX PTRATIO DIS CRIM TAX</p>
<p>Boosting: RM LSTAT DIS NOX PTRATIO CRIM AGE</p>


</section>
</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




</body></html>