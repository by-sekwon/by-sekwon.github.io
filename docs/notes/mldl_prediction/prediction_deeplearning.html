<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.32">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>MLDL 예측방법 | 딥러닝 회귀 – 세상의 모든 통계 이야기</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-37eea08aefeeee20ff55810ff984fec1.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-44962e3d41ec9ccc254fd50f1af5efbe.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-sidebar docked nav-fixed quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">세상의 모든 통계 이야기</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../notes/math_stat/index.html"> 
<span class="menu-text">기초수학·수리통계</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../notes/intro_stat/index.html"> 
<span class="menu-text">기초통계·조사방법</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../notes/linear_model/index.html"> 
<span class="menu-text">회귀·다변량</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../notes/mldl_intro/index.html"> 
<span class="menu-text">MLDL개념</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link active" href="../../notes/mldl_prediction/index.html" aria-current="page"> 
<span class="menu-text">MLDL예측</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../notes/mldl_classification/index.html"> 
<span class="menu-text">MLDL분류</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../notes/data_reduction/index.html"> 
<span class="menu-text">MLDL차원축소</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../cardnews/index.html"> 
<span class="menu-text">카드뉴스</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../consult.html"> 
<span class="menu-text">통계상담</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../notes/mldl_prediction/prediction_deeplearning.html">📄 딥러닝회귀</a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../notes/mldl_prediction/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">【머신·딥러닝 예측방법】</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../notes/mldl_prediction/prediction_intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">📄 예측문제 개요</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../notes/mldl_prediction/prediction_regression.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">📄[전통] 회귀분석</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../notes/mldl_prediction/prediction_regulation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">📄[전통] 규제회귀</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../notes/mldl_prediction/prediction_dimension.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">📄[전통] 차원축소</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../notes/mldl_prediction/prediction_nonreg.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">📄[머신러닝] 비선형회귀</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../notes/mldl_prediction/prediction_treebase.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">📄[머신러닝] 트리기반</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../notes/mldl_prediction/prediction_deeplearning.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">📄 딥러닝회귀</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">목차</h2>
   
  <ul>
  <li><a href="#딥러닝-회귀모형" id="toc-딥러닝-회귀모형" class="nav-link active" data-scroll-target="#딥러닝-회귀모형">딥러닝 회귀모형</a>
  <ul>
  <li><a href="#mlp-회귀-개념" id="toc-mlp-회귀-개념" class="nav-link" data-scroll-target="#mlp-회귀-개념">1. MLP 회귀 개념</a></li>
  <li><a href="#mlp-회귀-방법론" id="toc-mlp-회귀-방법론" class="nav-link" data-scroll-target="#mlp-회귀-방법론">2. MLP 회귀 방법론</a></li>
  <li><a href="#확장-quantile-분포-예측불확실성" id="toc-확장-quantile-분포-예측불확실성" class="nav-link" data-scroll-target="#확장-quantile-분포-예측불확실성">3. 확장: Quantile / 분포 예측(불확실성)</a></li>
  <li><a href="#mlp-회귀의-장점과-단점" id="toc-mlp-회귀의-장점과-단점" class="nav-link" data-scroll-target="#mlp-회귀의-장점과-단점">5. MLP 회귀의 장점과 단점</a></li>
  <li><a href="#mlp-변수-중요도" id="toc-mlp-변수-중요도" class="nav-link" data-scroll-target="#mlp-변수-중요도">6. MLP 변수 중요도</a></li>
  <li><a href="#사례분석" id="toc-사례분석" class="nav-link" data-scroll-target="#사례분석">7. 사례분석</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content column-body" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">MLDL 예측방법 | 딥러닝 회귀</h1>
</div>



<div class="quarto-title-meta column-body">

    
  
    
  </div>
  


</header>


<section id="딥러닝-회귀모형" class="level3">
<h3 class="anchored" data-anchor-id="딥러닝-회귀모형">딥러닝 회귀모형</h3>
<section id="mlp-회귀-개념" class="level4">
<h4 class="anchored" data-anchor-id="mlp-회귀-개념">1. MLP 회귀 개념</h4>
<section id="딥러닝-회귀란" class="level5">
<h5 class="anchored" data-anchor-id="딥러닝-회귀란">딥러닝 회귀란?</h5>
<p>딥러닝 회귀는 입력 x로부터 연속형 반응변수 y 를 예측하는 문제다. 목표는 관측치 <span class="math inline">\((x_{i},y_{i})\)</span>에 대해 예측값 <span class="math inline">\({\widehat{y}}_{i}\)</span>가 실제 <span class="math inline">\(y_{i}\)</span>와 가깝도록, 모형의 파라미터 <span class="math inline">\(\theta\)</span>를 학습(최적화)하는 것이다. 선형회귀는 x의 선형결합만으로 <span class="math inline">\(\widehat{y}\)</span>를 표현하지만, 딥러닝은 하나 이상의 은닉층(hidden layer) 과 비선형 활성화함수(activation) 를 통해 복잡한 비선형 관계, 변수 간 상호작용, 국소적 패턴을 데이터로부터 자동으로 학습할 수 있다.</p>
<p>특히 2차원(행x열) 데이터에서도 딥러닝이 의미 있는 성능을 보이는 경우가 있으나, 데이터의 규모와 잡음 수준, 범주형 변수의 구조와 처리 방식에 따라 전통적 트리 기반 앙상블(예: 랜덤포레스트, 부스팅) 이 더 강력한 기준선이 될 수 있다. 따라서 실제 적용에서는 딥러닝 모델을 사용하더라도 트리 앙상블과의 성능 비교를 함께 수행하는 것이 바람직하다.</p>
</section>
<section id="mlp-회귀란" class="level5">
<h5 class="anchored" data-anchor-id="mlp-회귀란">MLP 회귀란?</h5>
<p>그림은 MLP(다층 퍼셉트론)를 회귀 문제에 적용했을 때의 전형적인 구조를 보여준다. 목표는 입력 변수들로부터 연속형 반응변수 y 를 예측하는 것이며, 입력이 여러 층을 거치며 점점 더 <span dir="rtl">”</span>가공된 표현”으로 변환된 뒤 최종 예측값을 만든다.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/prediction_deep_mplconcept.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:70.0%"></p>
</figure>
</div>
<p>먼저 입력층은 데이터의 설명변수(특징)들이 신경망으로 들어오는 통로이다. 이 층은 보통 스스로 학습을 하지는 않고, 관측된 값을 다음 층으로 전달한다. 실제로 모델이 복잡한 패턴을 학습하는 능력은 입력층이 아니라 은닉층의 구조와 변환 방식에서 결정된다.</p>
<p>가운데의 은닉층(hidden layer) 들은 MLP의 핵심이다. 그림처럼 각 은닉층은 대개 완전연결(fully connected) 구조로 되어 있어, 한 층의 모든 노드가 다음 층의 모든 노드와 연결된다. 이 연결 강도(가중치)와 각 노드의 기준점(편향)이 학습을 통해 조정되며, 이를 통해 입력 정보가 층을 지날수록 단순한 원자료 형태에서 벗어나 예측에 유리한 새로운 특징 표현으로 변환된다.</p>
<p>또한 은닉층에는 활성화 함수(그림에서는 LReLU)가 적용되어, 단순한 직선 관계만 표현하는 것이 아니라 복잡한 비선형 관계나 변수 간 상호작용까지 포착할 수 있다. 이런 비선형 변환이 반복될수록 모델은 더 다양한 패턴을 담아낼 수 있는데, 이것이 <span dir="rtl">”</span>깊게 쌓는다”는 의미와 연결된다.</p>
<p>그림에 표시된 Dropout은 학습 과정에서 일부 연결 또는 노드를 임의로 제외하는 방식으로, 특정 경로에만 지나치게 의존하는 것을 막아 과적합을 완화하려는 장치다. 결과적으로 모델이 데이터의 우연한 잡음에 맞춰지기보다, 보다 일반적인 패턴을 학습하도록 돕는다.</p>
<p>마지막의 출력층(output layer) 은 최종 예측값을 내는 단계로, 문제 유형에 따라 설계가 달라진다. 이 그림은 회귀를 다루므로 출력은 보통 선형(항등) 형태로 두어 예측값이 실수 전 범위를 자유롭게 가질 수 있게 한다.</p>
<p>반면 분류 문제라면 출력이 확률처럼 해석되어야 하므로 sigmoid나 softmax처럼 값의 범위를 제한하는 형태를 사용한다. 요약하면 출력층은 <span dir="rtl">”</span>모델이 내놓는 값이 어떤 성격(연속값/확률)을 갖는가”를 결정하는 마지막 관문이다.</p>
<p>전체적으로 MLP 회귀는 입력 → 은닉층(표현 학습) → 출력이라는 흐름 속에서, 예측이 잘 되도록 내부 연결 강도와 기준점을 학습해 나가는 방식이다. 이후에는 손실함수 선택, 최적화 방법, 과적합 제어(드롭아웃 외 정규화 등)를 어떻게 설계하느냐가 실제 성능을 좌우하게 된다.</p>
</section>
<section id="mlp의-구성은닉층-출력층과-역할" class="level5">
<h5 class="anchored" data-anchor-id="mlp의-구성은닉층-출력층과-역할">MLP의 구성(은닉층, 출력층)과 역할</h5>
<p>MLP(Multi-Layer Perceptron)는 입력층–은닉층–출력층으로 이루어진 전형적인 순전파(feedforward) 신경망이다. 순전파 신경망에서는 입력 x가 주어졌을 때 예측값 <span class="math inline">\(\widehat{y}\)</span>를 계산하는 정보의 흐름이 입력층 → 은닉층 → 출력층 방향으로만 진행되며, 중간에 되돌아가는 연결(순환)은 없다.</p>
<p>예를 들어 2층(은닉층 1개, 출력층 1개) MLP는 다음과 같이 표현할 수 있다. 은닉층에서 <span class="math inline">\(z^{(1)} = W^{(1)}x + b^{(1)},a^{(1)} = \phi\left( z^{(1)} \right)\)</span>,</p>
<p>출력층에서 <span class="math inline">\(z^{(2)} = W^{(2)}a^{(1)} + b^{(2)},\widehat{y} = g\left( z^{(2)} \right)\)</span>로 계산한다.</p>
<p>회귀 문제의 경우 출력층 활성함수 g는 보통 항등함수(선형 출력) 를 사용하여 <span class="math inline">\(\widehat{y}\)</span>가 실수 전체 범위를 가질 수 있게 한다. 즉, 순전파의 목적은 <span dir="rtl">”</span>현재 파라미터 (W,b)로 예측을 얼마나 잘하는가”를 계산하는 데 있다.</p>
<p>반면 역전파(backpropagation) 는 손실 L을 줄이기 위해 각 가중치와 편향이 손실에 미치는 영향을 계산하는 과정이다. 구체적으로 <span class="math inline">\(\frac{\partial L}{\partial W},\frac{\partial L}{\partial b}\)</span>와 같은 기울기(gradient) 를 구하며, 핵심 원리는 연쇄법칙(chain rule) 이다.</p>
<p>따라서 오차(또는 손실의 변화)는 출력층에서부터 시작해 은닉층 방향으로 단계적으로 전파되며, 이를 통해 모든 층의 파라미터에 대한 기울기를 효율적으로 계산할 수 있다.</p>
<p>입력층은 데이터의 특징벡터 x가 신경망으로 들어오는 입구에 해당한다. 일반적으로 입력층 자체는 학습 가능한 변환을 수행하지 않고, 관측된 값을 다음 층으로 전달하는 역할을 한다. 따라서 모델이 어떤 형태의 관계를 표현할 수 있는지는 입력층보다는 은닉층의 구조와 변환 방식에 의해 좌우된다.</p>
<p>은닉층(hidden layer)은 하나 이상 둘 수 있으며, 각 은닉층은 이전 층의 출력을 입력으로 받아 선형변환과 비선형변환을 순차적으로 적용한다. 예를 들어 은닉층의 한 뉴런은 <span class="math inline">\(z = w^{\top}x + b,h = g(z)\)</span>와 같이 입력 x의 가중합 <span class="math inline">\(w^{\top}x\)</span>에 편향 b를 더해 z를 만든 뒤, 활성화 함수 <span class="math inline">\(g( \cdot )\)</span>를 통과시켜 출력 h를 생성한다.</p>
<p>이때 활성화 함수가 비선형이기 때문에, MLP는 단순 선형모형으로는 표현하기 어려운 복잡한 비선형 관계나 변수 간 상호작용까지 학습할 수 있다. 반대로 활성화 함수를 사용하지 않고 모든 층이 선형변환만 수행한다면, 여러 층을 쌓더라도 전체는 결국 하나의 선형변환과 동치가 되어 <span dir="rtl">”</span>깊게 쌓는 의미”가 크게 줄어든다.</p>
<p>결국 딥러닝에서 말하는 <span dir="rtl">”</span>딥(deep)“이란 은닉층을 여러 개 두고 비선형 변환을 반복 적용함으로써, 모델의 표현력(주어진 구조(층 수, 뉴런 수, 활성화 함수 등)가 얼마나 다양한 함수 f(x) 를 <span dir="rtl">”</span>근사해서” 표현할 수 있는가?)을 크게 확장하는 것을 의미한다.</p>
<p>MLP의 각 은닉층은 대개 완전연결(fully connected, dense) 구조로 구성된다. 이는 한 층의 모든 뉴런이 다음 층의 모든 뉴런과 연결되는 형태로, 층 간 연결은 가중치 행렬 W (및 편향 b)로 요약된다.</p>
<p>따라서 MLP의 학습은 데이터가 주어졌을 때 손실함수 L(예: MSE, MAE 등)를 최소화하도록 파라미터 (W,b) 를 반복적으로 조정하는 과정이다. 실제 업데이트는 역전파로 계산된 기울기 <span class="math inline">\(\nabla_{W,b}L\)</span>을 이용해 경사하강법 계열 알고리즘(예: SGD, Adam 등)으로 수행된다.</p>
<p>출력층(output layer)은 문제 유형에 맞게 출력값의 범위와 해석을 결정하도록 설계한다. 회귀 문제에서는 예측값이 실수 전 범위를 가질 수 있어야 하므로 출력층에 보통 항등함수(선형 출력) 를 두어 <span class="math inline">\(\widehat{y} \in \mathbb{R}\)</span>이 되게 한다.</p>
<p>반면 분류 문제에서는 출력이 확률로 해석되어야 하므로 제약을 만족시키는 활성화를 사용한다. 이진 분류는 <span class="math inline">\(0 \sim 1\)</span>범위의 확률을 위해 sigmoid를, 다중 분류는 각 클래스 확률의 합이 1이 되도록 softmax를 출력층에 적용한다. 요약하면 출력층은 <span dir="rtl">”</span>모형이 내놓는 값이 연속값인지, 확률인지”를 규정하는 최종 변환 단계로 이해할 수 있다.</p>
<p>시그모이드(sigmoid) 함수는 실수 입력을 받아 값을 0과 1 사이로 압축해 주는 대표적인 S자 형태의 함수입니다. 이진분류에서 <span dir="rtl">”</span>확률”처럼 해석 가능한 출력을 만들 때 사용된다. <span class="math inline">\(\sigma(z) = \frac{1}{1 + e^{- z}}\)</span>, 여기서 입력 <span class="math inline">\(z \in \mathbb{R}\)</span>, 출력은 <span class="math inline">\(\sigma(z) \in (0,1)\)</span>이다.</p>
</section>
</section>
<section id="mlp-회귀-방법론" class="level4">
<h4 class="anchored" data-anchor-id="mlp-회귀-방법론">2. MLP 회귀 방법론</h4>
<section id="문제-설정" class="level5">
<h5 class="anchored" data-anchor-id="문제-설정">(1) 문제 설정</h5>
<p>관측치 <span class="math inline">\(\{(x_{i},y_{i})\}_{i = 1}^{n}\)</span>가 주어졌을 때, <span class="math inline">\(x_{i} \in \mathbb{R}^{p},y_{i} \in \mathbb{R}\)</span>(연속형)라 하자. 딥러닝 회귀의 목표는 함수 <span class="math inline">\(f( \cdot ;\theta)\)</span>를 학습하여 <span class="math inline">\({\widehat{y}}_{i} = f(x_{i};\theta)\)</span>가 실제 <span class="math inline">\(y_{i}\)</span>와 가깝도록 파라미터 <span class="math inline">\(\theta\)</span>를 추정하는 것이다. 일반적으로 학습은 훈련 데이터에서의 평균 손실을 최소화하는 최적화 문제로 표현된다.</p>
<p><span class="math inline">\(\min_{\theta}\frac{1}{n}\overset{n}{\sum_{i = 1}}L\left( y_{i},f(x_{i};\theta) \right)\)</span>, 여기서 <span class="math inline">\(L( \cdot , \cdot )\)</span>는 손실함수(MSE/MAE/Huber 등)이다.</p>
</section>
<section id="모형-구조-완전연결-mlpfeedforward-network" class="level5">
<h5 class="anchored" data-anchor-id="모형-구조-완전연결-mlpfeedforward-network">(2) 모형 구조: 완전연결 MLP(Feedforward Network)</h5>
<p><strong>단일 은닉층(가장 단순한 MLP 회귀)</strong></p>
<p>은닉층이 1개인 MLP 회귀는 다음과 같이 쓸 수 있다.</p>
<p><span class="math inline">\(\widehat{y} = f(x) = W_{2}g(W_{1}x + b_{1}) + b_{2}\)</span>, <span class="math inline">\(x \in \mathbb{R}^{p}\)</span></p>
<p>은닉층 노드 수를 m이라 하면 <span class="math inline">\(W_{1} \in \mathbb{R}^{m \times p},b_{1} \in \mathbb{R}^{m}\)</span> 출력층(회귀)에서 <span class="math inline">\(W_{2} \in \mathbb{R}^{1 \times m},b_{2} \in \mathbb{R}\)</span>, <span class="math inline">\(g( \cdot ):ReLU,tanh\)</span> 등 비선형 활성화 함수이다.</p>
<p>핵심은 <span class="math inline">\(g( \cdot )\)</span>가 비선형이기 때문에, 단순한 선형회귀(<span class="math inline">\(\widehat{y} = \beta^{\top}x + \beta_{0}\)</span>)보다 훨씬 넓은 함수족을 표현할 수 있다는 점이다.</p>
<p><strong>다층 MLP(은닉층 L-1개, 출력층 1개)</strong></p>
<p>은닉층이 여러 개인 경우, 층 <span class="math inline">\(\ell\)</span>의 선형결합과 활성화 출력을</p>
<p><span class="math inline">\(z^{(\ell)} = W_{\ell}a^{(\ell - 1)} + b_{\ell},a^{(\ell)} = g_{\ell}\left( z^{(\ell)} \right)\)</span>로 두고 a^{(0)}=x라 하자. 회귀에서는 출력층을 보통 선형으로 두어 <span class="math inline">\(\widehat{y} = z^{(L)} = W_{L}a^{(L - 1)} + b_{L}\)</span>로 놓는다. (즉, 출력층 활성화는 항등함수)</p>
<p><strong><span dir="rtl">”</span>비선형이 없으면 깊이의 의미가 사라짐”</strong></p>
<p>만약 모든 층에서 활성화를 쓰지 않고 선형만 적용하면, <span class="math inline">\(W_{L}(W_{L - 1}(\cdots(W_{1}x)\cdots)) = \overset{˜}{W}x\)</span> 꼴로 합쳐져 결국 하나의 선형모형과 동치가 된다. 따라서 MLP의 표현력은 비선형 활성화의 반복 합성에서 나온다.</p>
</section>
<section id="활성화-함수activation-선택" class="level5">
<h5 class="anchored" data-anchor-id="활성화-함수activation-선택">(3) 활성화 함수(Activation) 선택</h5>
<p>대표 활성화들의 형태는 다음과 같다.</p>
<ul>
<li>ReLU <span class="math inline">\(ReLU(t) = \max(0,t)\)</span></li>
<li>Leaky ReLU <span class="math inline">\(LReLU(t) = \max(\alpha t,t)(\alpha \in (0,1))\)</span></li>
<li>tanh <span class="math inline">\(\tanh(t) = \frac{e^{t} - e^{- t}}{e^{t} + e^{- t}}\)</span></li>
</ul>
<p>실무적으로 표 데이터 MLP에서는 은닉층에 ReLU/LReLU가 기본 선택인 경우가 많다(학습 안정성과 기울기 소실 완화 측면).</p>
</section>
<section id="출력층-설계regression-output-design" class="level5">
<h5 class="anchored" data-anchor-id="출력층-설계regression-output-design">(4) 출력층 설계(Regression Output Design)</h5>
<p><strong>일반 회귀(범위 제약 없음)</strong></p>
<p><span class="math inline">\(y \in \mathbb{R}\)</span>이면 출력층은 선형(항등)으로 <span class="math inline">\(\widehat{y} \in \mathbb{R}\)</span>이 되게 한다.</p>
<p><strong>범위 제약이 있는 회귀(선택 사항)</strong></p>
<ul>
<li><span class="math inline">\(y \in (0,1)\)</span> (비율/확률형 반응): <span class="math inline">\(\widehat{y} = \sigma(z^{(L)})(\sigma:\text{sigmoid})\)</span></li>
<li><span class="math inline">\(y &gt; 0\)</span> (양수 반응): <span class="math inline">\(\widehat{y} = \exp(z^{(L)})\text{또는}\widehat{y} = \log(1 + e^{z^{(L)}})(\text{softplus})\)</span></li>
<li><span class="math inline">\(y \in \lbrack a,b\rbrack\)</span>: <span class="math inline">\(\widehat{y} = a + (b - a)\sigma(z^{(L)})\)</span></li>
</ul>
<p>이렇게 하면 예측값이 반응변수의 자연스러운 범위를 자동으로 만족한다.</p>
</section>
<section id="손실함수-mse-mae-huber" class="level5">
<h5 class="anchored" data-anchor-id="손실함수-mse-mae-huber">(5) 손실함수: MSE / MAE / Huber</h5>
<p>딥러닝 회귀에서 손실함수는 <span dir="rtl">”</span>무엇을 잘 맞출 것인가”를 결정한다. 같은 네트워크 구조라도 손실함수가 달라지면 학습되는 함수의 성질이 달라질 수 있으며, 특히 이상치(outlier), heavy-tail 오차, 비대칭 비용 구조가 있을 때 손실 선택은 성능과 해석을 좌우한다.</p>
<p><strong>MSE(평균제곱오차)</strong></p>
<p><span class="math inline">\(\text{MSE} = \frac{1}{n}\overset{n}{\sum_{i = 1}}(y_{i} - {\widehat{y}}_{i})^{2}\)</span>. MSE는 오차를 제곱하므로 큰 오차에 훨씬 큰 페널티를 부과한다. 즉 <span dir="rtl">”</span>크게 틀리는 것”을 강하게 억제하는 방향으로 학습이 진행된다. 오차가 정규분포(가우시안)이고 등분산이라는 가정에서는 MSE 최소화가 통계적으로 자연스러운 선택이며, 최적화 관점에서도 미분이 매끄러워 학습이 안정적으로 진행되는 경우가 많다. 반면 이상치가 존재하면 일부 관측치가 손실을 지배하여, 전체 패턴보다 특정 이상치에 맞춘 방향으로 모델이 끌려갈 수 있다.</p>
<p><strong>MAE(평균절대오차)</strong></p>
<p><span class="math inline">\(\text{MAE} = \frac{1}{n}\overset{n}{\sum_{i = 1}}|y_{i} - {\widehat{y}}_{i}|\)</span>. MAE는 오차를 절대값으로 평가하므로 MSE보다 이상치에 덜 민감하다. 따라서 노이즈가 크거나 오차분포가 heavy-tail인 데이터에서 더 강건한 예측을 제공하는 경우가 있다. 다만 절대값은 0에서 미분불가능하므로(실제로는 서브그라디언트로 처리), 학습이 다소 느리거나 불안정해지는 상황이 있을 수 있다. 또한 MAE는 큰 오차를 MSE만큼 강하게 벌하지 않으므로, <span dir="rtl">”</span>큰 실수를 반드시 줄여야 하는” 상황에서는 부적절할 수 있다.</p>
<p><strong>Huber 손실(절충형)</strong></p>
<p>Huber 손실은 작은 오차 구간에서는 제곱오차처럼, 큰 오차 구간에서는 절대오차처럼 동작하도록 설계된 절충형 손실이다.</p>
<p><span class="math display">\[L_{\delta}(r) = \{\begin{matrix}
\frac{1}{2}r^{2}, &amp; |r| \leq \delta \\
\lbrack 4pt\rbrack\delta\left( |r| - \frac{1}{2}\delta \right), &amp; |r| &gt; \delta
\end{matrix}(r = y - \widehat{y})\]</span></p>
<p><span class="math inline">\(\delta\)</span>는 <span dir="rtl">”</span>언제부터 큰 오차로 간주할 것인가”를 결정하는 임계값이다. Huber 손실은 MSE의 최적화 안정성과 MAE의 강건성을 일부씩 가져가며, 실제 회귀 문제에서 MSE와 MAE 중 어느 쪽도 확신하기 어려울 때 실무적으로 자주 선택된다. 특히 중간 정도의 이상치가 존재하거나, 소수의 큰 이상치가 전체 학습을 망치는 것을 방지하고 싶을 때 유용하다.</p>
<p><strong>손실 선택의 해석적 관점</strong></p>
<p>손실은 단순히 <span dir="rtl">”</span>평가척도”가 아니라 <span dir="rtl">”</span>학습 목표”다.</p>
<ul>
<li>MSE를 쓰면 모델은 조건부 평균 \mathbb{E}[Y|X=x]에 가까운 값을 맞추는 성질을 갖는다.</li>
<li>MAE를 쓰면 모델은 조건부 중앙값(median)에 가까운 값을 맞추는 성질이 강해진다.</li>
</ul>
<p>따라서 평균이 의미 있는지, 중앙값이 의미 있는지, 이상치가 구조적 정보인지(예: 실제 극단상황) 단순 오류인지에 따라 손실 선택을 해석적으로 결정할 수도 있다.</p>
</section>
<section id="정규화regularization와-과적합-제어" class="level5">
<h5 class="anchored" data-anchor-id="정규화regularization와-과적합-제어">(6) 정규화(Regularization)와 과적합 제어</h5>
<p>딥러닝은 매우 유연한 함수근사기이기 때문에 훈련 데이터에 지나치게 맞추는 과적합(overfitting)이 쉽게 발생한다. 전형적인 징후는 훈련 손실은 계속 감소하는데 검증(validation) 손실이 어느 시점부터 증가하거나 정체되는 현상이다. 따라서 딥러닝 회귀에서는 <span dir="rtl">”</span>모형을 크게 만드는 것”만큼이나 <span dir="rtl">”</span>일반화 성능을 유지하는 것”이 핵심이다.</p>
<p><strong>데이터 분할과 검증 기반 학습</strong></p>
<p>딥러닝은 학습 중 모델이 계속 변하므로, 학습 과정에서 검증셋을 사용하여 일반화 성능을 모니터링하는 것이 필수적이다. 일반적으로 Train/Validation/Test로 나누고, 학습 중 의사결정(조기종료, 하이퍼파라미터 선택)은 Validation 기준으로 수행하며, 최종 보고는 Test로 한다. 특히 데이터가 작을수록 검증셋의 분산이 커지므로, 여러 번의 반복 실험(시드 변경) 또는 교차검증적 사고방식이 필요하다.</p>
<p><strong>Early Stopping(조기 종료)</strong></p>
<p>검증 손실 <span class="math inline">\(L_{val}\)</span>이 더 이상 감소하지 않으면 학습을 중단한다. 이는 사실상 강력한 정규화로 작동한다(특히 표 데이터에서 효과적).</p>
<p>가장 강력하면서 구현도 쉬운 정규화 방법이 조기 종료다. 검증 손실이 더 이상 개선되지 않는 시점에서 학습을 중단하고, 그 이전의 <span dir="rtl">”</span>최고 성능 가중치”를 복원하여 사용한다. 조기 종료는 사실상 <span dir="rtl">”</span>학습을 너무 오래 하지 않음으로써 모델 복잡도를 제한”하는 효과가 있으며, 별도의 패널티를 추가하지 않아도 과적합을 크게 줄이는 경우가 많다. 실무에서는 patience(예: 10 epoch 동안 개선 없으면 종료)를 두어 일시적 흔들림에 과민 반응하지 않도록 한다.</p>
<p><strong>가중치 감쇠(Weight Decay, L2 정규화)</strong></p>
<p>가중치가 지나치게 커지면 모델 출력이 입력의 작은 변화에도 크게 흔들릴 수 있다. 이를 억제하기 위해 목적함수에 패널티를 추가한다.</p>
<p><span class="math inline">\(\min_{\theta}\frac{1}{n}\overset{n}{\sum_{i = 1}}L\left( y_{i},f(x_{i};\theta) \right) + \lambda\overset{L}{\sum_{\ell = 1}} \parallel W_{\ell} \parallel_{F}^{2}\)</span>, 여기서 <span class="math inline">\(\parallel W \parallel_{F}^{2} = \sum_{jk}W_{jk}^{2}\)</span>이다.<span class="math inline">\(\lambda\)</span>가 클수록 가중치를 더 강하게 줄여 모델이 매끄러워지고, 과적합이 줄어들 가능성이 커진다. 딥러닝 프레임워크에서는 보통 <span dir="rtl">”</span>weight decay”라는 이름으로 제공되며, AdamW처럼 옵티마이저 차원에서 더 안정적으로 구현된 방식이 널리 쓰인다.</p>
<p><strong>Dropout</strong></p>
<p>은닉층 활성값 <span class="math inline">\(a^{(\ell)}\)</span>에 대해 마스크 <span class="math inline">\(m^{(\ell)} \sim Bernoulli(q)\)</span>를 적용한다. <span class="math inline">\({\overset{˜}{a}}^{(\ell)} = m^{(\ell)} \odot a^{(\ell)}\)</span>(학습 시 일부 뉴런을 확률적으로 제거). 구현 방식에 따라 <span dir="rtl">”</span>inverted dropout”을 사용하면 학습 시 <span class="math inline">\({\overset{˜}{a}}^{(\ell)} = \frac{m^{(\ell)}}{q} \odot a^{(\ell)}\)</span>로 스케일을 보정하여 테스트 시 별도 보정 없이 사용한다.</p>
<p>Dropout은 학습 중 은닉 유닛을 일정 확률로 임의로 <span dir="rtl">”</span>끄는” 방식이다. 이렇게 하면 특정 유닛이나 특정 입력 패턴에 과도하게 의존하기 어렵게 되어 일반화가 좋아지는 효과가 나타난다. 직관적으로는 많은 서로 다른 하위 네트워크를 학습해 평균내는 앙상블 효과에 가깝다. 다만 회귀 문제에서 dropout 비율을 너무 크게 잡으면 예측이 불안정해질 수 있으므로, 검증 성능을 기준으로 적절한 범위를 탐색해야 한다.</p>
<p><strong>데이터 관점의 정규화: 증강과 잡음(Data-Centric Regularization)</strong></p>
<p>과적합은 모델 구조나 규제항만으로 결정되는 문제가 아니라, 데이터의 다양성과 품질에 의해 크게 좌우된다. 이미지 분야에서는 회전·자르기 등 데이터 증강(data augmentation) 이 대표적이지만, 회귀 문제에서도 유사한 관점의 <span dir="rtl">”</span>데이터 기반 정규화”를 적용할 수 있다. 예를 들어 입력 변수에 매우 작은 확률적 변동을 더하는 노이즈 주입(noise injection) 은 모델이 특정 관측치의 미세한 변동에 과도하게 맞춰지는 것을 완화하고, 보다 안정적인 패턴을 학습하도록 유도한다.</p>
<p>또한 일반화 성능은 학습 데이터 구성 방식에도 영향을 받는다. 문제 특성에 맞는 샘플링 전략(예: 불균형한 구간의 재표본화, 시간순 데이터의 적절한 분할) 을 통해 학습 분포를 안정화할 수 있으며, 데이터의 신뢰도를 떨어뜨리는 요소인 이상치(outlier), 누락값(missingness), 측정 오류, 스케일 불일치 등을 어떻게 처리하느냐가 모델의 과적합 여부를 실질적으로 좌우하는 경우가 많다.</p>
<p>특히 표 형태 데이터에서는 복잡한 정규화 기법을 추가하기보다, 변수 전처리(스케일링/변환), 누락값 처리, 이상치 처리, 범주형 인코딩의 적절성 같은 데이터 품질 관리가 성능에 더 큰 영향을 미치는 경우가 흔하다. 즉, 표 데이터 회귀에서는 <span dir="rtl">”</span>모델을 규제하는 것”만큼이나 <span dir="rtl">”</span>데이터를 정돈하고 신뢰도를 높이는 것”이 일반화 성능 향상의 핵심 전략이 된다.</p>
<p><strong>모델 용량 조절(Architecture Control)</strong></p>
<p>과적합은 단순히 정규화(regularization)만으로 항상 해결되지 않는다. 모델 자체의 용량(capacity) 이 데이터 규모에 비해 지나치게 크면, 규제를 걸더라도 훈련 데이터의 우연한 패턴(잡음)까지 학습해 훈련 성능만 좋아지고 검증 성능은 오히려 나빠지는 현상이 쉽게 나타난다. 특히 데이터가 크지 않은 표 형태(tabular) 데이터에서는 <span dir="rtl">”</span>깊고 넓은” MLP가 이런 문제를 유발하는 경우가 흔하다.</p>
<p>따라서 MLP 회귀에서는 학습 전에부터 모델 용량을 적절히 설계·제한하는 것이 중요하다. 구체적으로는 은닉층의 층 수(깊이) 와 각 층의 노드 수(너비) 를 조절하고, 활성화 함수의 선택, Batch Normalization 같은 정규화 레이어의 사용 여부, 그리고 드롭아웃/가중치 감쇠 등과의 조합을 함께 고려해야 한다.</p>
<p>기본 원칙은 <span dir="rtl">”</span>데이터가 작을수록 단순한 구조에서 시작한다” 이다. 검증 성능이 개선되지 않는데도 모델만 더 키우는 것은 대개 올바른 방향이 아니며, 이 경우에는 구조를 확장하기보다 정규화 강화, 조기 종료, 입력/타깃 스케일링 점검, 변수 설계(특히 범주형 처리) 개선 등 다른 요인을 먼저 점검하는 것이 합리적이다.</p>
</section>
<section id="입력타깃-전처리scaling-encoding" class="level5">
<h5 class="anchored" data-anchor-id="입력타깃-전처리scaling-encoding">(7) 입력/타깃 전처리(Scaling &amp; Encoding)</h5>
<p><strong>연속형 입력 표준화</strong></p>
<p>연속형 변수 <span class="math inline">\(x_{j}\)</span>는 보통 <span class="math inline">\(x_{ij}' = \frac{x_{ij} - \mu_{j}}{s_{j}}\)</span>로 표준화한다. (훈련 데이터 기준 <span class="math inline">\(\mu_{j}\)</span>, <span class="math inline">\(s_{j}\)</span> 추정 후 검증/테스트에 동일 적용)</p>
<p>경사하강법 기반 최적화는 스케일에 민감하므로 표준화는 수렴과 안정성을 크게 개선한다.</p>
<p><strong>범주형 변수: 원-핫과 임베딩</strong></p>
<p>원-핫: 범주 <span class="math inline">\(c \in \{ 1,\ldots,K\} - &gt; e_{c} \in \{ 0,1\}^{K}\)</span>로 변환</p>
<p>임베딩: 임베딩 행렬 <span class="math inline">\(E \in \mathbb{R}^{K \times d}\)</span>를 두고 범주 c에 대해 <span class="math inline">\(v = E_{c,:} \in \mathbb{R}^{d}\)</span>로 저차원 연속 표현을 학습한다(특히 K가 큰 경우 유리).</p>
<p><strong>타깃 y 표준화(선택)</strong></p>
<p><span class="math inline">\(y_{i}' = \frac{y_{i} - \mu_{y}}{s_{y}}\)</span>로 학습한 뒤 예측에서 역변환하면(<span class="math inline">\(\widehat{y} = s_{y}{\widehat{y}}' + \mu_{y}\)</span>) 수렴이 더 안정적인 경우가 있다.</p>
</section>
<section id="최적화optimization-미니배치-sgd와-adam" class="level5">
<h5 class="anchored" data-anchor-id="최적화optimization-미니배치-sgd와-adam">(8) 최적화(Optimization): 미니배치 SGD와 Adam</h5>
<p><strong>미니배치 목적함수</strong></p>
<p>미니배치 <span class="math inline">\(B \subset \{ 1,\ldots,n\}\)</span>에 대해 <span class="math inline">\({\widehat{\mathcal{L}}}_{B}(\theta) = \frac{1}{|B|}\sum_{i \in B}L\left( y_{i},f(x_{i};\theta) \right)\)</span>를 사용하여 반복 갱신한다.</p>
<p><strong>SGD 업데이트(기본형)</strong></p>
<p><span class="math inline">\(\theta_{t + 1} = \theta_{t} - \eta\nabla_{\theta}{\widehat{\mathcal{L}}}_{B}(\theta_{t})\)</span>, 여기서 <span class="math inline">\(\eta\)</span>는 학습률이다.</p>
<p><strong>Adam 업데이트(대표적 적응적 방법)</strong></p>
<p>기울기 <span class="math inline">\(g_{t} = \nabla_{\theta}{\widehat{\mathcal{L}}}_{B}(\theta_{t})\)</span>에 대해 <span class="math inline">\(m_{t} = \beta_{1}m_{t - 1} + (1 - \beta_{1})g_{t},v_{t} = \beta_{2}v_{t - 1} + (1 - \beta_{2})g_{t}^{2}\)</span></p>
<p>편향 보정: <span class="math inline">\({\widehat{m}}_{t} = \frac{m_{t}}{1 - \beta_{1}^{t}},{\widehat{v}}_{t} = \frac{v_{t}}{1 - \beta_{2}^{t}}\)</span></p>
<p>업데이트: <span class="math inline">\(\theta_{t + 1} = \theta_{t} - \eta\frac{{\widehat{m}}_{t}}{\sqrt{{\widehat{v}}_{t}} + \epsilon}\)</span>.</p>
<p>표 데이터 MLP에서 Adam/AdamW가 기본 출발점으로 자주 쓰인다.</p>
</section>
<section id="하이퍼파라미터-설계-포인트표-데이터-기준" class="level5">
<h5 class="anchored" data-anchor-id="하이퍼파라미터-설계-포인트표-데이터-기준">(9) 하이퍼파라미터 설계 포인트(표 데이터 기준)</h5>
<p>MLP 회귀의 성능은 구조·정규화·최적화의 조합에 크게 좌우된다.</p>
<ul>
<li>구조: L (층 수), 각 층 폭 m_\ell</li>
<li>활성화: ReLU/LReLU/tanh</li>
<li>손실: MSE/MAE/Huber(\delta)</li>
<li>정규화: weight decay \lambda, dropout rate, early stopping</li>
<li>최적화: Adam/AdamW, 학습률 \eta, 배치 크기, 스케줄링</li>
</ul>
</section>
<section id="역전파backpropagation" class="level5">
<h5 class="anchored" data-anchor-id="역전파backpropagation">(10) 역전파(Backpropagation)</h5>
<p>학습의 핵심은 손실의 기울기 <span class="math inline">\(\nabla_{\theta}L\)</span>를 효율적으로 계산하는 것이다. 다층 구조에서는 연쇄법칙을 이용해 출력층에서부터 오차 신호를 뒤로 전달한다.</p>
<p>역전파는 <span dir="rtl">”</span>분류 전용 기법”이 아니라, 신경망이 어떤 문제(회귀/분류)를 풀든 손실함수를 최소화하기 위해 가중치·편향의 기울기(gradient) 를 계산하는 표준 방법이에요.</p>
<ul>
<li>순전파(feedforward): 입력 x → 예측값 \hat y 계산</li>
<li>손실 계산: 회귀면 보통 MSE/MAE/Huber 등으로 y와 \hat y의 차이를 수치화</li>
<li>역전파(backprop): 그 손실이 각 층의 W,b에 얼마나 영향을 받는지 <span class="math inline">\(\partial L/\partial W,\partial L/\partial b\)</span>를 연쇄법칙으로 계산</li>
<li>최적화(Adam/SGD 등): 계산된 기울기로 파라미터 업데이트</li>
</ul>
<p><strong>표기(Forward 정의)</strong></p>
<p>L층 신경망(마지막 L층이 출력층)에서 입력 <span class="math inline">\(a^{(0)} = x\)</span>, 각 층 <span class="math inline">\(\ell = 1,\ldots,L\)</span>에서 <span class="math inline">\(z^{(\ell)} = W_{\ell}a^{(\ell - 1)} + b_{\ell},a^{(\ell)} = g_{\ell}\left( z^{(\ell)} \right)\)</span>이다.</p>
<p>회귀에서 마지막 층은 선형 출력이므로 <span class="math inline">\(g_{L}(t) = t, \Rightarrow a^{(L)} = z^{(L)} = \widehat{y}\)</span> . 벡터 출력 회귀면 <span class="math inline">\(\widehat{y} \in \mathbb{R}^{d}\)</span>로 동일하게 성립한다.</p>
<p><strong>손실(MSE)</strong></p>
<p>단일 관측치 기준으로 MSE를 <span class="math inline">\(\mathcal{L} = \frac{1}{2} \parallel \widehat{y} - y \parallel_{2}^{2}\)</span>로 두면(앞의 <span class="math inline">\(\frac{1}{2}\)</span>는 미분을 단순화하려는 관례)</p>
<p><strong><span class="math inline">\(\delta^{(\ell)}\)</span> 정의</strong></p>
<p>각 층의 오차 신호를 <span class="math inline">\(\delta^{(\ell)} \equiv \frac{\partial\mathcal{L}}{\partial z^{(\ell)}}\)</span>로 정의한다. 즉, <span class="math inline">\(\delta^{(\ell)}\)</span>는 <span dir="rtl">”</span>층 <span class="math inline">\(\ell\)</span>의 선형결합 <span class="math inline">\(z^{(\ell)}\)</span>에 대한 손실의 기울기”이다.</p>
<p><strong>출력층에서 시작(기저식)</strong></p>
<p>(A) 선형 출력 + MSE인 경우: <span class="math inline">\(\delta^{(L)} = \frac{\partial\mathcal{L}}{\partial z^{(L)}} = \frac{\partial\mathcal{L}}{\partial\widehat{y}} \cdot \frac{\partial\widehat{y}}{\partial z^{(L)}}\)</span>, 여기서 <span class="math inline">\(\frac{\partial\mathcal{L}}{\partial\widehat{y}} = \widehat{y} - y\)</span>이고 선형 출력이므로 <span class="math inline">\(\frac{\partial\widehat{y}}{\partial z^{(L)}} = I\)</span>이다. 따라서 <span class="math inline">\(\delta^{(L)} = \widehat{y} - y\)</span>이다. 참고로 만약 손실을 <span class="math inline">\(\parallel \widehat{y} - y \parallel^{2}\)</span>처럼 <span class="math inline">\(\frac{1}{2}\)</span>없이 정의하면 <span class="math inline">\(\delta^{(L)} = 2(\widehat{y} - y)\)</span>가 되므로 상수 계수 차이이다.</p>
<p><strong>은닉층으로의 전파(재귀식)</strong></p>
<p>은닉층 <span class="math inline">\(\ell = L - 1,\ldots,1\)</span>에 대해 <span class="math inline">\(\delta^{(\ell)} = \frac{\partial\mathcal{L}}{\partial z^{(\ell)}}\)</span>이다. 연쇄법칙을 적용하면 <span class="math inline">\(\delta^{(\ell)} = \left( W_{\ell + 1}^{\top}\delta^{(\ell + 1)} \right) \odot g_{\ell}'\left( z^{(\ell)} \right)\)</span>, 여기서 <span class="math inline">\(W_{\ell + 1}^{\top}\delta^{(\ell + 1)}\)</span>는 <span dir="rtl">”</span>다음 층의 오차를 현재 층으로 전달”, <span class="math inline">\(g_{\ell}'(z^{(\ell)})\)</span> 은 <span dir="rtl">”</span>현재 층 활성화의 기울기”, 그리고 <span class="math inline">\(\odot\)</span>는 원소별(elementwise) 곱이다.</p>
<p><strong>파라미터 기울기(각 층 <span class="math inline">\(W_{\ell},b_{\ell}\)</span>)</strong></p>
<p>각 층 <span class="math inline">\(\ell = 1,\ldots,L\)</span>에 대해 다음이 성립한다.</p>
<ul>
<li>가중치 기울기: <span class="math inline">\(\frac{\partial\mathcal{L}}{\partial W_{\ell}} = \delta^{(\ell)}\left( a^{(\ell - 1)} \right)^{\top}\)</span></li>
<li>편향 기울기: <span class="math inline">\(\frac{\partial\mathcal{L}}{\partial b_{\ell}} = \delta^{(\ell)}\)</span></li>
</ul>
<p>편향은 각 뉴런에 더해지는 항이므로 미분하면 그대로 <span class="math inline">\(\delta^{(\ell)}\)</span>가 남는다.</p>
<p><strong>미니배치/전체 데이터로 확장</strong></p>
<p>미니배치 B에서 평균 손실을 <span class="math inline">\(\mathcal{L}_{B} = \frac{1}{|B|}\sum_{i \in B}\mathcal{L}_{i}\)</span>로 두면, 기울기도 평균으로 합쳐진다.</p>
<p><span class="math display">\[\frac{\partial\mathcal{L}_{B}}{\partial W_{\ell}} = \frac{1}{|B|}\sum_{i \in B}\frac{\partial\mathcal{L}_{i}}{\partial W_{\ell}},\frac{\partial\mathcal{L}_{B}}{\partial b_{\ell}} = \frac{1}{|B|}\sum_{i \in B}\frac{\partial\mathcal{L}_{i}}{\partial b_{\ell}}\]</span></p>
<p><strong>요약</strong></p>
<p>정의: <span class="math inline">\(\delta^{(\ell)} = \partial\mathcal{L}/\partial z^{(\ell)}\)</span></p>
<p>출력층(선형+MSE): <span class="math inline">\(\delta^{(L)} = \widehat{y} - y\)</span></p>
<p>전파: <span class="math inline">\(\delta^{(\ell)} = (W_{\ell + 1}^{\top}\delta^{(\ell + 1)}) \odot g_{\ell}'(z^{(\ell)})\)</span></p>
<p>기울기: <span class="math inline">\(\frac{\partial\mathcal{L}}{\partial W_{\ell}} = \delta^{(\ell)}(a^{(\ell - 1)})^{\top},\frac{\partial\mathcal{L}}{\partial b_{\ell}} = \delta^{(\ell)}\)</span></p>
</section>
</section>
<section id="확장-quantile-분포-예측불확실성" class="level4">
<h4 class="anchored" data-anchor-id="확장-quantile-분포-예측불확실성">3. 확장: Quantile / 분포 예측(불확실성)</h4>
<p>MLP 회귀에서 흔히 하는 것은 <span class="math inline">\(\widehat{y}\)</span> 하나를 내는 점예측(point prediction) 이다. 그러나 실제 의사결정에서는 <span dir="rtl">”</span>예측값이 얼마인가”뿐 아니라 <span dir="rtl">”</span>얼마나 확신할 수 있는가”가 함께 중요하다. 예를 들어 재고관리에서는 평균 수요보다 상위 분위수(예: 0.9-quantile) 가 더 중요할 수 있고, 위험관리·품질관리에서는 예측의 불확실성 자체가 핵심 정보가 된다.</p>
<p>딥러닝 회귀는 이러한 요구를 반영하여 크게 두 방향으로 확장될 수 있다. (1) 조건부 분위수(quantile) 자체를 직접 예측하거나, (2) 조건부 분포 <span class="math inline">\(Y \mid X = x\)</span>를 가정하고 분포의 모수(또는 분포 전체)를 예측하는 방식이다.</p>
<section id="분위수quantile-회귀" class="level5">
<h5 class="anchored" data-anchor-id="분위수quantile-회귀">(1) 분위수(Quantile) 회귀</h5>
<p>분위수 회귀는 조건부 평균 <span class="math inline">\(\mathbb{E}\lbrack Y \mid X\rbrack\)</span> 대신, 조건부 분위수 <span class="math inline">\(Q_{\tau}(Y \mid X = x)\)</span>를 예측한다. 이를 위해 손실함수로 핀볼 손실(pinball loss) 을 사용한다. 한 관측치 <span class="math inline">\((y,\widehat{y})\)</span>에 대해</p>
<p><span class="math inline">\(L_{\tau}(y,\widehat{y}) = \{\begin{matrix}
\tau(y - \widehat{y}), &amp; y \geq \widehat{y} \\
\lbrack 4pt\rbrack(1 - \tau)(\widehat{y} - y), &amp; y &lt; \widehat{y}
\end{matrix}\)</span>로 정의한다.</p>
<p><span class="math inline">\(\tau = 0.5\)</span>이면 중앙값(중위수) 회귀에 해당한다.</p>
<p><span class="math inline">\(\tau = 0.9\)</span>이면 <span dir="rtl">”</span>상위 90% 수준”의 값을 예측하는 것이므로, 보수적인 의사결정(재고/서비스레벨 설정 등)에 유용하다.</p>
<p>또한 여러 <span class="math inline">\(\tau\)</span> (예: 0.1, 0.5, 0.9)를 동시에 학습하면 예측구간을 구성할 수 있어, 단일 점예측보다 훨씬 풍부한 정보를 제공한다. 예를 들어 <span class="math inline">\((\tau_{L},\tau_{U}) = (0.1,0.9)\)</span>라면 대략적인 80% 예측구간을 얻는 셈이다.</p>
<p>실무적으로는 <span class="math inline">\(\tau\)</span>가 여러 개인 경우 분위수 교차가 발생할 수 있으므로, 단조성 제약이나 후처리 등을 고려하기도 한다.</p>
</section>
<section id="분포distribution-예측-조건부-분포를-직접-모델링" class="level5">
<h5 class="anchored" data-anchor-id="분포distribution-예측-조건부-분포를-직접-모델링">(2) 분포(Distribution) 예측: 조건부 분포를 직접 모델링</h5>
<p>또 다른 접근은 <span class="math inline">\(Y \mid X = x\)</span>의 분포 형태를 가정하고(예: 정규분포), 그 분포의 모수를 신경망이 출력하도록 하는 것이다. 예를 들어 <span class="math inline">\(Y \mid X = x \sim \mathcal{N}(\mu(x),\sigma^{2}(x))\)</span>를 가정하면, 네트워크는 <span class="math inline">\(\mu(x)\)</span>와 <span class="math inline">\(\sigma(x)\)</span>를 예측한다. 분산은 항상 양수여야 하므로 보통 <span class="math inline">\(\log\sigma(x)\)</span> 또는 <span class="math inline">\(\log\sigma^{2}(x)\)</span>를 출력하게 설계한다(양수 제약을 자연스럽게 보장하기 위함이다).</p>
<p>이때 학습은 해당 분포의 음의 로그우도를 최소화하는 방식으로 진행된다. 정규분포 가정 하에서 한 관측치에 대한 NLL은 <span class="math inline">\(NLL(y \mid x) = \frac{1}{2}\log(2\pi\sigma^{2}(x)) + \frac{(y - \mu(x))^{2}}{2\sigma^{2}(x)}\)</span>이며, 전체 데이터에 대해 평균 NLL을 최소화한다.</p>
<p>이 방식의 장점은 예측값(예: 평균 <span class="math inline">\(\mu(x)\)</span>)만 주는 것이 아니라, 입력에 따라 달라지는 불확실성까지 함께 제공한다는 점이다. 특히 <span class="math inline">\(\sigma^{2}(x)\text{가}x\text{에 따라 변하는 경우}\)</span>, 즉 이분산 상황—입력 영역에 따라 오차 크기가 달라지는 상황—에서 점예측보다 더 현실적인 결과를 제공할 수 있다. 예를 들어 <span dir="rtl">”</span>특정 구간에서는 데이터 변동이 커서 예측이 불확실하다”는 구조를 모델이 명시적으로 반영하게 된다.</p>
</section>
<section id="mlp-외-딥러닝-회귀-대안선택" class="level5">
<h5 class="anchored" data-anchor-id="mlp-외-딥러닝-회귀-대안선택">4. MLP 외 딥러닝 회귀 대안(선택)</h5>
<p>표 형태 데이터에서 딥러닝 회귀는 오랫동안 MLP가 대표적인 선택지로 여겨졌지만, 최근에는 표 데이터가 갖는 구조적 특성—범주형 변수의 비중, 희소한 규칙성, 이산적 분기와 강한 상호작용—을 더 잘 반영하려는 다양한 설계가 등장했다. 또한 예측값 하나를 내는 점예측(point prediction) 을 넘어, 예측의 신뢰도를 함께 제시하는 불확실성(uncertainty) 추정까지 요구되는 경우가 늘면서, 딥러닝 회귀의 확장 방향도 함께 넓어지고 있다.</p>
</section>
<section id="wide-deep" class="level5">
<h5 class="anchored" data-anchor-id="wide-deep">(1) Wide &amp; Deep</h5>
<p>Wide &amp; Deep은 이름 그대로 선형 모형(wide) 과 딥러닝 모형(deep) 을 결합한 구조다. 두 구성요소는 서로 다른 강점을 담당하며, 이를 통해 표 데이터에서 흔히 나타나는 패턴을 보다 안정적으로 포착한다.</p>
<ul>
<li>Wide(선형) 부분은 원-핫 인코딩된 범주형 변수, 그리고 필요하다면 변수 간 교호작용(feature crosses)처럼 명시적으로 만들어진 규칙 기반 특징을 빠르게 학습한다. 이는 추천 시스템 문맥에서 흔히 말하는 암기(memorization) 에 해당하며, 특정 범주 조합에서만 나타나는 뚜렷한 규칙(예: <span dir="rtl">”</span>A 범주 &amp; B 범주일 때만 평균이 급변”)을 효과적으로 잡아낸다.</li>
<li>Deep(딥) 부분은 임베딩과 비선형 변환을 통해 유사한 사례들 간의 공통 구조를 학습하고, 보지 못한 조합에도 부드럽게 확장되는 일반화(generalization) 를 담당한다. 즉, 연속형 변수의 비선형 관계, 완만한 상호작용, 범주 간 유사성에 기반한 패턴을 MLP가 흡수한다.</li>
</ul>
<p>두 부분의 결합 핵심은 역할 분담이다. 표 데이터에서 자주 등장하는 <span dir="rtl">”</span>이산적·규칙 기반 패턴”은 선형(Wide) 쪽이 안정적으로 흡수하고, 그 밖의 복잡한 연속형 관계나 부드러운 상호작용은 딥(Deep) 쪽이 담당하도록 설계한다. 그 결과 MLP 단독보다 학습이 안정적이거나 일반화가 개선되는 경우가 있으며, 특히 범주형 변수가 많고 희소성이 큰 문제(원-핫 차원이 매우 커지는 상황) 에서 유의미한 대안이 될 수 있다.</p>
</section>
<section id="tabnet" class="level5">
<h5 class="anchored" data-anchor-id="tabnet">(2) TabNet</h5>
<p>TabNet은 표 형태 데이터에서 <span dir="rtl">”</span>모든 변수를 한꺼번에 쓰는 MLP”와 달리, 어떤 변수를 언제, 어느 정도까지 볼지를 모델이 스스로 학습하도록 설계된 딥러닝 구조다. 핵심 아이디어는 예측을 한 번에 만들기보다, 여러 번의 순차적 결정 단계(decision step) 를 거치면서 매 단계마다 attention 기반 마스킹(masking) 으로 일부 변수에 집중하고, 그 결과를 누적해 최종 예측으로 연결하는 방식이다. 이런 작동 방식은 트리 모델이 분할을 반복하면서 중요한 변수를 선택해 나가는 과정과 직관적으로 닮아 있어, 표 데이터에 더 <span dir="rtl">”</span>맞는” 구조로 이해할 수 있다.</p>
<p>TabNet의 특징은 다음과 같이 정리할 수 있다.</p>
<ul>
<li>내재적 변수 선택(implicit feature selection): 각 단계에서 마스크가 특정 변수(또는 변수 그룹)에 가중치를 부여하고 나머지는 덜 보게 만들기 때문에, 결과적으로 모델 내부에서 <span dir="rtl">”</span>중요한 변수에 집중하고 불필요한 변수는 배제하려는” 성향이 생긴다. 이는 표 데이터에서 흔한 잡음 변수, 중복 변수의 영향을 줄이는 데 도움이 될 수 있다.</li>
<li>부분적 설명 가능성(interpretability) 확보: 단계별로 어떤 변수들이 선택(또는 강조)되었는지를 마스크를 통해 확인할 수 있어, 일반적인 MLP보다 <span dir="rtl">”</span>어떤 변수가 예측에 기여했는가”를 추적하기가 상대적으로 쉽다. 즉 완전한 블랙박스라기보다, 변수 중요도/사용 패턴을 시각화할 수 있는 여지가 있다.</li>
</ul>
<p>다만 TabNet은 구조상 결정 단계 수, 마스킹의 희소성(sparsity)을 유도하는 관련 파라미터 등 하이퍼파라미터에 민감할 수 있다. 따라서 성능을 안정적으로 확보하려면 단순히 기본값으로 고정하기보다, 검증 성능을 기준으로 단계 수와 sparsity 정도를 포함한 설정을 체계적으로 튜닝하는 것이 중요하다.</p>
</section>
<section id="tabular-transformer표-데이터용-트랜스포머" class="level5">
<h5 class="anchored" data-anchor-id="tabular-transformer표-데이터용-트랜스포머">(3) Tabular Transformer(표 데이터용 트랜스포머)</h5>
<p>트랜스포머(Transformer)는 원래 자연어 처리에서 단어(토큰)들 사이의 관계를 attention으로 학습하는 구조인데, 최근에는 이 아이디어를 표 형태 데이터에 적용하려는 시도들이 활발하다. 표 데이터에서의 핵심은 문장 내 토큰 관계 대신, 변수(열, column)들 사이의 관계를 attention으로 모델링하는 것이다. 즉, <span dir="rtl">”</span>어떤 변수들이 함께 나타날 때 예측이 어떻게 달라지는가”를 네트워크가 보다 유연하게 학습하도록 만드는 접근이다.</p>
<p>표 데이터에 트랜스포머를 적용하려면 각 열을 <span dir="rtl">”</span>토큰”처럼 다루기 위한 표현이 필요하다. 범주형 변수는 보통 임베딩 벡터로 바꾸어 토큰처럼 처리할 수 있고, 연속형 변수도 단순히 숫자로 넣기보다는 프로젝션(선형 변환) 또는 임베딩 기반 인코딩을 통해 토큰화한다. 이렇게 구성하면 트랜스포머는 attention을 통해 변수들 간의 상호작용을 폭넓게 탐색하며, 특정 변수 조합에서만 나타나는 패턴이나 복잡한 의존 구조를 비교적 자연스럽게 포착할 수 있다.</p>
<p>다만 표 데이터에서 트랜스포머가 항상 MLP나 트리 앙상블보다 우월한 것은 아니다. 트랜스포머는 일반적으로 파라미터 수가 크고 학습이 민감한 편이라, 데이터 크기가 충분히 크지 않으면 과적합이 발생하기 쉽고, 실전에서는 여전히 트리 기반 앙상블(부스팅 계열) 이 강력한 기준선이 되는 경우가 많다. 따라서 Tabular Transformer는 <span dir="rtl">”</span>표 데이터 딥러닝의 대표적 대안”으로 소개할 가치가 충분하지만, 실제 적용에서는 동일한 데이터 분할과 튜닝 수준을 맞춘 상태에서 트리 앙상블 및 MLP와의 비교 실험을 함께 수행한다는 메시지를 동반하는 것이 바람직하다.</p>
</section>
<section id="시계열-대안-tcn과-transformer는-예고만" class="level5">
<h5 class="anchored" data-anchor-id="시계열-대안-tcn과-transformer는-예고만">(4) 시계열 대안: TCN과 Transformer는 <span dir="rtl">’</span>예고<span dir="rtl">’</span>만</h5>
<p>시계열 예측은 입력이 단순 벡터가 아니라 시간 순서(순차 구조)를 갖기 때문에, MLP를 그대로 적용하면 시간 의존성을 충분히 활용하기 어렵다. 이때 대표 대안이 TCN과 시계열 Transformer 계열이다. TCN(Temporal Convolutional Network)은 1차원 합성곱과 dilation을 통해 과거의 넓은 구간 정보를 효율적으로 반영하며, RNN보다 병렬화가 쉬워 학습이 안정적인 장점이 있다. Transformer 계열은 attention으로 장기 의존성을 직접 모델링하며, 계절성/추세/구간 패턴을 유연하게 학습할 수 있어 장기 예측에서 강점을 보인다는 보고가 많다. 다만 시계열은 데이터 구성, 누락, 계절성 처리, 롤링 검증 등 고유의 방법론이 필요하므로 시계열 분야에서 다루기로 한다.</p>
</section>
</section>
<section id="mlp-회귀의-장점과-단점" class="level4">
<h4 class="anchored" data-anchor-id="mlp-회귀의-장점과-단점">5. MLP 회귀의 장점과 단점</h4>
<p>MLP 회귀는 입력 특성 벡터 (x)를 여러 은닉층을 통해 비선형적으로 변환하여 연속형 반응변수 (y)를 예측하는 대표적인 딥러닝 방법이다. 표(tabular) 데이터에서는 전통적으로 트리 앙상블이 강력한 기준선이지만, MLP는 표현 학습(representation learning) 과 유연한 확장성을 바탕으로 특정 조건에서 경쟁력 있는 선택지가 된다.</p>
<section id="장점strengths" class="level5">
<h5 class="anchored" data-anchor-id="장점strengths">(1) 장점(Strengths)</h5>
<p><strong>복잡한 비선형성과 상호작용을 자동 학습</strong></p>
<p>선형회귀는 선형 결합으로만 관계를 표현하지만, MLP는 은닉층과 활성화함수를 통해 비선형 관계와 변수 간 상호작용을 자연스럽게 학습한다.<br>
즉, <span dir="rtl">”</span>어떤 형태의 상호작용을 넣어야 하는지”를 사람이 일일이 설계하지 않아도, 데이터가 충분하면 모델이 내부적으로 구조를 찾아낼 수 있다.</p>
<p><strong>입력 표현을 함께 학습(특히 범주형: 임베딩의 장점)</strong></p>
<p>범주형 변수가 많거나 범주 수가 큰 경우, 원-핫 인코딩은 차원을 급격히 늘려 비효율적이 된다. MLP는 범주형을 임베딩(embedding) 으로 표현해 저차원의 연속 벡터로 학습할 수 있어, 고카디널리티 범주형 처리에 유리하고 범주 간 유사성(예: 비슷한 브랜드/차종 등)을 데이터로부터 학습할 수 있다.</p>
<p><strong>목적함수/출력 설계의 유연성(확장성)</strong></p>
<p>MLP는 <span dir="rtl">”</span>평균값 예측”에만 머물지 않고, 목적과 상황에 따라 다양한 확장이 가능하다.</p>
<ul>
<li>분위수(quantile) 예측 → 예측구간 구성</li>
<li>분포 예측(NLL 기반) → 이분산(입력에 따라 불확실성이 달라짐) 반영</li>
<li>다중 출력/다중 과제(multi-task) → 여러 타깃을 동시에 학습<br>
이처럼 의사결정에 필요한 정보(불확실성 포함) 를 함께 제공하는 방향으로 확장하기가 쉽다.</li>
</ul>
<p><strong>미분 가능(differentiable) 구조 → 최적화 및 결합이 용이</strong></p>
<p>신경망은 전체가 미분 가능하므로, 손실 설계나 제약(예: 커스텀 손실, 가중치 패널티, 특정 형태의 규제)을 붙이기 좋다. 또한 다른 신경망 모듈(예: 임베딩, 어텐션, 게이팅 등)과 결합이 쉬워 <span dir="rtl">”</span>표 데이터 전용 딥러닝(예: Wide&amp;Deep, TabNet, Tabular Transformer)“로 확장하는 기반이 된다.</p>
<p><strong>대규모 데이터/온라인 학습 환경에서의 장점</strong></p>
<p>데이터가 매우 크거나 지속적으로 누적되는 상황에서는 MLP가 효율적으로 학습/업데이트되는 경우가 많다. 특히 대규모 추천·광고·수요예측 환경에서는 임베딩 + MLP 조합이 실무 표준으로 자리 잡은 사례가 많다.</p>
</section>
<section id="단점-및-주의점weaknesses" class="level5">
<h5 class="anchored" data-anchor-id="단점-및-주의점weaknesses">(2) 단점 및 주의점(Weaknesses)</h5>
<p><strong>표 데이터에서는 <span dir="rtl">”</span>데이터 크기”에 민감(과적합 위험)</strong></p>
<p>표 데이터(특히 샘플 수가 크지 않은 경우)에서 MLP는 쉽게 과적합한다. <span dir="rtl">”</span>깊고 넓은 모델”을 쓰면 훈련 성능은 좋아지지만 검증/테스트 성능이 악화되는 경우가 흔하다. 따라서 모델 용량(층 수·노드 수) 을 과하게 키우기보다, 작은 모델 + 정규화 + 조기종료로 안정적으로 접근해야 한다.</p>
<p><strong>전처리 의존도가 큼(스케일링·결측·이상치)</strong></p>
<p>MLP는 입력 스케일에 민감하며, 결측/이상치/변수 변환의 영향도 크게 받는다. 표 데이터에서는 종종 <span dir="rtl">”</span>모델을 바꾸는 것”보다 스케일링, 결측치 처리, 이상치 처리, 범주형 인코딩 전략이 성능에 더 큰 영향을 준다.</p>
<p><strong>하이퍼파라미터 민감 + 튜닝 비용</strong></p>
<p>학습률, 배치 크기, 은닉층 구조, 정규화 강도, 드롭아웃, early stopping 기준 등 튜닝 요소가 많고 민감하다. 동일 데이터에서도 설정에 따라 성능 변동이 커서, 검증 기반의 체계적 튜닝이 필요하다.</p>
<p><strong>해석 가능성이 낮음(설명력 요구에 취약)</strong></p>
<p>트리 기반 모델에 비해 <span dir="rtl">”</span>어떤 변수가 어떤 방식으로 작용했는가”를 직관적으로 설명하기 어렵다. SHAP/퍼뮤테이션 중요도 같은 사후적 설명 기법을 쓸 수는 있지만, 기본적으로는 블랙박스 성격이 강하다.</p>
<p><strong>표 데이터에서 트리 앙상블이 더 강한 경우가 많음</strong></p>
<p>실무적으로 XGBoost/LightGBM 같은 GBDT 계열은 표 데이터에서 매우 강력한 기준선이다. 특히 데이터 규모가 크지 않거나, 규칙 기반 분기/희소 패턴이 강한 문제에서는 트리 앙상블이 MLP를 앞서는 경우가 많다. 따라서 MLP를 적용할 때는 트리 앙상블과의 비교 실험이 사실상 필수다.</p>
<p><strong>재현성과 안정성 이슈(학습 변동)</strong></p>
<p>초기값, 미니배치 샘플링, 하드웨어/라이브러리 환경에 따라 결과가 달라질 수 있다. 따라서 여러 시드로 반복 실험하거나, 검증 성능 분포를 함께 제시하는 것이 바람직하다.</p>
</section>
<section id="언제-mlp가-유리한가-불리한가실무적-가이드" class="level5">
<h5 class="anchored" data-anchor-id="언제-mlp가-유리한가-불리한가실무적-가이드">(3) 언제 MLP가 유리한가 / 불리한가(실무적 가이드)</h5>
<p><strong>MLP가 유리해질 가능성이 큰 경우</strong></p>
<ul>
<li>데이터가 충분히 크고(또는 지속적으로 누적되고) 패턴이 복잡한 경우</li>
<li>범주형 고카디널리티 변수가 많아 임베딩이 효과적인 경우</li>
<li>점예측을 넘어 분위수/분포 등 불확실성 예측이 중요한 경우</li>
<li>다른 딥러닝 모듈(임베딩, 어텐션 등)과 결합해 확장할 계획이 있는 경우</li>
</ul>
<p><strong>MLP가 불리한 경우가 많은 조건</strong></p>
<ul>
<li>데이터가 작고 잡음이 큰 표 데이터(과적합 위험 큼)</li>
<li>결측/이상치/스케일 문제가 큰데 전처리가 충분히 정리되지 않은 경우</li>
<li>강한 규칙 기반 분기(<span dir="rtl">”</span>if-then” 패턴)가 핵심인 문제</li>
<li>설명 가능성이 핵심 요구사항인 경우(단순 모델/트리의 장점이 큼)</li>
</ul>
</section>
</section>
<section id="mlp-변수-중요도" class="level4">
<h4 class="anchored" data-anchor-id="mlp-변수-중요도">6. MLP 변수 중요도</h4>
<section id="회귀계수와-mlp-중요도" class="level5">
<h5 class="anchored" data-anchor-id="회귀계수와-mlp-중요도">(1) 회귀계수와 MLP 중요도</h5>
<p>선형회귀는 각 설명변수에 대해 하나의 회귀계수(β)가 존재하며, <span dir="rtl">”</span>다른 조건이 같을 때 해당 변수가 1 증가하면 예측값이 β만큼 변한다”처럼 고정된(전역적인) 효과로 해석할 수 있다.</p>
<p>반면 MLP 회귀는 여러 은닉층과 비선형 활성화가 연속적으로 결합된 함수이므로, 입력변수의 영향이 입력값의 위치(어떤 관측치인가)에 따라 달라지는 것이 일반적이다. 따라서 선형회귀처럼 <span dir="rtl">”</span>변수별로 하나의 고정된 회귀계수”를 제시하기 어렵고, MLP에서는 보통 <span dir="rtl">”</span>회귀계수” 대신 각 층의 가중치와 편향이 학습된다고 말한다.</p>
<p>다만 이는 <span dir="rtl">’</span>해석이 불가능하다<span dir="rtl">’</span>는 뜻은 아니다. MLP에서 변수의 영향은 보통 다음과 같은 방식으로 평가한다. (1) 특정 관측치 주변에서 변수를 조금 변화시켰을 때 예측이 얼마나 민감하게 변하는지(국소 민감도/기울기), (2) 변수를 무작위로 섞거나 제거했을 때 예측 성능이 얼마나 악화되는지(퍼뮤테이션 중요도), (3) 특정 변수를 변화시키며 평균 예측을 추적하는 방식(부분의존/ICE), (4) SHAP 같은 기여도 분해 방법을 통해 각 변수의 기여를 요약하는 방식 등이다.</p>
<p>정리하면, MLP 회귀는 <span dir="rtl">”</span>변수별 단일 계수” 대신 <span dir="rtl">”</span>입력에 따라 달라지는 비선형 효과”를 학습하므로, 해석은 계수표가 아니라 변수 중요도·민감도·부분의존·기여도 같은 도구로 수행하는 것이 적절하다.</p>
</section>
<section id="mlp-중요도" class="level5">
<h5 class="anchored" data-anchor-id="mlp-중요도">(2) MLP 중요도</h5>
<p>MLP는 선형회귀처럼 입력변수마다 하나의 <span dir="rtl">”</span>회귀계수” <span class="math inline">\(\beta_{j}\)</span>를 추정하는 대신, 각 층의 가중치와 편향 <span class="math inline">\(\{(W_{\ell},b_{\ell})\}_{\ell = 1}^{L}\)</span>을 학습한다. 중요한 점은 이 파라미터들이 원래 입력변수의 직접적 계수가 아니라, 은닉층에서 중간 특징(은닉표현) 을 생성하고 결합하기 위한 변환의 파라미터라는 것이다. 따라서 특정 W 값 하나를 보고 <span dir="rtl">”</span>변수 <span class="math inline">\(x_{j}\)</span>가 1 증가하면 y가 얼마 변한다”처럼 선형회귀식의 계수 해석을 그대로 적용하기는 어렵다.</p>
<p>그렇다고 MLP가 완전한 블랙박스인 것은 아니다. <span dir="rtl">”</span>변수 영향”을 보고 싶다면, (1) 국소적(local) 관점과 (2) 전역적(global) 관점을 구분하여 해석 도구를 선택하는 것이 표준적이다.</p>
<p><strong>국소 효과(민감도): <span dir="rtl">”</span>이 관측치 근처에서 변수 변화가 예측을 얼마나 바꾸나?“</strong></p>
<p>특정 관측치 x에서 변수 <span class="math inline">\(x_{j}\)</span>의 영향은 국소적으로 <span class="math inline">\(\frac{\partial\widehat{y}}{\partial x_{j}}\)</span>로 정의할 수 있다. 이는 <span dir="rtl">”</span>x 주변에서 <span class="math inline">\(x_{j}\)</span>를 아주 조금 변화시켰을 때 예측 <span class="math inline">\(\widehat{y}\)</span>가 얼마나 민감하게 변하는가?“를 수치화한 것이다.</p>
<ul>
<li>해석: <span class="math inline">\(\frac{\partial\widehat{y}}{\partial x_{j}}\)</span>가 크면, 그 관측치 주변에서는 <span class="math inline">\(x_{j}\)</span>가 예측에 큰 영향을 준다.</li>
<li>특징: 값이 관측치마다 달라질 수 있다. 즉 MLP에서는 변수 효과가 입력 공간에서 위치에 따라 달라지는 것이 자연스럽다.</li>
<li>주의: (i) 변수 스케일에 따라 크기가 달라지므로(표준화 여부 중요), (ii) 강한 상관변수/비선형 구간에서는 해석이 단순하지 않을 수 있다.</li>
</ul>
<p>실무적으로는 <span dir="rtl">”</span>기울기 기반 민감도” 외에도, <span class="math inline">\(x_{j}\)</span>를 작은 폭으로 증가/감소시켜 예측 변화량 <span class="math inline">\(\Delta\widehat{y}\)</span>를 보는 국소 perturbation 방식도 같은 철학의 국소 해석이다.</p>
<p><strong>전역 변수 중요도: <span dir="rtl">”</span>전체적으로 어떤 변수가 성능에 더 중요한가?“</strong></p>
<p>전역 관점에서는 <span dir="rtl">”</span>변수 하나가 모델 성능에 얼마나 기여하는가”를 기준으로 중요도를 정의한다. 이때 가장 널리 쓰이는 방법들이 다음이다.</p>
<p>(1) Permutation importance(퍼뮤테이션 중요도)</p>
<p>변수 <span class="math inline">\(x_{j}\)</span>의 값들을 표본 내에서 무작위로 섞어(관계성을 깨뜨린 뒤) 성능이 얼마나 악화되는지 측정한다.</p>
<ul>
<li>핵심 아이디어: <span dir="rtl">”</span>그 변수가 정말 중요하다면, 그 변수의 정보가 망가질 때 성능이 크게 떨어져야 한다.”</li>
<li>지표 예시: RMSE 증가량, <span class="math inline">\(R^{2}\)</span> 감소량 등</li>
<li>장점: 모델 종류에 무관(MLP/트리/선형 모두 가능), 원 변수 단위 해석이 직관적</li>
<li>주의: 상관된 변수들이 있으면 중요도가 분산되거나 과소평가될 수 있고, 시간/순서 구조가 있는 데이터는 섞는 방식 자체가 부적절할 수 있다.</li>
</ul>
<p>(2) SHAP(특징 기여도 분해)</p>
<p>예측값을 <span dir="rtl">”</span>기준값 + 변수별 기여” 형태로 분해하여, 각 관측치에서 어떤 변수가 예측을 올리거나 내렸는지까지 설명한다.</p>
<ul>
<li>전역 요약: 전체 데이터에서 평균적으로 기여가 큰 변수(중요도)</li>
<li>국소 설명: 특정 관측치에서 예측이 왜 그렇게 나왔는지(방향성과 크기)</li>
<li>장점: 방향성(증가/감소)까지 함께 보여주어 설명력이 높음(표 데이터에서 특히 자주 사용)</li>
<li>주의: 계산비용이 크고, 배경 표본 선택에 따라 결과가 달라질 수 있음(특히 모델-불문 SHAP).</li>
</ul>
<p>(3) Partial Dependence(PDP) / ICE</p>
<p>변수 <span class="math inline">\(x_{j}\)</span>를 여러 값으로 바꿔가며 예측이 어떻게 달라지는지 곡선으로 본다.</p>
<ul>
<li>PDP: 다른 변수들은 <span dir="rtl">”</span>평균적으로” 두었을 때 <span class="math inline">\(x_{j}\)</span>의 평균 효과 곡선</li>
<li>ICE: 개별 관측치마다의 효과 곡선(개별선 다발)</li>
<li>장점: 변수의 영향 <span dir="rtl">”</span>형태”(단조/비선형/구간별 변화)를 시각적으로 이해하기 좋음</li>
<li>주의: 강한 상관관계가 있으면 비현실적인 조합의 입력을 만들 수 있어 해석에 주의가 필요.</li>
</ul>
<p><strong>출력층 가중치의 <span dir="rtl">’</span>제한적<span dir="rtl">’</span> 해석</strong></p>
<p>회귀에서 출력층이 선형(항등)이라면, 마지막 층의 가중치는 <span dir="rtl">”</span>마지막 은닉표현(학습된 특징)“에 대한 계수로 볼 수 있다. 즉, 마지막 은닉표현 h(x)가 고정된 특징이라면, 출력은 그 특징의 선형 결합이다.</p>
<p>그러나 h(x) 자체가 입력 x의 비선형 함수로 학습되므로, 이 가중치를 곧바로 <span dir="rtl">”</span>입력변수 계수”로 해석할 수는 없다. 따라서 MLP의 해석은 <span dir="rtl">”</span>계수표”를 만드는 방식보다, (i) 민감도(기울기), (ii) 성능 기반 중요도(퍼뮤테이션), (iii) 기여도 분해(SHAP), (iv) 예측곡선(PDP/ICE) 를 목적에 맞게 사용하여 설명하는 것이 적절하다.</p>
</section>
</section>
<section id="사례분석" class="level4">
<h4 class="anchored" data-anchor-id="사례분석">7. 사례분석</h4>
<p><strong>MPG 데이터</strong></p>
<p>Y(예측 대상): mpg</p>
<p>X(설명변수): displacement, horsepower, weight, acceleration, model_year, origin, cylinders</p>
<section id="mlp-회귀" class="level5">
<h5 class="anchored" data-anchor-id="mlp-회귀">(1) MLP 회귀</h5>
<div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># =========================</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="co"># 0) 라이브러리 / 시드</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="co"># =========================</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.compose <span class="im">import</span> ColumnTransformer</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> OneHotEncoder, StandardScaler</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> mean_squared_error, mean_absolute_error, r2_score</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> tensorflow <span class="im">as</span> tf</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow <span class="im">import</span> keras</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow.keras <span class="im">import</span> layers</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>tf.random.set_seed(<span class="dv">42</span>)</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a><span class="co"># =========================</span></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a><span class="co"># 1) 데이터 로드 및 기본 정리</span></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a><span class="co"># =========================</span></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> sns.load_dataset(<span class="st">"mpg"</span>)</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> df.dropna()  <span class="co"># horsepower 결측 등 제거</span></span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a><span class="co"># 타깃/특징 분리</span></span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> df[<span class="st">"mpg"</span>].astype(<span class="bu">float</span>)</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> df.drop(columns<span class="op">=</span>[<span class="st">"mpg"</span>])</span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a><span class="co"># 'name'은 고유값이 많고 식별자 성격이 강해 일반화에 불리하므로 보통 제외</span></span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> X.drop(columns<span class="op">=</span>[<span class="st">"name"</span>])</span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a><span class="co"># =========================</span></span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a><span class="co"># 2) train/val/test 분할 (70/15/15)</span></span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a><span class="co"># =========================</span></span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a>X_train, X_temp, y_train, y_temp <span class="op">=</span> train_test_split(</span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a>    X, y, test_size<span class="op">=</span><span class="fl">0.30</span>, random_state<span class="op">=</span><span class="dv">42</span></span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a>X_val, X_test, y_val, y_test <span class="op">=</span> train_test_split(</span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a>    X_temp, y_temp, test_size<span class="op">=</span><span class="fl">0.50</span>, random_state<span class="op">=</span><span class="dv">42</span></span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Shapes"</span>)</span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"train:"</span>, X_train.shape, <span class="st">"val:"</span>, X_val.shape, <span class="st">"test:"</span>, X_test.shape)</span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a><span class="co"># =========================</span></span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a><span class="co"># 3) 전처리: 연속형 표준화 + 범주형 원-핫</span></span>
<span id="cb1-49"><a href="#cb1-49" aria-hidden="true" tabindex="-1"></a><span class="co"># =========================</span></span>
<span id="cb1-50"><a href="#cb1-50" aria-hidden="true" tabindex="-1"></a>num_cols <span class="op">=</span> X_train.select_dtypes(include<span class="op">=</span>[<span class="st">"int64"</span>, <span class="st">"float64"</span>]).columns.tolist()</span>
<span id="cb1-51"><a href="#cb1-51" aria-hidden="true" tabindex="-1"></a>cat_cols <span class="op">=</span> X_train.select_dtypes(include<span class="op">=</span>[<span class="st">"object"</span>, <span class="st">"category"</span>]).columns.tolist()</span>
<span id="cb1-52"><a href="#cb1-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-53"><a href="#cb1-53" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> make_ohe():</span>
<span id="cb1-54"><a href="#cb1-54" aria-hidden="true" tabindex="-1"></a>    <span class="co"># sklearn 버전 호환: sparse_output(False) vs sparse(False)</span></span>
<span id="cb1-55"><a href="#cb1-55" aria-hidden="true" tabindex="-1"></a>    <span class="cf">try</span>:</span>
<span id="cb1-56"><a href="#cb1-56" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> OneHotEncoder(handle_unknown<span class="op">=</span><span class="st">"ignore"</span>, sparse_output<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb1-57"><a href="#cb1-57" aria-hidden="true" tabindex="-1"></a>    <span class="cf">except</span> <span class="pp">TypeError</span>:</span>
<span id="cb1-58"><a href="#cb1-58" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> OneHotEncoder(handle_unknown<span class="op">=</span><span class="st">"ignore"</span>, sparse<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb1-59"><a href="#cb1-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-60"><a href="#cb1-60" aria-hidden="true" tabindex="-1"></a>preprocess <span class="op">=</span> ColumnTransformer(</span>
<span id="cb1-61"><a href="#cb1-61" aria-hidden="true" tabindex="-1"></a>    transformers<span class="op">=</span>[</span>
<span id="cb1-62"><a href="#cb1-62" aria-hidden="true" tabindex="-1"></a>        (<span class="st">"num"</span>, StandardScaler(), num_cols),</span>
<span id="cb1-63"><a href="#cb1-63" aria-hidden="true" tabindex="-1"></a>        (<span class="st">"cat"</span>, make_ohe(), cat_cols)</span>
<span id="cb1-64"><a href="#cb1-64" aria-hidden="true" tabindex="-1"></a>    ],</span>
<span id="cb1-65"><a href="#cb1-65" aria-hidden="true" tabindex="-1"></a>    remainder<span class="op">=</span><span class="st">"drop"</span></span>
<span id="cb1-66"><a href="#cb1-66" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb1-67"><a href="#cb1-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-68"><a href="#cb1-68" aria-hidden="true" tabindex="-1"></a>X_train_p <span class="op">=</span> preprocess.fit_transform(X_train)</span>
<span id="cb1-69"><a href="#cb1-69" aria-hidden="true" tabindex="-1"></a>X_val_p   <span class="op">=</span> preprocess.transform(X_val)</span>
<span id="cb1-70"><a href="#cb1-70" aria-hidden="true" tabindex="-1"></a>X_test_p  <span class="op">=</span> preprocess.transform(X_test)</span>
<span id="cb1-71"><a href="#cb1-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-72"><a href="#cb1-72" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Preprocessed shapes"</span>)</span>
<span id="cb1-73"><a href="#cb1-73" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"train_p:"</span>, X_train_p.shape, <span class="st">"val_p:"</span>, X_val_p.shape, <span class="st">"test_p:"</span>, X_test_p.shape)</span>
<span id="cb1-74"><a href="#cb1-74" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-75"><a href="#cb1-75" aria-hidden="true" tabindex="-1"></a><span class="co"># =========================</span></span>
<span id="cb1-76"><a href="#cb1-76" aria-hidden="true" tabindex="-1"></a><span class="co"># 4) MLP 모델 정의(표 데이터용: 과도하게 크지 않게)</span></span>
<span id="cb1-77"><a href="#cb1-77" aria-hidden="true" tabindex="-1"></a><span class="co"># =========================</span></span>
<span id="cb1-78"><a href="#cb1-78" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> build_mlp(input_dim, hidden_units<span class="op">=</span>(<span class="dv">64</span>, <span class="dv">32</span>), dropout<span class="op">=</span><span class="fl">0.2</span>, lr<span class="op">=</span><span class="fl">1e-3</span>, weight_decay<span class="op">=</span><span class="fl">1e-4</span>):</span>
<span id="cb1-79"><a href="#cb1-79" aria-hidden="true" tabindex="-1"></a>    inputs <span class="op">=</span> keras.Input(shape<span class="op">=</span>(input_dim,))</span>
<span id="cb1-80"><a href="#cb1-80" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> inputs</span>
<span id="cb1-81"><a href="#cb1-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-82"><a href="#cb1-82" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> u <span class="kw">in</span> hidden_units:</span>
<span id="cb1-83"><a href="#cb1-83" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> layers.Dense(</span>
<span id="cb1-84"><a href="#cb1-84" aria-hidden="true" tabindex="-1"></a>            u,</span>
<span id="cb1-85"><a href="#cb1-85" aria-hidden="true" tabindex="-1"></a>            activation<span class="op">=</span><span class="st">"relu"</span>,</span>
<span id="cb1-86"><a href="#cb1-86" aria-hidden="true" tabindex="-1"></a>            kernel_regularizer<span class="op">=</span>keras.regularizers.l2(weight_decay)</span>
<span id="cb1-87"><a href="#cb1-87" aria-hidden="true" tabindex="-1"></a>        )(x)</span>
<span id="cb1-88"><a href="#cb1-88" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> layers.Dropout(dropout)(x)</span>
<span id="cb1-89"><a href="#cb1-89" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-90"><a href="#cb1-90" aria-hidden="true" tabindex="-1"></a>    outputs <span class="op">=</span> layers.Dense(<span class="dv">1</span>, activation<span class="op">=</span><span class="st">"linear"</span>)(x)  <span class="co"># 회귀: 선형 출력</span></span>
<span id="cb1-91"><a href="#cb1-91" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> keras.Model(inputs, outputs)</span>
<span id="cb1-92"><a href="#cb1-92" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-93"><a href="#cb1-93" aria-hidden="true" tabindex="-1"></a>    model.<span class="bu">compile</span>(</span>
<span id="cb1-94"><a href="#cb1-94" aria-hidden="true" tabindex="-1"></a>        optimizer<span class="op">=</span>keras.optimizers.Adam(learning_rate<span class="op">=</span>lr),</span>
<span id="cb1-95"><a href="#cb1-95" aria-hidden="true" tabindex="-1"></a>        loss<span class="op">=</span><span class="st">"mse"</span>,</span>
<span id="cb1-96"><a href="#cb1-96" aria-hidden="true" tabindex="-1"></a>        metrics<span class="op">=</span>[keras.metrics.MeanAbsoluteError(name<span class="op">=</span><span class="st">"mae"</span>)]</span>
<span id="cb1-97"><a href="#cb1-97" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb1-98"><a href="#cb1-98" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> model</span>
<span id="cb1-99"><a href="#cb1-99" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-100"><a href="#cb1-100" aria-hidden="true" tabindex="-1"></a><span class="co"># =========================</span></span>
<span id="cb1-101"><a href="#cb1-101" aria-hidden="true" tabindex="-1"></a><span class="co"># 5) (1차) train/val로 학습 + best epoch 찾기</span></span>
<span id="cb1-102"><a href="#cb1-102" aria-hidden="true" tabindex="-1"></a><span class="co"># =========================</span></span>
<span id="cb1-103"><a href="#cb1-103" aria-hidden="true" tabindex="-1"></a>keras.backend.clear_session()</span>
<span id="cb1-104"><a href="#cb1-104" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-105"><a href="#cb1-105" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> build_mlp(input_dim<span class="op">=</span>X_train_p.shape[<span class="dv">1</span>])</span>
<span id="cb1-106"><a href="#cb1-106" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-107"><a href="#cb1-107" aria-hidden="true" tabindex="-1"></a>early <span class="op">=</span> keras.callbacks.EarlyStopping(</span>
<span id="cb1-108"><a href="#cb1-108" aria-hidden="true" tabindex="-1"></a>    monitor<span class="op">=</span><span class="st">"val_loss"</span>, patience<span class="op">=</span><span class="dv">30</span>, restore_best_weights<span class="op">=</span><span class="va">True</span></span>
<span id="cb1-109"><a href="#cb1-109" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb1-110"><a href="#cb1-110" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-111"><a href="#cb1-111" aria-hidden="true" tabindex="-1"></a>history <span class="op">=</span> model.fit(</span>
<span id="cb1-112"><a href="#cb1-112" aria-hidden="true" tabindex="-1"></a>    X_train_p, y_train.values,</span>
<span id="cb1-113"><a href="#cb1-113" aria-hidden="true" tabindex="-1"></a>    validation_data<span class="op">=</span>(X_val_p, y_val.values),</span>
<span id="cb1-114"><a href="#cb1-114" aria-hidden="true" tabindex="-1"></a>    epochs<span class="op">=</span><span class="dv">500</span>,</span>
<span id="cb1-115"><a href="#cb1-115" aria-hidden="true" tabindex="-1"></a>    batch_size<span class="op">=</span><span class="dv">32</span>,</span>
<span id="cb1-116"><a href="#cb1-116" aria-hidden="true" tabindex="-1"></a>    callbacks<span class="op">=</span>[early],</span>
<span id="cb1-117"><a href="#cb1-117" aria-hidden="true" tabindex="-1"></a>    verbose<span class="op">=</span><span class="dv">0</span></span>
<span id="cb1-118"><a href="#cb1-118" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb1-119"><a href="#cb1-119" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-120"><a href="#cb1-120" aria-hidden="true" tabindex="-1"></a>best_epoch <span class="op">=</span> <span class="bu">int</span>(np.argmin(history.history[<span class="st">"val_loss"</span>]) <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb1-121"><a href="#cb1-121" aria-hidden="true" tabindex="-1"></a>best_val_mse <span class="op">=</span> <span class="bu">float</span>(np.<span class="bu">min</span>(history.history[<span class="st">"val_loss"</span>]))</span>
<span id="cb1-122"><a href="#cb1-122" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-123"><a href="#cb1-123" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">(Train/Val) Best epoch:"</span>, best_epoch)</span>
<span id="cb1-124"><a href="#cb1-124" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"(Train/Val) Best val MSE:"</span>, best_val_mse)</span>
<span id="cb1-125"><a href="#cb1-125" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-126"><a href="#cb1-126" aria-hidden="true" tabindex="-1"></a><span class="co"># 학습곡선(선택)</span></span>
<span id="cb1-127"><a href="#cb1-127" aria-hidden="true" tabindex="-1"></a>plt.figure()</span>
<span id="cb1-128"><a href="#cb1-128" aria-hidden="true" tabindex="-1"></a>plt.plot(history.history[<span class="st">"loss"</span>], label<span class="op">=</span><span class="st">"train_loss"</span>)</span>
<span id="cb1-129"><a href="#cb1-129" aria-hidden="true" tabindex="-1"></a>plt.plot(history.history[<span class="st">"val_loss"</span>], label<span class="op">=</span><span class="st">"val_loss"</span>)</span>
<span id="cb1-130"><a href="#cb1-130" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Epoch"</span>)</span>
<span id="cb1-131"><a href="#cb1-131" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"MSE Loss"</span>)</span>
<span id="cb1-132"><a href="#cb1-132" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb1-133"><a href="#cb1-133" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb1-134"><a href="#cb1-134" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-135"><a href="#cb1-135" aria-hidden="true" tabindex="-1"></a><span class="co"># =========================</span></span>
<span id="cb1-136"><a href="#cb1-136" aria-hidden="true" tabindex="-1"></a><span class="co"># 6) (최종) train+val 합쳐서 재학습 (best_epoch만큼)</span></span>
<span id="cb1-137"><a href="#cb1-137" aria-hidden="true" tabindex="-1"></a><span class="co"># =========================</span></span>
<span id="cb1-138"><a href="#cb1-138" aria-hidden="true" tabindex="-1"></a>X_trainval <span class="op">=</span> pd.concat([X_train, X_val], axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb1-139"><a href="#cb1-139" aria-hidden="true" tabindex="-1"></a>y_trainval <span class="op">=</span> pd.concat([y_train, y_val], axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb1-140"><a href="#cb1-140" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-141"><a href="#cb1-141" aria-hidden="true" tabindex="-1"></a><span class="co"># 전처리기는 train+val로 다시 fit (최종모델 기준)</span></span>
<span id="cb1-142"><a href="#cb1-142" aria-hidden="true" tabindex="-1"></a>preprocess_final <span class="op">=</span> ColumnTransformer(</span>
<span id="cb1-143"><a href="#cb1-143" aria-hidden="true" tabindex="-1"></a>    transformers<span class="op">=</span>[</span>
<span id="cb1-144"><a href="#cb1-144" aria-hidden="true" tabindex="-1"></a>        (<span class="st">"num"</span>, StandardScaler(), num_cols),</span>
<span id="cb1-145"><a href="#cb1-145" aria-hidden="true" tabindex="-1"></a>        (<span class="st">"cat"</span>, make_ohe(), cat_cols)</span>
<span id="cb1-146"><a href="#cb1-146" aria-hidden="true" tabindex="-1"></a>    ],</span>
<span id="cb1-147"><a href="#cb1-147" aria-hidden="true" tabindex="-1"></a>    remainder<span class="op">=</span><span class="st">"drop"</span></span>
<span id="cb1-148"><a href="#cb1-148" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb1-149"><a href="#cb1-149" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-150"><a href="#cb1-150" aria-hidden="true" tabindex="-1"></a>X_trainval_p <span class="op">=</span> preprocess_final.fit_transform(X_trainval)</span>
<span id="cb1-151"><a href="#cb1-151" aria-hidden="true" tabindex="-1"></a>X_test_p_fin <span class="op">=</span> preprocess_final.transform(X_test)</span>
<span id="cb1-152"><a href="#cb1-152" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-153"><a href="#cb1-153" aria-hidden="true" tabindex="-1"></a>keras.backend.clear_session()</span>
<span id="cb1-154"><a href="#cb1-154" aria-hidden="true" tabindex="-1"></a>final_model <span class="op">=</span> build_mlp(input_dim<span class="op">=</span>X_trainval_p.shape[<span class="dv">1</span>])</span>
<span id="cb1-155"><a href="#cb1-155" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-156"><a href="#cb1-156" aria-hidden="true" tabindex="-1"></a>final_model.fit(</span>
<span id="cb1-157"><a href="#cb1-157" aria-hidden="true" tabindex="-1"></a>    X_trainval_p, y_trainval.values,</span>
<span id="cb1-158"><a href="#cb1-158" aria-hidden="true" tabindex="-1"></a>    epochs<span class="op">=</span>best_epoch,</span>
<span id="cb1-159"><a href="#cb1-159" aria-hidden="true" tabindex="-1"></a>    batch_size<span class="op">=</span><span class="dv">32</span>,</span>
<span id="cb1-160"><a href="#cb1-160" aria-hidden="true" tabindex="-1"></a>    verbose<span class="op">=</span><span class="dv">0</span></span>
<span id="cb1-161"><a href="#cb1-161" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb1-162"><a href="#cb1-162" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-163"><a href="#cb1-163" aria-hidden="true" tabindex="-1"></a><span class="co"># =========================</span></span>
<span id="cb1-164"><a href="#cb1-164" aria-hidden="true" tabindex="-1"></a><span class="co"># 7) 최종 평가: test 성능(RMSE/MAE/R^2) + 잔차 진단</span></span>
<span id="cb1-165"><a href="#cb1-165" aria-hidden="true" tabindex="-1"></a><span class="co"># =========================</span></span>
<span id="cb1-166"><a href="#cb1-166" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> final_model.predict(X_test_p_fin).ravel()</span>
<span id="cb1-167"><a href="#cb1-167" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-168"><a href="#cb1-168" aria-hidden="true" tabindex="-1"></a>mse  <span class="op">=</span> mean_squared_error(y_test, y_pred)</span>
<span id="cb1-169"><a href="#cb1-169" aria-hidden="true" tabindex="-1"></a>rmse <span class="op">=</span> np.sqrt(mse)</span>
<span id="cb1-170"><a href="#cb1-170" aria-hidden="true" tabindex="-1"></a>mae  <span class="op">=</span> mean_absolute_error(y_test, y_pred)</span>
<span id="cb1-171"><a href="#cb1-171" aria-hidden="true" tabindex="-1"></a>r2   <span class="op">=</span> r2_score(y_test, y_pred)</span>
<span id="cb1-172"><a href="#cb1-172" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-173"><a href="#cb1-173" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Test RMSE = </span><span class="sc">{</span>rmse<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb1-174"><a href="#cb1-174" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Test MAE  = </span><span class="sc">{</span>mae<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb1-175"><a href="#cb1-175" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Test R^2  = </span><span class="sc">{</span>r2<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb1-176"><a href="#cb1-176" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-177"><a href="#cb1-177" aria-hidden="true" tabindex="-1"></a><span class="co"># 큰 잔차(이상치 후보) 10개(선택)</span></span>
<span id="cb1-178"><a href="#cb1-178" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> X_test.copy()</span>
<span id="cb1-179"><a href="#cb1-179" aria-hidden="true" tabindex="-1"></a>result[<span class="st">"mpg"</span>] <span class="op">=</span> y_test.values</span>
<span id="cb1-180"><a href="#cb1-180" aria-hidden="true" tabindex="-1"></a>result[<span class="st">"yhat_mpg"</span>] <span class="op">=</span> y_pred</span>
<span id="cb1-181"><a href="#cb1-181" aria-hidden="true" tabindex="-1"></a>result[<span class="st">"resid"</span>] <span class="op">=</span> result[<span class="st">"mpg"</span>] <span class="op">-</span> result[<span class="st">"yhat_mpg"</span>]</span>
<span id="cb1-182"><a href="#cb1-182" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-183"><a href="#cb1-183" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Top 10 absolute residuals (outlier candidates)"</span>)</span>
<span id="cb1-184"><a href="#cb1-184" aria-hidden="true" tabindex="-1"></a>display(result.loc[result[<span class="st">"resid"</span>].<span class="bu">abs</span>().sort_values(ascending<span class="op">=</span><span class="va">False</span>).head(<span class="dv">10</span>).index])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>본 사례에서는 MPG 데이터(총 392개 관측치)를 학습/검증/테스트(약 70%/15%/15%)로 분할하여 MLP 회귀모형을 적합하였다. 연속형 변수는 표준화하고, 범주형 변수(origin)는 원-핫 인코딩한 뒤(전처리 후 입력 차원 9), 은닉층을 갖는 MLP를 학습하였다.</p>
<p>학습 과정에서는 검증 손실을 기준으로 조기종료를 적용하여 과적합을 억제했고, 그 결과 검증 MSE가 가장 낮아지는 시점이 약 126 epoch 부근에서 관측되었다.</p>
<p>이는 학습을 계속 진행해도 검증 성능이 더 이상 개선되지 않거나 오히려 악화될 수 있음을 의미하므로, 해당 시점의 파라미터를 <span dir="rtl">”</span>일반화 관점에서 가장 적절한 모델”로 선택한 것이다.</p>
<p>최종적으로 선택된 설정을 바탕으로 학습+검증 데이터를 합쳐 다시 학습한 뒤 테스트 데이터에서 성능을 평가한 결과, RMSE는 약 2.25, MAE는 약 1.65, 결정계수는 0.893로 나타났다.</p>
<p>이는 테스트 구간에서 실제 MPG와 예측 MPG의 평균적인 오차 규모가 대략 2–3 mpg 수준이며, 절대오차 기준으로는 평균적으로 약 1.7 mpg 정도의 차이를 보인다는 의미이다. 또한 결정계수는 관측된 MPG 변동의 약 90% 정도를 모델이 설명(예측)하고 있음을 시사하므로, 본 데이터에서 MLP가 비교적 높은 예측력을 확보했음을 알 수 있다.</p>
<p>Shapes <br> train: (274, 7) val: (59, 7) test: (59, 7)</p>
<p>Preprocessed shapes <br> train_p: (274, 9) val_p: (59, 9) test_p: (59, 9) <br> (Train/Val) Best epoch: 127 <br> (Train/Val) Best val MSE: 5.902000427246094</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/prediction_deep_mseloss.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:60.0%"></p>
</figure>
</div>
<p>Test RMSE = 2.302 <br> Test MAE = 1.684 <br> Test R^2 = 0.893</p>
<p>다만 성능 지표가 양호하더라도 일부 관측치에서는 큰 잔차가 발생하였다. 절대잔차 상위 사례를 보면, 예측이 실제보다 과대(잔차 음수) 혹은 과소(잔차 양수)로 크게 벗어나는 관측치가 존재하며, 최대 약 7–8 mpg 수준의 오차도 확인된다.</p>
<p>이러한 관측치는 (i) 데이터 자체의 이상치/측정오류 가능성, (ii) 특정 범주/연도/엔진 조합에서 나타나는 희귀 패턴, (iii) 현재 입력 변수로는 설명되지 않는 숨은 요인(예: 차량 상태, 세부 모델 특성) 등의 이유로 발생할 수 있다. 따라서 모델의 평균 성능뿐 아니라, 잔차가 큰 사례를 추가로 점검하여 데이터 품질(이상치/결측) 문제인지, 혹은 변수 추가/특성공학이 필요한 구조적 한계인지 진단하는 과정이 중요하다.</p>
<p>종합하면, 본 MLP 회귀는 MPG 데이터에서 높은 설명력과 낮은 평균 오차를 보이며 유효한 예측 성능을 달성하였다. 그러나 일부 관측치에서 큰 오차가 발생하므로, (1) 잔차 진단을 통한 이상치 점검, (2) 트리 앙상블(XGBoost/LightGBM)과의 비교를 통한 기준선 확인, (3) 필요 시 분위수 예측(예측구간) 등 불확실성 정보를 함께 제공하는 확장까지 고려하는 것이 바람직하다.</p>
<p>Top 10 absolute residuals (outlier candidates)</p>
<table class="caption-top table">
<colgroup>
<col style="width: 10%">
<col style="width: 12%">
<col style="width: 10%">
<col style="width: 7%">
<col style="width: 12%">
<col style="width: 10%">
<col style="width: 7%">
<col style="width: 5%">
<col style="width: 10%">
<col style="width: 10%">
</colgroup>
<tbody>
<tr class="odd">
<td style="text-align: left;">cylinders</td>
<td style="text-align: left;">displacement</td>
<td style="text-align: left;">horsepower</td>
<td style="text-align: left;">weight</td>
<td style="text-align: left;">acceleration</td>
<td style="text-align: left;">model_year</td>
<td style="text-align: left;">origin</td>
<td style="text-align: left;">mpg</td>
<td style="text-align: left;">yhat_mpg</td>
<td style="text-align: left;">resid</td>
</tr>
<tr class="even">
<td style="text-align: left;">3</td>
<td style="text-align: left;">70.0</td>
<td style="text-align: left;">90.0</td>
<td style="text-align: left;">2124</td>
<td style="text-align: left;">13.5</td>
<td style="text-align: left;">73</td>
<td style="text-align: left;">japan</td>
<td style="text-align: left;">18.0</td>
<td style="text-align: left;">26.876627</td>
<td style="text-align: left;">-8.876627</td>
</tr>
<tr class="odd">
<td style="text-align: left;">4</td>
<td style="text-align: left;">89.0</td>
<td style="text-align: left;">62.0</td>
<td style="text-align: left;">1845</td>
<td style="text-align: left;">15.3</td>
<td style="text-align: left;">80</td>
<td style="text-align: left;">europe</td>
<td style="text-align: left;">29.8</td>
<td style="text-align: left;">36.648178</td>
<td style="text-align: left;">-6.848178</td>
</tr>
<tr class="even">
<td style="text-align: left;">4</td>
<td style="text-align: left;">134.0</td>
<td style="text-align: left;">95.0</td>
<td style="text-align: left;">2515</td>
<td style="text-align: left;">14.8</td>
<td style="text-align: left;">78</td>
<td style="text-align: left;">japan</td>
<td style="text-align: left;">21.1</td>
<td style="text-align: left;">25.824297</td>
<td style="text-align: left;">-4.724297</td>
</tr>
<tr class="odd">
<td style="text-align: left;">4</td>
<td style="text-align: left;">91.0</td>
<td style="text-align: left;">69.0</td>
<td style="text-align: left;">2130</td>
<td style="text-align: left;">14.7</td>
<td style="text-align: left;">79</td>
<td style="text-align: left;">europe</td>
<td style="text-align: left;">37.3</td>
<td style="text-align: left;">33.204357</td>
<td style="text-align: left;">4.095643</td>
</tr>
<tr class="even">
<td style="text-align: left;">4</td>
<td style="text-align: left;">97.0</td>
<td style="text-align: left;">46.0</td>
<td style="text-align: left;">1950</td>
<td style="text-align: left;">21.0</td>
<td style="text-align: left;">73</td>
<td style="text-align: left;">europe</td>
<td style="text-align: left;">26.0</td>
<td style="text-align: left;">29.615395</td>
<td style="text-align: left;">-3.615395</td>
</tr>
<tr class="odd">
<td style="text-align: left;">6</td>
<td style="text-align: left;">173.0</td>
<td style="text-align: left;">115.0</td>
<td style="text-align: left;">2700</td>
<td style="text-align: left;">12.9</td>
<td style="text-align: left;">79</td>
<td style="text-align: left;">usa</td>
<td style="text-align: left;">26.8</td>
<td style="text-align: left;">23.383726</td>
<td style="text-align: left;">3.416274</td>
</tr>
<tr class="even">
<td style="text-align: left;">4</td>
<td style="text-align: left;">79.0</td>
<td style="text-align: left;">67.0</td>
<td style="text-align: left;">1963</td>
<td style="text-align: left;">15.5</td>
<td style="text-align: left;">74</td>
<td style="text-align: left;">europe</td>
<td style="text-align: left;">26.0</td>
<td style="text-align: left;">29.392462</td>
<td style="text-align: left;">-3.392462</td>
</tr>
<tr class="odd">
<td style="text-align: left;">4</td>
<td style="text-align: left;">97.0</td>
<td style="text-align: left;">75.0</td>
<td style="text-align: left;">2265</td>
<td style="text-align: left;">18.2</td>
<td style="text-align: left;">77</td>
<td style="text-align: left;">japan</td>
<td style="text-align: left;">26.0</td>
<td style="text-align: left;">29.010004</td>
<td style="text-align: left;">-3.010004</td>
</tr>
<tr class="even">
<td style="text-align: left;">4</td>
<td style="text-align: left;">114.0</td>
<td style="text-align: left;">91.0</td>
<td style="text-align: left;">2582</td>
<td style="text-align: left;">14.0</td>
<td style="text-align: left;">73</td>
<td style="text-align: left;">europe</td>
<td style="text-align: left;">20.0</td>
<td style="text-align: left;">22.949949</td>
<td style="text-align: left;">-2.949949</td>
</tr>
<tr class="odd">
<td style="text-align: left;">8</td>
<td style="text-align: left;">318.0</td>
<td style="text-align: left;">140.0</td>
<td style="text-align: left;">3735</td>
<td style="text-align: left;">13.2</td>
<td style="text-align: left;">78</td>
<td style="text-align: left;">usa</td>
<td style="text-align: left;">19.4</td>
<td style="text-align: left;">16.783535</td>
<td style="text-align: left;">2.616465</td>
</tr>
</tbody>
</table>
</section>
<section id="permutation-importance-중요도" class="level5">
<h5 class="anchored" data-anchor-id="permutation-importance-중요도">(2) Permutation importance 중요도</h5>
<div class="sourceCode" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co">#Permutation importance 중요도</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> mean_squared_error</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> perm_importance_by_column(</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>    model, preprocess, X_df, y_true,</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>    n_repeats<span class="op">=</span><span class="dv">30</span>, random_state<span class="op">=</span><span class="dv">42</span></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>):</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>    rng <span class="op">=</span> np.random.default_rng(random_state)</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>    <span class="co"># baseline RMSE</span></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>    X_base <span class="op">=</span> preprocess.transform(X_df)</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>    y_base <span class="op">=</span> model.predict(X_base).ravel()</span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>    rmse_base <span class="op">=</span> np.sqrt(mean_squared_error(y_true, y_base))</span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a>    importances <span class="op">=</span> []</span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> col <span class="kw">in</span> X_df.columns:</span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a>        deltas <span class="op">=</span> []</span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(n_repeats):</span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a>            X_perm <span class="op">=</span> X_df.copy()</span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a>            X_perm[col] <span class="op">=</span> rng.permutation(X_perm[col].values)</span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a>            Xp <span class="op">=</span> preprocess.transform(X_perm)</span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a>            yp <span class="op">=</span> model.predict(Xp).ravel()</span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a>            rmse_p <span class="op">=</span> np.sqrt(mean_squared_error(y_true, yp))</span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true" tabindex="-1"></a>            deltas.append(rmse_p <span class="op">-</span> rmse_base)  <span class="co"># RMSE 증가량</span></span>
<span id="cb2-30"><a href="#cb2-30" aria-hidden="true" tabindex="-1"></a>        importances.append((col, <span class="bu">float</span>(np.mean(deltas)), <span class="bu">float</span>(np.std(deltas))))</span>
<span id="cb2-31"><a href="#cb2-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-32"><a href="#cb2-32" aria-hidden="true" tabindex="-1"></a>    out <span class="op">=</span> pd.DataFrame(importances, columns<span class="op">=</span>[<span class="st">"feature"</span>, <span class="st">"rmse_increase_mean"</span>, <span class="st">"rmse_increase_sd"</span>])</span>
<span id="cb2-33"><a href="#cb2-33" aria-hidden="true" tabindex="-1"></a>    out <span class="op">=</span> out.sort_values(<span class="st">"rmse_increase_mean"</span>, ascending<span class="op">=</span><span class="va">False</span>).reset_index(drop<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb2-34"><a href="#cb2-34" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> rmse_base, out</span>
<span id="cb2-35"><a href="#cb2-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-36"><a href="#cb2-36" aria-hidden="true" tabindex="-1"></a>rmse_base, imp_df <span class="op">=</span> perm_importance_by_column(</span>
<span id="cb2-37"><a href="#cb2-37" aria-hidden="true" tabindex="-1"></a>    final_model, preprocess_final, X_test, y_test.values,</span>
<span id="cb2-38"><a href="#cb2-38" aria-hidden="true" tabindex="-1"></a>    n_repeats<span class="op">=</span><span class="dv">50</span>, random_state<span class="op">=</span><span class="dv">42</span></span>
<span id="cb2-39"><a href="#cb2-39" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb2-40"><a href="#cb2-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-41"><a href="#cb2-41" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Baseline Test RMSE:"</span>, rmse_base)</span>
<span id="cb2-42"><a href="#cb2-42" aria-hidden="true" tabindex="-1"></a>imp_df.head(<span class="dv">10</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Permutation importance는 <span dir="rtl">”</span>특정 변수를 무작위로 섞어(permutation) 그 변수에 담긴 정보(입력–출력 관계)를 일부러 깨뜨렸을 때, 모델 성능이 얼마나 악화되는가”로 변수 중요도를 정의한다. 즉 해당 변수가 예측에 많이 기여할수록, 그 변수를 섞는 순간 예측이 흔들리면서 오차(RMSE)가 크게 증가한다. 본 분석에서는 테스트셋에서의 기준 RMSE가 약 2.254였고, 각 변수를 한 번에 하나씩 섞었을 때 RMSE가 평균적으로 얼마나 증가하는지를 rmse_increase_mean으로 보고, 그 변동성을 rmse_increase_sd로 함께 제시하였다.</p>
<p><strong>결과 요약(중요도 순서)</strong></p>
<p><strong>model_year (RMSE +2.441)</strong>: model_year를 섞으면 RMSE가 약 2.44 증가하여, 기준 RMSE(2.254) 대비 오차가 거의 두 배 수준으로 악화된다. 이는 MLP가 mpg를 예측할 때 연식(모델 연도) 정보를 매우 핵심적으로 활용하고 있음을 의미한다. 연식은 배출규제·기술 발전·차량 경량화 등과 연관되어 연비와 구조적으로 연결되므로, 모델이 이를 강한 신호로 인식하는 결과로 해석할 수 있다.</p>
<p><strong>weight (RMSE +1.963), horsepower (RMSE +1.949)</strong>: 두 변수는 섞었을 때 RMSE가 약 2.0 가까이 증가하여, model_year 다음으로 매우 큰 중요도를 보인다. 이는 차량 중량과 출력(마력) 이 연비를 결정하는 물리적 요인임을 반영한다. 즉 MLP는 <span dir="rtl">”</span>무거울수록 연비가 낮아지는 경향”, <span dir="rtl">”</span>출력이 클수록 연비가 낮아질 수 있는 경향” 같은 패턴을 학습하고 있으며, 이 두 변수가 예측 성능을 지탱하는 핵심 축으로 작동한다.</p>
<p><strong>cylinders (RMSE +0.528), displacement (RMSE +0.452), acceleration (RMSE +0.436):</strong> 이들 변수는 중요도는 있으나 상위 3개(model_year, weight, horsepower)에 비해 영향이 작다. 해석상으로는 (i) 이들이 연비와 관련이 있으나, (ii) 중량/마력/연식 등과 강하게 연관되어 정보가 일부 중복되기 때문에, 하나를 섞더라도 나머지 변수들이 일정 부분 대체하여 성능 저하가 상대적으로 제한되는 상황으로 이해할 수 있다. 즉 <span dir="rtl">”</span>영향이 없다”기보다, 추가적으로 제공하는 독립 정보가 상대적으로 적다고 볼 수 있다.</p>
<p><strong>origin (RMSE +0.072)</strong>: origin을 섞었을 때 RMSE 증가가 매우 작아, 이 모델에서는 origin이 예측 성능에 기여하는 정도가 제한적임을 시사한다. 다만 이는 <span dir="rtl">”</span>origin이 연비와 무관하다”는 뜻이 아니다. origin의 효과가 weight·horsepower·model_year 등 다른 변수들에 이미 반영되어 있거나, 원-핫 인코딩된 범주 효과가 데이터 분할/표본 크기에서 상대적으로 약하게 드러났을 수 있다. 즉 퍼뮤테이션 중요도는 <span dir="rtl">”</span>관계 존재 여부”가 아니라 현재 모델이 실제 예측에서 얼마나 의존하고 있는가를 보여준다.</p>
<p><strong>rmse_increase_sd(표준편차)의 의미</strong></p>
<p>각 변수에 대해 여러 번 섞기를 반복했을 때 RMSE 증가량이 얼마나 흔들리는지를 나타낸다. 예를 들어 model_year, weight, horsepower의 표준편차는 약 0.28~0.31 수준으로, <span dir="rtl">”</span>대체로 중요하지만 섞는 방식(표본 재배열)에 따라 증가량이 조금 달라진다”는 의미이다. 반면 origin은 평균 증가량도 작고 변동도 작아, 중요도가 낮다는 결론이 비교적 안정적이다.</p>
<p><strong>해석 시 주의점(강의에서 꼭 언급할 사항)</strong></p>
<p>중요도는 <span dir="rtl">’</span>인과<span dir="rtl">’</span>가 아니라 <span dir="rtl">’</span>의존도<span dir="rtl">’</span>이다. 퍼뮤테이션 중요도는 <span dir="rtl">”</span>그 변수를 없앴을 때 성능이 얼마나 떨어지는가”이므로, 인과관계(원인→결과)를 직접 말해주지 않는다.</p>
<p>상관변수(중복 정보)가 있으면 중요도가 분산/과소평가될 수 있다. 예컨대 cylinders와 displacement는 weight/horsepower와 연관이 크기 때문에, 개별 중요도가 작게 나와도 실제로는 연비와 관련이 있을 수 있다.</p>
<p>데이터 구조를 깨는 섞기는 부적절할 수 있다(시계열/패널 등). 본 MPG는 i.i.d. 표본 가정이 비교적 자연스러운 표 데이터라 적용이 무난하다.</p>
<p><strong>결론</strong></p>
<p>본 MLP는 MPG 예측에서 연식(model_year), 중량(weight), 마력(horsepower) 에 가장 크게 의존하며, 나머지 변수들은 상위 변수들과의 정보 중복으로 인해 추가 기여가 상대적으로 작게 나타났다.</p>
<p>Baseline Test RMSE: 2.2540943739430226</p>
<table class="caption-top table">
<colgroup>
<col style="width: 21%">
<col style="width: 40%">
<col style="width: 36%">
</colgroup>
<tbody>
<tr class="odd">
<td style="text-align: left;">feature</td>
<td style="text-align: left;">rmse_increase_mean</td>
<td style="text-align: left;">rmse_increase_sd</td>
</tr>
<tr class="even">
<td style="text-align: left;">model_year</td>
<td style="text-align: left;">2.440691</td>
<td style="text-align: left;">0.310676</td>
</tr>
<tr class="odd">
<td style="text-align: left;">weight</td>
<td style="text-align: left;">1.963253</td>
<td style="text-align: left;">0.312264</td>
</tr>
<tr class="even">
<td style="text-align: left;">horsepower</td>
<td style="text-align: left;">1.948955</td>
<td style="text-align: left;">0.284075</td>
</tr>
<tr class="odd">
<td style="text-align: left;">cylinders</td>
<td style="text-align: left;">0.528403</td>
<td style="text-align: left;">0.115467</td>
</tr>
<tr class="even">
<td style="text-align: left;">displacement</td>
<td style="text-align: left;">0.451576</td>
<td style="text-align: left;">0.207818</td>
</tr>
<tr class="odd">
<td style="text-align: left;">acceleration</td>
<td style="text-align: left;">0.435880</td>
<td style="text-align: left;">0.094955</td>
</tr>
<tr class="even">
<td style="text-align: left;">origin</td>
<td style="text-align: left;">0.071696</td>
<td style="text-align: left;">0.064527</td>
</tr>
</tbody>
</table>
</section>
<section id="shap-변수-기여도전역-관측치별" class="level5">
<h5 class="anchored" data-anchor-id="shap-변수-기여도전역-관측치별">(3) SHAP: 변수 기여도(전역 + 관측치별)</h5>
<div class="sourceCode" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># SHAP: 변수 기여도(전역 + 관측치별)</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="co"># 설치(최초 1회)</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>pip <span class="op">-</span>q install shap</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> shap</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a><span class="co"># (1) 전처리된 배열 준비: train+val fit된 preprocess_final을 사용</span></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>X_trainval <span class="op">=</span> pd.concat([X_train, X_val], axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>X_trainval_p <span class="op">=</span> preprocess_final.transform(X_trainval)</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>X_test_p_fin <span class="op">=</span> preprocess_final.transform(X_test)</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a><span class="co"># (2) SHAP 계산용 background / 설명 대상 샘플(속도 위해 일부만)</span></span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>rng <span class="op">=</span> np.random.default_rng(<span class="dv">42</span>)</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>bg_size <span class="op">=</span> <span class="bu">min</span>(<span class="dv">100</span>, X_trainval_p.shape[<span class="dv">0</span>])</span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>explain_size <span class="op">=</span> <span class="bu">min</span>(<span class="dv">200</span>, X_test_p_fin.shape[<span class="dv">0</span>])</span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a>bg_idx <span class="op">=</span> rng.choice(X_trainval_p.shape[<span class="dv">0</span>], size<span class="op">=</span>bg_size, replace<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a>ex_idx <span class="op">=</span> rng.choice(X_test_p_fin.shape[<span class="dv">0</span>], size<span class="op">=</span>explain_size, replace<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a>X_bg <span class="op">=</span> X_trainval_p[bg_idx]</span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a>X_ex <span class="op">=</span> X_test_p_fin[ex_idx]</span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-26"><a href="#cb3-26" aria-hidden="true" tabindex="-1"></a><span class="co"># (3) 모델 예측 함수(2D -&gt; 1D)</span></span>
<span id="cb3-27"><a href="#cb3-27" aria-hidden="true" tabindex="-1"></a>f <span class="op">=</span> <span class="kw">lambda</span> X: final_model.predict(X).ravel()</span>
<span id="cb3-28"><a href="#cb3-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-29"><a href="#cb3-29" aria-hidden="true" tabindex="-1"></a><span class="co"># (4) Explainer (기본적으로 permutation 기반으로 안정적)</span></span>
<span id="cb3-30"><a href="#cb3-30" aria-hidden="true" tabindex="-1"></a>explainer <span class="op">=</span> shap.Explainer(f, X_bg)</span>
<span id="cb3-31"><a href="#cb3-31" aria-hidden="true" tabindex="-1"></a>shap_values <span class="op">=</span> explainer(X_ex)  <span class="co"># shap_values.values: (n, p)</span></span>
<span id="cb3-32"><a href="#cb3-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-33"><a href="#cb3-33" aria-hidden="true" tabindex="-1"></a><span class="co"># (5) feature 이름 추출(가능하면)</span></span>
<span id="cb3-34"><a href="#cb3-34" aria-hidden="true" tabindex="-1"></a><span class="cf">try</span>:</span>
<span id="cb3-35"><a href="#cb3-35" aria-hidden="true" tabindex="-1"></a>    feature_names <span class="op">=</span> preprocess_final.get_feature_names_out()</span>
<span id="cb3-36"><a href="#cb3-36" aria-hidden="true" tabindex="-1"></a><span class="cf">except</span> <span class="pp">Exception</span>:</span>
<span id="cb3-37"><a href="#cb3-37" aria-hidden="true" tabindex="-1"></a>    feature_names <span class="op">=</span> [<span class="ss">f"x</span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">"</span> <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(X_ex.shape[<span class="dv">1</span>])]</span>
<span id="cb3-38"><a href="#cb3-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-39"><a href="#cb3-39" aria-hidden="true" tabindex="-1"></a><span class="co"># (6) 전역 중요도(bar)</span></span>
<span id="cb3-40"><a href="#cb3-40" aria-hidden="true" tabindex="-1"></a>plt.figure()</span>
<span id="cb3-41"><a href="#cb3-41" aria-hidden="true" tabindex="-1"></a>shap.summary_plot(shap_values.values, X_ex, feature_names<span class="op">=</span>feature_names, plot_type<span class="op">=</span><span class="st">"bar"</span>, show<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb3-42"><a href="#cb3-42" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb3-43"><a href="#cb3-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-44"><a href="#cb3-44" aria-hidden="true" tabindex="-1"></a><span class="co"># (7) 전역 분포(beeswarm: 방향성까지)</span></span>
<span id="cb3-45"><a href="#cb3-45" aria-hidden="true" tabindex="-1"></a>plt.figure()</span>
<span id="cb3-46"><a href="#cb3-46" aria-hidden="true" tabindex="-1"></a>shap.summary_plot(shap_values.values, X_ex, feature_names<span class="op">=</span>feature_names, show<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb3-47"><a href="#cb3-47" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb3-48"><a href="#cb3-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-49"><a href="#cb3-49" aria-hidden="true" tabindex="-1"></a><span class="co"># (8) 특정 관측치 1개(로컬 설명)</span></span>
<span id="cb3-50"><a href="#cb3-50" aria-hidden="true" tabindex="-1"></a>i <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb3-51"><a href="#cb3-51" aria-hidden="true" tabindex="-1"></a>plt.figure()</span>
<span id="cb3-52"><a href="#cb3-52" aria-hidden="true" tabindex="-1"></a>shap.plots.waterfall(shap_values[i], max_display<span class="op">=</span><span class="dv">12</span>, show<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb3-53"><a href="#cb3-53" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><strong>전역 해석: SHAP bar plot(평균 절대 기여도)</strong></p>
<p>SHAP 요약 막대그래프는 각 변수가 전체 표본에서 평균적으로 예측값을 얼마나 크게 흔드는지(mean |SHAP|)를 보여준다. 값이 클수록 <span dir="rtl">”</span>그 변수를 바꿔도 예측이 많이 달라진다”는 뜻이며, 중요도(전역 영향력)로 해석한다.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/prediction_deep_shapbar.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:80.0%"></p>
</figure>
</div>
<p>origin(특히 USA 더미)</p>
<ul>
<li>모델이 MPG를 예측할 때 제조국 범주 자체가 예측을 크게 이동시키는 경향이 있음을 시사한다.</li>
<li>주의: 원-핫 인코딩에서는 origin_usa, origin_europe, origin_japan이 서로 배타적으로 작동하므로, <span dir="rtl">”</span>USA가 중요하다”는 말은 곧 <span dir="rtl">”</span>origin이라는 범주형 효과가 크다”로 읽는 것이 더 자연스럽다(기준범주 설정/더미 구성에 따라 막대 순서는 달라질 수 있음).</li>
</ul>
<p>weight, horsepower, model_year</p>
<ul>
<li>물리적으로도 타당한 핵심 요인들이다.</li>
<li>SHAP 관점에서는 이 변수들이 예측을 가장 크게 좌우하는 연속형 신호로 작동하며, 모델이 이들 값의 변화에 민감하다는 뜻이다.</li>
</ul>
<p>displacement, acceleration, cylinders</p>
<ul>
<li>중요도는 상대적으로 낮지만 여전히 기여가 존재한다.</li>
<li>특히 cylinders와 displacement는 weight/horsepower와 상관이 큰 편이라, 개별 중요도가 낮게 나오더라도 <span dir="rtl">”</span>정보가 중복되어 분산된 결과”일 수 있다(중요도 0과는 다름).</li>
</ul>
<p>정리하면, 이 MLP는 (i) origin에 따른 범주 효과 + (ii) weight/horsepower/model_year의 연속형 구조를 함께 이용해 MPG를 예측하고 있다고 해석할 수 있다.</p>
<p><strong>전역 해석: SHAP beeswarm(방향성까지)</strong></p>
<p>beeswarm은 각 변수가 <span dir="rtl">”</span>중요한가”뿐 아니라, 값이 커질 때 예측을 올리는지/내리는지 방향도 보여준다. 해석 원칙은 다음과 같다.</p>
<ul>
<li>점 하나 = 관측치 하나</li>
<li>x축(좌/우): 예측에 더해진 기여(음수면 mpg를 낮추는 방향, 양수면 높이는 방향)</li>
<li>색(낮음/높음): 해당 변수값의 크기</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/prediction_deep_shapbeeswarm.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:80.0%"></p>
</figure>
</div>
<p>일반적으로 MPG 데이터에서는 다음 패턴이 기대되며(모형이 학습했다면 beeswarm에서 잘 드러남):</p>
<ul>
<li>weight가 높을수록(High) → 예측 MPG를 낮추는 방향(음의 SHAP)으로 이동</li>
<li>horsepower가 높을수록(High) → 예측 MPG를 낮추는 방향</li>
<li>model_year가 높을수록(High, 최신 연식) → 예측 MPG를 높이는 방향(양의 SHAP)</li>
</ul>
<p>범주형(origin)은 각 더미가 <span dir="rtl">”</span>해당 범주일 때” 예측을 어느 방향으로 이동시키는지가 나타난다. 예: origin_usa=1일 때 점들이 주로 음쪽에 있으면 <span dir="rtl">”</span>미국차 범주가 MPG를 낮추는 방향의 편향을 갖는다”로 읽는다(반대면 높이는 방향).</p>
<p><strong>관측치별 해석: SHAP waterfall(로컬 설명)</strong></p>
<p>waterfall은 특정 1개 관측치의 예측값이 어떻게 만들어졌는지를 분해해서 보여준다. 읽는 방법은 다음과 같다.</p>
<ul>
<li>기준값(대개 전체 평균 예측 또는 base value)에서 출발</li>
<li>각 변수의 SHAP 값이 더해지거나(+) 빼지면서(-) 최종 예측값으로 이동</li>
<li>막대가 클수록 그 관측치에서 해당 변수가 <span dir="rtl">”</span>결정적으로” 작동한 것이다.</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/prediction_deep_shapwaterfall.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:80.0%"></p>
</figure>
</div>
<p>따라서 로컬 해석은 <span dir="rtl">”</span>이 차량의 mpg 예측이 왜 이렇게 나왔는가?“에 대한 서술로 바꾸면 된다. 예시는 이런 스타일이다.</p>
<p><span dir="rtl">”</span>이 관측치는 weight가 큰 편이라 mpg를 낮추는 방향으로 크게 작용했고, model_year가 상대적으로 높아 mpg를 올리는 방향으로 일부 상쇄되었으며, origin이 USA라 추가로 mpg를 낮추는 방향의 기여가 더해져 최종 예측이 낮아졌다.”</p>
<p><strong>해석 시 주의</strong></p>
<ul>
<li>SHAP은 예측 기여도(모델 내부에서의 영향)이지, 인과관계를 보장하지 않는다.</li>
<li>원-핫 범주형은 더미가 여러 개로 쪼개지므로, 개별 더미 해석보다 <span dir="rtl">”</span>origin 전체 효과”로 요약하는 편이 안정적이다.</li>
<li>상관이 큰 변수들(예: cylinders–displacement–horsepower–weight)은 기여도가 서로 분산될 수 있다.</li>
</ul>
<p><strong>결론</strong></p>
<p>이 MLP의 SHAP 결과는 MPG 예측이 연식(model_year), 중량(weight), 출력(horsepower) 같은 핵심 물리 변수와 제조국(origin) 범주 효과에 의해 주로 결정되며, 개별 차량 수준에서는 이 변수들의 조합이 예측값을 기준점에서 위/아래로 이동시키는 방식으로 설명될 수 있음을 보여준다.</p>
</section>
<section id="범주형-변수-pdp-예-범주형-origin" class="level5">
<h5 class="anchored" data-anchor-id="범주형-변수-pdp-예-범주형-origin">(4) 범주형 변수 PDP (예: 범주형 origin)</h5>
<div class="sourceCode" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> pdp_categorical_annot(model, preprocess, X_df, feature, digits<span class="op">=</span><span class="dv">2</span>):</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>    cats <span class="op">=</span> <span class="bu">sorted</span>(X_df[feature].dropna().unique().tolist())</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>    means <span class="op">=</span> []</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> c <span class="kw">in</span> cats:</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>        X_tmp <span class="op">=</span> X_df.copy()</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>        X_tmp[feature] <span class="op">=</span> c</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>        Xp <span class="op">=</span> preprocess.transform(X_tmp)</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>        yp <span class="op">=</span> model.predict(Xp).ravel()</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>        means.append(yp.mean())</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>    plt.figure()</span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a>    bars <span class="op">=</span> plt.bar([<span class="bu">str</span>(c) <span class="cf">for</span> c <span class="kw">in</span> cats], means)</span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>    plt.xlabel(feature)</span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>    plt.ylabel(<span class="st">"Average predicted mpg"</span>)</span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 숫자 라벨</span></span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> b, m <span class="kw">in</span> <span class="bu">zip</span>(bars, means):</span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a>        plt.text(b.get_x() <span class="op">+</span> b.get_width()<span class="op">/</span><span class="dv">2</span>, b.get_height(),</span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a>                 <span class="ss">f"</span><span class="sc">{</span>m<span class="sc">:.</span>{digits}f<span class="sc">}</span><span class="ss">"</span>, ha<span class="op">=</span><span class="st">"center"</span>, va<span class="op">=</span><span class="st">"bottom"</span>)</span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a>    plt.show()</span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a>pdp_categorical_annot(final_model, preprocess_final, X_test, feature<span class="op">=</span><span class="st">"origin"</span>, digits<span class="op">=</span><span class="dv">2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>이번 범주형 PDP는 <span dir="rtl">”</span>다른 변수들은 그대로 둔 채, origin만 특정 범주로 고정했을 때 평균 예측 mpg가 얼마나 달라지는가”를 본 결과입니다. 막대 위 숫자는 그때의 평균 예측 mpg이다.</p>
<p>europe: 22.74, japan: 22.98, usa: 22.65으로 평균 예측값은 japan &gt; europe &gt; usa 순으로 나타났다.</p>
<p>범주 간 최대 차이는 japan − usa = 22.98 − 22.65 = 0.33 mpg, 다른 쌍도 japan − europe = 0.24 mpg, europe − usa = 0.09 mp로, 전체적으로 0.1~0.3 mpg 수준의 작은 차이이다. → 결론적으로, 이 모델에서는 origin을 바꿔도 평균 예측 mpg가 크게 이동하지 않다.<br>
즉, origin의 <span dir="rtl">”</span>단독 평균 효과”는 약하게 학습된 것으로 해석할 수 있습니다.</p>
<p>origin을 europe/japan/usa로 고정했을 때 평균 예측 mpg는 각각 22.74, 22.98, 22.65로 나타났으며, 범주 간 최대 차이는 0.33 mpg로 작아 origin의 단독 평균 효과는 제한적이었다. 이는 MLP가 origin 자체보다는 중량·마력·연식 등 연속형 변수에 더 크게 의존해 예측하는 경향을 시사한다.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/prediction_deep_pdpbar.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:60.0%"></p>
</figure>
</div>
</section>
<section id="범주형-변수-pdp-예-측정형-weight" class="level5">
<h5 class="anchored" data-anchor-id="범주형-변수-pdp-예-측정형-weight">(5) 범주형 변수 PDP (예: 측정형 weight)</h5>
<div class="sourceCode" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 연속형 변수 PDP (예: weight)</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> pdp_ice_continuous(model, preprocess, X_df, feature,</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>                       grid_size<span class="op">=</span><span class="dv">25</span>, ice_n<span class="op">=</span><span class="dv">80</span>, use_ice<span class="op">=</span><span class="va">True</span>, random_state<span class="op">=</span><span class="dv">42</span>):</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>    rng <span class="op">=</span> np.random.default_rng(random_state)</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># ICE용 샘플(너무 많으면 선이 과해짐)</span></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>    X_s <span class="op">=</span> X_df.copy()</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="bu">len</span>(X_s) <span class="op">&gt;</span> ice_n:</span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>        X_s <span class="op">=</span> X_s.iloc[rng.choice(<span class="bu">len</span>(X_s), size<span class="op">=</span>ice_n, replace<span class="op">=</span><span class="va">False</span>)]</span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 극단값 영향 줄이려고 5%~95% 구간에서 그리드 생성(권장)</span></span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a>    vmin, vmax <span class="op">=</span> np.quantile(X_df[feature].dropna().values, [<span class="fl">0.05</span>, <span class="fl">0.95</span>])</span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a>    grid <span class="op">=</span> np.linspace(vmin, vmax, grid_size)</span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a>    preds <span class="op">=</span> []</span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> v <span class="kw">in</span> grid:</span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a>        X_tmp <span class="op">=</span> X_s.copy()</span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a>        X_tmp[feature] <span class="op">=</span> v</span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a>        Xp <span class="op">=</span> preprocess.transform(X_tmp)</span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a>        yp <span class="op">=</span> model.predict(Xp).ravel()</span>
<span id="cb5-25"><a href="#cb5-25" aria-hidden="true" tabindex="-1"></a>        preds.append(yp)</span>
<span id="cb5-26"><a href="#cb5-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-27"><a href="#cb5-27" aria-hidden="true" tabindex="-1"></a>    preds <span class="op">=</span> np.array(preds)          <span class="co"># (grid_size, ice_n)</span></span>
<span id="cb5-28"><a href="#cb5-28" aria-hidden="true" tabindex="-1"></a>    pdp <span class="op">=</span> preds.mean(axis<span class="op">=</span><span class="dv">1</span>)         <span class="co"># (grid_size,)</span></span>
<span id="cb5-29"><a href="#cb5-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-30"><a href="#cb5-30" aria-hidden="true" tabindex="-1"></a>    plt.figure()</span>
<span id="cb5-31"><a href="#cb5-31" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> use_ice:</span>
<span id="cb5-32"><a href="#cb5-32" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(preds.shape[<span class="dv">1</span>]):</span>
<span id="cb5-33"><a href="#cb5-33" aria-hidden="true" tabindex="-1"></a>            plt.plot(grid, preds[:, j], alpha<span class="op">=</span><span class="fl">0.15</span>)  <span class="co"># ICE</span></span>
<span id="cb5-34"><a href="#cb5-34" aria-hidden="true" tabindex="-1"></a>    plt.plot(grid, pdp, linewidth<span class="op">=</span><span class="dv">2</span>)                 <span class="co"># PDP</span></span>
<span id="cb5-35"><a href="#cb5-35" aria-hidden="true" tabindex="-1"></a>    plt.xlabel(feature)</span>
<span id="cb5-36"><a href="#cb5-36" aria-hidden="true" tabindex="-1"></a>    plt.ylabel(<span class="st">"Predicted mpg"</span>)</span>
<span id="cb5-37"><a href="#cb5-37" aria-hidden="true" tabindex="-1"></a>    plt.title(<span class="ss">f"PDP/ICE for </span><span class="sc">{</span>feature<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb5-38"><a href="#cb5-38" aria-hidden="true" tabindex="-1"></a>    plt.show()</span>
<span id="cb5-39"><a href="#cb5-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-40"><a href="#cb5-40" aria-hidden="true" tabindex="-1"></a><span class="co"># 실행: weight에 대한 PDP(+ICE)</span></span>
<span id="cb5-41"><a href="#cb5-41" aria-hidden="true" tabindex="-1"></a>pdp_ice_continuous(final_model, preprocess_final, X_test, feature<span class="op">=</span><span class="st">"weight"</span>, use_ice<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>weight는 범주형이 아니라 연속형(측정형) 변수이므로 <span dir="rtl">’</span>범주형 PDP<span dir="rtl">’</span>가 아니라 연속형 PDP(및 ICE)로 해석한다.</p>
<p><strong>그래프가 의미하는 것</strong></p>
<p>PDP(굵은 선): 다른 변수들은 데이터 분포대로 두고, weight만 일정 값으로 고정했을 때의 평균 예측 mpg를 나타낸다.<br>
→ <span dir="rtl">”</span>weight가 변하면 평균적으로 예측 mpg가 어떻게 변하는가?“에 해당한다.</p>
<p>ICE(얇은 선들): 동일한 조작을 각 관측치별로 적용한 결과로, 차량마다 weight 효과가 얼마나 다른지(이질성)를 보여준다.<br>
→ 선들이 서로 많이 벌어지면, weight 효과가 다른 변수(예: horsepower, model_year, origin)와 상호작용을 가진다는 신호로 해석할 수 있다.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/prediction_deep_pdpweight.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:60.0%"></p>
</figure>
</div>
<p><strong>관측된 패턴(핵심 해석)</strong></p>
<p>그래프에서 PDP가 전반적으로 하강(감소)하는 형태라면, 이는 차량 중량(weight)이 증가할수록 예측 mpg가 감소한다는 것을 의미한다.<br>
이는 물리적 직관과도 일치하며(무거울수록 연비 불리), MLP가 표 데이터에서도 이러한 구조적 관계를 학습했음을 보여준다.</p>
<p>또한 PDP 곡선이 직선이 아니라 구간별 기울기가 달라지는 완만한 곡선이라면, 저중량 구간에서는 weight 변화가 mpg를 크게 흔들지만, 고중량 구간에서는 감소 폭이 다소 완만해지는 등 비선형 관계(구간별 민감도 차이)를 MLP가 반영하고 있다는 뜻이다.</p>
<p><strong>ICE 선들의 퍼짐이 말해주는 것</strong></p>
<p>ICE 선들이 한 줄로 겹치지 않고 여러 개의 밴드처럼 퍼져 있으면, 이는 동일한 weight라도 엔진 출력(horsepower), 실린더(cylinders), 연식(model_year), 제조국(origin) 같은 다른 변수 조합에 따라 mpg 수준이 달라지고, weight의 효과도 관측치에 따라 달라질 수 있음을 보여준다.</p>
<p>즉, 이 그림은 <span dir="rtl">”</span>weight가 중요하다”를 말하기보다, weight가 예측에 미치는 평균적 방향(감소)과, 그 효과가 관측치마다 달라지는 정도(이질성)를 동시에 보여준다.</p>
<p><strong>해석 시 주의점</strong></p>
<p>PDP는 모든 관측치에서 weight를 강제로 바꾸는 <span dir="rtl">’</span>가상 조작<span dir="rtl">’</span>을 하므로, 실제 데이터에서 드문 조합(예: 매우 무거운데도 다른 특성이 경량차 조합)을 만들 수 있다. 따라서 PDP는 인과가 아니라 <span dir="rtl">”</span>모델이 학습한 평균적 반응”으로 해석하는 것이 안전하다. 또한 weight와 다른 변수들이 상관되어 있으면, PDP가 <span dir="rtl">”</span>순수한 단독 효과”라기보다 상관 구조를 포함한 평균 효과로 나타날 수 있다.</p>
<p><strong>요약</strong></p>
<p>weight의 PDP/ICE는 <span dir="rtl">”</span>중량이 증가할수록 평균 예측 mpg가 감소”하는 뚜렷한 구조를 보여주며, ICE의 퍼짐은 차량 특성 조합에 따라 weight 효과가 달라지는 상호작용/이질성이 존재함을 시사한다.</p>
</section>
<section id="quantile-예측" class="level5">
<h5 class="anchored" data-anchor-id="quantile-예측">(6) Quantile 예측</h5>
<div class="sourceCode" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co"># =========================================</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Quantile Regression (MLP): q0.1 / q0.5 / q0.9 동시 예측 + 예측구간</span></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="co"># - train/val로 best epoch 선택 -&gt; train+val로 재학습 -&gt; test 평가</span></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a><span class="co"># =========================================</span></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.compose <span class="im">import</span> ColumnTransformer</span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> OneHotEncoder, StandardScaler</span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> mean_squared_error, mean_absolute_error</span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> tensorflow <span class="im">as</span> tf</span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow <span class="im">import</span> keras</span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow.keras <span class="im">import</span> layers</span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a><span class="co"># -------------------------</span></span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a><span class="co"># 0) 시드</span></span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a><span class="co"># -------------------------</span></span>
<span id="cb6-22"><a href="#cb6-22" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb6-23"><a href="#cb6-23" aria-hidden="true" tabindex="-1"></a>tf.random.set_seed(<span class="dv">42</span>)</span>
<span id="cb6-24"><a href="#cb6-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-25"><a href="#cb6-25" aria-hidden="true" tabindex="-1"></a><span class="co"># -------------------------</span></span>
<span id="cb6-26"><a href="#cb6-26" aria-hidden="true" tabindex="-1"></a><span class="co"># 1) 데이터 준비 (이미 df가 있으면 아래 2줄은 생략 가능)</span></span>
<span id="cb6-27"><a href="#cb6-27" aria-hidden="true" tabindex="-1"></a><span class="co"># -------------------------</span></span>
<span id="cb6-28"><a href="#cb6-28" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> sns.load_dataset(<span class="st">"mpg"</span>).dropna()</span>
<span id="cb6-29"><a href="#cb6-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-30"><a href="#cb6-30" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> df[<span class="st">"mpg"</span>].astype(<span class="bu">float</span>)</span>
<span id="cb6-31"><a href="#cb6-31" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> df.drop(columns<span class="op">=</span>[<span class="st">"mpg"</span>]).drop(columns<span class="op">=</span>[<span class="st">"name"</span>])  <span class="co"># name은 보통 제외 권장</span></span>
<span id="cb6-32"><a href="#cb6-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-33"><a href="#cb6-33" aria-hidden="true" tabindex="-1"></a><span class="co"># -------------------------</span></span>
<span id="cb6-34"><a href="#cb6-34" aria-hidden="true" tabindex="-1"></a><span class="co"># 2) train/val/test 분할 (70/15/15)</span></span>
<span id="cb6-35"><a href="#cb6-35" aria-hidden="true" tabindex="-1"></a><span class="co"># -------------------------</span></span>
<span id="cb6-36"><a href="#cb6-36" aria-hidden="true" tabindex="-1"></a>X_train, X_temp, y_train, y_temp <span class="op">=</span> train_test_split(X, y, test_size<span class="op">=</span><span class="fl">0.30</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb6-37"><a href="#cb6-37" aria-hidden="true" tabindex="-1"></a>X_val, X_test, y_val, y_test <span class="op">=</span> train_test_split(X_temp, y_temp, test_size<span class="op">=</span><span class="fl">0.50</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb6-38"><a href="#cb6-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-39"><a href="#cb6-39" aria-hidden="true" tabindex="-1"></a>X_trainval <span class="op">=</span> pd.concat([X_train, X_val], axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb6-40"><a href="#cb6-40" aria-hidden="true" tabindex="-1"></a>y_trainval <span class="op">=</span> pd.concat([y_train, y_val], axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb6-41"><a href="#cb6-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-42"><a href="#cb6-42" aria-hidden="true" tabindex="-1"></a><span class="co"># -------------------------</span></span>
<span id="cb6-43"><a href="#cb6-43" aria-hidden="true" tabindex="-1"></a><span class="co"># 3) 전처리: 연속형 표준화 + 범주형 원-핫</span></span>
<span id="cb6-44"><a href="#cb6-44" aria-hidden="true" tabindex="-1"></a><span class="co"># -------------------------</span></span>
<span id="cb6-45"><a href="#cb6-45" aria-hidden="true" tabindex="-1"></a>num_cols <span class="op">=</span> X_train.select_dtypes(include<span class="op">=</span>[<span class="st">"int64"</span>, <span class="st">"float64"</span>]).columns.tolist()</span>
<span id="cb6-46"><a href="#cb6-46" aria-hidden="true" tabindex="-1"></a>cat_cols <span class="op">=</span> X_train.select_dtypes(include<span class="op">=</span>[<span class="st">"object"</span>, <span class="st">"category"</span>]).columns.tolist()</span>
<span id="cb6-47"><a href="#cb6-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-48"><a href="#cb6-48" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> make_ohe():</span>
<span id="cb6-49"><a href="#cb6-49" aria-hidden="true" tabindex="-1"></a>    <span class="co"># sklearn 버전 호환</span></span>
<span id="cb6-50"><a href="#cb6-50" aria-hidden="true" tabindex="-1"></a>    <span class="cf">try</span>:</span>
<span id="cb6-51"><a href="#cb6-51" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> OneHotEncoder(handle_unknown<span class="op">=</span><span class="st">"ignore"</span>, sparse_output<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb6-52"><a href="#cb6-52" aria-hidden="true" tabindex="-1"></a>    <span class="cf">except</span> <span class="pp">TypeError</span>:</span>
<span id="cb6-53"><a href="#cb6-53" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> OneHotEncoder(handle_unknown<span class="op">=</span><span class="st">"ignore"</span>, sparse<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb6-54"><a href="#cb6-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-55"><a href="#cb6-55" aria-hidden="true" tabindex="-1"></a>preprocess <span class="op">=</span> ColumnTransformer(</span>
<span id="cb6-56"><a href="#cb6-56" aria-hidden="true" tabindex="-1"></a>    transformers<span class="op">=</span>[</span>
<span id="cb6-57"><a href="#cb6-57" aria-hidden="true" tabindex="-1"></a>        (<span class="st">"num"</span>, StandardScaler(), num_cols),</span>
<span id="cb6-58"><a href="#cb6-58" aria-hidden="true" tabindex="-1"></a>        (<span class="st">"cat"</span>, make_ohe(), cat_cols)</span>
<span id="cb6-59"><a href="#cb6-59" aria-hidden="true" tabindex="-1"></a>    ],</span>
<span id="cb6-60"><a href="#cb6-60" aria-hidden="true" tabindex="-1"></a>    remainder<span class="op">=</span><span class="st">"drop"</span></span>
<span id="cb6-61"><a href="#cb6-61" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb6-62"><a href="#cb6-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-63"><a href="#cb6-63" aria-hidden="true" tabindex="-1"></a><span class="co"># best epoch 선택용(train에 fit, val/test transform)</span></span>
<span id="cb6-64"><a href="#cb6-64" aria-hidden="true" tabindex="-1"></a>Xtr <span class="op">=</span> preprocess.fit_transform(X_train)</span>
<span id="cb6-65"><a href="#cb6-65" aria-hidden="true" tabindex="-1"></a>Xva <span class="op">=</span> preprocess.transform(X_val)</span>
<span id="cb6-66"><a href="#cb6-66" aria-hidden="true" tabindex="-1"></a>Xte <span class="op">=</span> preprocess.transform(X_test)</span>
<span id="cb6-67"><a href="#cb6-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-68"><a href="#cb6-68" aria-hidden="true" tabindex="-1"></a><span class="co"># 최종 재학습용(train+val에 fit)</span></span>
<span id="cb6-69"><a href="#cb6-69" aria-hidden="true" tabindex="-1"></a>preprocess_final <span class="op">=</span> ColumnTransformer(</span>
<span id="cb6-70"><a href="#cb6-70" aria-hidden="true" tabindex="-1"></a>    transformers<span class="op">=</span>[</span>
<span id="cb6-71"><a href="#cb6-71" aria-hidden="true" tabindex="-1"></a>        (<span class="st">"num"</span>, StandardScaler(), num_cols),</span>
<span id="cb6-72"><a href="#cb6-72" aria-hidden="true" tabindex="-1"></a>        (<span class="st">"cat"</span>, make_ohe(), cat_cols)</span>
<span id="cb6-73"><a href="#cb6-73" aria-hidden="true" tabindex="-1"></a>    ],</span>
<span id="cb6-74"><a href="#cb6-74" aria-hidden="true" tabindex="-1"></a>    remainder<span class="op">=</span><span class="st">"drop"</span></span>
<span id="cb6-75"><a href="#cb6-75" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb6-76"><a href="#cb6-76" aria-hidden="true" tabindex="-1"></a>Xtrva <span class="op">=</span> preprocess_final.fit_transform(X_trainval)</span>
<span id="cb6-77"><a href="#cb6-77" aria-hidden="true" tabindex="-1"></a>Xte2  <span class="op">=</span> preprocess_final.transform(X_test)</span>
<span id="cb6-78"><a href="#cb6-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-79"><a href="#cb6-79" aria-hidden="true" tabindex="-1"></a><span class="co"># -------------------------</span></span>
<span id="cb6-80"><a href="#cb6-80" aria-hidden="true" tabindex="-1"></a><span class="co"># 4) Pinball loss (다중 분위수)</span></span>
<span id="cb6-81"><a href="#cb6-81" aria-hidden="true" tabindex="-1"></a><span class="co"># -------------------------</span></span>
<span id="cb6-82"><a href="#cb6-82" aria-hidden="true" tabindex="-1"></a>taus <span class="op">=</span> tf.constant([<span class="fl">0.1</span>, <span class="fl">0.5</span>, <span class="fl">0.9</span>], dtype<span class="op">=</span>tf.float32)  <span class="co"># 필요시 변경</span></span>
<span id="cb6-83"><a href="#cb6-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-84"><a href="#cb6-84" aria-hidden="true" tabindex="-1"></a><span class="at">@tf.function</span></span>
<span id="cb6-85"><a href="#cb6-85" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> multi_pinball_loss(y_true, y_pred):</span>
<span id="cb6-86"><a href="#cb6-86" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb6-87"><a href="#cb6-87" aria-hidden="true" tabindex="-1"></a><span class="co">    y_true: (batch,) or (batch,1)</span></span>
<span id="cb6-88"><a href="#cb6-88" aria-hidden="true" tabindex="-1"></a><span class="co">    y_pred: (batch,3) -&gt; [q0.1, q0.5, q0.9]</span></span>
<span id="cb6-89"><a href="#cb6-89" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb6-90"><a href="#cb6-90" aria-hidden="true" tabindex="-1"></a>    y_true <span class="op">=</span> tf.cast(tf.reshape(y_true, (<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)), tf.float32)  <span class="co"># (batch,1)</span></span>
<span id="cb6-91"><a href="#cb6-91" aria-hidden="true" tabindex="-1"></a>    y_pred <span class="op">=</span> tf.cast(y_pred, tf.float32)                      <span class="co"># (batch,3)</span></span>
<span id="cb6-92"><a href="#cb6-92" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-93"><a href="#cb6-93" aria-hidden="true" tabindex="-1"></a>    e <span class="op">=</span> y_true <span class="op">-</span> y_pred  <span class="co"># (batch,3)</span></span>
<span id="cb6-94"><a href="#cb6-94" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> tf.maximum(taus <span class="op">*</span> e, (taus <span class="op">-</span> <span class="fl">1.0</span>) <span class="op">*</span> e)             <span class="co"># pinball</span></span>
<span id="cb6-95"><a href="#cb6-95" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> tf.reduce_mean(loss, axis<span class="op">=</span><span class="dv">1</span>)                       <span class="co"># (batch,)</span></span>
<span id="cb6-96"><a href="#cb6-96" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-97"><a href="#cb6-97" aria-hidden="true" tabindex="-1"></a><span class="at">@tf.function</span></span>
<span id="cb6-98"><a href="#cb6-98" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> mae_q50(y_true, y_pred):</span>
<span id="cb6-99"><a href="#cb6-99" aria-hidden="true" tabindex="-1"></a>    y_true <span class="op">=</span> tf.cast(tf.reshape(y_true, (<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)), tf.float32)</span>
<span id="cb6-100"><a href="#cb6-100" aria-hidden="true" tabindex="-1"></a>    q50 <span class="op">=</span> tf.reshape(y_pred[:, <span class="dv">1</span>], (<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>))</span>
<span id="cb6-101"><a href="#cb6-101" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> tf.reduce_mean(tf.<span class="bu">abs</span>(y_true <span class="op">-</span> q50))</span>
<span id="cb6-102"><a href="#cb6-102" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-103"><a href="#cb6-103" aria-hidden="true" tabindex="-1"></a><span class="at">@tf.function</span></span>
<span id="cb6-104"><a href="#cb6-104" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> crossing_rate(y_true, y_pred):</span>
<span id="cb6-105"><a href="#cb6-105" aria-hidden="true" tabindex="-1"></a>    <span class="co"># q0.1 &lt;= q0.5 &lt;= q0.9 위배 비율(참고용)</span></span>
<span id="cb6-106"><a href="#cb6-106" aria-hidden="true" tabindex="-1"></a>    q10 <span class="op">=</span> y_pred[:, <span class="dv">0</span>]</span>
<span id="cb6-107"><a href="#cb6-107" aria-hidden="true" tabindex="-1"></a>    q50 <span class="op">=</span> y_pred[:, <span class="dv">1</span>]</span>
<span id="cb6-108"><a href="#cb6-108" aria-hidden="true" tabindex="-1"></a>    q90 <span class="op">=</span> y_pred[:, <span class="dv">2</span>]</span>
<span id="cb6-109"><a href="#cb6-109" aria-hidden="true" tabindex="-1"></a>    ok <span class="op">=</span> tf.logical_and(q10 <span class="op">&lt;=</span> q50, q50 <span class="op">&lt;=</span> q90)</span>
<span id="cb6-110"><a href="#cb6-110" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="fl">1.0</span> <span class="op">-</span> tf.reduce_mean(tf.cast(ok, tf.float32))</span>
<span id="cb6-111"><a href="#cb6-111" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-112"><a href="#cb6-112" aria-hidden="true" tabindex="-1"></a><span class="co"># -------------------------</span></span>
<span id="cb6-113"><a href="#cb6-113" aria-hidden="true" tabindex="-1"></a><span class="co"># 5) 모델: 분위수 교차 방지(단조성 보장)</span></span>
<span id="cb6-114"><a href="#cb6-114" aria-hidden="true" tabindex="-1"></a><span class="co">#   q10_base + softplus(d1) + softplus(d2) 구조로 q10&lt;=q50&lt;=q90 보장</span></span>
<span id="cb6-115"><a href="#cb6-115" aria-hidden="true" tabindex="-1"></a><span class="co"># -------------------------</span></span>
<span id="cb6-116"><a href="#cb6-116" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> build_quantile_mlp(input_dim, hidden_units<span class="op">=</span>(<span class="dv">64</span>, <span class="dv">32</span>), dropout<span class="op">=</span><span class="fl">0.2</span>, lr<span class="op">=</span><span class="fl">1e-3</span>, weight_decay<span class="op">=</span><span class="fl">1e-4</span>):</span>
<span id="cb6-117"><a href="#cb6-117" aria-hidden="true" tabindex="-1"></a>    inp <span class="op">=</span> keras.Input(shape<span class="op">=</span>(input_dim,))</span>
<span id="cb6-118"><a href="#cb6-118" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> inp</span>
<span id="cb6-119"><a href="#cb6-119" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> u <span class="kw">in</span> hidden_units:</span>
<span id="cb6-120"><a href="#cb6-120" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> layers.Dense(u, activation<span class="op">=</span><span class="st">"relu"</span>,</span>
<span id="cb6-121"><a href="#cb6-121" aria-hidden="true" tabindex="-1"></a>                         kernel_regularizer<span class="op">=</span>keras.regularizers.l2(weight_decay))(x)</span>
<span id="cb6-122"><a href="#cb6-122" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> layers.Dropout(dropout)(x)</span>
<span id="cb6-123"><a href="#cb6-123" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-124"><a href="#cb6-124" aria-hidden="true" tabindex="-1"></a>    q10_base <span class="op">=</span> layers.Dense(<span class="dv">1</span>, activation<span class="op">=</span><span class="st">"linear"</span>)(x)</span>
<span id="cb6-125"><a href="#cb6-125" aria-hidden="true" tabindex="-1"></a>    d1 <span class="op">=</span> layers.Dense(<span class="dv">1</span>, activation<span class="op">=</span><span class="st">"linear"</span>)(x)</span>
<span id="cb6-126"><a href="#cb6-126" aria-hidden="true" tabindex="-1"></a>    d2 <span class="op">=</span> layers.Dense(<span class="dv">1</span>, activation<span class="op">=</span><span class="st">"linear"</span>)(x)</span>
<span id="cb6-127"><a href="#cb6-127" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-128"><a href="#cb6-128" aria-hidden="true" tabindex="-1"></a>    sp1 <span class="op">=</span> layers.Activation(tf.nn.softplus)(d1)</span>
<span id="cb6-129"><a href="#cb6-129" aria-hidden="true" tabindex="-1"></a>    sp2 <span class="op">=</span> layers.Activation(tf.nn.softplus)(d2)</span>
<span id="cb6-130"><a href="#cb6-130" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-131"><a href="#cb6-131" aria-hidden="true" tabindex="-1"></a>    q10 <span class="op">=</span> q10_base</span>
<span id="cb6-132"><a href="#cb6-132" aria-hidden="true" tabindex="-1"></a>    q50 <span class="op">=</span> layers.Add()([q10, sp1])</span>
<span id="cb6-133"><a href="#cb6-133" aria-hidden="true" tabindex="-1"></a>    q90 <span class="op">=</span> layers.Add()([q50, sp2])</span>
<span id="cb6-134"><a href="#cb6-134" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-135"><a href="#cb6-135" aria-hidden="true" tabindex="-1"></a>    out <span class="op">=</span> layers.Concatenate(axis<span class="op">=</span><span class="dv">1</span>)([q10, q50, q90])  <span class="co"># (batch,3)</span></span>
<span id="cb6-136"><a href="#cb6-136" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-137"><a href="#cb6-137" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> keras.Model(inp, out)</span>
<span id="cb6-138"><a href="#cb6-138" aria-hidden="true" tabindex="-1"></a>    model.<span class="bu">compile</span>(</span>
<span id="cb6-139"><a href="#cb6-139" aria-hidden="true" tabindex="-1"></a>        optimizer<span class="op">=</span>keras.optimizers.Adam(learning_rate<span class="op">=</span>lr),</span>
<span id="cb6-140"><a href="#cb6-140" aria-hidden="true" tabindex="-1"></a>        loss<span class="op">=</span>multi_pinball_loss,</span>
<span id="cb6-141"><a href="#cb6-141" aria-hidden="true" tabindex="-1"></a>        metrics<span class="op">=</span>[mae_q50, crossing_rate]</span>
<span id="cb6-142"><a href="#cb6-142" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb6-143"><a href="#cb6-143" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> model</span>
<span id="cb6-144"><a href="#cb6-144" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-145"><a href="#cb6-145" aria-hidden="true" tabindex="-1"></a><span class="co"># -------------------------</span></span>
<span id="cb6-146"><a href="#cb6-146" aria-hidden="true" tabindex="-1"></a><span class="co"># 6) (1차) train/val로 best epoch 찾기</span></span>
<span id="cb6-147"><a href="#cb6-147" aria-hidden="true" tabindex="-1"></a><span class="co"># -------------------------</span></span>
<span id="cb6-148"><a href="#cb6-148" aria-hidden="true" tabindex="-1"></a>keras.backend.clear_session()</span>
<span id="cb6-149"><a href="#cb6-149" aria-hidden="true" tabindex="-1"></a>tf.random.set_seed(<span class="dv">42</span>)</span>
<span id="cb6-150"><a href="#cb6-150" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-151"><a href="#cb6-151" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> build_quantile_mlp(input_dim<span class="op">=</span>Xtr.shape[<span class="dv">1</span>])</span>
<span id="cb6-152"><a href="#cb6-152" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-153"><a href="#cb6-153" aria-hidden="true" tabindex="-1"></a>early <span class="op">=</span> keras.callbacks.EarlyStopping(</span>
<span id="cb6-154"><a href="#cb6-154" aria-hidden="true" tabindex="-1"></a>    monitor<span class="op">=</span><span class="st">"val_loss"</span>, patience<span class="op">=</span><span class="dv">30</span>, restore_best_weights<span class="op">=</span><span class="va">True</span></span>
<span id="cb6-155"><a href="#cb6-155" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb6-156"><a href="#cb6-156" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-157"><a href="#cb6-157" aria-hidden="true" tabindex="-1"></a>hist <span class="op">=</span> model.fit(</span>
<span id="cb6-158"><a href="#cb6-158" aria-hidden="true" tabindex="-1"></a>    Xtr, y_train.values,</span>
<span id="cb6-159"><a href="#cb6-159" aria-hidden="true" tabindex="-1"></a>    validation_data<span class="op">=</span>(Xva, y_val.values),</span>
<span id="cb6-160"><a href="#cb6-160" aria-hidden="true" tabindex="-1"></a>    epochs<span class="op">=</span><span class="dv">500</span>, batch_size<span class="op">=</span><span class="dv">32</span>,</span>
<span id="cb6-161"><a href="#cb6-161" aria-hidden="true" tabindex="-1"></a>    callbacks<span class="op">=</span>[early],</span>
<span id="cb6-162"><a href="#cb6-162" aria-hidden="true" tabindex="-1"></a>    verbose<span class="op">=</span><span class="dv">0</span></span>
<span id="cb6-163"><a href="#cb6-163" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb6-164"><a href="#cb6-164" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-165"><a href="#cb6-165" aria-hidden="true" tabindex="-1"></a>best_epoch <span class="op">=</span> <span class="bu">int</span>(np.argmin(hist.history[<span class="st">"val_loss"</span>]) <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb6-166"><a href="#cb6-166" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Best epoch:"</span>, best_epoch, <span class="st">"| best val loss:"</span>, <span class="bu">float</span>(np.<span class="bu">min</span>(hist.history[<span class="st">"val_loss"</span>])))</span>
<span id="cb6-167"><a href="#cb6-167" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-168"><a href="#cb6-168" aria-hidden="true" tabindex="-1"></a><span class="co"># -------------------------</span></span>
<span id="cb6-169"><a href="#cb6-169" aria-hidden="true" tabindex="-1"></a><span class="co"># 7) (최종) train+val로 재학습(best_epoch만큼)</span></span>
<span id="cb6-170"><a href="#cb6-170" aria-hidden="true" tabindex="-1"></a><span class="co"># -------------------------</span></span>
<span id="cb6-171"><a href="#cb6-171" aria-hidden="true" tabindex="-1"></a>keras.backend.clear_session()</span>
<span id="cb6-172"><a href="#cb6-172" aria-hidden="true" tabindex="-1"></a>tf.random.set_seed(<span class="dv">42</span>)</span>
<span id="cb6-173"><a href="#cb6-173" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-174"><a href="#cb6-174" aria-hidden="true" tabindex="-1"></a>final_qmodel <span class="op">=</span> build_quantile_mlp(input_dim<span class="op">=</span>Xtrva.shape[<span class="dv">1</span>])</span>
<span id="cb6-175"><a href="#cb6-175" aria-hidden="true" tabindex="-1"></a>final_qmodel.fit(Xtrva, y_trainval.values, epochs<span class="op">=</span>best_epoch, batch_size<span class="op">=</span><span class="dv">32</span>, verbose<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb6-176"><a href="#cb6-176" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-177"><a href="#cb6-177" aria-hidden="true" tabindex="-1"></a><span class="co"># -------------------------</span></span>
<span id="cb6-178"><a href="#cb6-178" aria-hidden="true" tabindex="-1"></a><span class="co"># 8) test 예측 + 예측구간 평가</span></span>
<span id="cb6-179"><a href="#cb6-179" aria-hidden="true" tabindex="-1"></a><span class="co"># -------------------------</span></span>
<span id="cb6-180"><a href="#cb6-180" aria-hidden="true" tabindex="-1"></a>pred <span class="op">=</span> final_qmodel.predict(Xte2)  <span class="co"># (n,3)</span></span>
<span id="cb6-181"><a href="#cb6-181" aria-hidden="true" tabindex="-1"></a>q10, q50, q90 <span class="op">=</span> pred[:, <span class="dv">0</span>], pred[:, <span class="dv">1</span>], pred[:, <span class="dv">2</span>]</span>
<span id="cb6-182"><a href="#cb6-182" aria-hidden="true" tabindex="-1"></a>y_true <span class="op">=</span> y_test.values</span>
<span id="cb6-183"><a href="#cb6-183" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-184"><a href="#cb6-184" aria-hidden="true" tabindex="-1"></a><span class="co"># (중앙값 예측 성능)</span></span>
<span id="cb6-185"><a href="#cb6-185" aria-hidden="true" tabindex="-1"></a>rmse_q50 <span class="op">=</span> np.sqrt(mean_squared_error(y_true, q50))</span>
<span id="cb6-186"><a href="#cb6-186" aria-hidden="true" tabindex="-1"></a>mae_q50_ <span class="op">=</span> mean_absolute_error(y_true, q50)</span>
<span id="cb6-187"><a href="#cb6-187" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-188"><a href="#cb6-188" aria-hidden="true" tabindex="-1"></a><span class="co"># (예측구간: 80% PI = [q0.1, q0.9])</span></span>
<span id="cb6-189"><a href="#cb6-189" aria-hidden="true" tabindex="-1"></a>coverage_80 <span class="op">=</span> np.mean((y_true <span class="op">&gt;=</span> q10) <span class="op">&amp;</span> (y_true <span class="op">&lt;=</span> q90))</span>
<span id="cb6-190"><a href="#cb6-190" aria-hidden="true" tabindex="-1"></a>avg_width   <span class="op">=</span> np.mean(q90 <span class="op">-</span> q10)</span>
<span id="cb6-191"><a href="#cb6-191" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-192"><a href="#cb6-192" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">=== Quantile MLP Test Summary ==="</span>)</span>
<span id="cb6-193"><a href="#cb6-193" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Median(q0.5) RMSE = </span><span class="sc">{</span>rmse_q50<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb6-194"><a href="#cb6-194" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Median(q0.5) MAE  = </span><span class="sc">{</span>mae_q50_<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb6-195"><a href="#cb6-195" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"80% PI coverage (q0.1~q0.9) = </span><span class="sc">{</span>coverage_80<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb6-196"><a href="#cb6-196" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Avg PI width (q0.9 - q0.1)  = </span><span class="sc">{</span>avg_width<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb6-197"><a href="#cb6-197" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-198"><a href="#cb6-198" aria-hidden="true" tabindex="-1"></a><span class="co"># 결과 테이블(상위 10개 확인)</span></span>
<span id="cb6-199"><a href="#cb6-199" aria-hidden="true" tabindex="-1"></a>out_df <span class="op">=</span> X_test.copy()</span>
<span id="cb6-200"><a href="#cb6-200" aria-hidden="true" tabindex="-1"></a>out_df[<span class="st">"mpg_true"</span>] <span class="op">=</span> y_true</span>
<span id="cb6-201"><a href="#cb6-201" aria-hidden="true" tabindex="-1"></a>out_df[<span class="st">"q10"</span>] <span class="op">=</span> q10</span>
<span id="cb6-202"><a href="#cb6-202" aria-hidden="true" tabindex="-1"></a>out_df[<span class="st">"q50"</span>] <span class="op">=</span> q50</span>
<span id="cb6-203"><a href="#cb6-203" aria-hidden="true" tabindex="-1"></a>out_df[<span class="st">"q90"</span>] <span class="op">=</span> q90</span>
<span id="cb6-204"><a href="#cb6-204" aria-hidden="true" tabindex="-1"></a>out_df[<span class="st">"in_80PI"</span>] <span class="op">=</span> (y_true <span class="op">&gt;=</span> q10) <span class="op">&amp;</span> (y_true <span class="op">&lt;=</span> q90)</span>
<span id="cb6-205"><a href="#cb6-205" aria-hidden="true" tabindex="-1"></a>out_df.head(<span class="dv">10</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>이번 분석은 MLP가 중앙값(q0.5) 뿐 아니라 하위 10%(q0.1), 상위 90%(q0.9) 분위수까지 동시에 예측하도록 학습하여, 각 관측치에 대해 예측구간(80% PI = [q0.1, q0.9])을 제공한 것이다. 즉 <span dir="rtl">”</span>점예측이 얼마나 맞는가”와 함께 <span dir="rtl">”</span>예측의 불확실성이 어느 정도인가”를 함께 평가한다.</p>
<p><strong>학습 종료 시점(best epoch = 217)의 의미</strong></p>
<p>검증 손실(val loss)이 가장 작아진 시점이 217 epoch였고, 이후에는 과적합 또는 개선 정체가 나타났다는 뜻이다. 따라서 217 epoch 부근의 파라미터가 일반화 성능(새 데이터에서의 성능) 관점에서 가장 적절한 선택으로 간주된다.</p>
<p><strong>중앙값 예측 성능(점예측 관점)</strong></p>
<p>Median(q0.5) RMSE = 2.334, Median(q0.5) MAE = 1.651</p>
<p>이는 테스트 데이터에서 예측 mpg(중앙값)가 실제 mpg와 평균적으로 약 2.33 mpg 정도의 제곱평균오차 규모, 절대오차로는 약 1.65 mpg 정도 차이 난다는 의미다. 즉 중앙값 예측만 보더라도 성능은 준수하며, 이전의 단일 MLP 점예측과 유사한 수준의 정확도를 유지하면서 <span dir="rtl">”</span>구간 정보”를 추가로 제공하게 된다.</p>
<p><strong>80% 예측구간 성능(불확실성 관점)</strong></p>
<p>80% PI coverage(q0.1~q0.9) = 0.966, Avg PI width(q0.9 - q0.1) = 8.909</p>
<p>여기서 coverage는 <span dir="rtl">”</span>실제 mpg가 예측구간 [q0.1, q0.9] 안에 들어간 비율”이다. 이론적으로는 80% 구간이므로 coverage가 0.80 근처면 적절한데, 실제로는 0.966(96.6%)로 매우 높게 나왔다. 이는 모델이 구간을 상당히 넓게 잡아(width 평균 8.91 mpg) 실제값을 대부분 포함시키고 있음을 뜻한다.</p>
<p>정리하면 실제값을 놓치지 않는 <span dir="rtl">”</span>안전한” 구간(리스크 관리에는 유리)이지만 구간이 넓어 실무적으로 정보가 덜 날카로울 수 있다. 즉 현재 설정에서는 calibration(포함률)은 과하게 높고, sharpness(구간의 날카로움)는 다소 희생된 상태로 해석할 수 있다.</p>
<p><strong>개별 관측치 해석(표 예시 기반)</strong></p>
<p>표의 각 행은 한 차량에 대해 q10: <span dir="rtl">”</span>낮게 나올 가능성이 있는 수준(하위 10%)“, q50: <span dir="rtl">”</span>대표값(중앙값 예측)”, q90: <span dir="rtl">”</span>높게 나올 가능성이 있는 수준(상위 90%)“을 뜻한다.</p>
<p>대부분의 행에서 in_80PI=True로 나타나 실제 mpg가 [q10, q90] 안에 들어갔다. 반면 마지막 예시(인덱스 111)는 실제 mpg = 18.0, 예측구간 [q10, q90] = [22.25, 32.56] 으로 실제값이 구간보다 훨씬 낮아 False가 되었다.</p>
<p>이는 해당 관측치에 대해 모델이 <span dir="rtl">”</span>전반적으로 mpg를 과대평가”했음을 의미하며, 원인은 보통 다음 중 하나다. 이 차량 조합이 학습 데이터에서 드문 패턴(희귀 조합), 입력 변수로 설명되지 않는 요인이 존재(예: 상태/세부 트림 차이), 범주(origin)와 연속형 변수 관계가 이 관측치에서는 비정상적으로 나타남. 따라서 이런 사례는 잔차/오차 큰 관측치처럼 따로 모아 점검하는 것이 바람직하다.</p>
<p><strong>결론</strong></p>
<p>Quantile MLP는 중앙값 예측 정확도(RMSE 2.33, MAE 1.65)를 유지하면서, 각 관측치에 대해 80% 예측구간을 제공하였다. 다만 본 결과에서는 포함률이 96.6%로 목표(80%)보다 과도하게 높고 평균 구간폭이 8.91 mpg로 넓어, <span dir="rtl">”</span>구간이 안전하지만 다소 보수적”인 예측이 나타났다. 따라서 실제 활용에서는 목표 포함률에 맞게 구간폭을 조정(모델/튜닝/검증)하여 calibration과 sharpness의 균형을 맞추는 것이 중요하다.</p>
<p>Best epoch: 217 | best val loss: 0.641271710395813 <br> === Quantile MLP Test Summary === <br> Median(q0.5) RMSE = 2.334 <br> Median(q0.5) MAE = 1.651 <br> 80% PI coverage (q0.1~q0.9) = 0.966 <br> Avg PI width (q0.9 - q0.1) = 8.909</p>
<table class="caption-top table">
<colgroup>
<col style="width: 8%">
<col style="width: 10%">
<col style="width: 8%">
<col style="width: 6%">
<col style="width: 10%">
<col style="width: 8%">
<col style="width: 6%">
<col style="width: 7%">
<col style="width: 8%">
<col style="width: 8%">
<col style="width: 8%">
<col style="width: 8%">
</colgroup>
<tbody>
<tr class="odd">
<td style="text-align: left;">cylinders</td>
<td style="text-align: left;">displacement</td>
<td style="text-align: left;">horsepower</td>
<td style="text-align: left;">weight</td>
<td style="text-align: left;">acceleration</td>
<td style="text-align: left;">model_year</td>
<td style="text-align: left;">origin</td>
<td style="text-align: left;">mpg_true</td>
<td style="text-align: left;">q10</td>
<td style="text-align: left;">q50</td>
<td style="text-align: left;">q90</td>
<td style="text-align: left;">in_80PI</td>
</tr>
<tr class="even">
<td style="text-align: left;">8</td>
<td style="text-align: left;">262.0</td>
<td style="text-align: left;">110.0</td>
<td style="text-align: left;">3221</td>
<td style="text-align: left;">13.5</td>
<td style="text-align: left;">75</td>
<td style="text-align: left;">usa</td>
<td style="text-align: left;">20.0</td>
<td style="text-align: left;">15.051061</td>
<td style="text-align: left;">18.198753</td>
<td style="text-align: left;">21.884342</td>
<td style="text-align: left;">1</td>
</tr>
<tr class="odd">
<td style="text-align: left;">6</td>
<td style="text-align: left;">250.0</td>
<td style="text-align: left;">100.0</td>
<td style="text-align: left;">3781</td>
<td style="text-align: left;">17.0</td>
<td style="text-align: left;">74</td>
<td style="text-align: left;">usa</td>
<td style="text-align: left;">16.0</td>
<td style="text-align: left;">13.367275</td>
<td style="text-align: left;">15.983590</td>
<td style="text-align: left;">19.326855</td>
<td style="text-align: left;">1</td>
</tr>
<tr class="even">
<td style="text-align: left;">4</td>
<td style="text-align: left;">140.0</td>
<td style="text-align: left;">86.0</td>
<td style="text-align: left;">2790</td>
<td style="text-align: left;">15.6</td>
<td style="text-align: left;">82</td>
<td style="text-align: left;">usa</td>
<td style="text-align: left;">27.0</td>
<td style="text-align: left;">22.570227</td>
<td style="text-align: left;">27.181686</td>
<td style="text-align: left;">33.161285</td>
<td style="text-align: left;">1</td>
</tr>
<tr class="odd">
<td style="text-align: left;">8</td>
<td style="text-align: left;">440.0</td>
<td style="text-align: left;">215.0</td>
<td style="text-align: left;">4735</td>
<td style="text-align: left;">11.0</td>
<td style="text-align: left;">73</td>
<td style="text-align: left;">usa</td>
<td style="text-align: left;">13.0</td>
<td style="text-align: left;">9.721265</td>
<td style="text-align: left;">12.028823</td>
<td style="text-align: left;">14.587757</td>
<td style="text-align: left;">1</td>
</tr>
<tr class="even">
<td style="text-align: left;">4</td>
<td style="text-align: left;">120.0</td>
<td style="text-align: left;">87.0</td>
<td style="text-align: left;">2979</td>
<td style="text-align: left;">19.5</td>
<td style="text-align: left;">72</td>
<td style="text-align: left;">europe</td>
<td style="text-align: left;">21.0</td>
<td style="text-align: left;">15.746195</td>
<td style="text-align: left;">19.154026</td>
<td style="text-align: left;">23.161297</td>
<td style="text-align: left;">1</td>
</tr>
<tr class="odd">
<td style="text-align: left;">8</td>
<td style="text-align: left;">318.0</td>
<td style="text-align: left;">140.0</td>
<td style="text-align: left;">3735</td>
<td style="text-align: left;">13.2</td>
<td style="text-align: left;">78</td>
<td style="text-align: left;">usa</td>
<td style="text-align: left;">19.4</td>
<td style="text-align: left;">13.708342</td>
<td style="text-align: left;">16.710249</td>
<td style="text-align: left;">20.392799</td>
<td style="text-align: left;">1</td>
</tr>
<tr class="even">
<td style="text-align: left;">8</td>
<td style="text-align: left;">429.0</td>
<td style="text-align: left;">198.0</td>
<td style="text-align: left;">4341</td>
<td style="text-align: left;">10.0</td>
<td style="text-align: left;">70</td>
<td style="text-align: left;">usa</td>
<td style="text-align: left;">15.0</td>
<td style="text-align: left;">10.665980</td>
<td style="text-align: left;">12.984769</td>
<td style="text-align: left;">15.593925</td>
<td style="text-align: left;">1</td>
</tr>
<tr class="odd">
<td style="text-align: left;">8</td>
<td style="text-align: left;">390.0</td>
<td style="text-align: left;">190.0</td>
<td style="text-align: left;">3850</td>
<td style="text-align: left;">8.5</td>
<td style="text-align: left;">70</td>
<td style="text-align: left;">usa</td>
<td style="text-align: left;">15.0</td>
<td style="text-align: left;">12.255207</td>
<td style="text-align: left;">14.866547</td>
<td style="text-align: left;">17.737793</td>
<td style="text-align: left;">1</td>
</tr>
<tr class="even">
<td style="text-align: left;">4</td>
<td style="text-align: left;">140.0</td>
<td style="text-align: left;">88.0</td>
<td style="text-align: left;">2890</td>
<td style="text-align: left;">17.3</td>
<td style="text-align: left;">79</td>
<td style="text-align: left;">usa</td>
<td style="text-align: left;">22.3</td>
<td style="text-align: left;">19.344568</td>
<td style="text-align: left;">23.277502</td>
<td style="text-align: left;">28.300762</td>
<td style="text-align: left;">1</td>
</tr>
<tr class="odd">
<td style="text-align: left;">3</td>
<td style="text-align: left;">70.0</td>
<td style="text-align: left;">90.0</td>
<td style="text-align: left;">2124</td>
<td style="text-align: left;">13.5</td>
<td style="text-align: left;">73</td>
<td style="text-align: left;">japan</td>
<td style="text-align: left;">18.0</td>
<td style="text-align: left;">22.247568</td>
<td style="text-align: left;">27.097534</td>
<td style="text-align: left;">32.563396</td>
<td style="text-align: left;">0</td>
</tr>
</tbody>
</table>
</section>
<section id="분포-예측" class="level5">
<h5 class="anchored" data-anchor-id="분포-예측">(7) 분포 예측</h5>
<div class="sourceCode" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="co"># =========================================</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Distribution Prediction (Gaussian NLL)</span></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="co"># - output: mu(x), sigma(x)  (heteroscedastic)</span></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a><span class="co"># - loss: Negative Log-Likelihood (NLL)</span></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a><span class="co"># - train/val로 best epoch 선택 -&gt; train+val로 재학습 -&gt; test 평가</span></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a><span class="co"># =========================================</span></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.compose <span class="im">import</span> ColumnTransformer</span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> OneHotEncoder, StandardScaler</span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> mean_squared_error, mean_absolute_error</span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> tensorflow <span class="im">as</span> tf</span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow <span class="im">import</span> keras</span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow.keras <span class="im">import</span> layers</span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb7-22"><a href="#cb7-22" aria-hidden="true" tabindex="-1"></a>tf.random.set_seed(<span class="dv">42</span>)</span>
<span id="cb7-23"><a href="#cb7-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-24"><a href="#cb7-24" aria-hidden="true" tabindex="-1"></a><span class="co"># -------------------------</span></span>
<span id="cb7-25"><a href="#cb7-25" aria-hidden="true" tabindex="-1"></a><span class="co"># 1) 데이터/분할(없으면 생성)</span></span>
<span id="cb7-26"><a href="#cb7-26" aria-hidden="true" tabindex="-1"></a><span class="co"># -------------------------</span></span>
<span id="cb7-27"><a href="#cb7-27" aria-hidden="true" tabindex="-1"></a><span class="cf">try</span>:</span>
<span id="cb7-28"><a href="#cb7-28" aria-hidden="true" tabindex="-1"></a>    X_train</span>
<span id="cb7-29"><a href="#cb7-29" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Using existing split: X_train/X_val/X_test already defined."</span>)</span>
<span id="cb7-30"><a href="#cb7-30" aria-hidden="true" tabindex="-1"></a><span class="cf">except</span> <span class="pp">NameError</span>:</span>
<span id="cb7-31"><a href="#cb7-31" aria-hidden="true" tabindex="-1"></a>    df <span class="op">=</span> sns.load_dataset(<span class="st">"mpg"</span>).dropna()</span>
<span id="cb7-32"><a href="#cb7-32" aria-hidden="true" tabindex="-1"></a>    y <span class="op">=</span> df[<span class="st">"mpg"</span>].astype(<span class="bu">float</span>)</span>
<span id="cb7-33"><a href="#cb7-33" aria-hidden="true" tabindex="-1"></a>    X <span class="op">=</span> df.drop(columns<span class="op">=</span>[<span class="st">"mpg"</span>, <span class="st">"name"</span>])</span>
<span id="cb7-34"><a href="#cb7-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-35"><a href="#cb7-35" aria-hidden="true" tabindex="-1"></a>    X_train, X_temp, y_train, y_temp <span class="op">=</span> train_test_split(X, y, test_size<span class="op">=</span><span class="fl">0.30</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb7-36"><a href="#cb7-36" aria-hidden="true" tabindex="-1"></a>    X_val, X_test, y_val, y_test <span class="op">=</span> train_test_split(X_temp, y_temp, test_size<span class="op">=</span><span class="fl">0.50</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb7-37"><a href="#cb7-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-38"><a href="#cb7-38" aria-hidden="true" tabindex="-1"></a>X_trainval <span class="op">=</span> pd.concat([X_train, X_val], axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb7-39"><a href="#cb7-39" aria-hidden="true" tabindex="-1"></a>y_trainval <span class="op">=</span> pd.concat([y_train, y_val], axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb7-40"><a href="#cb7-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-41"><a href="#cb7-41" aria-hidden="true" tabindex="-1"></a><span class="co"># -------------------------</span></span>
<span id="cb7-42"><a href="#cb7-42" aria-hidden="true" tabindex="-1"></a><span class="co"># 2) 전처리(없으면 생성)</span></span>
<span id="cb7-43"><a href="#cb7-43" aria-hidden="true" tabindex="-1"></a><span class="co"># -------------------------</span></span>
<span id="cb7-44"><a href="#cb7-44" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> make_ohe():</span>
<span id="cb7-45"><a href="#cb7-45" aria-hidden="true" tabindex="-1"></a>    <span class="cf">try</span>:</span>
<span id="cb7-46"><a href="#cb7-46" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> OneHotEncoder(handle_unknown<span class="op">=</span><span class="st">"ignore"</span>, sparse_output<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb7-47"><a href="#cb7-47" aria-hidden="true" tabindex="-1"></a>    <span class="cf">except</span> <span class="pp">TypeError</span>:</span>
<span id="cb7-48"><a href="#cb7-48" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> OneHotEncoder(handle_unknown<span class="op">=</span><span class="st">"ignore"</span>, sparse<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb7-49"><a href="#cb7-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-50"><a href="#cb7-50" aria-hidden="true" tabindex="-1"></a>num_cols <span class="op">=</span> X_train.select_dtypes(include<span class="op">=</span>[<span class="st">"int64"</span>,<span class="st">"float64"</span>]).columns.tolist()</span>
<span id="cb7-51"><a href="#cb7-51" aria-hidden="true" tabindex="-1"></a>cat_cols <span class="op">=</span> X_train.select_dtypes(include<span class="op">=</span>[<span class="st">"object"</span>,<span class="st">"category"</span>]).columns.tolist()</span>
<span id="cb7-52"><a href="#cb7-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-53"><a href="#cb7-53" aria-hidden="true" tabindex="-1"></a><span class="cf">try</span>:</span>
<span id="cb7-54"><a href="#cb7-54" aria-hidden="true" tabindex="-1"></a>    preprocess_final</span>
<span id="cb7-55"><a href="#cb7-55" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Using existing preprocess_final."</span>)</span>
<span id="cb7-56"><a href="#cb7-56" aria-hidden="true" tabindex="-1"></a><span class="cf">except</span> <span class="pp">NameError</span>:</span>
<span id="cb7-57"><a href="#cb7-57" aria-hidden="true" tabindex="-1"></a>    preprocess_final <span class="op">=</span> ColumnTransformer(</span>
<span id="cb7-58"><a href="#cb7-58" aria-hidden="true" tabindex="-1"></a>        transformers<span class="op">=</span>[</span>
<span id="cb7-59"><a href="#cb7-59" aria-hidden="true" tabindex="-1"></a>            (<span class="st">"num"</span>, StandardScaler(), num_cols),</span>
<span id="cb7-60"><a href="#cb7-60" aria-hidden="true" tabindex="-1"></a>            (<span class="st">"cat"</span>, make_ohe(), cat_cols)</span>
<span id="cb7-61"><a href="#cb7-61" aria-hidden="true" tabindex="-1"></a>        ],</span>
<span id="cb7-62"><a href="#cb7-62" aria-hidden="true" tabindex="-1"></a>        remainder<span class="op">=</span><span class="st">"drop"</span></span>
<span id="cb7-63"><a href="#cb7-63" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb7-64"><a href="#cb7-64" aria-hidden="true" tabindex="-1"></a>    preprocess_final.fit(X_trainval)</span>
<span id="cb7-65"><a href="#cb7-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-66"><a href="#cb7-66" aria-hidden="true" tabindex="-1"></a><span class="co"># (best epoch 찾기용) train 기반 전처리기</span></span>
<span id="cb7-67"><a href="#cb7-67" aria-hidden="true" tabindex="-1"></a>preprocess <span class="op">=</span> ColumnTransformer(</span>
<span id="cb7-68"><a href="#cb7-68" aria-hidden="true" tabindex="-1"></a>    transformers<span class="op">=</span>[</span>
<span id="cb7-69"><a href="#cb7-69" aria-hidden="true" tabindex="-1"></a>        (<span class="st">"num"</span>, StandardScaler(), num_cols),</span>
<span id="cb7-70"><a href="#cb7-70" aria-hidden="true" tabindex="-1"></a>        (<span class="st">"cat"</span>, make_ohe(), cat_cols)</span>
<span id="cb7-71"><a href="#cb7-71" aria-hidden="true" tabindex="-1"></a>    ],</span>
<span id="cb7-72"><a href="#cb7-72" aria-hidden="true" tabindex="-1"></a>    remainder<span class="op">=</span><span class="st">"drop"</span></span>
<span id="cb7-73"><a href="#cb7-73" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb7-74"><a href="#cb7-74" aria-hidden="true" tabindex="-1"></a>Xtr <span class="op">=</span> preprocess.fit_transform(X_train)</span>
<span id="cb7-75"><a href="#cb7-75" aria-hidden="true" tabindex="-1"></a>Xva <span class="op">=</span> preprocess.transform(X_val)</span>
<span id="cb7-76"><a href="#cb7-76" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-77"><a href="#cb7-77" aria-hidden="true" tabindex="-1"></a><span class="co"># (최종 재학습용) train+val 기반 전처리</span></span>
<span id="cb7-78"><a href="#cb7-78" aria-hidden="true" tabindex="-1"></a>Xtrva <span class="op">=</span> preprocess_final.transform(X_trainval)</span>
<span id="cb7-79"><a href="#cb7-79" aria-hidden="true" tabindex="-1"></a>Xte   <span class="op">=</span> preprocess_final.transform(X_test)</span>
<span id="cb7-80"><a href="#cb7-80" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-81"><a href="#cb7-81" aria-hidden="true" tabindex="-1"></a><span class="co"># -------------------------</span></span>
<span id="cb7-82"><a href="#cb7-82" aria-hidden="true" tabindex="-1"></a><span class="co"># 3) Gaussian NLL loss</span></span>
<span id="cb7-83"><a href="#cb7-83" aria-hidden="true" tabindex="-1"></a><span class="co"># -------------------------</span></span>
<span id="cb7-84"><a href="#cb7-84" aria-hidden="true" tabindex="-1"></a>LOG_2PI <span class="op">=</span> np.log(<span class="fl">2.0</span> <span class="op">*</span> np.pi).astype(<span class="st">"float32"</span>)</span>
<span id="cb7-85"><a href="#cb7-85" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-86"><a href="#cb7-86" aria-hidden="true" tabindex="-1"></a><span class="at">@tf.function</span></span>
<span id="cb7-87"><a href="#cb7-87" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> gaussian_nll(y_true, y_pred):</span>
<span id="cb7-88"><a href="#cb7-88" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb7-89"><a href="#cb7-89" aria-hidden="true" tabindex="-1"></a><span class="co">    y_pred: (batch,2) -&gt; [mu, raw_sigma]</span></span>
<span id="cb7-90"><a href="#cb7-90" aria-hidden="true" tabindex="-1"></a><span class="co">    sigma = softplus(raw_sigma) + eps  (양수 보장)</span></span>
<span id="cb7-91"><a href="#cb7-91" aria-hidden="true" tabindex="-1"></a><span class="co">    NLL = 0.5*log(2pi) + log(sigma) + 0.5*((y-mu)/sigma)^2</span></span>
<span id="cb7-92"><a href="#cb7-92" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb7-93"><a href="#cb7-93" aria-hidden="true" tabindex="-1"></a>    y_true <span class="op">=</span> tf.cast(tf.reshape(y_true, (<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)), tf.float32)   <span class="co"># (batch,1)</span></span>
<span id="cb7-94"><a href="#cb7-94" aria-hidden="true" tabindex="-1"></a>    mu <span class="op">=</span> tf.reshape(y_pred[:, <span class="dv">0</span>], (<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>))</span>
<span id="cb7-95"><a href="#cb7-95" aria-hidden="true" tabindex="-1"></a>    raw <span class="op">=</span> tf.reshape(y_pred[:, <span class="dv">1</span>], (<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>))</span>
<span id="cb7-96"><a href="#cb7-96" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-97"><a href="#cb7-97" aria-hidden="true" tabindex="-1"></a>    sigma <span class="op">=</span> tf.nn.softplus(raw) <span class="op">+</span> <span class="fl">1e-6</span>  <span class="co"># 안정화(0 방지)</span></span>
<span id="cb7-98"><a href="#cb7-98" aria-hidden="true" tabindex="-1"></a>    z <span class="op">=</span> (y_true <span class="op">-</span> mu) <span class="op">/</span> sigma</span>
<span id="cb7-99"><a href="#cb7-99" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-100"><a href="#cb7-100" aria-hidden="true" tabindex="-1"></a>    nll <span class="op">=</span> <span class="fl">0.5</span> <span class="op">*</span> LOG_2PI <span class="op">+</span> tf.math.log(sigma) <span class="op">+</span> <span class="fl">0.5</span> <span class="op">*</span> tf.square(z)</span>
<span id="cb7-101"><a href="#cb7-101" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> tf.reduce_mean(nll)</span>
<span id="cb7-102"><a href="#cb7-102" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-103"><a href="#cb7-103" aria-hidden="true" tabindex="-1"></a><span class="at">@tf.function</span></span>
<span id="cb7-104"><a href="#cb7-104" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> rmse_mu(y_true, y_pred):</span>
<span id="cb7-105"><a href="#cb7-105" aria-hidden="true" tabindex="-1"></a>    y_true <span class="op">=</span> tf.cast(tf.reshape(y_true, (<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)), tf.float32)</span>
<span id="cb7-106"><a href="#cb7-106" aria-hidden="true" tabindex="-1"></a>    mu <span class="op">=</span> tf.reshape(y_pred[:, <span class="dv">0</span>], (<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>))</span>
<span id="cb7-107"><a href="#cb7-107" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> tf.sqrt(tf.reduce_mean(tf.square(y_true <span class="op">-</span> mu)))</span>
<span id="cb7-108"><a href="#cb7-108" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-109"><a href="#cb7-109" aria-hidden="true" tabindex="-1"></a><span class="at">@tf.function</span></span>
<span id="cb7-110"><a href="#cb7-110" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> mae_mu(y_true, y_pred):</span>
<span id="cb7-111"><a href="#cb7-111" aria-hidden="true" tabindex="-1"></a>    y_true <span class="op">=</span> tf.cast(tf.reshape(y_true, (<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)), tf.float32)</span>
<span id="cb7-112"><a href="#cb7-112" aria-hidden="true" tabindex="-1"></a>    mu <span class="op">=</span> tf.reshape(y_pred[:, <span class="dv">0</span>], (<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>))</span>
<span id="cb7-113"><a href="#cb7-113" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> tf.reduce_mean(tf.<span class="bu">abs</span>(y_true <span class="op">-</span> mu))</span>
<span id="cb7-114"><a href="#cb7-114" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-115"><a href="#cb7-115" aria-hidden="true" tabindex="-1"></a><span class="co"># -------------------------</span></span>
<span id="cb7-116"><a href="#cb7-116" aria-hidden="true" tabindex="-1"></a><span class="co"># 4) 모델(평균+표준편차 출력)</span></span>
<span id="cb7-117"><a href="#cb7-117" aria-hidden="true" tabindex="-1"></a><span class="co"># -------------------------</span></span>
<span id="cb7-118"><a href="#cb7-118" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> build_dist_mlp(input_dim, hidden_units<span class="op">=</span>(<span class="dv">64</span>,<span class="dv">32</span>), dropout<span class="op">=</span><span class="fl">0.2</span>, lr<span class="op">=</span><span class="fl">1e-3</span>, weight_decay<span class="op">=</span><span class="fl">1e-4</span>):</span>
<span id="cb7-119"><a href="#cb7-119" aria-hidden="true" tabindex="-1"></a>    inp <span class="op">=</span> keras.Input(shape<span class="op">=</span>(input_dim,))</span>
<span id="cb7-120"><a href="#cb7-120" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> inp</span>
<span id="cb7-121"><a href="#cb7-121" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> u <span class="kw">in</span> hidden_units:</span>
<span id="cb7-122"><a href="#cb7-122" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> layers.Dense(u, activation<span class="op">=</span><span class="st">"relu"</span>,</span>
<span id="cb7-123"><a href="#cb7-123" aria-hidden="true" tabindex="-1"></a>                         kernel_regularizer<span class="op">=</span>keras.regularizers.l2(weight_decay))(x)</span>
<span id="cb7-124"><a href="#cb7-124" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> layers.Dropout(dropout)(x)</span>
<span id="cb7-125"><a href="#cb7-125" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-126"><a href="#cb7-126" aria-hidden="true" tabindex="-1"></a>    mu <span class="op">=</span> layers.Dense(<span class="dv">1</span>, activation<span class="op">=</span><span class="st">"linear"</span>, name<span class="op">=</span><span class="st">"mu"</span>)(x)</span>
<span id="cb7-127"><a href="#cb7-127" aria-hidden="true" tabindex="-1"></a>    raw_sigma <span class="op">=</span> layers.Dense(<span class="dv">1</span>, activation<span class="op">=</span><span class="st">"linear"</span>, name<span class="op">=</span><span class="st">"raw_sigma"</span>)(x)  <span class="co"># softplus로 양수화</span></span>
<span id="cb7-128"><a href="#cb7-128" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-129"><a href="#cb7-129" aria-hidden="true" tabindex="-1"></a>    out <span class="op">=</span> layers.Concatenate(axis<span class="op">=</span><span class="dv">1</span>)([mu, raw_sigma])  <span class="co"># (batch,2)</span></span>
<span id="cb7-130"><a href="#cb7-130" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-131"><a href="#cb7-131" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> keras.Model(inp, out)</span>
<span id="cb7-132"><a href="#cb7-132" aria-hidden="true" tabindex="-1"></a>    model.<span class="bu">compile</span>(</span>
<span id="cb7-133"><a href="#cb7-133" aria-hidden="true" tabindex="-1"></a>        optimizer<span class="op">=</span>keras.optimizers.Adam(learning_rate<span class="op">=</span>lr),</span>
<span id="cb7-134"><a href="#cb7-134" aria-hidden="true" tabindex="-1"></a>        loss<span class="op">=</span>gaussian_nll,</span>
<span id="cb7-135"><a href="#cb7-135" aria-hidden="true" tabindex="-1"></a>        metrics<span class="op">=</span>[rmse_mu, mae_mu]</span>
<span id="cb7-136"><a href="#cb7-136" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb7-137"><a href="#cb7-137" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> model</span>
<span id="cb7-138"><a href="#cb7-138" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-139"><a href="#cb7-139" aria-hidden="true" tabindex="-1"></a><span class="co"># -------------------------</span></span>
<span id="cb7-140"><a href="#cb7-140" aria-hidden="true" tabindex="-1"></a><span class="co"># 5) (1차) train/val로 best epoch 찾기</span></span>
<span id="cb7-141"><a href="#cb7-141" aria-hidden="true" tabindex="-1"></a><span class="co"># -------------------------</span></span>
<span id="cb7-142"><a href="#cb7-142" aria-hidden="true" tabindex="-1"></a>keras.backend.clear_session()</span>
<span id="cb7-143"><a href="#cb7-143" aria-hidden="true" tabindex="-1"></a>tf.random.set_seed(<span class="dv">42</span>)</span>
<span id="cb7-144"><a href="#cb7-144" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-145"><a href="#cb7-145" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> build_dist_mlp(input_dim<span class="op">=</span>Xtr.shape[<span class="dv">1</span>])</span>
<span id="cb7-146"><a href="#cb7-146" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-147"><a href="#cb7-147" aria-hidden="true" tabindex="-1"></a>early <span class="op">=</span> keras.callbacks.EarlyStopping(</span>
<span id="cb7-148"><a href="#cb7-148" aria-hidden="true" tabindex="-1"></a>    monitor<span class="op">=</span><span class="st">"val_loss"</span>, patience<span class="op">=</span><span class="dv">30</span>, restore_best_weights<span class="op">=</span><span class="va">True</span></span>
<span id="cb7-149"><a href="#cb7-149" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb7-150"><a href="#cb7-150" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-151"><a href="#cb7-151" aria-hidden="true" tabindex="-1"></a>hist <span class="op">=</span> model.fit(</span>
<span id="cb7-152"><a href="#cb7-152" aria-hidden="true" tabindex="-1"></a>    Xtr, y_train.values,</span>
<span id="cb7-153"><a href="#cb7-153" aria-hidden="true" tabindex="-1"></a>    validation_data<span class="op">=</span>(Xva, y_val.values),</span>
<span id="cb7-154"><a href="#cb7-154" aria-hidden="true" tabindex="-1"></a>    epochs<span class="op">=</span><span class="dv">500</span>, batch_size<span class="op">=</span><span class="dv">32</span>,</span>
<span id="cb7-155"><a href="#cb7-155" aria-hidden="true" tabindex="-1"></a>    callbacks<span class="op">=</span>[early],</span>
<span id="cb7-156"><a href="#cb7-156" aria-hidden="true" tabindex="-1"></a>    verbose<span class="op">=</span><span class="dv">0</span></span>
<span id="cb7-157"><a href="#cb7-157" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb7-158"><a href="#cb7-158" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-159"><a href="#cb7-159" aria-hidden="true" tabindex="-1"></a>best_epoch <span class="op">=</span> <span class="bu">int</span>(np.argmin(hist.history[<span class="st">"val_loss"</span>]) <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb7-160"><a href="#cb7-160" aria-hidden="true" tabindex="-1"></a>best_val <span class="op">=</span> <span class="bu">float</span>(np.<span class="bu">min</span>(hist.history[<span class="st">"val_loss"</span>]))</span>
<span id="cb7-161"><a href="#cb7-161" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Best epoch:"</span>, best_epoch, <span class="st">"| best val NLL:"</span>, best_val)</span>
<span id="cb7-162"><a href="#cb7-162" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-163"><a href="#cb7-163" aria-hidden="true" tabindex="-1"></a><span class="co"># -------------------------</span></span>
<span id="cb7-164"><a href="#cb7-164" aria-hidden="true" tabindex="-1"></a><span class="co"># 6) (최종) train+val로 재학습(best_epoch만큼)</span></span>
<span id="cb7-165"><a href="#cb7-165" aria-hidden="true" tabindex="-1"></a><span class="co"># -------------------------</span></span>
<span id="cb7-166"><a href="#cb7-166" aria-hidden="true" tabindex="-1"></a>keras.backend.clear_session()</span>
<span id="cb7-167"><a href="#cb7-167" aria-hidden="true" tabindex="-1"></a>tf.random.set_seed(<span class="dv">42</span>)</span>
<span id="cb7-168"><a href="#cb7-168" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-169"><a href="#cb7-169" aria-hidden="true" tabindex="-1"></a>final_dmodel <span class="op">=</span> build_dist_mlp(input_dim<span class="op">=</span>Xtrva.shape[<span class="dv">1</span>])</span>
<span id="cb7-170"><a href="#cb7-170" aria-hidden="true" tabindex="-1"></a>final_dmodel.fit(Xtrva, y_trainval.values, epochs<span class="op">=</span>best_epoch, batch_size<span class="op">=</span><span class="dv">32</span>, verbose<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb7-171"><a href="#cb7-171" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-172"><a href="#cb7-172" aria-hidden="true" tabindex="-1"></a><span class="co"># -------------------------</span></span>
<span id="cb7-173"><a href="#cb7-173" aria-hidden="true" tabindex="-1"></a><span class="co"># 7) test 평가 + 예측구간(정규 가정)</span></span>
<span id="cb7-174"><a href="#cb7-174" aria-hidden="true" tabindex="-1"></a><span class="co"># -------------------------</span></span>
<span id="cb7-175"><a href="#cb7-175" aria-hidden="true" tabindex="-1"></a>pred <span class="op">=</span> final_dmodel.predict(Xte)  <span class="co"># (n,2)</span></span>
<span id="cb7-176"><a href="#cb7-176" aria-hidden="true" tabindex="-1"></a>mu <span class="op">=</span> pred[:, <span class="dv">0</span>]</span>
<span id="cb7-177"><a href="#cb7-177" aria-hidden="true" tabindex="-1"></a>raw <span class="op">=</span> pred[:, <span class="dv">1</span>]</span>
<span id="cb7-178"><a href="#cb7-178" aria-hidden="true" tabindex="-1"></a>sigma <span class="op">=</span> tf.nn.softplus(raw).numpy() <span class="op">+</span> <span class="fl">1e-6</span></span>
<span id="cb7-179"><a href="#cb7-179" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-180"><a href="#cb7-180" aria-hidden="true" tabindex="-1"></a>y_true <span class="op">=</span> y_test.values</span>
<span id="cb7-181"><a href="#cb7-181" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-182"><a href="#cb7-182" aria-hidden="true" tabindex="-1"></a><span class="co"># 점예측 성능(평균 예측 기준)</span></span>
<span id="cb7-183"><a href="#cb7-183" aria-hidden="true" tabindex="-1"></a>rmse <span class="op">=</span> np.sqrt(mean_squared_error(y_true, mu))</span>
<span id="cb7-184"><a href="#cb7-184" aria-hidden="true" tabindex="-1"></a>mae  <span class="op">=</span> mean_absolute_error(y_true, mu)</span>
<span id="cb7-185"><a href="#cb7-185" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-186"><a href="#cb7-186" aria-hidden="true" tabindex="-1"></a><span class="co"># test NLL(평균)</span></span>
<span id="cb7-187"><a href="#cb7-187" aria-hidden="true" tabindex="-1"></a><span class="co"># (같은 식을 numpy로 계산)</span></span>
<span id="cb7-188"><a href="#cb7-188" aria-hidden="true" tabindex="-1"></a>nll <span class="op">=</span> <span class="fl">0.5</span><span class="op">*</span>np.log(<span class="dv">2</span><span class="op">*</span>np.pi) <span class="op">+</span> np.log(sigma) <span class="op">+</span> <span class="fl">0.5</span><span class="op">*</span>((y_true <span class="op">-</span> mu)<span class="op">/</span>sigma)<span class="op">**</span><span class="dv">2</span></span>
<span id="cb7-189"><a href="#cb7-189" aria-hidden="true" tabindex="-1"></a>test_nll <span class="op">=</span> <span class="bu">float</span>(np.mean(nll))</span>
<span id="cb7-190"><a href="#cb7-190" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-191"><a href="#cb7-191" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">=== Distribution (Gaussian) MLP Test Summary ==="</span>)</span>
<span id="cb7-192"><a href="#cb7-192" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"RMSE(mu) = </span><span class="sc">{</span>rmse<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb7-193"><a href="#cb7-193" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"MAE(mu)  = </span><span class="sc">{</span>mae<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb7-194"><a href="#cb7-194" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Test NLL = </span><span class="sc">{</span>test_nll<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb7-195"><a href="#cb7-195" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-196"><a href="#cb7-196" aria-hidden="true" tabindex="-1"></a><span class="co"># 예측구간: 중앙 80% (0.1~0.9), 중앙 90% (0.05~0.95)</span></span>
<span id="cb7-197"><a href="#cb7-197" aria-hidden="true" tabindex="-1"></a>z80 <span class="op">=</span> <span class="fl">1.2815515655446004</span>  <span class="co"># norm.ppf(0.90)</span></span>
<span id="cb7-198"><a href="#cb7-198" aria-hidden="true" tabindex="-1"></a>z90 <span class="op">=</span> <span class="fl">1.6448536269514722</span>  <span class="co"># norm.ppf(0.95)</span></span>
<span id="cb7-199"><a href="#cb7-199" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-200"><a href="#cb7-200" aria-hidden="true" tabindex="-1"></a>l80, u80 <span class="op">=</span> mu <span class="op">-</span> z80<span class="op">*</span>sigma, mu <span class="op">+</span> z80<span class="op">*</span>sigma</span>
<span id="cb7-201"><a href="#cb7-201" aria-hidden="true" tabindex="-1"></a>l90, u90 <span class="op">=</span> mu <span class="op">-</span> z90<span class="op">*</span>sigma, mu <span class="op">+</span> z90<span class="op">*</span>sigma</span>
<span id="cb7-202"><a href="#cb7-202" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-203"><a href="#cb7-203" aria-hidden="true" tabindex="-1"></a>cov80 <span class="op">=</span> np.mean((y_true <span class="op">&gt;=</span> l80) <span class="op">&amp;</span> (y_true <span class="op">&lt;=</span> u80))</span>
<span id="cb7-204"><a href="#cb7-204" aria-hidden="true" tabindex="-1"></a>cov90 <span class="op">=</span> np.mean((y_true <span class="op">&gt;=</span> l90) <span class="op">&amp;</span> (y_true <span class="op">&lt;=</span> u90))</span>
<span id="cb7-205"><a href="#cb7-205" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-206"><a href="#cb7-206" aria-hidden="true" tabindex="-1"></a>w80 <span class="op">=</span> np.mean(u80 <span class="op">-</span> l80)</span>
<span id="cb7-207"><a href="#cb7-207" aria-hidden="true" tabindex="-1"></a>w90 <span class="op">=</span> np.mean(u90 <span class="op">-</span> l90)</span>
<span id="cb7-208"><a href="#cb7-208" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-209"><a href="#cb7-209" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"80% PI coverage = </span><span class="sc">{</span>cov80<span class="sc">:.3f}</span><span class="ss"> | avg width = </span><span class="sc">{</span>w80<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb7-210"><a href="#cb7-210" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"90% PI coverage = </span><span class="sc">{</span>cov90<span class="sc">:.3f}</span><span class="ss"> | avg width = </span><span class="sc">{</span>w90<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb7-211"><a href="#cb7-211" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-212"><a href="#cb7-212" aria-hidden="true" tabindex="-1"></a><span class="co"># 결과 테이블</span></span>
<span id="cb7-213"><a href="#cb7-213" aria-hidden="true" tabindex="-1"></a>out_df <span class="op">=</span> X_test.copy()</span>
<span id="cb7-214"><a href="#cb7-214" aria-hidden="true" tabindex="-1"></a>out_df[<span class="st">"mpg_true"</span>] <span class="op">=</span> y_true</span>
<span id="cb7-215"><a href="#cb7-215" aria-hidden="true" tabindex="-1"></a>out_df[<span class="st">"mu"</span>] <span class="op">=</span> mu</span>
<span id="cb7-216"><a href="#cb7-216" aria-hidden="true" tabindex="-1"></a>out_df[<span class="st">"sigma"</span>] <span class="op">=</span> sigma</span>
<span id="cb7-217"><a href="#cb7-217" aria-hidden="true" tabindex="-1"></a>out_df[<span class="st">"l80"</span>] <span class="op">=</span> l80</span>
<span id="cb7-218"><a href="#cb7-218" aria-hidden="true" tabindex="-1"></a>out_df[<span class="st">"u80"</span>] <span class="op">=</span> u80</span>
<span id="cb7-219"><a href="#cb7-219" aria-hidden="true" tabindex="-1"></a>out_df[<span class="st">"in_80PI"</span>] <span class="op">=</span> (y_true <span class="op">&gt;=</span> l80) <span class="op">&amp;</span> (y_true <span class="op">&lt;=</span> u80)</span>
<span id="cb7-220"><a href="#cb7-220" aria-hidden="true" tabindex="-1"></a>out_df.head(<span class="dv">10</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>이번 분포 예측 모형은 MLP가 점예측 (\mu(x)) 뿐 아니라, 입력 (x)에 따라 달라지는 불확실성(표준편차) (\sigma(x)) 도 함께 출력하도록 학습한 것이다. 따라서 각 관측치에 대해 <span dir="rtl">”</span>예측값이 얼마인가”뿐 아니라 <span dir="rtl">”</span>그 예측이 얼마나 불확실한가”까지 제공할 수 있다. 손실은 정규분포 가정 하에서의 음의 로그우도(NLL) 로 학습되며, 이는 (i) 평균 예측이 실제값과 얼마나 가까운지, (ii) 분산을 과도하게 키우지 않았는지(불필요하게 넓은 구간을 피하도록) 두 요소를 동시에 반영한다.</p>
<p><strong>점예측 성능(평균 <span class="math inline">\(\mu\)</span>기준)</strong></p>
<p>RMSE((\mu)) = 2.734, MAE((\mu)) = 1.969</p>
<p>이는 <span dir="rtl">”</span>분포 예측 모델의 평균(μ)을 점예측으로 사용했을 때” 실제 mpg와 평균적으로 약 2–3 mpg 수준의 오차가 난다는 의미이다. 즉 분포 예측은 단순히 <span dir="rtl">’</span>정확도<span dir="rtl">’</span>만이 아니라, 이후의 예측구간과 NLL까지 함께 보아야 한다.</p>
<p><strong>NLL(Test NLL = 2.649)의 의미</strong></p>
<p>NLL은 낮을수록 좋다. 평균 예측 오차가 큰 관측치에서는 NLL이 커지며, 반대로 오차가 큰데도 (<span class="math inline">\(\sigma\)</span>)를 지나치게 크게 잡으면(구간을 과도하게 넓히면) (<span class="math inline">\(\log\sigma\)</span>) 항 때문에 역시 NLL이 커진다.</p>
<p>따라서 NLL은 <span dir="rtl">”</span>맞추는 정도”와 <span dir="rtl">”</span>불확실성 추정의 절제” 사이의 균형을 평가하는 지표로 해석할 수 있다. (단, NLL 값 자체는 비교 기준(다른 모델/설정)과 함께 비교할 때 의미가 커진다.)</p>
<p><strong>예측구간(PI) 결과: coverage와 width의 해석</strong></p>
<p>출력된 예측구간 성능은 다음과 같다.</p>
<ul>
<li>80% PI coverage = 0.983, 평균 폭(width) = 13.913</li>
<li>90% PI coverage = 0.983, 평균 폭(width) = 17.857</li>
</ul>
<p>여기서 coverage는 <span dir="rtl">”</span>실제 mpg가 예측구간 안에 들어간 비율”이다.<br>
원래 80% 구간이면 coverage가 0.80 부근이 자연스러운 목표인데, 실제로는 **0.983(98.3%)**로 매우 높게 나타났다. 즉 이 모델은 평균적으로 구간을 상당히 넓게 잡아, 실제값을 대부분 포함시키는 <span dir="rtl">”</span>보수적인(안전한)” 구간을 만든 것으로 해석된다.</p>
<p>또한 흥미로운 점은 80%와 90% coverage가 동일(0.983) 하다는 것이다. 이는 <span dir="rtl">”</span>구간을 90%로 넓혀도 추가로 포함되는 관측치가 거의 없었다”는 뜻이며, 구간 밖으로 나간 소수 관측치들은 단순한 경계 근처가 아니라 평균 예측 자체가 크게 빗나간(구조적으로 어려운) 사례일 가능성이 크다.</p>
<p><strong>개별 관측치 표(out_df) 읽는 법과 사례 해석</strong></p>
<ul>
<li><span class="math inline">\(\mu\)</span>: 평균 예측(점예측)</li>
<li><span class="math inline">\(\sigma\)</span>: 해당 관측치에서의 예측 불확실성(표준편차)</li>
<li>[l80, u80]: 80% 예측구간</li>
<li>in_80PI: 실제 mpg가 80% 구간에 포함되는지 여부</li>
</ul>
<p>대부분의 관측치가 True로 나타나 coverage가 높게 나온다. 하지만 예시로 마지막 행(인덱스 111)은 실제 mpg = 18.0, (\mu) = 28.54 (평균 예측이 크게 과대), 80% PI = [20.44, 36.63] → 실제값 18.0이 구간 아래로 벗어나 False</p>
<p>이 사례는 <span dir="rtl">”</span>불확실성이 작아서 실패했다”기보다, 평균 예측(μ) 자체가 크게 잘못된 케이스에 가깝다. 즉 (\sigma)를 어느 정도 크게 잡아도, 평균이 크게 치우치면 구간이 커버하지 못하는 상황이 발생한다. 이런 관측치는 데이터의 희귀 조합/이상치 가능성, 또는 현재 입력 변수로는 설명되지 않는 숨은 요인(상태/세부 트림 등) 때문일 수 있어 별도 점검 대상이다.</p>
<p><strong>결론</strong></p>
<p>본 Gaussian 분포 예측 MLP는 평균 예측 기준으로 RMSE 2.734, MAE 1.969의 성능을 보였고, 80% 예측구간의 포함률은 98.3%로 매우 높게 나타났다. 이는 모델이 평균적으로 (<span class="math inline">\(\sigma(x)\)</span>)를 크게 추정하여 예측구간을 넓게 설정한 <span dir="rtl">”</span>보수적 불확실성” 추정임을 시사한다. 다만 80%와 90% coverage가 동일하게 나타난 점은, 구간 밖으로 벗어나는 소수 사례가 단순 경계가 아니라 평균 예측이 크게 빗나간 관측치일 가능성이 크며, 이러한 사례는 데이터/특성/모형의 한계 관점에서 별도로 진단할 필요가 있다.</p>
<p>=== Distribution (Gaussian) MLP Test Summary === <br> RMSE(mu) = 2.734 <br> MAE(mu) = 1.969 <br> Test NLL = 2.649 <br> 80% PI coverage = 0.983 | avg width = 13.913 <br> 90% PI coverage = 0.983 | avg width = 17.857</p>
<table class="caption-top table">
<colgroup>
<col style="width: 7%">
<col style="width: 9%">
<col style="width: 8%">
<col style="width: 5%">
<col style="width: 9%">
<col style="width: 8%">
<col style="width: 5%">
<col style="width: 6%">
<col style="width: 7%">
<col style="width: 6%">
<col style="width: 7%">
<col style="width: 7%">
<col style="width: 7%">
</colgroup>
<tbody>
<tr class="odd">
<td style="text-align: left;">cylinders</td>
<td style="text-align: left;">displacement</td>
<td style="text-align: left;">horsepower</td>
<td style="text-align: left;">weight</td>
<td style="text-align: left;">acceleration</td>
<td style="text-align: left;">model_year</td>
<td style="text-align: left;">origin</td>
<td style="text-align: left;">mpg_true</td>
<td style="text-align: left;">mu</td>
<td style="text-align: left;">sigma</td>
<td style="text-align: left;">l80</td>
<td style="text-align: left;">u80</td>
<td style="text-align: left;">in_80PI</td>
</tr>
<tr class="even">
<td style="text-align: left;">8</td>
<td style="text-align: left;">262.0</td>
<td style="text-align: left;">110.0</td>
<td style="text-align: left;">3221</td>
<td style="text-align: left;">13.5</td>
<td style="text-align: left;">75</td>
<td style="text-align: left;">usa</td>
<td style="text-align: left;">20.0</td>
<td style="text-align: left;">19.115173</td>
<td style="text-align: left;">4.173835</td>
<td style="text-align: left;">13.766189</td>
<td style="text-align: left;">24.464157</td>
<td style="text-align: left;">1</td>
</tr>
<tr class="odd">
<td style="text-align: left;">6</td>
<td style="text-align: left;">250.0</td>
<td style="text-align: left;">100.0</td>
<td style="text-align: left;">3781</td>
<td style="text-align: left;">17.0</td>
<td style="text-align: left;">74</td>
<td style="text-align: left;">usa</td>
<td style="text-align: left;">16.0</td>
<td style="text-align: left;">16.239368</td>
<td style="text-align: left;">3.377981</td>
<td style="text-align: left;">11.910311</td>
<td style="text-align: left;">20.568426</td>
<td style="text-align: left;">1</td>
</tr>
<tr class="even">
<td style="text-align: left;">4</td>
<td style="text-align: left;">140.0</td>
<td style="text-align: left;">86.0</td>
<td style="text-align: left;">2790</td>
<td style="text-align: left;">15.6</td>
<td style="text-align: left;">82</td>
<td style="text-align: left;">usa</td>
<td style="text-align: left;">27.0</td>
<td style="text-align: left;">29.843834</td>
<td style="text-align: left;">6.828448</td>
<td style="text-align: left;">21.092825</td>
<td style="text-align: left;">38.594841</td>
<td style="text-align: left;">1</td>
</tr>
<tr class="odd">
<td style="text-align: left;">8</td>
<td style="text-align: left;">440.0</td>
<td style="text-align: left;">215.0</td>
<td style="text-align: left;">4735</td>
<td style="text-align: left;">11.0</td>
<td style="text-align: left;">73</td>
<td style="text-align: left;">usa</td>
<td style="text-align: left;">13.0</td>
<td style="text-align: left;">12.908395</td>
<td style="text-align: left;">3.016389</td>
<td style="text-align: left;">9.042737</td>
<td style="text-align: left;">16.774054</td>
<td style="text-align: left;">1</td>
</tr>
<tr class="even">
<td style="text-align: left;">4</td>
<td style="text-align: left;">120.0</td>
<td style="text-align: left;">87.0</td>
<td style="text-align: left;">2979</td>
<td style="text-align: left;">19.5</td>
<td style="text-align: left;">72</td>
<td style="text-align: left;">europe</td>
<td style="text-align: left;">21.0</td>
<td style="text-align: left;">21.415955</td>
<td style="text-align: left;">4.981795</td>
<td style="text-align: left;">15.031527</td>
<td style="text-align: left;">27.800383</td>
<td style="text-align: left;">1</td>
</tr>
<tr class="odd">
<td style="text-align: left;">8</td>
<td style="text-align: left;">318.0</td>
<td style="text-align: left;">140.0</td>
<td style="text-align: left;">3735</td>
<td style="text-align: left;">13.2</td>
<td style="text-align: left;">78</td>
<td style="text-align: left;">usa</td>
<td style="text-align: left;">19.4</td>
<td style="text-align: left;">17.502491</td>
<td style="text-align: left;">4.099450</td>
<td style="text-align: left;">12.248835</td>
<td style="text-align: left;">22.756147</td>
<td style="text-align: left;">1</td>
</tr>
<tr class="even">
<td style="text-align: left;">8</td>
<td style="text-align: left;">429.0</td>
<td style="text-align: left;">198.0</td>
<td style="text-align: left;">4341</td>
<td style="text-align: left;">10.0</td>
<td style="text-align: left;">70</td>
<td style="text-align: left;">usa</td>
<td style="text-align: left;">15.0</td>
<td style="text-align: left;">13.396416</td>
<td style="text-align: left;">2.915562</td>
<td style="text-align: left;">9.659973</td>
<td style="text-align: left;">17.132858</td>
<td style="text-align: left;">1</td>
</tr>
<tr class="odd">
<td style="text-align: left;">8</td>
<td style="text-align: left;">390.0</td>
<td style="text-align: left;">190.0</td>
<td style="text-align: left;">3850</td>
<td style="text-align: left;">8.5</td>
<td style="text-align: left;">70</td>
<td style="text-align: left;">usa</td>
<td style="text-align: left;">15.0</td>
<td style="text-align: left;">15.099515</td>
<td style="text-align: left;">3.304760</td>
<td style="text-align: left;">10.864294</td>
<td style="text-align: left;">19.334736</td>
<td style="text-align: left;">1</td>
</tr>
<tr class="even">
<td style="text-align: left;">4</td>
<td style="text-align: left;">140.0</td>
<td style="text-align: left;">88.0</td>
<td style="text-align: left;">2890</td>
<td style="text-align: left;">17.3</td>
<td style="text-align: left;">79</td>
<td style="text-align: left;">usa</td>
<td style="text-align: left;">22.3</td>
<td style="text-align: left;">25.006130</td>
<td style="text-align: left;">5.945368</td>
<td style="text-align: left;">17.386835</td>
<td style="text-align: left;">32.625427</td>
<td style="text-align: left;">1</td>
</tr>
<tr class="odd">
<td style="text-align: left;">3</td>
<td style="text-align: left;">70.0</td>
<td style="text-align: left;">90.0</td>
<td style="text-align: left;">2124</td>
<td style="text-align: left;">13.5</td>
<td style="text-align: left;">73</td>
<td style="text-align: left;">japan</td>
<td style="text-align: left;">18.0</td>
<td style="text-align: left;">28.536776</td>
<td style="text-align: left;">6.317631</td>
<td style="text-align: left;">20.440407</td>
<td style="text-align: left;">36.633144</td>
<td style="text-align: left;">0</td>
</tr>
</tbody>
</table>


</section>
</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




</body></html>